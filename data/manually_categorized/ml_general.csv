year,title,authors,link
1967,Nearest Neighbor Pattern Classification,T. M. Cover and P. E. Hart,https://ieeexplore.ieee.org/document/1053964
1986,Induction of Decision Trees,J. R. Quinlan,https://link.springer.com/article/10.1007/BF00116251
1988,Indexing by Latent Semantic Analysis,Scott Deerwester et al.,"A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher?order structure in the association of terms with documents (�semantic structure�) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular?value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo?document vectors formed from weighted combinations of terms, and documents with supra?threshold cosine values are returned. Initial tests find this completely automatic method for retrieval to be promising. � 1990 John Wiley & Sons, Inc."
1989,Fast learning in multi-resolution hierarchies,John Moody,https://proceedings.neurips.cc/paper_files/paper/1988/hash/82161242827b703e6acf9c726942a1e4-Abstract.html
1990,The Strength of Weak Learnability,Robert E. Schapire,https://link.springer.com/article/10.1007/BF00116037
1991,Bagging Predictors,Leo Breiman,https://link.springer.com/article/10.1007/BF00058655
1991,Multivariate Adaptive Regression Splines,Jerome H. Friedman,A new method is presented for flexible regression modeling of high dimensional data. The model takes the form of an expansion in product spline basis functions  where the number of basis functions as well as the parameters associated with each one (product degree and knot locations) are automatically determined by the data. This procedure is motivated by the recursive partitioning approach to regression and shares its attractive properties. Unlike recursive partitioning  however  this method produces continuous models with continuous derivatives. It has more power and flexibility to model relationships that are nearly additive or involve interactions in at most a few variables. In addition  the model can be represented in a form that separately identifies the additive contributions and those associated with the different multivariable interactions.
1992,A Training Algorithm for Optimal Margin Classifier,Bernhard E. Boser et al.,https://dl.acm.org/doi/10.1145/130385.130401
1996,Regression Shrinkage and Selection via the Lasso,Robert Tibshirani,We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.
1997,A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting,Yoav Freund and Robert E Schapire,https://www.sciencedirect.com/science/article/pii/S002200009791504X
1997,Bias Plus Variance Decomposition for Zero-One Loss Functions,Ron Kohavi and David H. Wolpert,We present a bias-variance decomposition of expected misclassification rate  the most commonly used loss function in supervised classification learning. The bias-variance decomposition for quadratic loss functions is well known and serves as an important tool for analyzing learning algorithms  yet no decomposition was offered for the more commonly used zero-one (misclassification) loss functions until the recent work of Kong & Dietterich (1995) and Breiman (1996). Their decomposition suffers from some major shortcomings though (eg  potentially negative variance)  which our decomposition avoids. We show that  in practice  the naive frequency-based estimation of the decomposition terms is by itself biased and show how to correct for this bias. We illustrate the decomposition on various algorithms and datasets from the UCI repository.
1999,Improved Boosting Algorithms Using Confidence-rated Predictions,Yoav Freund and Robert E Schapire,https://link.springer.com/article/10.1023/A:1007614523901
2001,Random Forests,Leo Breiman,https://link.springer.com/article/10.1023/A:1010933404324
2002,On Discriminative vs. Generative classifiers: A comparison of logistic regression and naive Bayes,Andrew Ng and Michael Jordan,https://papers.nips.cc/paper_files/paper/2001/hash/7b7a53e239400a13bd6be6c91c4f6c4e-Abstract.html
2003,Latent Dirichlet Allocation,David M. Blei et al.,We describe latent Dirichlet allocation (LDA)  a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model  in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is  in turn  modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling  the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling  text classification  and collaborative filtering  comparing to a mixture of unigrams model and the probabilistic LSI model.
2003,Document Clustering Based On Non-negative Matrix Factorization,Wei Xu et al.,https://dl.acm.org/doi/10.1145/860435.860485
2016,XGBoost: A Scalable Tree Boosting System,Tianqi Chen and Carlos Guestrin,https://arxiv.org/abs/1603.02754
2019,Reconciling modern machine-learning practice and the classical bias–variance trade-off,Mikhail Belkin et al.,https://arxiv.org/abs/1812.11118
2020,The generalization error of random features regression: Precise asymptotics and double descent curve,Song Mei and Andrea Montanari,https://arxiv.org/abs/1908.05355
2021,A Farewell to the Bias-Variance Tradeoff? An Overview of the Theory of Overparameterized Machine Learning,Yehuda Dar et al.,https://arxiv.org/pdf/2109.02355.pdf
2013,Identifying and attacking the saddle point problem in high-dimensional non-convex optimization,Yann Dauphin et al.,https://arxiv.org/abs/1406.2572
2014,Adam: A Method for Stochastic Optimization,Diederik P. Kingma and Jimmy Ba,https://arxiv.org/abs/1412.6980
2015,Optimizing Neural Networks with Kronecker-factored Approximate Curvature,James Martens and Roger Grosse,https://arxiv.org/abs/1503.05671
2019,ZeRO: Memory Optimizations Toward Training Trillion Parameter Models,Samyam Rajbhandari et al.,https://arxiv.org/abs/1910.02054
2015,Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning,Yarin Gal and Zoubin Ghahramani,https://arxiv.org/pdf/1506.02142.pdf
2017,Stochastic Gradient Descent as Approximate Bayesian Inference,Stephan Mandt et al.,https://arxiv.org/pdf/1704.04289.pdf
2020,Bayesian workflow,Andrew Gelman et al.,https://arxiv.org/abs/2011.01808
1960,A New Approach to Linear Filtering and Prediction Problems,R. E. Kalman,The classical filtering and prediction problem is re-examined using the Bode-Shannon representation of random processes and the state-transition method of analysis of dynamic systems. New results are: (1) The formulation and methods of solution of the problem apply without modification to stationary and nonstationary statistics and to growing-memory and infinite-memory filters. (2) A nonlinear difference (or differential) equation is derived for the covariance matrix of the optimal estimation error. From the solution of this equation the co-efficients of the difference (or differential) equation of the optimal linear filter are obtained without further calculations. (3) The filtering problem is shown to be the dual of the noise-free regulator problem. The new method developed here is applied to two well-known problems  confirming and extending earlier results. The discussion is largely self-contained and proceeds from first principles; basic concepts of the theory of random processes are reviewed in the Appendix.
1951,A Stochastic Approximation Method,H. Robbins and S. Monro,Let denote the expected value at level of the response to a certain experiment. is assumed to be a monotone function of but is unknown to the experimenter  and it is desired to find the solution of the equation   where is a given constant. We give a method for making successive experiments at levels in such a way that will tend to in probability.
1958,The perceptron: A probabilistic model for information storage and organization in the brain., Rosenblatt F., To answer the questions of how information about the physical world is sensed  in what form is information remembered  and how does information retained in memory influence recognition and behavior  a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems
1986,Learning representations by back-propagating errors,David Rumelhart et al.,https://www.nature.com/articles/323533a0
1998,Efficient Backprop,Yann LeCun et al.,https://link.springer.com/chapter/10.1007/978-3-642-35289-8_3
2010,Understanding the difficulty of training deep feedforward neural networks,Xavier Glorot and Yoshua Bengio,Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained  since then several algorithms have been shown to successfully train them  with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks  to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value  which can drive especially the top hidden layer into saturation. Surprisingly  we find that saturated units can move out of saturation by themselves  albeit slowly  and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally  we study how activations and gradients vary across layers and during training  with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations  we propose a new initialization scheme that brings substantially faster convergence.
2011,Deep Sparse Rectifier Neural Networks,Xavier Glorot et al.,While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons  the latter work better for training multi-layer neural networks. This pa-per shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiable at zero  creating sparse representations with true zeros  which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data  deep rectifier net-works can reach their best performance with-out requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence  these results can be seen as a new milestone in the attempts at under-standing the difficulty in training deep but purely supervised neural networks  and closing the performance gap between neural net-works learnt with and without unsupervised pre-training.
2011,HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent,Feng Niu et al.,https://arxiv.org/abs/1106.5730
2014,Neural Machine Translation by Jointly Learning to Align and Translate,Dzmitry Bahdanau et al.,https://arxiv.org/pdf/1409.0473v7.pdf
2014,Dropout: A Simple Way to Prevent Neural Networks from Overfitting,Nitish Srivastava et al.,https://jmlr.org/papers/v15/srivastava14a.html
2015,Highway Networks,Rupesh Kumar Srivastava et al.,https://arxiv.org/abs/1505.00387
2015,Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,Sergey Ioffe and Christian Szegedy,https://arxiv.org/abs/1502.03167
2016,Gaussian Error Linear Units (GELUs),Dan Hendrycks and Kevin Gimpel,https://arxiv.org/pdf/1606.08415v4.pdf
2016,Categorical Reparameterization with Gumbel-Softmax,Eric Jang et al.,https://arxiv.org/abs/1611.01144v5
2016,On the Properties of the Softmax Function with Application in Game Theory and Reinforcement Learning,Bolin Gao and Lacra Pavel,https://arxiv.org/pdf/1704.00805.pdf
2017,Understanding the Disharmony Between Dropout and Batch Normalization by Variance Shift,Xiang Li et al.,https://arxiv.org/pdf/1801.05134.pdf
2013,Maxout networks,I. Goodfellow et al.,http://arxiv.org/pdf/1302.4389v4
1991,Face Recognition Using Eigenfaces,Matthew Turk and Alex Pentland,https://ieeexplore.ieee.org/document/139758
2015,Deep Unsupervised Learning using Nonequilibrium Thermodynamics,Jascha Sohl-Dickstein et al.,https://arxiv.org/abs/1503.03585
1989,Learnability and the Vapnik-Chervonenkis Dimension,Blumer et al.,https://dl.acm.org/doi/10.1145/76359.76371
1989,Multilayer Feedforward Networks are Universal Approximators,Hornik et al.,https://www.sciencedirect.com/science/article/abs/pii/0893608089900208
1991,Approximation capabilities of multilayer feedforward networks,Kurt Hornik,https://www.sciencedirect.com/science/article/abs/pii/089360809190009T
1992,Principles of Risk Minimization for Learning Theory,V. Vapnik,https://proceedings.neurips.cc/paper/1991/file/ff4d5fbbafdf976cfdc032e3bde78de5-Paper.pdf
1993,Multilayer Feedforward Networks With a Nonpolynomial Activation Function Can Approximate Any Function,Moshe Leshno et al.,https://www.sciencedirect.com/science/article/abs/pii/S0893608005801315
1997,No Free Lunch Theorems for Optimization,David H. Wolpert and William G. Macready,https://ieeexplore.ieee.org/document/585893
1997,Computational Power of Neural Networks: A Characterization in Terms of Kolmogorov Complexity,José L. Balcázar et al.,The computational power of recurrent neural networks is shown to depend ultimately on the complexity of the real constants (weights) of the network. The complexity  or information contents  of the weights is measured by a variant of resource-bounded Kolmogorov (1965) complexity  taking into account the time required for constructing the numbers. In particular  we reveal a full and proper hierarchy of nonuniform complexity classes associated with networks having weights of increasing Kolmogorov complexity
1999,Generalization in a linear perceptron in the presence of noise,Anders Krogh and John A Hertz,The authors study the evolution of the generalization ability of a simple linear perceptron with N inputs which learns to imitate a 'teacher perceptron'. The system is trained on p= alpha N example inputs drawn from some distribution and the generalization ability is measured by the average agreement with the teacher on test examples drawn from the same distribution. The dynamics may be solved analytically and exhibits a phase transition from imperfect to perfect generalization at alpha =1  when there are no errors (static noise) in the training examples. If the examples are produced by an erroneous teacher  overfitting is observed  i.e. the generalization error starts to increase after a finite time of training. It is shown that a weight decay of the same size as the variance of the noise (errors) on the teacher improves on the generalization and suppresses the overfitting. The generalization error as a function of time is calculated numerically for various values of the parameters. Finally dynamic noise in the training is considered. White noise on the input corresponds on average to a weight decay  and can thus improve generalization  whereas white noise on the weights or the output degrades generalization. Generalization is particularly sensitive to noise on the weights (for alpha (1) where it makes the error constantly increase with time  but this effect is also shown to be damped by a weight decay. Weight noise and output noise acts similarly above the transition at alpha =1.
2014,Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images,Anh Nguyen et al.,https://arxiv.org/pdf/1412.1897.pdf
2015,Deep Learning and the Information Bottleneck Principle,Naftali Tishby and Noga Zaslavsky,https://arxiv.org/abs/1503.02406
2016,Understanding deep learning requires rethinking generalization,Chiyuan Zhang et al.,https://arxiv.org/pdf/1611.03530.pdf
2017,On Calibration of Modern Neural Networks,Chuan Guo et al.,http://proceedings.mlr.press/v70/guo17a.html
2017,GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium,Martin Heusel et al.,https://arxiv.org/abs/1706.08500v6
2018,Neural Tangent Kernel: Convergence and Generalization in Neural Networks,Arthur Jacot et al.,https://arxiv.org/abs/1806.07572
2018,"The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks",Jonathan Frankle and Michael Carbin,https://arxiv.org/abs/1803.03635
2018,"Relational inductive biases, deep learning, and graph networks",Peter W. Battaglia et al.,https://arxiv.org/pdf/1806.01261.pdf
2018,A Spline Theory of Deep Networks,Randall Balestriero and Richard G. Baraniuk,https://proceedings.mlr.press/v80/balestriero18b.html
2019,Deep Double Descent: Where Bigger Models and More Data Hurt,Preetum Nakkiran et al.,https://arxiv.org/abs/1912.02292
2019,On the covariance-Hessian relation in evolution strategies,Ofer M. Shir and Amir Yehudayoff,https://arxiv.org/abs/1806.03674
2019,On the Measure of Intelligence,François Chollet,https://arxiv.org/abs/1911.01547
2020,The Neural Tangent Kernel in High Dimensions: Triple Descent and a Multi-Scale Theory of Generalization,Ben Adlam and Jeffrey Pennington,https://arxiv.org/abs/2008.06786
2020,Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning,Armen Aghajanyan et al.,https://arxiv.org/abs/2012.13255
2021,Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets,Alethea Power et al.,https://arxiv.org/abs/2201.02177
2021,Learning in High Dimension Always Amounts to Extrapolation,Randall Balestriero et al.,https://arxiv.org/pdf/2110.09485.pdf
2022,Towards Understanding Grokking: An Effective Theory of Representation Learning,Ziming Liu et al.,https://arxiv.org/abs/2205.10343
2021,"Highly accurate protein structure prediction with AlphaFold",John Jumper et al.,https://www.nature.com/articles/s41586-021-03819-2
1982,Self-Organized Formation of Topologically Correct Feature Maps,Teuvo Kohonen,https://link.springer.com/article/10.1007/BF00337288
1991,Nonlinear principal component analysis using autoassociative neural networks,Mark A. Kramer,Nonlinear principal component analysis is a novel technique for multivariate data analysis  similar to the well-known method of principal component analysis. NLPCA  like PCA  is used to identify and remove correlations among problem variables as an aid to dimensionality reduction  visualization  and exploratory data analysis. While PCA identifies only linear correlations between variables  NLPCA uncovers both linear and nonlinear correlations  without restriction on the character of the nonlinearities present in the data. NLPCA operates by training a feedforward neural network to perform the identity mapping  where the network inputs are reproduced at the output layer. The network contains an internal “bottleneck” layer (containing fewer nodes than input or output layers)  which forces the network to develop a compact representation of the input data  and two additional hidden layers. The NLPCA method is demonstrated using time-dependent  simulated batch reaction data. Results show that NLPCA successfully reduces dimensionality and produces a feature space map resembling the actual distribution of the underlying system parameters.
2002,Stochastic Neighbor Embedding,Geoffrey Hinton and Sam Roweis,We describe a probabilistic approach to the task of placing objects  de- scribed by high-dimensional vectors or by pairwise dissimilarities  in a low-dimensional space in a way that preserves neighbor identities. A Gaussian is centered on each object in the high-dimensional space and the densities under this Gaussian (or the given dissimilarities) are used to de?ne a probability distribution over all the potential neighbors of the object. The aim of the embedding is to approximate this distribu- tion as well as possible when the same operation is performed on the low-dimensional of the objects. A natural cost function is a sum of Kullback-Leibler divergences  one per object  which leads to a simple gradient for adjusting the positions of the low-dimensional im- ages. Unlike other dimensionality reduction methods  this probabilistic framework makes it easy to represent each object by a mixture of widely separated low-dimensional images. This allows ambiguous objects  like the document count vector for the word bank  to have versions close to the images of both river and nance without forcing the images of outdoor concepts to be located close to those of corporate concepts.
2006,Reducing the Dimensionality of Data with Neural Networks,Geoff Hinton and R. R. Salakhutdinov,High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.
2008,Visualizing Data using t-SNE,Laurens van der Maaten and Geoffrey Hinton,"We present a new technique called ""t-SNE"" that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis  2002) that is much easier to optimize  and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different  but related  low-dimensional manifolds  such as images ofobjects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets  we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques  including Sammon mapping  Isomap  and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets"
2009,Feature Hashing for Large Scale Multitask Learning,Kilian Weinberger et al.,https://arxiv.org/abs/0902.2206
2013,Auto-Encoding Variational Bayes,Diederik P Kingma and Max Welling,https://arxiv.org/abs/1312.6114
2014,Locality Preserving Hashing,Yi-Hsuan Tsai and Ming-Hsuan Yang,The spectral hashing algorithm relaxes and solves an objective function for generating hash codes such that data similarity is preserved in the Hamming space. However  the assumption of uniform global data distribution limits its applicability. In the paper  we introduce locality preserving projection to determine the data distribution adaptively  and a spectral method is adopted to estimate the eigen functions of the under-lying graph Laplacian. Furthermore  pairwise label similarity can be further incorporated in the weight matrix to bridge the semantic gap between data and hash codes. Experiments on three benchmark datasets show the proposed algorithm per-forms favorably against state-of-the-art hashing methods
2015,Siamese Neural Networks for One-Shot Image Recognition,Gregory Koch,The process of learning good features for machine learning applications can be very computationally expensive and may prove difficult in cases where little data is available. A prototypical example of this is the one-shot learning setting  in which we must correctly make predictions given only a single example of each new class. In this paper  we explore a method for learning siamese neural networks which employ a unique structure to naturally rank similarity between inputs. Once a network has been tuned  we can then capitalize on powerful discriminative features to generalize the predictive power of the network not just to new data  but to entirely new classes from unknown distributions. Using a convolutional architecture  we are able to achieve strong results which exceed those of other deep learning models with near state-of-the-art performance on one-shot classification tasks.
2018,UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction,Leland McInnes et al.,https://arxiv.org/abs/1802.03426
2018,Representation Learning with Contrastive Predictive Coding,Aaron van den Oord et al.,https://arxiv.org/abs/1807.03748
2020,Implicit Neural Representations with Periodic Activation Functions,Vincent Sitzmann et al.,https://arxiv.org/abs/2006.09661
2020,Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains,Matthew Tancik et al.,https://arxiv.org/abs/2006.10739
2021,An Introduction to Johnson-Lindenstrauss Transforms,Casper Benjamin Freksen,https://arxiv.org/abs/2103.00564
1979,Bootstrap Methods: Another Look at the Jackknife,Bradley Effron,https://link.springer.com/chapter/10.1007/978-1-4612-4380-9_41
1987,Hybrid Monte Carlo,Simon Duane et al.,https://www.sciencedirect.com/science/article/abs/pii/037026938791197X
2002,SMOTE: Synthetic Minority Over-sampling Technique,Nitesh V Chawla et al.,https://arxiv.org/abs/1106.1813
2009,Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design,Niranjan Srinivas et al.,https://arxiv.org/abs/0912.3995
2012,Random Search for Hyper-Parameter Optimization,James Bergstra and Yoshua Bengio,https://www.jmlr.org/papers/v13/bergstra12a.html
2016,Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization,Lisha Li et al.,https://arxiv.org/pdf/1603.06560.pdf
2017,SMASH: One-Shot Model Architecture Search through HyperNetworks,Andrew Brock et al.,https://arxiv.org/abs/1708.05344