Year,Paper,Authors,Link
1982,Self-Organized Formation of Topologically Correct Feature Maps,Teuvo Kohonen,https://link.springer.com/article/10.1007/BF00337288
1991,Nonlinear principal component analysis using autoassociative neural networks,Mark A. Kramer,Nonlinear principal component analysis is a novel technique for multivariate data analysis  similar to the well-known method of principal component analysis. NLPCA  like PCA  is used to identify and remove correlations among problem variables as an aid to dimensionality reduction  visualization  and exploratory data analysis. While PCA identifies only linear correlations between variables  NLPCA uncovers both linear and nonlinear correlations  without restriction on the character of the nonlinearities present in the data. NLPCA operates by training a feedforward neural network to perform the identity mapping  where the network inputs are reproduced at the output layer. The network contains an internal “bottleneck” layer (containing fewer nodes than input or output layers)  which forces the network to develop a compact representation of the input data  and two additional hidden layers. The NLPCA method is demonstrated using time-dependent  simulated batch reaction data. Results show that NLPCA successfully reduces dimensionality and produces a feature space map resembling the actual distribution of the underlying system parameters.
2002,Stochastic Neighbor Embedding,Geoffrey Hinton and Sam Roweis,We describe a probabilistic approach to the task of placing objects  de- scribed by high-dimensional vectors or by pairwise dissimilarities  in a low-dimensional space in a way that preserves neighbor identities. A Gaussian is centered on each object in the high-dimensional space and the densities under this Gaussian (or the given dissimilarities) are used to de?ne a probability distribution over all the potential neighbors of the object. The aim of the embedding is to approximate this distribu- tion as well as possible when the same operation is performed on the low-dimensional of the objects. A natural cost function is a sum of Kullback-Leibler divergences  one per object  which leads to a simple gradient for adjusting the positions of the low-dimensional im- ages. Unlike other dimensionality reduction methods  this probabilistic framework makes it easy to represent each object by a mixture of widely separated low-dimensional images. This allows ambiguous objects  like the document count vector for the word bank  to have versions close to the images of both river and nance without forcing the images of outdoor concepts to be located close to those of corporate concepts.
2006,Reducing the Dimensionality of Data with Neural Networks,Geoff Hinton and R. R. Salakhutdinov,High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.
2008,Visualizing Data using t-SNE,Laurens van der Maaten and Geoffrey Hinton,"We present a new technique called ""t-SNE"" that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis  2002) that is much easier to optimize  and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different  but related  low-dimensional manifolds  such as images ofobjects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets  we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques  including Sammon mapping  Isomap  and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets"
2009,Feature Hashing for Large Scale Multitask Learning,Kilian Weinberger et al.,https://arxiv.org/abs/0902.2206
2013,Auto-Encoding Variational Bayes,Diederik P Kingma and Max Welling,https://arxiv.org/abs/1312.6114
2014,Locality Preserving Hashing,Yi-Hsuan Tsai and Ming-Hsuan Yang,The spectral hashing algorithm relaxes and solves an objective function for generating hash codes such that data similarity is preserved in the Hamming space. However  the assumption of uniform global data distribution limits its applicability. In the paper  we introduce locality preserving projection to determine the data distribution adaptively  and a spectral method is adopted to estimate the eigen functions of the under-lying graph Laplacian. Furthermore  pairwise label similarity can be further incorporated in the weight matrix to bridge the semantic gap between data and hash codes. Experiments on three benchmark datasets show the proposed algorithm per-forms favorably against state-of-the-art hashing methods
2014,On the Properties of Neural Machine Translation: Encoder–Decoder Approaches,Kyunghyun Cho et al.,https://arxiv.org/pdf/1409.1259.pdf
2015,Siamese Neural Networks for One-Shot Image Recognition,Gregory Koch,http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf
2018,UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction,Leland McInnes et al.,https://arxiv.org/abs/1802.03426
2018,Representation Learning with Contrastive Predictive Coding,Aaron van den Oord et al.,https://arxiv.org/abs/1807.03748
2018,Occupancy Networks: Learning 3D Reconstruction in Function Space,Lars Mescheder et al.,https://arxiv.org/abs/1812.03828
2020,Implicit Neural Representations with Periodic Activation Functions,Vincent Sitzmann et al.,https://arxiv.org/abs/2006.09661
2020,NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis,Ben Mildenhall et al.,https://arxiv.org/abs/2003.08934
2020,Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains,Matthew Tancik et al.,https://arxiv.org/abs/2006.10739
2021,Emerging Properties in Self-Supervised Vision Transformers,Mathilde Caron et al.,https://arxiv.org/abs/2104.14294
2021,Learning Transferable Visual Models From Natural Language Supervision,Alec Radford et al.,https://arxiv.org/abs/2103.00020
2021,An Introduction to Johnson-Lindenstrauss Transforms,Casper Benjamin Freksen,https://arxiv.org/abs/2103.00564
2022,Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning,Weixin Liang et al.,https://arxiv.org/abs/2203.02053
2023,DINOv2: Learning Robust Visual Features without Supervision,Maxime Oquab et al.,https://arxiv.org/abs/2304.07193
