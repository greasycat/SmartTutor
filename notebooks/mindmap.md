# Cluster 0 (cluster_7)
## Cluster 1 (cluster_25)
### Cluster 6 (cluster_50)
- dropout a simple way to prevent neural networks from overfitting
- maxout networks
- dropout as a bayesian approximation representing model uncertainty in deep learning
- gaussian error linear units gelus
- understanding the disharmony between dropout and batch normalization by variance shift
- the lottery ticket hypothesis finding sparse trainable neural networks
- understanding the difficulty of training deep feedforward neural networks
- deep sparse rectifier neural networks

### Cluster 8 (cluster_50)
- deep learning and the information bottleneck principle
- understanding deep learning requires rethinking generalization
- neural tangent kernel convergence and generalization in neural networks
- reconciling modern machine learning practice and the classical bias variance trade off
- the generalization error of random features regression precise asymptotics and double descent curve
- the neural tangent kernel in high dimensions triple descent and a multi scale theory of generalization
- deep double descent where bigger models and more data hurt
- a farewell to the bias variance tradeoff an overview of the theory of overparameterized machine learning
- learning in high dimension always amounts to extrapolation
- grokking generalization beyond overfitting on small algorithmic datasets
- towards understanding grokking an effective theory of representation learning
- generalization in a linear perceptron in the presence of noise

### Cluster 37 (cluster_50)
- efficient backprop
- identifying and attacking the saddle point problem in high dimensional non convex optimization
- optimizing neural networks with kronecker factored approximate curvature

## Cluster 5 (cluster_25)
### Cluster 36 (cluster_50)
- multilayer feedforward networks with a nonpolynomial activation function can approximate any function
- multilayer feedforward networks are universal approximators
- approximation capabilities of multilayer feedforward networks

### Cluster 46 (cluster_50)
- learning representations by back propagating errors
- a signal flow graph approach to on line gradient calculation
- on line learning algorithms for locally recurrent neural networks
- self organized formation of topologically correct feature maps
- the perceptron a probabilistic model for information storage and organization in the brain
- a local learning algorithm for dynamic feedforward and recurrent networks

## Cluster 6 (cluster_25)
### Cluster 13 (cluster_50)
- hybrid monte carlo
- on the properties of the softmax function with application in game theory and reinforcement learning
- stochastic gradient descent as approximate bayesian inference
- bayesian workflow

### Cluster 24 (cluster_50)
- no free lunch theorems for optimization
- random search for hyper parameter optimization
- gaussian process optimization in the bandit setting no regret and experimental design
- hogwild a lock free approach to parallelizing stochastic gradient descent
- adam a method for stochastic optimization
- hyperband a novel bandit based approach to hyperparameter optimization
- on the covariance hessian relation in evolution strategies
- a stochastic approximation method

### Cluster 42 (cluster_50)
- multivariate adaptive regression splines
- a new approach to linear filtering and prediction problems

## Cluster 7 (cluster_25)
### Cluster 4 (cluster_50)
- auto encoding variational bayes
- deep unsupervised learning using nonequilibrium thermodynamics
- categorical reparameterization with gumbel softmax
- neural discrete representation learning
- representation learning with contrastive predictive coding

### Cluster 33 (cluster_50)
- implicit neural representations with periodic activation functions

## Cluster 16 (cluster_25)
### Cluster 32 (cluster_50)
- improved boosting algorithms using confidence rated predictions
- induction of decision trees
- bagging predictors
- random forests
- bootstrap methods another look at the jackknife
- a decision theoretic generalization of on line learning and an application to boosting
- smote synthetic minority over sampling technique
- xgboost a scalable tree boosting system
- regression shrinkage and selection via the lasso

### Cluster 49 (cluster_50)
- nearest neighbor pattern classification
- on discriminative vs generative classifiers a comparison of logistic regression and naive bayes
- the strength of weak learnability
- a training algorithm for optimal margin classifier
- learnability and the vapnik chervonenkis dimension
- bias plus variance decomposition for zero one loss functions

## Cluster 19 (cluster_25)
### Cluster 19 (cluster_50)
- document clustering based on non negative matrix factorization
- indexing by latent semantic analysis
- latent dirichlet allocation

## Cluster 20 (cluster_25)
### Cluster 20 (cluster_50)
- an introduction to johnson lindenstrauss transforms
- feature hashing for large scale multitask learning
- umap uniform manifold approximation and projection for dimension reduction
- fourier features let networks learn high frequency functions in low dimensional domains
- nonlinear principal component analysis using autoassociative neural networks
- stochastic neighbor embedding
- reducing the dimensionality of data with neural networks
- visualizing data using t sne
- locality preserving hashing

# Cluster 1 (cluster_7)
## Cluster 0 (cluster_25)
### Cluster 5 (cluster_50)
- long short term memory
- bidirectional recurrent neural networks
- bidirectional recurrent neural networks
- lstm recurrent networks learn simple context free and context sensitive languages
- recurrent networks and narma modeling
- constructing deterministic finite state automata in recurrent neural networks
- learning precise timing with lstm recurrent networks
- kalman filters improve lstm network performance in problems unsolvable by traditional recurrent nets
- generating sequences with recurrent neural networks
- recurrent neural networks for time series forecasting
- predictive business process monitoring with lstm neural networks
- simplified minimal gated unit variations for recurrent neural networks
- gate variants of gated recurrent unit gru neural networks
- a multi horizon quantile recurrent forecaster
- independently recurrent neural network indrnn building a longer and deeper rnn
- forecasting cpi inflation components with hierarchical recurrent neural networks

### Cluster 12 (cluster_50)
- hybrid computing using a neural network with dynamic external memory
- learning task dependent distributed representations by backpropagation through structure
- neural turing machines
- relational inductive biases deep learning and graph networks

### Cluster 40 (cluster_50)
- learning the long term structure of the blues

### Cluster 48 (cluster_50)
- learning complex extended sequences using the principle of history compression
- computational power of neural networks a characterization in terms of kolmogorov complexity

## Cluster 8 (cluster_25)
### Cluster 1 (cluster_50)
- learning phrase representations using rnn encoder decoder for statistical machine translation
- neural machine translation by jointly learning to align and translate
- on the properties of neural machine translation encoder decoder approaches
- sequence to sequence learning with neural networks
- neural machine translation of rare words with subword units
- multilingual language processing from bytes
- exploring the limits of language modeling
- google s neural machine translation system bridging the gap between human and machine translation
- attention is all you need

### Cluster 7 (cluster_50)
- neural word embedding as implicit matrix factorization
- distributed representations of words and phrases and their compositionality
- a structured self attentive sentence embedding
- deep contextualized word representations

## Cluster 18 (cluster_25)
### Cluster 16 (cluster_50)
- photo real talking head with deep bidirectional lstm
- deep neural networks for acoustic modeling in speech recognition the shared views of four research groups
- context dependent pre trained deep neural networks for large vocabulary speech recognition
- acoustic modeling using deep belief networks
- an application of recurrent neural networks to discriminative keyword spotting
- framewise phoneme classification with bidirectional lstm and other neural network architectures
- speech recognition with deep recurrent neural networks
- constructing long short term memory based deep recurrent neural networks for large vocabulary speech recognition
- end to end attention based large vocabulary speech recognition
- deep speech 2 end to end speech recognition in english and mandarin
- recurrent neural network based language model

### Cluster 44 (cluster_50)
- gradientbased learning applied to document recognition
- offline handwriting recognition with multidimensional recurrent neural networks
- unconstrained online handwriting recognition with recurrent neural networks

## Cluster 23 (cluster_25)
### Cluster 47 (cluster_50)
- stabilizing transformer training by preventing attention entropy collapse

# Cluster 2 (cluster_7)
## Cluster 3 (cluster_25)
### Cluster 30 (cluster_50)
- deep image prior

### Cluster 34 (cluster_50)
- a neural algorithm of artistic style
- arbitrary style transfer in real time with adaptive instance normalization
- gans trained by a two time scale update rule converge to a local nash equilibrium
- generating diverse high fidelity images with vq vae 2
- denoising diffusion probabilistic models

### Cluster 45 (cluster_50)
- tensorf tensorial radiance fields
- 3d gaussian splatting for real time radiance field rendering
- occupancy networks learning 3d reconstruction in function space
- block nerf scalable large scene neural view synthesis
- neuralangelo high fidelity neural surface reconstruction

## Cluster 10 (cluster_25)
### Cluster 3 (cluster_50)
- high resolution image synthesis with latent diffusion models
- hierarchical text conditional image generation with clip latents dall e 2
- photorealistic text to image diffusion models with deep language understanding imagen
- classifier free diffusion guidance
- multi concept customization of text to image diffusion custom diffusion
- dreambooth fine tuning text to image diffusion models for subject driven generation
- dreamfusion text to 3d using 2d diffusion
- scalable diffusion models with transformers dit
- on distillation of guided diffusion models
- imagic text based real image editing with diffusion models
- instructpix2pix learning to follow image editing instructions
- magic3d high resolution text to 3d content creation
- muse text to image generation via masked generative transformers
- adding conditional control to text to image diffusion models controlnet
- scaling up gans for text to image synthesis gigagan
- synthetic data from diffusion models improves imagenet classification
- drag your gan interactive point based manipulation on the generative image manifold
- sdxl improving latent diffusion models for high resolution image synthesis
- mvdream multi view diffusion for 3d generation

### Cluster 35 (cluster_50)
- taming transformers for high resolution image synthesis

### Cluster 43 (cluster_50)
- make a video text to video generation without text video data
- structure and content guided video synthesis with diffusion models gen 1
- align your latents high resolution video synthesis with latent diffusion models
- videopoet a large language model for zero shot video generation

## Cluster 14 (cluster_25)
### Cluster 18 (cluster_50)
- git a generative image to text transformer for vision and language
- laion 5b an open large scale dataset for training next generation image text models
- visual chatgpt talking drawing and editing with visual foundation models

### Cluster 23 (cluster_50)
- show and tell a neural image caption generator
- deep visual semantic alignments for generating image descriptions
- show attend and tell neural image caption generation with visual attention
- vqa visual question answering
- learning transferable visual models from natural language supervision
- qwen vl a versatile vision language model for understanding localization and generation
- florence 2 advancing a unified representation for a variety of vision tasks

## Cluster 22 (cluster_25)
### Cluster 10 (cluster_50)
- swin unetr swin transformers for semantic segmentation of brain tumors in mri images
- visual prompt tuning
- spatial transformer network
- an image is worth 16x16 words transformers for image recognition at scale
- cmt convolutional neural networks meet vision transformers
- a convnet for the 2020s
- patches are all you need convmixer
- scaling up your kernels to 31 31 revisiting large kernel design in cnns
- maxvit multi axis vision transformer
- better plain vit baselines for imagenet 1k
- scaling vision transformers to 22 billion parameters
- dinov2 learning robust visual features without supervision
- vision transformers need registers

# Cluster 3 (cluster_7)
## Cluster 4 (cluster_25)
### Cluster 22 (cluster_50)
- on the measure of intelligence
- chain of thought prompting elicits reasoning in large language models
- sparks of artificial general intelligence early experiments with gpt 4

### Cluster 29 (cluster_50)
- do as i can not as i say grounding language in robotic affordances
- react synergizing reasoning and acting in language models
- grounding large language models in interactive environments with online rl glam
- reward design with language models
- hugginggpt solving ai tasks with chatgpt and its friends in huggingface
- generative agents interactive simulacra of human behavior
- tree of thoughts deliberate problem solving with large language models
- voyager an open ended embodied agent with large language models
- direct preference optimization your language model is secretly a reward model
- metagpt meta programming for multi agent collaborative framework
- retroformer retrospective large language agents with policy gradient optimization
- rlaif scaling reinforcement learning from human feedback with ai feedback
- eureka human level reward design via coding large language models

### Cluster 31 (cluster_50)
- bc z zero shot task generalization with robotic imitation learning

## Cluster 15 (cluster_25)
### Cluster 0 (cluster_50)
- zero memory optimizations toward training trillion parameter models
- language models are few shot learners
- lora low rank adaptation of large language models
- finetuned language models are zero shot learners
- multitask prompted training enables zero shot task generalization
- training compute optimal large language models
- palm scaling language modeling with pathways
- gpt neox 20b an open source autoregressive language model
- beyond the imitation game quantifying and extrapolating the capabilities of language models
- solving quantitative reasoning problems with language models
- bloom a 176b parameter open access multilingual language model
- large language models encode clinical knowledge
- detectgpt zero shot machine generated text detection using probability curvature
- llama open and efficient foundation language models
- bloomberggpt a large language model for finance
- palm 2 technical report
- qlora efficient finetuning of quantized llms
- efficient streaming language models with attention sinks

### Cluster 15 (cluster_50)
- mathematical discoveries from program search with large language models funsearch
- lambda language models for dialog applications
- training language models to follow human instructions with human feedback
- competition level code generation with alphacode
- toolformer language models can teach themselves to use tools
- instruction tuning with gpt 4
- lima less is more for alignment
- toolllm facilitating large language models to master 16000 real world apis
- code llama open foundation models for code
- large language models as optimizers

### Cluster 38 (cluster_50)
- visual instruction tuning

## Cluster 24 (cluster_25)
### Cluster 2 (cluster_50)
- universal language model fine tuning for text classification
- bert pre training of deep bidirectional transformers for language understanding
- intrinsic dimensionality explains the effectiveness of language model fine tuning
- the impact of positional encoding on length generalization in transformers
- improving language understanding by generative pre training

# Cluster 4 (cluster_7)
## Cluster 9 (cluster_25)
### Cluster 9 (cluster_50)
- learning hierarchical features for scene labeling
- rich feature hierarchies for accurate object detection and semantic segmentation
- overfeat integrated recognition localization and detection using convolutional networks
- spatial pyramid pooling in deep convolutional networks for visual recognition
- fully convolutional networks for semantic segmentation
- semantic image segmentation with deep convolutional nets and fully connected crfs
- fast r cnn
- faster r cnn towards real time object detection with region proposal networks
- you only look once unified real time object detection
- feature pyramid networks for object detection
- mask r cnn
- dino detr with improved denoising anchor boxes for end to end object detection
- diffusiondet diffusion model for object detection

## Cluster 12 (cluster_25)
### Cluster 25 (cluster_50)
- segment anything in medical images

## Cluster 13 (cluster_25)
### Cluster 27 (cluster_50)
- sequential deep learning for human action recognition
- 3d convolutional neural networks for human action recognition
- large scale video classification with convolutional neural networks
- two stream convolutional networks for action recognition in videos
- long term recurrent convolutional networks for visual recognition and description
- tracking emerges by colorizing videos

## Cluster 17 (cluster_25)
### Cluster 11 (cluster_50)
- face recognition using eigenfaces
- learning and transferring mid level image representations using convolutional neural networks
- deepface closing the gap to human level performance in face verification
- imagenet classification with deep convolutional neural networks
- decaf a deep convolutional activation feature for generic visual recognition
- visualizing and understanding convolutional networks
- cnn features off the shelf an astounding baseline for recognition
- return of the devil in the details delving deep into convolutional nets
- very deep convolutional networks for large scale image recognition
- understanding deep image representations by inverting them
- deep neural networks are easily fooled high confidence predictions for unrecognizable images
- image super resolution using deep convolutional networks
- deep residual learning for image recognition
- siamese neural networks for one shot image recognition
- inceptionism going deeper into neural networks

### Cluster 39 (cluster_50)
- network in network
- going deeper with convolutions
- smash one shot model architecture search through hypernetworks
- rethinking the inception architecture for computer vision
- inception v4 inception resnet and the impact of residual connections on learning
- identity mappings in deep residual networks
- mobilenets efficient convolutional neural networks for mobile vision applications
- squeeze and excitation networks
- mobilenetv2 inverted residuals and linear bottlenecks
- efficientnet rethinking model scaling for convolutional neural networks

## Cluster 21 (cluster_25)
### Cluster 21 (cluster_50)
- batch normalization accelerating deep network training by reducing internal covariate shift
- highway networks

# Cluster 5 (cluster_7)
## Cluster 11 (cluster_25)
### Cluster 14 (cluster_50)
- wavlm large scale self supervised pre training for full stack speech processing
- robust speech recognition via large scale weak supervision whisper
- bigssl exploring the frontier of large scale semi supervised learning for asr
- speecht5 unified modal encoder decoder pre training for spoken language processing
- mslam massively multilingual joint pre training for speech and text
- add 2022 the first audio deep synthesis detection challenge
- maestro matched speech text representations through modality matching
- neural codec language models are zero shot text to speech synthesizers vall e
- google usm scaling automatic speech recognition beyond 100 languages
- scaling speech technology to 1000 languages mms
- audiopalm a large language model that can speak and listen
- voicebox text guided multilingual universal speech generation at scale

### Cluster 41 (cluster_50)
- efficient training of audio transformers with patchout passt
- mulan a joint embedding of music audio and natural language
- audiolm a language modeling approach to audio generation
- audiogen textually guided audio generation
- musiclm generating music from text
- high fidelity neural audio compression encodec
- audioldm text to audio generation with latent diffusion models
- simple and controllable music generation musicgen

# Cluster 6 (cluster_7)
## Cluster 2 (cluster_25)
### Cluster 17 (cluster_50)
- human level control through deep reinforcement learning
- outracing champion gran turismo drivers with deep reinforcement learning sophy
- magnetic control of tokamak plasmas through deep reinforcement learning
- discovering faster matrix multiplication algorithms with reinforcement learning alphatensor
- faster sorting algorithms discovered using deep reinforcement learning alphadev
- playing atari with deep reinforcement learning
- proximal policy optimization algorithms
- learning to walk in minutes using massively parallel deep reinforcement learning anymal
- learning robust perceptive locomotion for quadrupedal robots in the wild
- mastering diverse domains through world models dreamerv3
- efficient online reinforcement learning with offline data rlpd
- learning from delayed rewards

### Cluster 26 (cluster_50)
- highly accurate protein structure prediction with alphafold

### Cluster 28 (cluster_50)
- learning to predict by the methods of temporal differences
