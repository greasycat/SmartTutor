Year,Paper,Authors,Link
1967,Nearest Neighbor Pattern Classification,T. M. Cover and P. E. Hart,https://ieeexplore.ieee.org/document/1053964
1986,Induction of Decision Trees,J. R. Quinlan,https://link.springer.com/article/10.1007/BF00116251
1988,Indexing by Latent Semantic Analysis,Scott Deerwester et al.,"A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher?order structure in the association of terms with documents (�semantic structure�) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular?value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo?document vectors formed from weighted combinations of terms, and documents with supra?threshold cosine values are returned. Initial tests find this completely automatic method for retrieval to be promising. � 1990 John Wiley & Sons, Inc."
1989,Fast learning in multi-resolution hierarchies,John Moody,https://proceedings.neurips.cc/paper_files/paper/1988/hash/82161242827b703e6acf9c726942a1e4-Abstract.html
1990,The Strength of Weak Learnability,Robert E. Schapire,https://link.springer.com/article/10.1007/BF00116037
1991,Bagging Predictors,Leo Breiman,https://link.springer.com/article/10.1007/BF00058655
1991,Multivariate Adaptive Regression Splines,Jerome H. Friedman,A new method is presented for flexible regression modeling of high dimensional data. The model takes the form of an expansion in product spline basis functions  where the number of basis functions as well as the parameters associated with each one (product degree and knot locations) are automatically determined by the data. This procedure is motivated by the recursive partitioning approach to regression and shares its attractive properties. Unlike recursive partitioning  however  this method produces continuous models with continuous derivatives. It has more power and flexibility to model relationships that are nearly additive or involve interactions in at most a few variables. In addition  the model can be represented in a form that separately identifies the additive contributions and those associated with the different multivariable interactions.
1992,A Training Algorithm for Optimal Margin Classifier,Bernhard E. Boser et al.,https://dl.acm.org/doi/10.1145/130385.130401
1996,Regression Shrinkage and Selection via the Lasso,Robert Tibshirani,https://www.jstor.org/stable/2346178
1997,A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting,Yoav Freund and Robert E Schapire,https://www.sciencedirect.com/science/article/pii/S002200009791504X
1997,Bias Plus Variance Decomposition for Zero-One Loss Functions,Ron Kohavi and David H. Wolpert,We present a bias-variance decomposition of expected misclassification rate  the most commonly used loss function in supervised classification learning. The bias-variance decomposition for quadratic loss functions is well known and serves as an important tool for analyzing learning algorithms  yet no decomposition was offered for the more commonly used zero-one (misclassification) loss functions until the recent work of Kong & Dietterich (1995) and Breiman (1996). Their decomposition suffers from some major shortcomings though (eg  potentially negative variance)  which our decomposition avoids. We show that  in practice  the naive frequency-based estimation of the decomposition terms is by itself biased and show how to correct for this bias. We illustrate the decomposition on various algorithms and datasets from the UCI repository.
1999,Improved Boosting Algorithms Using Confidence-rated Predictions,Yoav Freund and Robert E Schapire,https://link.springer.com/article/10.1023/A:1007614523901
2001,Random Forests,Leo Breiman,https://link.springer.com/article/10.1023/A:1010933404324
2002,On Discriminative vs. Generative classifiers: A comparison of logistic regression and naive Bayes,Andrew Ng and Michael Jordan,https://proceedings.neurips.cc/paper/2001/file/7b7a53e239400a13bd6be6c91c4f6c4e-Paper.pdf
2003,Latent Dirichlet Allocation,David M. Blei et al.,We describe latent Dirichlet allocation (LDA)  a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model  in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is  in turn  modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling  the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling  text classification  and collaborative filtering  comparing to a mixture of unigrams model and the probabilistic LSI model.
2003,Document Clustering Based On Non-negative Matrix Factorization,Wei Xu et al.,https://dl.acm.org/doi/10.1145/860435.860485
2016,XGBoost: A Scalable Tree Boosting System,Tianqi Chen and Carlos Guestrin,https://arxiv.org/abs/1603.02754
2019,Reconciling modern machine-learning practice and the classical bias–variance trade-off,Mikhail Belkin et al.,https://www.pnas.org/doi/10.1073/pnas.1903070116
2020,The generalization error of random features regression: Precise asymptotics and double descent curve,Song Mei and Andrea Montanari,https://arxiv.org/abs/1908.05355
2021,A Farewell to the Bias-Variance Tradeoff? An Overview of the Theory of Overparameterized Machine Learning,Yehuda Dar et al.,https://arxiv.org/pdf/2109.02355.pdf
