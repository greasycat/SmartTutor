year,title,authors,link
1991,Face Recognition Using Eigenfaces,Matthew Turk and Alex Pentland,https://ieeexplore.ieee.org/document/139758
1998,GradientBased Learning Applied to Document Recognition,Yann LeCun et al.,https://ieeexplore.ieee.org/document/726791
2012,ImageNet Classification with Deep Convolutional Neural Networks,Alex Krizhevsky et al.,https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html
2013,"OverFeat: Integrated recognition, localization and detection using convolutional networks",P. Sermanet et al.,http://arxiv.org/pdf/1312.6229
2013,Maxout networks,I. Goodfellow et al.,http://arxiv.org/pdf/1302.4389v4
2013,Network in network,M. Lin et al.,http://arxiv.org/pdf/1312.4400
2013,Learning hierarchical features for scene labeling,C. Farabet et al.,https://ieeexplore.ieee.org/document/6338939
2013,3D convolutional neural networks for human action recognition,S. Ji et al.,https://ieeexplore.ieee.org/document/6165309
2014,Very Deep Convolutional Networks for Large-Scale Image Recognition,Karen Simonyan and Andrew Zisserman,https://arxiv.org/abs/1409.1556
2014,Understanding Deep Image Representations by Inverting Them,Aravindh Mahendran and Andrea Vedaldi,https://arxiv.org/abs/1412.0035
2014,CNN features off-the-Shelf: An astounding baseline for recognition,A. Razavian et al.,https://arxiv.org/abs/1403.6382
2014,Learning and transferring mid-Level image representations using convolutional neural networks,M. Oquab et al.,https://ieeexplore.ieee.org/document/6909618
2014,Visualizing and understanding convolutional networks,M. Zeiler and R. Fergus,http://arxiv.org/pdf/1311.2901
2014,Decaf: A deep convolutional activation feature for generic visual recognition,J. Donahue et al.,http://arxiv.org/pdf/1310.1531
2014,Return of the devil in the details: delving deep into convolutional nets,K. Chatfield et al.,http://arxiv.org/pdf/1405.3531
2014,Rich feature hierarchies for accurate object detection and semantic segmentation,R. Girshick et al.,https://arxiv.org/abs/1311.2524
2014,Spatial pyramid pooling in deep convolutional networks for visual recognition,K. He et al.,http://arxiv.org/pdf/1406.4729
2014,Semantic image segmentation with deep convolutional nets and fully connected CRFs,L. Chen et al.,https://arxiv.org/pdf/1412.7062
2014,DeepFace: Closing the gap to human-level performance in face verification,Y. Taigman et al.,https://ieeexplore.ieee.org/document/6909616
2014,Large-scale video classification with convolutional neural networks,A. Karpathy et al.,https://ieeexplore.ieee.org/document/6909619
2014,Two-stream convolutional networks for action recognition in videos,K. Simonyan et al.,https://arxiv.org/abs/1406.2199
2015,Inceptionism: Going Deeper into Neural Networks,Alexander Mordvintsev et al.,Artificialneural networks have spurred remarkable recent progress in image classification and speech recognition but even though these are very useful tools based on wellknown mathematical methods we actually understand surprisingly little of why certain models work and others dont so lets take a look at some simple techniques for peeking inside these networkswe train an artificial neural network by showing it millions of training examples and gradually adjusting the network parameters until it gives the classifications we want the network typically consists of 1030 stacked layers of artificial neurons each image is fed into the input layer which then talks to the next layer until eventually the output layer is reached the networks answer comes from this final output layer
2015,Deep Unsupervised Learning using Nonequilibrium Thermodynamics,Jascha Sohl-Dickstein et al.,https://arxiv.org/abs/1503.03585
2015,Deep Residual Learning for Image Recognition,Kaiming He et al.,https://arxiv.org/pdf/1512.03385.pdf
2015,Spatial transformer network,M. Jaderberg et al.,https://arxiv.org/abs/1506.02025
2015,Going deeper with convolutions,C. Szegedy et al.,https://arxiv.org/abs/1409.4842
2015,Fully convolutional networks for semantic segmentation,J. Long et al.,https://arxiv.org/abs/1411.4038
2015,Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,S. Ren et al.,https://arxiv.org/abs/1506.01497
2015,Fast R-CNN,R. Girshick,https://arxiv.org/abs/1504.08083
2015,A neural algorithm of artistic style,L. Gatys et al.,https://arxiv.org/pdf/1508.06576
2015,Deep visual-semantic alignments for generating image descriptions,A. Karpathy and L. Fei-Fei,https://arxiv.org/abs/1412.2306
2015,"Show, attend and tell: Neural image caption generation with visual attention",K. Xu et al.,http://arxiv.org/pdf/1502.03044
2015,Show and tell: A neural image caption generator,O. Vinyals et al.,https://arxiv.org/abs/1411.4555
2015,Long-term recurrent convolutional networks for visual recognition and description,J. Donahue et al.,https://arxiv.org/abs/1411.4389
2015,VQA: Visual question answering,S. Antol et al.,https://arxiv.org/abs/1505.00468
2016,Feature Pyramid Networks for Object Detection,Tsung-Yi Lin et al.,https://arxiv.org/pdf/1612.03144.pdf
2016,Rethinking the inception architecture for computer vision,C. Szegedy et al.,https://arxiv.org/abs/1512.00567
2016,"Inception-v4, inception-resnet and the impact of residual connections on learning",C. Szegedy et al.,http://arxiv.org/pdf/1602.07261
2016,Identity Mappings in Deep Residual Networks,K. He et al.,https://arxiv.org/pdf/1603.05027v2.pdf
2016,Deep residual learning for image recognition,K. He et al.,http://arxiv.org/pdf/1512.03385
2016,"You only look once: Unified, real-time object detection",J. Redmon et al.,https://arxiv.org/abs/1506.02640
2016,Image Super-Resolution Using Deep Convolutional Networks,C. Dong et al.,https://arxiv.org/pdf/1501.00092v3.pdf
2017,Mask R-CNN,Kaiming He et al.,https://arxiv.org/pdf/1703.06870.pdf
2017,MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications,Andrew G. Howard et al.,https://arxiv.org/pdf/1704.04861.pdf
2017,Neural Discrete Representation Learning,Aaron van den Oord et al.,https://arxiv.org/pdf/1711.00937.pdf
2018,Tracking Emerges by Colorizing Videos,Carl Vondrick et al.,https://arxiv.org/abs/1806.09594
2018,MobileNetV2: Inverted Residuals and Linear Bottlenecks,Mark Sandler et al.,https://arxiv.org/abs/1801.04381
2018,Deep Image Prior,Dmitry Ulyanov et al.,https://arxiv.org/abs/1711.10925
2019,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,Mingxing Tan and Quoc V. Le,https://arxiv.org/abs/1905.11946
2019,Generating Diverse High-Fidelity Images with VQ-VAE-2,Ali Razavi et al.,https://arxiv.org/pdf/1906.00446.pdf
2020,Denoising Diffusion Probabilistic Models,Jonathan Ho et al.,https://arxiv.org/abs/2006.11239
2020,Taming Transformers for High-Resolution Image Synthesis,Patrick Esser et al.,https://arxiv.org/abs/2012.09841
2020,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,Alexey Dosovitskiy et al.,https://arxiv.org/abs/2010.11929
2022,High-Resolution Image Synthesis with Latent Diffusion Models,Robin Rombach et al.,https://arxiv.org/abs/2112.10752
2022,Better plain ViT baselines for ImageNet-1k,Lucas Beyer et al.,https://arxiv.org/abs/2205.01580
2022,DreamFusion: Text-to-3D using 2D Diffusion,Ben Poole et al.,https://arxiv.org/abs/2209.14988
2022,A ConvNet for the 2020s,Zhuang Liu et al.,https://arxiv.org/abs/2201.03545
2022,Patches Are All You Need (ConvMixer),Asher Trockman et al.,https://arxiv.org/abs/2201.09792
2022,Block-NeRF: Scalable Large Scene Neural View Synthesis,Matthew Tancik et al.,https://arxiv.org/abs/2202.05263
2022,DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection,Hao Zhang et al.,https://arxiv.org/abs/2203.03605
2022,Scaling Up Your Kernels to 31Ã—31: Revisiting Large Kernel Design in CNNs,Xiaohan Ding et al.,https://arxiv.org/abs/2203.06717
2022,TensoRF: Tensorial Radiance Fields,Anpei Chen et al.,https://link.springer.com/chapter/10.1007/978-3-031-19824-3_20
2022,MaxViT: Multi-Axis Vision Transformer,Zhengzhong Tu et al.,https://arxiv.org/abs/2204.01697
2022,Hierarchical Text-Conditional Image Generation with CLIP Latents (DALL-E 2),Aditya Ramesh et al.,https://arxiv.org/abs/2204.06125
2022,Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding (Imagen),Chitwan Saharia et al.,https://arxiv.org/abs/2205.11487
2022,GIT: A Generative Image-to-text Transformer for Vision and Language,Jianfeng Wang et al.,https://arxiv.org/abs/2205.14100
2022,CMT: Convolutional Neural Networks Meet Vision Transformers,Jianyuan Guo et al.,https://arxiv.org/abs/2107.06263
2022,Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images,Ali Hatamizadeh et al.,https://link.springer.com/chapter/10.1007/978-3-031-08999-2_22
2022,Classifier-Free Diffusion Guidance,Jonathan Ho and Tim Salimans,https://arxiv.org/abs/2207.12598
2022,DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation,Nataniel Ruiz et al.,https://arxiv.org/abs/2208.12242
2022,DreamFusion: Text-to-3D using 2D Diffusion,Ben Poole et al.,https://arxiv.org/abs/2209.14988
2022,Make-A-Video: Text-to-Video Generation without Text-Video Data,Uriel Singer et al.,https://arxiv.org/abs/2209.14792
2022,On Distillation of Guided Diffusion Models,Chenlin Meng et al.,https://arxiv.org/abs/2210.03142
2022,LAION-5B: An open large-scale dataset for training next generation image-text models,Christoph Schuhmann et al.,https://arxiv.org/abs/2210.08402
2022,Imagic: Text-Based Real Image Editing with Diffusion Models,Bahjat Kawar et al.,https://arxiv.org/abs/2210.09276
2022,Visual Prompt Tuning,Menglin Jia et al.,https://link.springer.com/chapter/10.1007/978-3-031-19827-4_41
2022,Magic3D: High-Resolution Text-to-3D Content Creation,Chen-Hsuan Lin et al.,https://arxiv.org/abs/2211.10440
2022,DiffusionDet: Diffusion Model for Object Detection,Shoufa Chen et al.,https://arxiv.org/abs/2211.09788
2022,InstructPix2Pix: Learning to Follow Image Editing Instructions,Tim Brooks et al.,https://arxiv.org/abs/2211.09800
2022,Multi-Concept Customization of Text-to-Image Diffusion (Custom Diffusion),Nupur Kumari et al.,https://arxiv.org/abs/2212.04488
2022,Scalable Diffusion Models with Transformers (DiT),William Peebles and Saining Xie,https://arxiv.org/abs/2212.09748
2023,Vision Transformers Need Registers, T. Darcet el al., https://arxiv.org/abs/2309.16588
2023,Muse: Text-To-Image Generation via Masked Generative Transformers,Huiwen Chang et al.,https://arxiv.org/abs/2301.00704
2023,Structure and Content-Guided Video Synthesis with Diffusion Models (Gen-1),Omer Bar-Tal et al.,https://arxiv.org/abs/2302.03011
2023,Scaling Vision Transformers to 22 Billion Parameters,Mostafa Dehghani et al.,https://arxiv.org/abs/2302.05442
2023,Adding Conditional Control to Text-to-Image Diffusion Models (ControlNet),Lvmin Zhang et al.,https://arxiv.org/abs/2302.05543
2023,"Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models",Chenfei Wu et al.,https://arxiv.org/abs/2303.04671
2023,Scaling up GANs for Text-to-Image Synthesis (GigaGAN),Minguk Kang et al.,https://arxiv.org/abs/2303.05511
2023,DINOv2: Learning Robust Visual Features without Supervision,Maxime Oquab et al.,https://arxiv.org/abs/2304.07193
2023,Visual Instruction Tuning,Haotian Liu et al.,https://arxiv.org/abs/2304.08485
2023,Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models,Andreas Blattmann et al.,https://arxiv.org/abs/2304.08818
2023,Synthetic Data from Diffusion Models Improves ImageNet Classification,Shekoofeh Azizi et al.,https://arxiv.org/abs/2304.08466
2023,Segment Anything in Medical Images,Junlong Cheng et al.,https://arxiv.org/abs/2304.12306
2023,Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold,Xingang Pan et al.,https://arxiv.org/abs/2305.10973
2023,Neuralangelo: High-Fidelity Neural Surface Reconstruction,Zhaoshuo Li et al.,https://arxiv.org/abs/2306.03092
2023,SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis,Dustin Podell et al.,https://arxiv.org/abs/2307.01952
2023,3D Gaussian Splatting for Real-Time Radiance Field Rendering,Bernhard Kerbl et al.,https://dl.acm.org/doi/abs/10.1145/3592433
2023,"Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, and Generation",Jinze Bai et al.,https://arxiv.org/abs/2308.12966
2023,MVDream: Multi-view Diffusion for 3D Generation,Yichun Shi et al.,https://arxiv.org/abs/2308.16512
2023,Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks,Bin Xiao et al.,https://arxiv.org/abs/2311.06242
2023,VideoPoet: A Large Language Model for Zero-Shot Video Generation,Paul Vicol et al.,https://arxiv.org/abs/2312.14125
