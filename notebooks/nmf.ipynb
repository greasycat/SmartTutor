{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming 'embeddings' is your pre-computed embedding matrix\n",
    "# and 'df' contains your documents with columns 'title', 'summary', etc.\n",
    "\n",
    "def perform_nmf_topic_modeling(embeddings, n_topics=10, random_state=42):\n",
    "    \"\"\"\n",
    "    Perform NMF topic modeling on embedding matrix.\n",
    "    \n",
    "    Parameters:\n",
    "        embeddings: numpy array of shape (n_documents, embedding_dim)\n",
    "        n_topics: number of topics to extract\n",
    "        random_state: random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        W: document-topic matrix\n",
    "        H: topic-feature matrix\n",
    "        model: trained NMF model\n",
    "    \"\"\"\n",
    "    # Apply NMF to the embeddings\n",
    "    model = NMF(\n",
    "        n_components=n_topics,\n",
    "        init='nndsvd',  # Use Non-negative Double SVD for better initialization\n",
    "        max_iter=500,\n",
    "        random_state=random_state,\n",
    "        alpha=0.1  # L2 regularization to avoid overfitting\n",
    "    )\n",
    "    \n",
    "    # W is document-topic matrix, H is topic-feature matrix\n",
    "    W = model.fit_transform(embeddings)\n",
    "    H = model.components_\n",
    "    \n",
    "    return W, H, model\n",
    "\n",
    "def get_dominant_topics(W):\n",
    "    \"\"\"Get the dominant topic for each document.\"\"\"\n",
    "    return np.argmax(W, axis=1)\n",
    "\n",
    "def evaluate_topic_coherence(W, topic_idx, df, text_column='text', top_n=10):\n",
    "    \"\"\"\n",
    "    Find the top N documents for a given topic and print them to evaluate coherence.\n",
    "    \"\"\"\n",
    "    # Get affinity scores for this topic across all documents\n",
    "    topic_scores = W[:, topic_idx]\n",
    "    \n",
    "    # Get indices of top N documents for this topic\n",
    "    top_doc_indices = np.argsort(-topic_scores)[:top_n]\n",
    "    \n",
    "    # Return the documents\n",
    "    return df.iloc[top_doc_indices][['title', text_column]].values\n",
    "\n",
    "def visualize_topics(embeddings, topic_assignments, output_file='topic_visualization.png'):\n",
    "    \"\"\"\n",
    "    Visualize the topics using dimensionality reduction.\n",
    "    \"\"\"\n",
    "    # Use UMAP for dimensionality reduction\n",
    "    reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42)\n",
    "    reduced_embeddings = reducer.fit_transform(embeddings)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    scatter = plt.scatter(\n",
    "        reduced_embeddings[:, 0], \n",
    "        reduced_embeddings[:, 1], \n",
    "        c=topic_assignments, \n",
    "        cmap='tab10', \n",
    "        alpha=0.7,\n",
    "        s=10\n",
    "    )\n",
    "    plt.colorbar(scatter, label='Topic')\n",
    "    plt.title('Topic Visualization')\n",
    "    plt.xlabel('UMAP Dimension 1')\n",
    "    plt.ylabel('UMAP Dimension 2')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "def analyze_topic_distribution(W):\n",
    "    \"\"\"\n",
    "    Analyze the distribution of documents across topics.\n",
    "    \"\"\"\n",
    "    # Get dominant topic for each document\n",
    "    dominant_topics = get_dominant_topics(W)\n",
    "    \n",
    "    # Count documents per topic\n",
    "    topic_counts = np.bincount(dominant_topics, minlength=W.shape[1])\n",
    "    \n",
    "    # Create a percentage distribution\n",
    "    topic_distribution = 100 * topic_counts / len(dominant_topics)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'Topic': range(len(topic_counts)),\n",
    "        'Documents': topic_counts,\n",
    "        'Percentage': topic_distribution\n",
    "    }).sort_values('Documents', ascending=False)\n",
    "\n",
    "# Main execution\n",
    "n_topics = 10  # Adjust based on your dataset\n",
    "\n",
    "# Perform NMF topic modeling\n",
    "W, H, model = perform_nmf_topic_modeling(embeddings, n_topics=n_topics)\n",
    "\n",
    "# Get dominant topic for each document\n",
    "dominant_topics = get_dominant_topics(W)\n",
    "\n",
    "# Analyze topic distribution\n",
    "topic_distribution = analyze_topic_distribution(W)\n",
    "print(topic_distribution)\n",
    "\n",
    "# Visualize topics\n",
    "visualize_topics(embeddings, dominant_topics)\n",
    "\n",
    "# # If you want to extract topic keywords (assuming you have access to vocabulary)\n",
    "# # For instance, if you had a CountVectorizer used before getting embeddings:\n",
    "# def extract_topic_words(H, feature_names, n_top_words=10):\n",
    "#     \"\"\"Extract the top words for each topic.\"\"\"\n",
    "#     topics = []\n",
    "#     for i, topic_vector in enumerate(H):\n",
    "#         topic_words = [feature_names[j] for j in topic_vector.argsort()[:-n_top_words-1:-1]]\n",
    "#         topics.append({f'Topic {i}': topic_words})\n",
    "#     return topics\n",
    "# \n",
    "# # feature_names = count_vectorizer.get_feature_names_out()\n",
    "# # topic_words = extract_topic_words(H, feature_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
