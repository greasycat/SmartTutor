<?xml version='1.0' encoding='utf-8'?>
<feed><entry><id>https://machinelearningmastery.com/statistical-methods-for-evaluating-llm-performance/</id><title>Statistical Methods for Evaluating LLM Performance</title><author>Cornellius Yudha Wijaya</author><summary>The large language model (LLM) has become a cornerstone of many AI applications.</summary><published>2025-03-14</published><source>MachineLearningMastery.com</source></entry><entry><id>https://machinelearningmastery.com/understanding-rag-part-vii-vector-databases-indexing-strategies/</id><title>Understanding RAG Part VII: Vector Databases &amp; Indexing Strategies</title><author>Iv√°n Palomares Carrascosa</author><summary>Be sure to check out the previous articles in this series: ‚Ä¢ &lt;a href="https://machinelearningmastery.</summary><published>2025-03-12</published><source>MachineLearningMastery.com</source></entry><entry><id>https://machinelearningmastery.com/mastering-time-series-forecasting-from-arima-to-lstm/</id><title>Mastering Time Series Forecasting: From ARIMA to LSTM</title><author>Jayita Gulati</author><summary>Time series forecasting is a statistical technique used to analyze historical data points and predict future values based on temporal patterns.</summary><published>2025-03-12</published><source>MachineLearningMastery.com</source></entry><entry><id>https://machinelearningmastery.com/a-complete-guide-to-matrices-for-machine-learning-with-python/</id><title>A Complete Guide to Matrices for Machine Learning with Python</title><author>Iv√°n Palomares Carrascosa</author><summary>Matrices are a key concept not only in linear algebra but also with regard to their prominent application and use in machine learning (ML) and data science.</summary><published>2025-03-11</published><source>MachineLearningMastery.com</source></entry><entry><id>https://machinelearningmastery.com/the-beginners-guide-to-language-models-with-python/</id><title>The Beginner‚Äôs Guide to Language Models with Python</title><author>Iv√°n Palomares Carrascosa</author><summary>Language models ‚Äî often known for the acronym LLM for Large Language Models, their large-scale version ‚Äî fuel powerful AI applications like conversational chatbots, AI assistants, and other intelligent text and content generation apps.</summary><published>2025-03-10</published><source>MachineLearningMastery.com</source></entry><entry><id>https://machinelearningmastery.com/understanding-the-distilbart-model-and-rouge-metric/</id><title>Understanding the DistilBart Model and ROUGE Metric</title><author>Muhammad Asad Iqbal Khan</author><summary>This post is in two parts; they are: ‚Ä¢ Understanding the Encoder-Decoder Architecture ‚Ä¢ Evaluating the Result of Summarization using ROUGE DistilBart is a "distilled" version of the BART model, a powerful sequence-to-sequence model for natural language generation, translation, and comprehension.</summary><published>2025-03-10</published><source>MachineLearningMastery.com</source></entry><entry><id>https://machinelearningmastery.com/text-summarization-with-distillbart-model/</id><title>Text Summarization with DistillBart Model</title><author>Muhammad Asad Iqbal Khan</author><summary>This tutorial is in two parts; they are: ‚Ä¢ Using DistilBart for Summarization ‚Ä¢ Improving the Summarization Process Let's start with a fundamental implementation that demonstrates the key concepts of text summarization with DistilBart: import torch from transformers import AutoTokenizer, AutoModelForSeq2SeqLM class TextSummarizer: def __init__(self, model_name="sshleifer/distilbart-cnn-12-6"): """Initialize the summarizer with a pre-trained model.</summary><published>2025-03-08</published><source>MachineLearningMastery.com</source></entry><entry><id>https://machinelearningmastery.com/diagnosing-and-fixing-overfitting-in-machine-learning-with-python/</id><title>Diagnosing and Fixing Overfitting in Machine Learning with Python</title><author>Iv√°n Palomares Carrascosa</author><summary>Overfitting is one of the most (if not the most!) common problems encountered when building machine learning (ML) models.</summary><published>2025-03-07</published><source>MachineLearningMastery.com</source></entry><entry><id>https://machinelearningmastery.com/building-llm-applications-with-hugging-face-endpoints-and-fastapi/</id><title>Building LLM Applications with Hugging Face Endpoints and FastAPI</title><author>Iv√°n Palomares Carrascosa</author><summary>FastAPI is a modern and high-performance compliant web framework for building APIs with Python.</summary><published>2025-03-04</published><source>MachineLearningMastery.com</source></entry><entry><id>https://machinelearningmastery.com/10-python-one-liners-that-will-boost-your-data-preparation-workflow/</id><title>10 Python One-Liners That Will Boost Your Data Preparation Workflow</title><author>Cornellius Yudha Wijaya</author><summary>Data preparation is a step within the data project lifecycle where we prepare the raw data for subsequent processes, such as data analysis and machine learning modeling.</summary><published>2025-03-03</published><source>MachineLearningMastery.com</source></entry><entry><id>https://huggingface.co/blog/gemma3</id><title>Welcome Gemma 3: Google's all new multimodal, multilingual, long context open LLM</title><author>Aritra Roy Gosthipaty, Merve Noyan, Pedro Cuenca, Vaibhav Srivastav</author><summary>Today Google releases Gemma 3, a new iteration of their Gemma family of models. The models range from 1B to 27B parameters, have a context window up to 128k tokens, can accept images and text, and support 140+ languages. Try out Gemma 3 now üëâüèª Gemma 3 Space</summary><published>2025-03-12</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/lerobot-goes-to-driving-school</id><title>LeRobot goes to driving school: World‚Äôs largest open-source self-driving dataset</title><author>Harsimrat Sandhawalia, Remi Cadene</author><summary>OpenStreetMap

Multimodal search

LeRobot

LeRobot driver

Appendix

A.1 Route tasks

A.2 LLM prompts

A.2 Data collection hardware

State-of-the art Vision Language Models and Large Language Models are trained on open-source
image-text corpora sourced from the internet, which spearheaded the recent acceleration of open-source AI. Despite these
breakthroughs, the adoption of end-to-end AI within the robotics and automotive community remains low, primarily due to a
lack of high quality, large scale multimodal datasets like OXE.
To unlock the potential for robotics AI, Yaak teamed up with the LeRobot team at ü§ó and is excited to announce
Learning to Drive (L2D) to the robotics AI community. L2D is the world‚Äôs largest multimodal dataset aimed at
building an open-sourced spatial intelligence for the automotive domain with first class support for ü§ó‚Äôs LeRobot
training pipeline and models. Drawing inspiration from the best practices of
source version control, Yaak also invites the AI
community to search and discover
novel episodes in our entire dataset (&gt; 1 PetaBytes), and queue their collection for review to be merged into
future release (R5+). Table 1: Open source self-driving datasets (*excluding lidar and radar). Source</summary><published>2025-03-11</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/llm-inference-on-edge</id><title>LLM Inference on Edge: A Fun and Easy Guide to run LLMs via React Native on your Phone!</title><author>Mohamed Mekkouri, Marc Sun</author><summary>Why You Should Follow This Tutorial?

0. Choosing the Right Models
Model Size Considerations

GGUF Quantization Formats

Recommended Models to Try

Finding More Models


1. Setting Up Your Environment
Tools You Need

Virtual Device Setup


2. Create the App
Project Structure


3. Running the Demo &amp; Project
Running the Demo

Running the Project


4. App Implementation
Installing Dependencies

State Management

Fetching available GGUF models from the Hub

Model Download Implementation

Model Loading and Initialization

Chat Implementation

The UI &amp; Logic

The other Functionnalities


5. How to Debug
Chrome DevTools Debugging

Common Debugging Tips


6. Additional Features we can add

7. Acknowledgments

8. Conclusion

As LLMs continue to evolve, they are becoming smaller and smarter, enabling them to run directly on your phone. Take, for instance, the DeepSeek R1 Distil Qwen 2.5 with 1.5 billion parameters, this model really shows how advanced AI can now fit into the palm of your hand! In this blog, we will guide you through creating a mobile app that allows you to chat with these powerful models locally. The complete code for this tutorial is available in our EdgeLLM repository. If you've ever felt overwhelmed by the complexity of open-source projects, fear not! Inspired by the Pocket Pal app, we will help you build a straightforward React Native application that downloads LLMs from the Hugging Face hub, ensuring everything remains private and runs on your device. We will utilize llama.rn, a binding for llama.cpp, to load GGUF files efficiently!</summary><published>2025-03-07</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/jfrog</id><title>Hugging Face and JFrog partner to make AI Security more transparent</title><author>Luc Georges, Shachar M</author><summary>Model security refresher

Integration

We are pleased to announce our partnership with JFrog, creators of the JFrog Software Supply Chain Platform, as part of our long-standing commitment to provide a safe and reliable platform for the ML community. We have decided to add JFrog's scanner to our platform to continue improving security on the Hugging Face Hub. JFrog's scanner brings new functionality to scanning, aimed at reducing false positives on the Hub. Indeed, what we currently observe is that model weights can contain code that is executed upon deserialization and sometimes at inference time, depending on the format. This code is oftentimes a non harmful practicality for the developer. As our picklescan scanner only performs pattern matching on module names, we cannot always confirm that usage of a given function or module is malicious.
JFrog goes a step deeper and will parse and analyze code it finds in models weights to check for potential malicious usage.</summary><published>2025-03-04</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/aya-vision</id><title>A Deepdive into Aya Vision: Advancing the Frontier of Multilingual Multimodality</title><author>Saurabh Dash, Yiyang Nan, Arash Ahmadian, John Dang</author><summary>Aya Vision Architecture and Training
Training process


Multimodal Data Enhancement and Expanding Language Coverage

Multimodal Model Merging

Scaling up to 32B

Aya Vision Benchmark ‚Äì a multilingual evaluation data

Designed for real-world applications

Getting Started with Aya

Acknowledgments

References

With the release of the Aya Vision family, our new 8B and 32B parameter vision-language models (VLMs), we are addressing one of the biggest challenges in AI: bringing multilingual performance to multimodal models. Aya Vision is Cohere For AI's latest open-weight multilingual and multimodal model family, designed to be a strong foundation for language and vision understanding across 23 languages. It builds on the success of Aya Expanse, state-of-the-art multilingual language models, and extends it using a combination of advanced techniques. These include synthetic annotations, scaling up multilingual data through translation and rephrasing, and multimodal model merging ‚Äì key methods that improve both language and vision understanding in a multilingual setting.</summary><published>2025-03-04</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/smolagents-phoenix</id><title>Trace &amp; Evaluate your Agent with Arize Phoenix</title><author>Sri Chavali, John Gilhuly, Aymeric Roucher</author><summary>Make An Agent
Step 1: Install the Required Libraries

Step 2: Import all the Essential Building Blocks

Step 3: Set Up Our Base Models

Step 4: Create the Tool-Calling Agent

Step 5: Run the Agent


Trace Your Agent

Evaluate Your Agent
Step 1: Install OpenAI

Step 2: Retrieve Tool Execution Spans

Step 3: Import Prompt Template

Step 4: Run the Evaluation

Step 5: Send Evaluation Results to Phoenix


So, you‚Äôve built your agent. It takes in inputs and tools, processes them, and generates responses. Maybe it‚Äôs making decisions, retrieving information, executing tasks autonomously, or all three. But now comes the big question ‚Äì how effectively is it performing? And more importantly, how do you know? Building an agent is one thing; understanding its behavior is another. That‚Äôs where tracing and evaluations come in. Tracing allows you to see exactly what your agent is doing step by step‚Äîwhat inputs it receives, how it processes information, and how it arrives at its final output. Think of it like having an X-ray for your agent‚Äôs decision-making process. Meanwhile, evaluation helps you measure performance, ensuring your agent isn‚Äôt just functional, but actually effective. Is it producing the right answers? How relevant are its findings at  each step? How well-crafted is the agent‚Äôs response? Does it align with your goals?</summary><published>2025-02-28</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/iisc-huggingface-collab</id><title>HuggingFace, IISc partner to supercharge model building on India's diverse languages</title><author>Prasanta Kumar Ghosh, Nihar Desai, Sanka, Sujith Pulikodan</author><summary>Partnership

About Vaani Dataset
District wise language distribution

Transcribed subset


Utility of Vaani in the Age of LLMs

What's next

How You Can Contribute

The Indian Institute of Science IISc and ARTPARK  partner with Hugging Face to enable developers across the globe to access Vaani, India's most diverse open-source, multi-modal, multi-lingual dataset. Both organisations share a commitment to building inclusive, accessible, and state-of-the-art AI technologies that honor linguistic and cultural diversity. The partnership between Hugging Face and IISc/ARTPARK aims to increase the accessibility and improve usability of the Vaani dataset, encouraging the development of AI systems that better understand India's diverse languages and cater to the digital needs of its people.</summary><published>2025-02-27</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/fastrtc</id><title>FastRTC: The Real-Time Communication Library for Python</title><author>Freddy Boulton, Abubakar Abid</author><summary>Getting Started

Leveling-Up: LLM Voice Chat

Bonus: Call via Phone

Next Steps

In the last few months, many new real-time speech models have been released and entire companies have been founded around both open and closed source models. To name a few milestones: Despite the explosion on the model and funding side, it's still difficult to build real-time AI applications that stream audio and video, especially in Python.</summary><published>2025-02-25</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/remote_vae</id><title>Remote VAEs for decoding with HF endpoints ü§ó</title><author>hlky, Sayak Paul</author><summary>Getting started
Code

Basic example

Generation

Queueing


Available VAEs

Advantages of using a remote VAE

Provide feedback
Steps:


When operating with latent-space diffusion models for high-resolution image and video synthesis, the VAE decoder can consume quite a bit more memory. This makes it hard for the users to run these models on consumer GPUs without going through latency sacrifices and others alike. For example, with offloading, there is a device transfer overhead, causing delays in the overall inference latency. Tiling is another solution that lets us operate on so-called ‚Äútiles‚Äù of inputs. However, it can have a negative impact on the quality of the final image.</summary><published>2025-02-24</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/siglip2</id><title>SigLIP 2: A better multilingual vision language encoder</title><author>Aritra Roy Gosthipaty, Merve Noyan, Pavel Iakubovskii</author><summary>Today Google releases a new and better family of multilingual vision-language encoders, SigLIP 2. The authors have extended the training objective of SigLIP (sigmoid loss) with additional objectives for improved semantic understanding, localization, and dense features. SigLIP 2 models outperform the older SigLIP ones at all model scales in core capabilities, including zero-shot classification, image-text retrieval, and transfer performance when extracting visual representations for Vision-Language Models (VLMs).</summary><published>2025-02-21</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/smolvlm2</id><title>SmolVLM2: Bringing Video Understanding to Every Device</title><author>Orr Zohar, Miquel Farr√©, Andres Marafioti, Merve Noyan, Pedro Cuenca, Cyril, Joshua</author><summary>SmolVLM2 represents a fundamental shift in how we think about video understanding - moving from massive models that require substantial computing resources to efficient models that can run anywhere. Our goal is simple: make video understanding accessible across all devices and use cases, from phones to servers. We are releasing models in three sizes (2.2B, 500M and 256M), MLX ready (Python and Swift APIs) from day zero.
We've made all models and demos available in this collection.</summary><published>2025-02-20</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/paligemma2mix</id><title>PaliGemma 2 Mix - New Instruction Vision Language Models by Google</title><author>Merve Noyan, Aritra Roy Gosthipaty, Andreas P. Steiner</author><summary>Last December, Google released PaliGemma 2: a new family of pre-trained (pt) PaliGemma vision language models (VLMs) based on SigLIP and Gemma 2. The models come in three different sizes (3B, 10B, 28B) and three different resolutions (224x224, 448x448, 896x896). Today, Google is releasing PaliGemma 2 mix: fine-tuned on a mix of vision language tasks, including OCR, long and short captioning and more.</summary><published>2025-02-19</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/inference-providers-nebius-novita-hyperbolic</id><title>Introducing Three New Serverless Inference Providers: Hyperbolic, Nebius AI Studio, and Novita üî•</title><author>Julien Chaumond, Bertrand Chevrier, Vaibhav Srivastav, Simon Brandeis, Albert Abdulmanov, Viktor Hu, Connor Chevli</author><summary>How it works
In the website UI

From the client SDKs


Billing

Feedback and next steps

We‚Äôre thrilled to announce the addition of three more outstanding serverless Inference Providers to the Hugging Face Hub: Hyperbolic, Nebius AI Studio, and Novita. These providers join our growing ecosystem, enhancing the breadth and capabilities of serverless inference directly on the Hub‚Äôs model pages. They‚Äôre also seamlessly integrated into our client SDKs (for both JS and Python), making it super easy to use a wide variety of models with your preferred providers. These partners join the ranks of our existing providers, including Together AI, Sambanova, Replicate, fal and Fireworks.ai.</summary><published>2025-02-18</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/fireworks-ai</id><title>Welcome Fireworks.ai on the Hub üéÜ</title><author>Teo Feliu, Shaunak Godbole, Julien Chaumond</author><summary>How it works
In the website UI

From the client SDKs

From HTTP calls


Billing

Following our recent announcement on Inference Providers on the Hub, we're thrilled to share that Fireworks.ai is now a supported Inference Provider on HF Hub! Fireworks.ai delivers blazing-fast serverless inference directly on model pages, as well as throughout the whole HF ecosystem of libraries and tools, making it easier than ever to run inference on your favorite models.</summary><published>2025-02-14</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/math_verify_leaderboard</id><title>Fixing Open LLM Leaderboard with Math-Verify</title><author>Hynek Kydlicek, Alina Lozovskaya, Nathan Habib, Cl√©mentine Fourrier</author><summary>Why math evaluation on the Open LLM Leaderboard was broken

Which model is the best at math? A complete reshuffling of cards thanks to fairer evaluations
Impact of the change

Model Family Changes

Changes in the MATH-Hard Leaderboard

Changes in the Leaderboard


Wrapping Up

3 weeks ago, we showed how hard it is to correctly evaluate LLM performance on math problems, and introduced Math-Verify, a better solution to validate models on math (read more in the announcement)! Today, we‚Äôre thrilled to share that we‚Äôve used Math-Verify to thoroughly re-evaluate all 3,751 models ever submitted to the Open LLM Leaderboard, for even fairer and more robust model comparisons!</summary><published>2025-02-14</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/billion-classifications</id><title>1 Billion Classifications</title><author>Derek Thomas</author><summary>Approach

Optimization
Setup

Load Testing Parameters

K6

Orchestration


Classification
Introduction

Experiment

Results


Embedding
Introduction

Experiment

Results


Vision Embedding
Introduction

Experiment

Results


Analysis

Conclusion

References

Appendix
Sanity Checks

Cost Analysis

Infinity Client

Other Lessons Learned

Future Improvements


You‚Äôve optimized your model. Your pipeline is running smoothly. But now, your cloud bill has skyrocketed. Running 1B+ classifications or embeddings per day isn‚Äôt just a technical challenge‚Äîit‚Äôs a financial one. How do you process at this scale without blowing your budget? Whether you're running large-scale document classification or bulk embedding pipelines for Retrieval-Augmented Generation (RAG), you need cost-efficient, high-throughput inference to make it feasible, and you get that from having a well optimized configuration. These tasks often use encoder models, which are much smaller than modern LLMs, but at the 1B+ inference request scale it's still quite a non-trivial task. Just to be clear, that's English Wikipedia 144x over. I haven‚Äôt seen much information on how to approach this with cost in mind and I want to tackle that. This blog breaks down HOW to calculate cost and latency for large scale classification and embedding. We‚Äôll analyze different model architectures, benchmark costs across hardware choices, and give you a clear framework for optimizing your own setup. Additionally we should be able to build some intuition if you don't feel like going through the process yourself.</summary><published>2025-02-13</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/from-chunks-to-blocks</id><title>From Chunks to Blocks: Accelerating Uploads and Downloads on the Hub</title><author>Jared Sulzdorf, yuchenglow, Zach Nation, saba noorassa</author><summary>The Realities of Scaling Deduplication

Design Principles for Deduplication at Scale

Scaling Deduplication with Aggregation

Aggregated Deduplication in Practice

Content-defined chunking (CDC) plays a central role in enabling deduplication within a Xet-backed repository. The idea is straightforward: break each file‚Äôs data into chunks, store only unique ones, reap the benefits. In practice, it's more complex. If we focused solely on maximizing deduplication, the design would call for the smallest possible chunk size. By doing that, we‚Äôd create significant overheads for the infrastructure and the builders on the Hub.</summary><published>2025-02-12</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/vid_ds_scripts</id><title>Build awesome datasets for video generation</title><author>hlky, Sayak Paul</author><summary>Tooling
Stage 1 (Acquisition)

Stage 2 (Pre-processing/filtering)

Stage 3 (Processing)


Filtering examples
OCR/Caption


Putting this tooling to use üë®‚Äçüç≥

Your Turn

Tooling for image generation datasets is well established, with img2dataset being a fundamental tool used for large scale dataset preparation, and complemented with various community guides, scripts and UIs that cover smaller scale initiatives. Our ambition is to make tooling for video generation datasets equally established, by creating open video dataset scripts suited for small scale, and leveraging video2dataset for large scale use cases.</summary><published>2025-02-12</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/leaderboard-arabic-v2</id><title>The Open Arabic LLM Leaderboard 2</title><author>Ali El Filali, Manel ALOUI, Tarique Husaain, Ahmed Alzubaidi, Basma Boussaha, Ruxandra Cojocaru, Hakim Hacid, Cl√©mentine Fourrier</author><summary>Current status of Arabic LLMs leaderboards

Impact of the previous leaderboard

Why do we need a new leaderboard?

What's new in this version?

Results from v1 and v2

Conclusion and future work

Acknowledgments

Citations

References The growing availability of LLMs supporting Arabic, both as monolingual and multilingual models, prompted the community to create dedicated Arabic language leaderboards. Previously, Arabic-focused leaderboards were typically confined to narrow benchmarks introduced by specific authors, often as demos for their work. In these cases, the authors would set up leaderboards to demonstrate how models performed on a particular task or dataset. Alternatively, other leaderboards required users to run evaluations on their own computing resources and then submit a JSON file containing their results for display.</summary><published>2025-02-10</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/open-deep-research</id><title>Open-source DeepResearch ‚Äì Freeing our search agents</title><author>Aymeric Roucher, Albert Villanova del Moral, Merve Noyan, Thomas Wolf, Cl√©mentine Fourrier</author><summary>Yesterday, OpenAI released Deep Research, a system that browses the web to summarize content and answer questions based on the summary. The system is impressive and blew our minds when we tried it for the first time. One of the main results in the blog post is a strong improvement of performances on the General AI Assistants benchmark (GAIA), a benchmark we‚Äôve been playing with recently as well, where they successfully reached near 67% correct answers on 1-shot on average, and 47.6% on especially challenging ‚Äúlevel 3‚Äù questions that involve multiple steps of reasoning and tool usage (see below for a presentation of GAIA).</summary><published>2025-02-04</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/pi0</id><title>œÄ0 and œÄ0-FAST: Vision-Language-Action Models for General Robot Control</title><author>Dana Aubakirova, Pablo Montalvo, Mustafa Shukor, Remi Cadene</author><summary>Introduction

üîç What is œÄ0?

How to Use œÄ0 in LeRobot?
Inference on œÄ0 pretrained model

Fine-tuning the œÄ0 Pretrained Model


What is the difference between VLMs and VLAs?

Attention Mechanisms in Robotics Policies
Key Idea


‚ö° Towards the Faster Attention in œÄ0
Handling 2D Attention Masks

Can we use FlashAttention2?

Using FlexAttention in PyTorch


How to effectively represent Actions?

üöÄ What is œÄ0-FAST?
Key Advantages of œÄ0-FAST:


How does FAST work?

How to use FAST tokenizer?

What‚Äôs Next for Generalist Robot Intelligence?

Additional Resources

References

We have ported the first robotics foundation models to Hugging Face LeRobot! Both œÄ0 and œÄ0-FAST, developed by Physical Intelligence, are now available in the LeRobot repository, bringing generalist robotic intelligence to the Hugging Face ecosystem. If you are curious about how Vision-Language-Action (VLA) models differ from Vision-Language Models (VLMs) and how actions are represented, dive into this blog post to find out! Explore the model collection and the PyTorch Version of the model in our repository:
Huggingface collection of Pi0 models | LeRobot repo</summary><published>2025-02-04</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/dabstep</id><title>DABStep: Data Agent Benchmark for Multi-step Reasoning</title><author>Alex Egg, Martin Iglesias Goyanes, Friso Kingma, Andreu Mora, Leandro von Werra, Thomas Wolf</author><summary>Motivation

Introducing DABstep

What's inside the DABstep?
Data

Tasks

Evaluations

Real-time leaderboard

Baselines


Getting Started and Infra

Future direction

Related Works

Language models are becoming increasingly capable and can solve tasks autonomously as agents. There are many exciting use cases, especially at the intersection of reasoning, code, and data. However, proper evaluation benchmarks on real-world problems are lacking and hinder progress in the field. To tackle this challenge, Adyen and Hugging Face built the Data Agent Benchmark for Multi-step Reasoning (DABstep) together. DABstep consists of over 450 data analysis tasks designed to evaluate the capabilities of state-of-the-art LLMs and AI agents.</summary><published>2025-02-04</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/ai-art-newsletter-jan-25</id><title>The AI tools for Art Newsletter - Issue 1</title><author>Linoy Tsaban, Apolin√°rio from multimodal AI art</author><summary>Table of Contents

Major Releases of 2024

Image Generation
Text-to-image generation

Personalization &amp; stylization


Video Generation

Audio Generation

Creative Tools that Shined in 2024

What should we expect for AI &amp; Art in 2025?

Starting off strong - Open source releases of January 25

Announcing Our Newsletter üóûÔ∏è

The AI space is moving so fast it‚Äôs hard to believe that a year ago we still struggled to generate people with the correct amount of fingers üòÇ. The last couple of years have been pivotal for open source models and tools for artistic usage. 
AI tools for creative expression have never been more accessible, and we‚Äôre only scratching the surface. 
Join us as we look back at the key milestones, tools, and breakthroughs in AI &amp; Arts from 2024, 
and forward for what‚Äôs to come in 2025 (spoiler üëÄ: we‚Äôre starting a new monthly roundup üëá).</summary><published>2025-01-31</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/deepseek-r1-aws</id><title>How to deploy and fine-tune DeepSeek models on AWS</title><author>Simon Pagezy, Jeff Boudier, David Corvoysier</author><summary>What is DeepSeek-R1?

Deploy DeepSeek R1 models
Deploy on AWS with Hugging Face Inference Endpoints

Deploy on Amazon Bedrock Marketplace

Deploy on Amazon Sagemaker AI with Hugging Face LLM DLCs

Deploy on EC2 Neuron with the Hugging Face Neuron Deep Learning AMI


Fine-tune DeepSeek R1 models
Fine tune on Amazon SageMaker AI with Hugging Face Training DLCs

Fine tune on EC2  Neuron with the Hugging Face Neuron Deep Learning AMI


A running document to showcase how to deploy and fine-tune DeepSeek R1 models with Hugging Face on AWS. If you‚Äôve ever struggled with a tough math problem, you know how useful it is to think a little longer and work through it carefully.¬†OpenAI‚Äôs o1 model¬†showed that when LLMs are trained to do the same‚Äîby using more compute during inference‚Äîthey get significantly better at solving reasoning tasks like mathematics, coding, and logic.</summary><published>2025-01-30</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/inference-providers</id><title>Welcome to Inference Providers on the Hub üî•</title><author>Burkay Gur, Zeke Sikelianos, Anton McGonnell, Hassan El Mghari, Simon Brandeis, Bertrand Chevrier, Julien Chaumond</author><summary>How it works
In the website UI

From the client SDKs

From HTTP calls


Billing

Feedback and next steps

Today, we are launching the integration of four awesome serverless Inference Providers ‚Äì fal, Replicate, Sambanova, Together AI ‚Äì directly on the Hub‚Äôs model pages. They are also seamlessly integrated into our client SDKs (for JS and Python), making it easier than ever to explore serverless inference of a wide variety of models that run on your favorite providers. We‚Äôve been hosting a serverless Inference API on the Hub for a long time (we launched the v1 in summer 2020 ‚Äì wow, time flies ü§Ø). While this has enabled easy exploration and prototyping, we‚Äôve refined our core value proposition towards collaboration, storage, versioning, and distribution of large datasets and models with the community. At the same time, serverless providers have flourished, and the time was right for Hugging Face to offer easy and unified access to serverless inference through a set of great providers.</summary><published>2025-01-28</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/open-r1</id><title>Open-R1: a fully open reproduction of DeepSeek-R1</title><author>Elie Bakouch, Leandro von Werra, Lewis Tunstall</author><summary>If you‚Äôve ever struggled with a tough math problem, you know how useful it is to think a little longer and work through it carefully. OpenAI‚Äôs o1 model showed that when LLMs are trained to do the same‚Äîby using more compute during inference‚Äîthey get significantly better at solving reasoning tasks like mathematics, coding, and logic. However, the recipe behind OpenAI‚Äôs reasoning models has been a well kept secret. That is, until last week, when DeepSeek released their DeepSeek-R1 model and promptly broke the internet (and the stock market!).</summary><published>2025-01-28</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/video_gen</id><title>State of open video generation models in Diffusers</title><author>Sayak Paul, Aryan V S, Dhruv Nair</author><summary>Today‚Äôs Video Generation Models and their Limitations

Why is Video Generation Hard?

Open Video Generation Models

Video Generation with Diffusers
Memory requirements

Suite of optimizations

Fine-tuning


Looking ahead

Resources

OpenAI‚Äôs Sora demo marked a striking advance in AI-generated video last year and gave us a glimpse of the potential capabilities of video generation models. The impact was immediate and since that demo, the video generation space has become increasingly competitive with major players and startups producing their own highly capable models such as Google‚Äôs Veo2, Haliluo‚Äôs Minimax, Runway‚Äôs Gen3 Alpha, Kling, Pika, and Luma Lab‚Äôs Dream Machine. Open-source has also had its own surge of video generation models with CogVideoX, Mochi-1, Hunyuan, Allegro, and LTX Video. Is the video community having its ‚ÄúStable Diffusion moment‚Äù?</summary><published>2025-01-27</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/smolagents-can-see</id><title>We now support VLMs in smolagents!</title><author>Aymeric Roucher, Merve Noyan, Albert Villanova del Moral</author><summary>TL;DR

Table of Contents

Overview

How we gave sight to smolagents
How to create a Web browsing agent with vision

Running the agent


Next Steps

You hypocrite, first take the log out of your own eye, and then you will see clearly to take the speck out of your brother's eye. Matthew 7, 3-5 We have added vision support to smolagents, which unlocks the use of vision language models in agentic pipelines natively.</summary><published>2025-01-24</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/smolervlm</id><title>SmolVLM Grows Smaller ‚Äì Introducing the 250M &amp; 500M Models!</title><author>Andres Marafioti, Miquel Farr√©, Merve Noyan</author><summary>We‚Äôre excited to announce two new additions to the SmolVLM family: SmolVLM-256M and SmolVLM-500M. That‚Äôs right‚Äî256M parameters, making it the smallest Vision Language Model in the world! We built on everything we learned from SmolVLM 2B while focusing on efficiency, data mixtures, and new design trade-offs. We are excited to introduce a pair of models that preserve strong multimodal performance in a fraction of the footprint.</summary><published>2025-01-23</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/friendliai-partnership</id><title>Hugging Face and FriendliAI partner to supercharge model deployment on the Hub</title><author>Ahnjae Shin, Soomin Chun, Byung-Gon Chun, Julien Chaumond</author><summary>A Collaboration to Advance AI Innovation

Simplifying Model Deployment
Deploy models with NVIDIA H100 in Friendli Dedicated Endpoints

Inference Open-Source Models with Friendli Serverless Endpoints


What‚Äôs Next

FriendliAI‚Äôs inference infrastructure is now integrated into the Hugging Face Hub as an option in the ‚ÄúDeploy this model‚Äù button, simplifying and accelerating generative AI model serving. Hugging Face empowers developers, researchers, and businesses to innovate in AI. Our common priority is building impactful partnerships that simplify workflows and provide cutting-edge tools for the AI community.</summary><published>2025-01-22</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/timm-transformers</id><title>Timm ‚ù§Ô∏è Transformers: Use any timm model with transformers</title><author>Aritra Roy Gosthipaty, Ross Wightman, Pavel Iakubovskii, Pedro Cuenca, Vaibhav Srivastav</author><summary>What is timm?

Why Use the timm integration?

Pipeline API: Using timm Models for Image Classification

Gradio Integration: Building a Food Classifier Demo üç£

Auto Classes: Simplifying Model Loading

Running quantized timm models

Supervised Fine-Tuning of timm models
Standard Fine-Tuning with the Trainer API


LoRA Fine-Tuning for Efficient Training
Inference with LoRA Fine-Tuned Model


Round trip integration

Torch Compile: Instant Speedup

Wrapping Up

Acknowledgments

Get lightning-fast inference, quick quantization, torch.compile boosts, and effortless fine-tuning
for any timm model‚Äîall within the friendly ü§ó transformers ecosystem. Enter TimmWrapper‚Äîa simple,
yet powerful tool that unlocks this potential.</summary><published>2025-01-16</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/tgi-multi-backend</id><title>Introducing multi-backends (TRT-LLM, vLLM) support for Text Generation Inference</title><author>Morgan Funtowicz, Hugo Larcher</author><summary>Since its initial release in 2022, Text-Generation-Inference (TGI) has provided Hugging Face and the AI Community with a performance-focused solution to easily deploy large-language models (LLMs). TGI initially offered an almost no-code solution to load models from the Hugging Face Hub and deploy them in production on NVIDIA GPUs. Over time, support expanded to include AMD Instinct GPUs, Intel GPUs, AWS Trainium/Inferentia, Google TPU, and Intel Gaudi.Over the years, multiple inferencing solutions have emerged, including vLLM, SGLang, llama.cpp, TensorRT-LLM, etc., splitting up the overall ecosystem. Different models, hardware, and use cases may require a specific backend to achieve optimal performance. However, configuring each backend correctly, managing licenses, and integrating them into existing infrastructure can be challenging for users. To address this, we are excited to introduce the concept of TGI Backends. This new architecture gives the flexibility to integrate with any of the solutions above through TGI as a single unified frontend layer. This change makes it easier for the community to get the best performance for their production workloads, switching backends according to their modeling, hardware, and performance requirements.</summary><published>2025-01-16</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/static-embeddings</id><title>Train 400x faster Static Embedding Models with Sentence Transformers</title><author>Tom Aarsen</author><summary>This blog post introduces a method to train static embedding models that run 100x to 400x faster on CPU than state-of-the-art embedding models, while retaining most of the quality. This unlocks a lot of exciting use cases, including on-device and in-browser execution, edge computing, low power and embedded applications. We apply this recipe to train two extremely efficient embedding models: sentence-transformers/static-retrieval-mrl-en-v1 for English Retrieval, and sentence-transformers/static-similarity-mrl-multilingual-v1 for Multilingual Similarity tasks. These models are 100x to 400x faster on CPU than common counterparts like all-mpnet-base-v2 and multilingual-e5-small, while reaching at least 85% of their performance on various benchmarks.</summary><published>2025-01-15</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/run-comfyui-workflows-on-spaces</id><title>Run ComfyUI workflows for free on Spaces</title><author>Apolin√°rio from multimodal AI art, Charles Bensimon</author><summary>Intro
Prerequisites


1. Exporting your ComfyUI workflow to run on pure Python

2. Create a Gradio app for the exported Python

3. Preparing it to run Hugging Face Spaces

4. Exporting to Spaces and running on ZeroGPU
Fix requirements

Move models outside the decorated function (ZeroGPU only)

If you are not a PRO subscriber (skip this step if you are)

The demo is running


5. Conclusion

Index: In this tutorial I will present a step-by-step guide on how to convert a complex ComfyUI workflow to a simple Gradio application, and how to deploy this application on Hugging Face Spaces ZeroGPU serverless structure, which allows for it to be deployed and run for free in a serverless manner. In this tutorial, we are going to work with Nathan Shipley's Flux[dev] Redux + Flux[dev] Depth ComfyUI workflow, but you can follow the tutorial with any workflow that you would like.</summary><published>2024-01-14</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/ethics-soc-7</id><title>AI Agents Are Here. What Now?</title><author>Margaret Mitchell, Avijit Ghosh, Sasha Luccioni, Giada Pistilli</author><summary>Introduction

What is an AI agent?
Overview

The Spectra of AI Agents


Risks, Benefits, and Uses: A Values-Based Analysis
Value: Accuracy

Value: Assistiveness

Value: Consistency

Value: Efficiency

Value: Equity

Value: Humanlikeness

Value: Interoperability

Value: Privacy

Value: Relevance

Value: Safety

Value: Scientific Progress

Value: Security

Value: Speed

Value: Sustainability

Value: Trust

Value: Truthfulness


AI Agents at HF

Recommendations &amp; What Comes Next The sudden, rapid advancement of LLM capabilities ‚Äì such as writing fluent sentences and achieving increasingly high scores on benchmarks ‚Äì has led AI developers and businesses alike to look towards what comes next: What game-changing technology is just on the horizon? One technology very recently taking off is ‚ÄúAI agents‚Äù, systems that can take actions in the digital world aligned with a deployer‚Äôs goals. Most of today‚Äôs AI agents are built by incorporating large language models (LLMs) into larger systems that can perform multiple functions. A fundamental idea underlying this new wave of technology is that computer programs no longer need to function as human-controlled tools, confined to specialized tasks: They can now combine multiple tasks without human input.</summary><published>2025-01-13</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/vdr-2b-multilingual</id><title>Visual Document Retrieval Goes Multilingual</title><author>Marco Cimolai, Logan Markewich</author><summary>Usage

Training Dataset
Data Gathering

Synthetic Generation

Filtering and Hard-Negative Mining

Download


Evaluations
Faster Inference

Cross-Lingual Retrieval

MRL and Binary Embeddings


Conclusions and Next Steps

Links

TL;DR: We present vdr-2b-multi-v1, the best multilingual embedding model for visual document retrieval. We also release its English-only twin vdr-2b-v1 and open-source the new vdr-multilingual-train dataset. With 500k high-quality samples, it's the largest open-source multilingual synthetic dataset for visual document retrieval. Introducing vdr-2b-multi-v1 (ü§ó), a multilingual embedding model designed for visual document retrieval across multiple languages and domains. This model is designed to encode document page screenshots into dense single-vector representations, this will effectively allow to search and query visually rich multilingual documents without the need for any OCR, data extraction pipelines, chunking...</summary><published>2025-01-10</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/leaderboard-emissions-analysis</id><title>CO‚ÇÇ Emissions and Models Performance: Insights from the Open LLM Leaderboard</title><author>Alina Lozovskaya, Nathan Habib, Albert Villanova del Moral, Cl√©mentine Fourrier</author><summary>Computing CO‚ÇÇ cost

General Trends

"Official Providers" Models

Community Releases

Detailed Insights
High-Parameter Language Models

Compact Language Models

Analyzing Emission Patterns in Qwen2 Models

Analyzing Emission Patterns in Llama Models


Conclusion

Open Questions

Since June 2024, we have evaluated more than 3,000 models on the Open LLM Leaderboard, a worldwide ranking of open language models performance. Even though we‚Äôre trying to run evaluations without wasting resources (we use the spare cycles of our cluster, in other words the GPUs which are active but waiting between jobs), this still represents quite a big amount of energy spent for model inference! In the last year, people have become more and more aware that using large language models (LLMs) to generate text has a significant environmental impact, beyond the already important impact of training. Recent research (see the Towards Greener LLMs article) highlights the challenges of managing resources efficiently at inference due to dynamic and diverse workloads.</summary><published>2025-01-09</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/smolagents</id><title>Introducing smolagents: simple agents that write actions in code.</title><author>Aymeric Roucher, Merve Noyan, Thomas Wolf</author><summary>Table of Contents

ü§î¬†What are agents?

‚úÖ¬†When to use agents / ‚õî¬†when to avoid them

Code agents

Introducing smolagents: making agents simple ü•≥
Building an agent

How strong are open models for agentic workflows?


Next steps üöÄ

Today we are launching smolagents, a very simple library that unlocks agentic capabilities for language models. Here‚Äôs a glimpse: Any efficient system using AI will need to provide LLMs some kind of access to the real world: for instance the possibility to call a search tool to get external information, or to act on certain programs in order to solve a task. In other words, LLMs should have agency. Agentic programs are the gateway to the outside world for LLMs.</summary><published>2024-12-31</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/train_memory</id><title>Visualize and understand GPU memory in PyTorch</title><author>Quentin Gallou√©dec</author><summary>üîé The PyTorch visualizer

üìä Visualizing Memory During Training

üìê Estimating Memory Requirements
Model parameters

Optimizer State

Activations

Gradients

Optimizer Intermediates

Total Memory


üöÄ Next steps

ü§ù Acknowledgements

You must be familiar with this message ü§¨: While it's easy to see that GPU memory is full, understanding why and how to fix it can be more challenging. In this tutorial, we'll go step by step on how to visualize and understand GPU memory usage in PyTorch during training. We‚Äôll also see how to estimate memory requirements and optimize GPU memory usage.</summary><published>2024-12-24</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/logits-processor-zoo</id><title>Controlling Language Model Generation with NVIDIA's LogitsProcessorZoo</title><author>Aritra Roy Gosthipaty, Ahmet Erdem</author><summary>What Are Logits in Language Models?

Why Process Logits?

NVIDIA's LogitsProcessorZoo
1. GenLengthLogitsProcessor

2. CiteFromPromptLogitsProcessor

3. ForceLastPhraseLogitsProcessor

4. MultipleChoiceLogitsProcessor


Wrapping Up

Generating text with language models often involves selecting the next token based on a distribution of probabilities.
A straightforward approach like greedy search selects the most probable token, but this can result in generic or repetitive outputs.
To add diversity and control, more advanced decoding strategies, such as beam search, nucleus sampling, and top-k sampling, are widely used.
These strategies, supported by the ü§ó Transformers library,
give us flexibility in shaping the model's outputs. But what if we wanted to go a step further and control the text generation process itself by directly modifying the probability distribution?
That‚Äôs where logit processing comes into play. Hugging Face's LogitsProcessor API
lets you customize the prediction scores of the language model head, providing granular control over model behavior.
The ü§ó Transformers library not only offers a rich set of built-in logits processors but also empowers the community
to create and share custom processors tailored to unique use cases.</summary><published>2024-12-23</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/big-bench-audio-release</id><title>Evaluating Audio Reasoning with Big Bench Audio</title><author>Micah Hill-Smith, George Cameron</author><summary>The Big Bench Audio Dataset

Evaluating Audio Reasoning
Evaluation Methodology


Examples
Speech to Speech

Speech to Speech Pipeline

Speech to Text

Text to Speech

Text to Text


Results
The Audio Reasoning Gap

Speech to Speech Pipelines Currently Outperform Native Audio for Reasoning


How to Contribute or Get in Touch

The emergence of native Speech to Speech models offers exciting opportunities to increase voice agent capabilities and simplify speech-enabled workflows. However, it's crucial to evaluate whether this simplification comes at the cost of model performance or introduces other trade-offs. To support analysis of this, Artificial Analysis is releasing Big Bench Audio, a new evaluation dataset for assessing the reasoning capabilities of audio language models. This dataset adapts questions from Big Bench Hard - chosen for its rigorous testing of advanced reasoning - into the audio domain.</summary><published>2024-12-20</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/modernbert</id><title>Finally, a Replacement for BERT: Introducing ModernBERT</title><author>Benjamin Warner, Antoine Chaffin, Benjamin Clavi√©, Orion Weller, Oskar Hallstr√∂m, Said Taghadouini, Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom Aarsen, Nathan Cooper, Griffin Adams, Jeremy Howard, Jonathan Whitaker, Iacopo Poli</author><summary>This blog post introduces ModernBERT, a family of state-of-the-art encoder-only models representing improvements over older generation encoders across the board, with a 8192 sequence length, better downstream performance and much faster processing. ModernBERT is available as a slot-in replacement for any BERT-like models, with both a base (149M params) and large (395M params) model size.</summary><published>2024-12-19</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/bamba</id><title>Bamba: Inference-Efficient Hybrid Mamba2 Model</title><author>LINSONG CHU, Divya Kumari, Tri Dao, Albert Gu, Raghu Ganti, Dakshi Agrawal, Mudhakar Srivatsa, Davis Wertheimer, Yu Chin Fabian Lim, Antoni Viros, Nelson Gonzalez, Tuan HoangTrong, Ofir Arviv, Yotam Perlitz, michal shmueli, Haochen Shen, Zhang, Gabe Goodhart, Naigang Wang, Nick Hill, Joshua Rosenkranz, Chi-Chun (Charlie) Liu, Adnan Hoque, Chih-Chieh Yang, Sukriti Sharma, Anh Uong, Jay Gala, Syed Zawad, Ryan Gordon</author><summary>We introduce Bamba-9B, an inference-efficient Hybrid Mamba2 model trained by IBM, Princeton, CMU, and UIUC on completely open data. At inference time, the model demonstrates 2.5x throughput improvement and 2x latency speedup compared to standard transformers in vLLM. To foster community experimentation, the model is immediately available to use in transformers, vLLM, TRL, and llama.cpp. We also release tuning, training, and extended pretraining recipes with a stateful data loader, and invite the community to further improve this model. Let's overcome the KV-cache bottleneck together! Transformer models are increasingly used in real-world applications, but they face memory-bandwidth bottlenecks during inference, particularly during per-token decoding in longer context-length models. Techniques like lower precision, layer pruning, and compression can alleviate the problem, but do not address the root cause, which is the increasing amount of memory required by the KV-cache as the context length increases. Emerging architectures such as Mamba, Griffin, and DeltaNet eliminate this bottleneck by making the KV-cache size constant. The Mamba architecture has gained significant traction in the community in the recent past. For example, Jamba and Samba interleave Mamba layers with transformer layers and explore the resulting hybrid Mamba models. Codestral Mamba, a pure Mamba2 model, demonstrates state-of-the-art (SOTA) results on coding tasks, while NVIDIA's hybrid Mamba2 model achieves competitive performance across long-context and traditional LLM benchmarks. Recent innovations, like Falcon Mamba and Falcon 3 Mamba achieve SOTA rankings on Hugging Face leaderboards at the time of their releases.</summary><published>2024-12-18</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/falcon3</id><title>Welcome the Falcon 3 Family of Open Models!</title><author>Falcon LLM TII UAE</author><summary>Key Highlights

Enhanced Capabilities

Models' Specs and Benchmark Results
Instruct models


Open Source Commitment

Useful links

Acknowledgments

Citation

We introduce Falcon3, a family of decoder-only large language models under 10 billion parameters, developed by
Technology Innovation Institute (TII) in Abu Dhabi. By pushing the
boundaries of performance and training efficiency, this release reflects our ongoing commitment to advancing open
and accessible large foundation models. Falcon3 represents a natural evolution from previous releases, emphasizing expanding the models' science, math, and code capabilities.</summary><published>2024-12-17</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/intel-gcp-c4</id><title>Benchmarking Language Model Performance on 5th Gen Xeon at GCP</title><author>Matrix Yao, Ke Ding, Ilyas Moutawwakil</author><summary>Introduction

Create instance
N2


C4

Set up environment

Benchmark
text embedding

text generation


Results and Conclusion
Text Embedding Results

Text Generation Results

Conclusion


TL;DR: We benchmark 2 representative agentic AI workload components, text embedding and text generation, on two Google Cloud Compute Engine Xeon-based CPU instances, namely N2 and C4. The results consistently shows that C4 has 10x to 24x higher throughput over N2 in text embedding and 2.3x to 3.6x higher throughput over N2 in text generation. Taking price into consideration, C4's hourly price is about 1.3x of N2, in this sense, C4 keeps 7x ~ 19x TCO(Total Cost of Ownership) advantage over N2 in text embedding and 1.7x ~ 2.9x TCO advantage in text generation. The results indicate that it is possible to deploy light-weight Agentic AI solutions wholly on CPUs. People believe the next frontier of artificial intelligence lies in agentic AI. The new paradigm uses the perceive - reason - action pipeline to combine LLM's sophisticated reasoning and iterative planning capabilities with a strong context understanding enhancement. The context understanding capability is provided by tools like vector databases and sensor input, to ceate more context-aware AI systems which can autonomously solve complex, multi-step problems. Moreover, the function calling capability of LLMs make it possible for the AI agent to directly take the action, going far beyond the chatting a chatbot offers. Agentic AI offers exciting prospects to enhance productivity and operations across industries.</summary><published>2024-12-17</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/synthetic-data-generator</id><title>Introducing the Synthetic Data Generator - Build Datasets with Natural Language</title><author>David Berenstein, Sara Han D√≠az, Leire Aguirre, Daniel Vila, Ame Vi, ben burtenshaw</author><summary>From Prompt to dataset to model
Supported Tasks

Let‚Äôs generate our first dataset

Reviewing the Dataset

Training a Model


Advanced Features
Improving Speed and Accuracy

Local Deployment

Customising Pipelines


What‚Äôs Next?

Introducing the Synthetic Data Generator, a user-friendly application that takes a no-code approach to creating custom datasets with Large Language Models (LLMs). The best part: A simple step-by-step process, making dataset creation a non-technical breeze, allowing anyone to create datasets and models in minutes and without any code. Synthetic data is artificially generated information that mimics real-world data. It allows overcoming data limitations by expanding or enhancing datasets.</summary><published>2024-12-16</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/lematerial</id><title>LeMaterial: an open source initiative to accelerate materials discovery and research</title><author>Alexandre Duval, Lucile Ritchie, Martin Siron, Inel DJAFAR, Etienne du Fayet, Amandine Rossello, Ali Ramlaoui, JB D., Leandro von Werra, Thomas Wolf</author><summary>Why LeMaterial?

Achieving a clean, unified and standardized dataset

Integrating a well-benchmarked materials fingerprint

LeMaterial in Action: applications and impact

Take-aways

Citations Today, we are thrilled to announce the launch of LeMaterial, an open-source collaborative project led by Entalpic and Hugging Face. LeMaterial aims to simplify and accelerate materials research, making it easier to train ML models, identify novel materials and explore chemical spaces. ‚öõÔ∏èü§ó</summary><published>2024-12-10</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/bedrock-marketplace</id><title>Hugging Face models in Amazon Bedrock</title><author>Simon Pagezy, Philipp Schmid, Jeff Boudier, Violette</author><summary>Deploy Google Gemma 2 27B Instruct

Use the model with Amazon Bedrock APIs

Clean up We are excited to announce that popular open models from Hugging Face are now available on Amazon Bedrock in the new Bedrock Marketplace! AWS customers can now deploy 83 open models with Bedrock Marketplace to build their Generative AI applications.</summary><published>2024-12-09</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/image-preferences</id><title>Open Preference Dataset for Text-to-Image Generation by the ü§ó Community</title><author>David Berenstein, ben burtenshaw, Daniel Vila, Daniel van Strien, Sayak Paul, Ame Vi, Linoy Tsaban</author><summary>The input dataset
Input prompts

Reducing toxicity

Synthetic prompt enhancement

Image generation


The results
Annotator alignment

Model performance

Model-finetune


The community

What is next?

The Data is Better Together community releases yet another important dataset for open source development. Due to the lack of open preference datasets for text-to-image generation, we set out to release an Apache 2.0 licensed dataset for text-to-image generation. This dataset is focused on text-to-image preference pairs across common image generation categories, while mixing different model families and varying prompt complexities. TL;DR? All results can be found in this collection on the Hugging Face Hub and code for pre- and post-processing can be found in this GitHub repository. Most importantly, there is a ready-to-go preference dataset and a flux-dev-lora-finetune. If you want to show your support already, don‚Äôt forget to like, subscribe and follow us before you continue reading further.</summary><published>2024-12-09</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/paligemma2</id><title>Welcome PaliGemma 2 ‚Äì New vision language models by Google</title><author>Merve Noyan, Andreas P. Steiner, Pedro Cuenca, Aritra Roy Gosthipaty</author><summary>Table of Content

Introducing PaliGemma 2

Model Capabilities

Demo

How to Use with Transformers

Fine-tuning

Conclusion

Resources

We are excited to welcome Google's all-new vision language models, PaliGemma 2, a new iteration of PaliGemma. Like its predecessor, PaliGemma 2 uses the same powerful SigLIP for vision, but it upgrades to the latest Gemma 2 for the text decoder part. PaliGemma 2 comes with new pre-trained (pt) models, in sizes of 3B, 10B, and 28B parameters. All of them support various input resolutions: 224x224, 448x448, and 896x896. These combinations provide a lot of flexibility for different use cases, so practitioners can choose the balance they need in the quality / efficiency space. In contrast, the previous PaliGemma was only available in the 3B variant.</summary><published>2024-12-05</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/keras-chatbot-arena</id><title>How good are LLMs at fixing their mistakes? A chatbot arena experiment with Keras and TPUs</title><author>Martin G√∂rner</author><summary>üëâ You can play with the Keras chatbot arenawhile you read. Click here to open it in a new tab. üëà Table of contents
¬†¬†¬†1. Introduction
¬†¬†¬†2. The experiment
¬†¬†¬†3. Keras chatbot arena tech: Spaces, Gradio, TPUs, JAX and Keras
¬†¬†¬†¬†¬†¬†3.1 Why TPUs?
¬†¬†¬†¬†¬†¬†3.2 Why JAX and Keras?
¬†¬†¬†¬†¬†¬†3.3 Sharding Models?
¬†¬†¬†¬†¬†¬†3.4 Which models?
¬†¬†¬†4. Results
¬†¬†¬†¬†¬†¬†4.1 Reliability
¬†¬†¬†¬†¬†¬†4.2 The complete chat - fixing mistakes
¬†¬†¬†¬†¬†¬†4.3 More mistake fixing
¬†¬†¬†5. Recap</summary><published>2024-12-05</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/leaderboard-3c3h-aragen</id><title>Rethinking LLM Evaluation with 3C3H: AraGen Benchmark and Leaderboard</title><author>Ali El Filali, Neha Sengupta, Abouelseoud, Preslav Nakov, Cl√©mentine Fourrier</author><summary>Summary

AraGen: A Generative Benchmark and Leaderboard for Arabic LLMs

The AraGen Leaderboard
Evaluation Pipeline

3C3H: Our new evaluation measure for LLMs

Dynamic Leaderboard for Robustness

Dataset Design


Judge Evaluation and Selection
Agreement with Human as a Judge

Score Consistency Analysis


Average Standard Deviation for Judge gpt-4o-mini: 0.043604 
 

Average Standard Deviation for Judge gpt-4o: 0.02870 
 

Average Standard Deviation for Judge claude-3.5-sonnet: 0.00629 
 

Average Standard Deviation for Judge llama3.1-405b: 0.00915 
 

Average Standard Deviation for Judge Jury: 0.00489 
 
Self Bias Analysis

Hallucination Analysis

Jury: Limitations and Insights

Judge Selection


Conclusion

In the rapidly evolving landscape of large language models (LLMs), comprehensive and robust evaluation methodologies remain a critical challenge, particularly for low-resource languages. In this blog, we introduce AraGen, a generative tasks benchmark and leaderboard for Arabic LLMs, based on 3C3H, a new evaluation measure for NLG which we hope will inspire work for other languages as well. The AraGen leaderboard makes three key contributions:</summary><published>2024-12-04</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/cfm-case-study</id><title>Investing in Performance: Fine-tune small models with LLM insights  - a CFM case study</title><author>Oussama Ahouzi, Florent Gbelidji, champonnois, J√©r√©my L'Hour, Pirashanth Ratnamogan, B√©reng√®re Patault, Morgane Goibert</author><summary>Table of Content

NER on the Financial News and Stock Price Integration Dataset

LLM-Assisted data labeling with Llama
Deploy Llama3.1-70b-Instruct with Hugging Face Inference Endpoints

Prompting Llama for NER

Get predictions from the endpoint

Review predictions with Argilla


Performance of zero-shot approaches for financial NER

Improving the performance of compact models with fine-tuning on LLM-assisted labeled dataset

Weak Supervision vs. LLM-assisted labeling 

Conclusion

Overview: This article presents a deep dive into Capital Fund Management‚Äôs (CFM) use of open-source large language models (LLMs) and the Hugging Face (HF) ecosystem to optimize Named Entity Recognition (NER) for financial data. By leveraging LLM-assisted labeling with HF Inference Endpoints and refining data with Argilla, the team improved accuracy by up to 6.4% and reduced operational costs, achieving solutions up to 80x cheaper than large LLMs alone. This structured approach combines accuracy and cost-effectiveness, making it ideal for real-world financial applications.</summary><published>2024-12-03</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/rearchitecting-uploads-and-downloads</id><title>Rearchitecting Hugging Face Uploads and Downloads</title><author>Banerjee, Jared Sulzdorf, Ann Huang</author><summary>A Custom Protocol for Uploads and Downloads

Designing for Global Access

Validating and Vetting

Beyond the Bytes

As part of Hugging Face's Xet team‚Äôs work to improve Hugging Face Hub‚Äôs storage backend, we analyzed a 24 hour window of Hugging Face upload requests to better understand access patterns. On October 11th, 2024, we saw: The map below visualizes this activity, with countries colored by bytes uploaded per hour.</summary><published>2024-11-26</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/smolvlm</id><title>SmolVLM - small yet mighty Vision Language Model</title><author>Andres Marafioti, Merve Noyan, Miquel Farr√©, Elie Bakouch, Pedro Cuenca</author><summary>This blog post introduces SmolVLM, a 2B VLM, SOTA for its memory footprint. SmolVLM is small, fast, memory-efficient, and fully open-source. All model checkpoints, VLM datasets, training recipes and tools are released under the Apache 2.0 license. This year has seen a boom in multimodal AI with many large vision language models released. The trends were to initially scale up compute, later scale up the data diversity by generating synthetic data with large models, and, recently, scale down to make these models more efficient. Small open models allow local deployment to browser or edge devices, cut inference costs, and enable user customization. Some notable examples of these models include PaliGemma 3B, moondream2, and Qwen2VL.</summary><published>2024-11-26</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/designing-positional-encoding</id><title>You could have designed state of the art positional encoding</title><author>Christopher Fleetwood</author><summary>Problem Statement

Motivating Example

Desirable Properties

Integer Position Encoding

Binary Position Encoding

Sinusoidal positional encoding

Absolute vs Relative Position Encoding

Positional encoding in context

Rotary Postional Encoding

Extending RoPE to nnn-Dimensions

The future of positional encoding

Conclusion

References

Gall's Law A complex system that works is invariably found to have evolved from a simple
system that worked John Gall This post walks you through the step-by-step discovery of state-of-the-art positional encoding in transformer models. We will achieve
this by iteratively improving our approach to encoding position, arriving at Rotary Postional Encoding (RoPE) used in the latest LLama 3.2 release and most modern transformers. This post intends to limit the mathematical knowledge required to follow along, but some basic linear algebra, trigonometry and understanding of self attention is expected.</summary><published>2024-11-25</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/debate</id><title>Letting Large Models Debate: The First Multilingual LLM Debate Competition</title><author>Richeng Xuan, jingshu, Xi Yang, Yonghua Lin, Zheqi He, lixuejing, Gray, daiteng01, Sherlock, liu, Tiezhen WANG, Cl√©mentine Fourrier</author><summary>Background: The Need to Redefine LLM Evaluation Protocols

Key Features and Innovations of FlagEval Debate
Multilingual Support: Enabling Comprehensive Global Evaluation

Developer Customization: Flexible Model Configuration and Optimization

Dual Evaluation Metrics: Expert Reviews and User Feedback


Experimental Results: Assessing the Impact of Multi-Model Debates

How to add your model into this competition?

Conclusion

About BAAI &amp; FlagEval

References

Current static evaluations and user-driven arenas have exhibited their limitations and biases in the previous year. Here, we explore a novel way to evaluate LLMs: debate.
Debate is an excellent way to showcase reasoning strength and language abilities, used all across history, from the debates in the Athenian Ecclesia in the 5th century BCE to today's World Universities Debating Championship.
Do today's large language models exhibit debate skills similar to humans? Which model is currently the best at debating? What can we learn from models when they debate against one another?
To answer this question, BAAI has created a "Debate Arena", allowing large models to compete against each other. Currently, it supports debate competitions in English, Chinese, Arabic and Korean. The advancement of multimodal and multilingual technologies has exposed the limitations of traditional static evaluation protocols in capturing LLMs‚Äô performance in complex interactive scenarios. Inspired by OpenAI‚Äôs ‚ÄúAI Safety via Debate‚Äù framework‚Äîwhich emphasizes enhancing models‚Äô reasoning and logic through multi-model interactions ([1])‚ÄîBAAI‚Äôs FlagEval Debate platform introduces a dynamic evaluation methodology to address these limitations.
Recent research has demonstrated the potential of multi-agent debates in improving models‚Äô reasoning capabilities and factual accuracy. For example, studies have shown that multi-agent interactions can significantly enhance models‚Äô consistency and accuracy in logical reasoning and factual judgments ([2]), while others have indicated that multi-model debates encourage models to generate more truthful and coherent responses ([3]).
While existing platforms like LMSYS Chatbot Arena offer foundational settings for multi-model interactions, they present certain limitations in practical evaluation:</summary><published>2024-11-20</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/from-files-to-chunks</id><title>From Files to Chunks: Improving Hugging Face Storage Efficiency</title><author>Jared Sulzdorf, Ann Huang</author><summary>Content-Defined Chunking Foundations

Insertions and Deletions

What CDC means for the Hub

Hugging Face stores over 30 PB of models, datasets, and spaces in Git LFS repositories. Because Git stores and versions at the file level, any change to a file requires re-uploading the full asset ‚Äì expensive operations when average Parquet and CSV files on the Hub range between 200-300 MB, average Safetensor files around 1 GB, and GGUF files can exceed 8 GB. Imagine modifying just a single line of metadata in a GGUF file and waiting for the multi-gigabyte file to upload; in addition to user time and transfer costs, Git LFS also then needs to save full versions of both files, bloating storage costs. The plot below illustrates the growth of LFS storage in model, dataset, and space repositories on the Hub between March 2022 and September 2024:</summary><published>2024-11-20</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/layerskip</id><title>Faster Text Generation with Self-Speculative Decoding</title><author>Aritra Roy Gosthipaty, Mostafa Elhoushi, Pedro Cuenca, Vaibhav Srivastav</author><summary>Speculative Decoding and Self-Speculative Decoding

Usage with transformers
Benchmarking


Early Exit and Unembedding

Training Modifications: Layer Dropout and Early Exit Loss

Self-Drafting and Self-Verification

Optimizations: Shared Weights, Shared KV Cache, and Shared Compute

How Early Can We Exit?

Conclusion

Self-speculative decoding, proposed in
LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding
is a novel approach to text generation. It combines the strengths of speculative decoding with early
exiting from a large language model (LLM). This method allows for efficient generation
by using the same model's early layers for drafting tokens, and later layers for verification. This technique not only speeds up text generation, but it also achieves significant
memory savings and reduces computational latency. In order to obtain an end-to-end speedup, the
output of the earlier layers need to be close enough to the last layer. This is achieved by a
training recipe which, as described in the paper, can be applied during pretraining,
and also while fine-tuning on a specific domain. Self-speculative decoding is
especially efficient for real-world applications, enabling deployment on smaller GPUs and lowering
the overall hardware footprint needed for large-scale inference.</summary><published>2024-11-20</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/leaderboard-japanese</id><title>Introduction to the Open Leaderboard for Japanese LLMs</title><author>Akim Mousterou, Yusuke Miyao, Namgi Han, Takumi Okamoto, Shigeki Ishida, hysts, Cl√©mentine Fourrier</author><summary>Introduction to the Leaderboard Tasks
Jamp

JEMHopQA

jcommonsenseqa

chABSA

mbpp-ja

mawps

JMMLU

XL-Sum


Technical Setup

Observations

Future directions

Acknowledgements

LLMs are now increasingly capable in English, but it's quite hard to know how well they perform in other national languages, widely spoken but which present their own set of linguistic challenges. Today, we are excited to fill this gap for Japanese! We'd like to announce the Open Japanese LLM Leaderboard, composed of more than 20 datasets from classical to modern NLP tasks to understand underlying mechanisms of Japanese LLMs. The Open Japanese LLM Leaderboard was built by the LLM-jp, a cross-organizational project for the research and development of Japanese large language models (LLMs) in partnership with Hugging Face.</summary><published>2024-11-20</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/arena-atla</id><title>Judge Arena: Benchmarking LLMs as Evaluators</title><author>kyle, Maurice, Roman Engeler, Max Bartolo, Cl√©mentine Fourrier, Toby Drane, Mathias Leys, Jake Golden</author><summary>Judge Arena
How it works


Selected Models

The Leaderboard

Early Insights

How to contribute

Credits

LLM-as-a-Judge has emerged as a popular way to grade natural language outputs from LLM applications, but how do we know which models make the best judges? We‚Äôre excited to launch Judge Arena - a platform that lets anyone easily compare models as judges side-by-side. Just run the judges on a test sample and vote which judge you agree with most. The results will be organized into a leaderboard that displays the best judges.</summary><published>2024-11-19</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/eu-ai-act-for-oss-developers</id><title>Open Source Developers Guide to the EU AI Act</title><author>Bruna Trevelin, Lucie-Aim√©e Kaffee, Yacine Jernite</author><summary>ü§ó Scope

ü§ó How to prepare for compliance
For limited risk AI systems

For open source non-systemic risk GPAI models


ü§ó Get involved

The EU AI Act, the world‚Äôs first comprehensive legislation on artificial intelligence, has officially come into force, and it‚Äôs set to impact the way we develop and use AI ‚Äì including in the open source community. If you‚Äôre an open source developer navigating this new landscape, you‚Äôre probably wondering what this means for your projects. This guide breaks down key points of the regulation with a focus on open source development, offering a clear introduction to this legislation and directing you to tools that may help you prepare to comply with it. Disclaimer: The information provided in this guide is for informational purposes only, and should not be considered as any form of legal advice.</summary><published>2024-12-02</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/researcher-dataset-sharing</id><title>Share your open ML datasets on Hugging Face Hub!</title><author>Daniel van Strien, Caleb Fahlgren, Quentin Lhoest, Ann Huang</author><summary>Generous Limits
Support for large datasets


Dataset Viewer
Full Text Search

Sorting


Third Party Library Support

SQL Console

Security
Access Controls

Built-in Security Scanning


Reach and Visibility
Better Community Engagement

Wider Reach

Improved Documentation


How can I host my dataset on the Hugging Face Hub?

If you're working on data-intensive research or machine learning projects, you need a reliable way to share and host your datasets. Public datasets such as Common Crawl, ImageNet, Common Voice and more are critical to the open ML ecosystem, yet they can be challenging to host and share. Hugging Face Hub makes it seamless to host and share datasets, trusted by many leading research institutions, companies, and government agencies, including Nvidia, Google, Stanford, NASA, THUDM and Barcelona Supercomputing Center.</summary><published>2024-11-12</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/pycharm-integration</id><title>Hugging Face + PyCharm</title><author>Matthew Carrigan</author><summary>The Hugging Face Is Inside Your House

Instant Model Cards

The Local Model Cache

Python in the age of AI

It‚Äôs a Tuesday morning. As a Transformers maintainer, I‚Äôm doing the same thing I do most weekday mornings: Opening PyCharm, loading up the Transformers codebase and gazing lovingly at the chat template documentation while ignoring the 50 user issues I was pinged on that day. But this time, something feels different: Something is‚Ä¶ wait! Computer! Enhance!
Is that..?Those user issues are definitely not getting responses today. Let‚Äôs talk about the Hugging Face integration in PyCharm.</summary><published>2024-11-05</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/argilla-ui-hub</id><title>Argilla 2.4: Easily Build Fine-Tuning and Evaluation datasets on the Hub ‚Äî No Code Required</title><author>Natalia Elvira, ben burtenshaw, Daniel Vila</author><summary>Use cases

How it works

We are incredibly excited to share the most impactful feature since Argilla joined Hugging Face: you can prepare your AI datasets without any code, getting started from any Hub dataset! Using Argilla‚Äôs UI, you can easily import a dataset from the Hugging Face Hub, define questions, and start collecting human feedback. Not familiar with Argilla? Argilla is a free, open-source data-centric tool. Using Argilla, AI developers and domain experts can collaborate and build high-quality datasets. Argilla is part of the Hugging Face family and fully integrated with the Hub. Want to know more? Here‚Äôs an intro blog post.</summary><published>2024-11-04</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/universal_assisted_generation</id><title>Universal Assisted Generation: Faster Decoding with Any Assistant Model</title><author>Daniel Korat, Oren Pereg, Moshe Berchansky, Jonathan Mamou, Joao Gante, Lewis Tunstall, Nadav Timor, Moshe Wasserblat</author><summary>Introduction

Assisted Generation

Universal Assisted Generation

Benchmarks

Code

Future Directions

References

TL;DR: Many LLMs such as gemma-2-9b and Mixtral-8x22B-Instruct-v0.1 lack a much smaller version to use for assisted generation. In this blog post, we present Universal Assisted Generation: a method developed by Intel Labs and Hugging Face which extends assisted generation to work with a small language model from any model family ü§Ø. As a result, it is now possible to accelerate inference from any decoder or Mixture of Experts model by 1.5x-2.0x with almost zero overhead üî•üî•üî•. Let's dive in! Nowadays, the strongest open weight LLMs typically have billions to hundreds of billions parameters (hello Llama-3.1-405B üëã), and deploying these beasts in production environments poses a range of engineering challenges. One such challenge is that generating text from these large models is slow, which has prompted the community to develop a wide range of techniques to accelerate the decoding process. Assisted generation, also known as speculative decoding, is a very popular and practical approach for accelerating LLM inference without accuracy loss. In this blog post, we take a look at how assisted generation works and share our   research to extend it towards any of the 140,000 language models on the Hugging Face Hub üöÄ!</summary><published>2024-10-29</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/digital-green-llm-judge</id><title>Expert Support case study: Bolstering a RAG app with LLM-as-a-Judge</title><author>Vineet Singh, Rajsekar Manokaran, Aymeric Roucher</author><summary>System architecture

The Power of LLMs-as-Judges

Results: benchmarking LLMs for RAG

Conclusion

This is a guest blog post authored by Digital Green. Digital green is participating in a CGIAR-led collaboration to bring agricultural support to smallholder farmers. There are an estimated 500 million smallholder farmers globally: they play a critical role in global food security. Timely access to accurate information is essential for these farmers to make informed decisions and improve their yields.</summary><published>2024-10-28</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/protectai</id><title>Hugging Face Teams Up with Protect AI: Enhancing Model Security for the Community</title><author>Luc Georges, Sean Morgan</author><summary>Model security refresher

Integration

We are pleased to announce our partnership with Protect AI, as part of our long-standing commitment to provide a safe and reliable platform for the ML community. Protect AI is a company founded with a mission to create a safer AI-powered world. They are developing powerful tools, namely Guardian, to ensure that the rapid pace of AI innovation can continue without compromising security.</summary><published>2024-10-22</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/aya-expanse</id><title>A Deepdive into Aya Expanse: Advancing the Frontier of Multilinguality</title><author>John Dang, Shivalika Singh, Daniel D'souza, Arash Ahmadian</author><summary>Avoiding Model Collapse in Synthetic Data

Iteratively Improving with Global Preferences

Maximizing Performance through Model Merging

Bringing it all Together

Acknowledgements

References

This is a guest blog post by the Cohere For AI team. Cohere For AI is Cohere's research lab that seeks to solve complex machine learning problems. With the release of the Aya Expanse family, featuring 8B and 32B parameter models, we are addressing one of the most urgent challenges in AI: the lack of highly performant multilingual models that can rival the capabilities of monolingual ones. While AI has made tremendous progress, there remains a stark gap in the performance of models across multiple languages. Aya Expanse is the result of several years of dedicated research at C4AI --- data arbitrage, multilingual preference training, safety tuning, and model merging.</summary><published>2024-10-24</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/synthid-text</id><title>Introducing SynthID Text</title><author>Sumedh Ghaisas, Sumanth Dathathri, Ryan Mullins, Joao Gante, Marc Sun, Raushan Turganbay</author><summary>How it works

Configuring a watermark

Applying a watermark

Detecting a watermark

Limitations

Acknowledgements

Do you find it difficult to tell if text was written by a human or generated by
AI? Being able to identify AI-generated content is essential to promoting trust
in information, and helping to address problems such as misattribution and
misinformation. Today, Google DeepMind and Hugging
Face are excited to launch
SynthID Text in Transformers
v4.46.0, releasing later today. This technology allows you to apply watermarks
to AI-generated text using a
logits processor
for generation tasks, and detect those watermarks with a
classifier. Check out the SynthID Text
paper in Nature for the
complete technical details of this algorithm, and Google‚Äôs
Responsible GenAI Toolkit
for more on how to apply SynthID Text in your products.</summary><published>2024-10-23</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/hugs</id><title>Introducing HUGS - Scale your AI with Open Models</title><author>Philipp Schmid, Jeff Boudier, Alvaro Bartolome, Simon Pagezy, Violette</author><summary>Zero-Configuration Optimized Inference for Open Models

Why HUGS?

How it Works
Where to find HUGS

Pricing

Running Inference


Supported Models and Hardware

Get Started with HUGS Today

Today, we are thrilled to announce the launch of Hugging Face Generative AI Services a.k.a. HUGS: optimized, zero-configuration inference microservices designed to simplify and accelerate the development of AI applications with open models. Built on open-source Hugging Face technologies such as Text Generation Inference and Transformers, HUGS provides the best solution to efficiently build and scale Generative AI Applications in your own infrastructure. HUGS is optimized to run open models on a variety of hardware accelerators, including NVIDIA GPUs, AMD GPUs, and soon AWS Inferentia and Google TPUs. HUGS simplifies the optimized deployment of open models in your own infrastructure and on a wide variety of hardware. One key challenge developers and organizations face is the engineering complexity of optimizing inference workloads for LLMs on a particular GPU or AI accelerator. With HUGS, we enable maximum throughput deployments for the most popular open LLMs with zero configuration required. Each deployment configuration offered by HUGS is fully tested and maintained to work out of the box.</summary><published>2024-10-23</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/cinepile2</id><title>CinePile 2.0 - making stronger datasets with adversarial refinement</title><author>Ruchit Rawal, Miquel Farr√©, Gowthami Somepalli, Leandro von Werra</author><summary>Wait. What is CinePile?
Taking a look at a data sample

Tell me more. How did you put together the original dataset?

Inspecting the quality of the first results


CinePile 2.0
Issues in CinePile 1.0

Adversarial Refinement

Evaluations

Leaderboard


In this blog post we share the journey of releasing CinePile 2.0, a significantly improved version of our long video QA dataset. The improvements in the new dataset rely on a new approach that we coined adversarial dataset refinement. We're excited to share both CinePile 2.0 and our adversarial refinement method implementation, which we believe can strengthen many existing datasets and directly be part of future dataset creation pipelines.</summary><published>2024-10-23</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/transformersjs-v3</id><title>Transformers.js v3: WebGPU support, new models &amp; tasks, and more‚Ä¶</title><author>Joshua</author><summary>Installation

WebGPU support
Usage in Transformers.js v3


New quantization formats (dtypes)
Basic usage

Per-module dtypes


120 supported architectures

Example projects and templates

Over 1200 pre-converted models

Node.js (ESM + CJS), Deno, and Bun compatibility

A new home on NPM and GitHub

After more than a year of development, we're excited to announce the release of ü§ó Transformers.js v3! You can get started by installing Transformers.js v3 from NPM using:</summary><published>2024-10-22</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/sd3-5</id><title>üß® Diffusers welcomes Stable Diffusion 3.5 Large</title><author>YiYi Xu, Aryan V S, Dhruv Nair, Sayak Paul, Linoy Tsaban, Apolin√°rio from multimodal AI art, Alvaro Somoza, Aritra Roy Gosthipaty</author><summary>Table Of Contents

Architectural changes

Using SD3.5 with Diffusers

Running inference with quantization

Training LoRAs with SD3.5 Large with quantization

Using single-file loading with the Stable Diffusion 3.5 Transformer
Important links


Stable Diffusion 3.5 is the improved variant of its predecessor, Stable Diffusion 3. 
As of today, the models are available on the Hugging Face Hub and can be used with üß®¬†Diffusers. The release comes with two checkpoints:</summary><published>2024-10-22</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/outlines-core</id><title>Releasing Outlines-core 0.1.0: structured generation in Rust and Python</title><author>bwillard, David Holtz, Erik Kaunism√§ki, Kaustubh Chaudhari, Remi Louf, Umut ≈ûahin, Will Kurt</author><summary>A quick primer on structured generation üßë‚Äçüéì
How it works

Why it‚Äôs important


Why rewrite in Rust? ü¶Ä
Speed

Safety and Reliability

Separation of concerns

Portability


Contribute

dottxt and Hugging Face are excited to announce that we have been collaborating on outlines-core, a Rust port of outlines‚Äôs core algorithms for structured generation. On top of getting reliable output from LLMs with outlines, this Rust port offers several further benefits to users of outlines: These improvements should not only improve the performance for existing outlines users, but also dramatically increase the ways users can incorporate structured generation into their LLM workflows. outlines-core is now public, integrated in outlines, and the version 0.1.0 of the Python bindings are out. You can find the repo here.</summary><published>2024-10-22</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/s2s_endpoint</id><title>Deploying Speech-to-Speech on Hugging Face</title><author>Andres Marafioti, Derek Thomas, Diego Maniloff, Eustache Le Bihan</author><summary>Speech-to-Speech (S2S) is an exciting new project from Hugging Face that combines several advanced models to create a seamless, almost magical experience: you speak, and the system responds with a synthesized voice. The project implements a cascaded pipeline leveraging models available through the Transformers library on the Hugging Face hub. The pipeline consists of the following components:</summary><published>2024-10-22</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/keras-llama-32</id><title>Llama 3.2 in Keras</title><author>Martin G√∂rner</author><summary>Keras is multi-backend

What is keras-hub?

LLMs in Keras come "batteries included"

Chatting with an LLM

Lower level access: Tokenizer, Backbone

Wait, Tokenizer, Preprocessor? I'm confused

Keras has a built-in trainer

You can upload to the Hub

Distributed model parallelism for inference or training

This is going to be the shortest blog post ever. Question: Llama 3.2 landed two weeks ago on Hugging Face / Transformers. When will it be available in Keras?</summary><published>2024-10-21</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/gradient_accumulation</id><title>Fixing Gradient Accumulation</title><author>Lysandre, Arthur Zucker, Zachary Mueller, Yih-Dar SHIEH, Benjamin Bossan, Pedro Cuenca</author><summary>Where does it stem from?

How we're fixing it

Our friends at Unsloth shared an issue regarding gradient accumulation yesterday that is affecting the transformers Trainer. The initial report comes from @bnjmn_marie (kudos to him!). Gradient accumulation is supposed to be mathematically equivalent to full batch training; however, losses did not match between training runs where the setting was toggled on and off.</summary><published>2024-10-16</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/huggingface-amd-turin</id><title>Introducing the AMD 5th Gen EPYC‚Ñ¢ CPU</title><author>Mohit Sharma, Morgan Funtowicz</author><summary>AMD Turin vs AMD Genoa Performance - A 2X speedup
Results for Llama 3.1 8B Instruct


Conclusion

Useful Resources

AMD has just unveiled its 5th generation of server-grade EPYC CPU based on Zen5 architecture - also known as Turin. It provides a significant boost in performance, especially with a higher number of core count reaching up to 192 and 384 threads. From Large Language Models (LLMs) to RAG scenarios, Hugging Face users can leverage this new generation of servers to enhance their performance capabilities:</summary><published>2024-10-10</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/gradio-5-security</id><title>A Security Review of Gradio 5</title><author>Abubakar Abid, Pete</author><summary>Why a security audit?

Major findings

Going forward

We audited Gradio 5 so that your machine learning apps are safe! In the past few years, Gradio (&gt;6 million monthly Pypi installs) has become the default way to build machine learning web applications in Python. In just a few lines of code, you can create a user interface for an image generation app, a chatbot, or any other kind of ML app and share it with others using Gradio‚Äôs built-in share links or Hugging Face Spaces.</summary><published>2024-10-10</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/gradio-5</id><title>Welcome, Gradio 5</title><author>Abubakar Abid</author><summary>Gradio 5: production-ready machine learning apps

Breaking changes

What‚Äôs next for Gradio?

Try Gradio 5 right now

We‚Äôve been hard at work over the past few months, and we are excited to now announce the stable release of Gradio 5. With Gradio 5, developers can build production-ready machine learning web applications that are performant, scalable, beautifully designed, accessible, and follow best web security practices, all in a few lines of Python.</summary><published>2024-10-09</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/dask-scaling</id><title>Scaling AI-based Data Processing with Hugging Face + Dask</title><author>Sarah Johnson, James Bourbeau, Quentin Lhoest, Daniel van Strien</author><summary>Processing 100 Rows with Pandas

Scaling to 211 Million Rows with Dask
Multi-GPU Parallel Model Inference


Conclusion

The Hugging Face platform has many datasets and pre-trained models that make using and training state-of-the-art machine learning models increasingly accessible. However, it can be hard to scale AI tasks because AI datasets are often large (100s GBs to TBs) and using Hugging Face transformers for model inference can sometimes be computationally expensive. Dask, a Python library for distributed computing, can handle out-of-core computing (processing data that doesn‚Äôt fit in memory) by breaking datasets into manageable chunks. This makes it easy to do things like:</summary><published>2024-10-09</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/dynamic_speculation_lookahead</id><title>Faster Assisted Generation with Dynamic Speculation</title><author>Jonathan Mamou, Oren Pereg, Joao Gante, Lewis Tunstall, Daniel Korat, Nadav Timor, Moshe Wasserblat</author><summary>Speculative Decoding

Dynamic Speculative Decoding

Benchmarking

Code

What‚Äôs next?

References

Citation

‚≠ê In this blog post, we‚Äôll explore dynamic speculative decoding ‚Äîa novel method developed by Intel labs and Hugging Face that accelerates text generation by up to 2.7x, depending on the task. This method is the default operational mode for assisted generation starting from Transformersü§ó release 4.45.0 ‚≠ê Speculative decoding is a popular technique to accelerate the inference of large language models, while preserving their accuracy. As shown in the figure below, speculative decoding works by splitting the generative process into two stages. In the first stage, a fast, but less accurate draft model (AKA assistant) autoregressively generates a sequence of tokens. In the second stage, a large, but more accurate target model conducts parallelized verification over the generated draft tokens. This process allows the target model to produce multiple tokens in a single forward pass and thus accelerate autoregressive decoding. The success of speculative decoding largely hinges on the speculation lookahead (SL), i.e. the number of tokens produced by the draft model in each iteration. In practice, the SL is either a static value or based on heuristics, neither of which is optimal for squeezing out maximium performance during inference.</summary><published>2024-10-08</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/improve_parquet_dedupe</id><title>Improving Parquet Dedupe on Hugging Face Hub</title><author>yuchenglow, Di Xiao</author><summary>Background

Append

Modification

Deletion

Content Defined Row Groups

The Xet team at Hugging Face is working on improving the efficiency of the Hub's
storage architecture to make it easier and quicker for users to
store and update data and models. As Hugging Face hosts nearly 11PB of datasets
with Parquet files alone accounting for over 2.2PB of that storage,
optimizing Parquet storage is of pretty high priority. Most Parquet files are bulk exports from various data analysis pipelines
or databases, often appearing as full snapshots rather than incremental
updates. Data deduplication becomes critical for efficiency when users want to
update their datasets on a regular basis. Only by deduplicating can we store
all versions as compactly as possible, without requiring everything to be uploaded
again on every update. In an ideal case, we should be able to store every version
of a growing dataset with only a little more space than the size of its largest version.</summary><published>2024-10-05</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/leaderboard-finbench</id><title>Introducing the Open FinLLM Leaderboard</title><author>Xie, Jimin Huang, Sophia Ananiadou, Xiao-Yang Liu Yanglet, Alejandro Lopez-Lira, Wang, ldruth, Ruoyu Xiang, chenzhengyu, Yangyang Yu, Colin Lin, Andy Zhu, Cl√©mentine Fourrier</author><summary>Key Features of the Open Financial LLM Leaderboard

Supported Tasks and Metric
Categories

Metrics

Individual Tasks


How to Use the Open Financial LLM Leaderboard
Selecting Tasks to Display

Selecting Models to Display

Viewing Results in the Task Table

Submitting a Model for Evaluation


Current Best Models and Surprising Results

Acknowledgments

Finding the best LLM models for finance use cases The growing complexity of financial language models (LLMs) necessitates evaluations that go beyond general NLP benchmarks. While traditional leaderboards focus on broader NLP tasks like translation or summarization, they often fall short in addressing the specific needs of the finance industry. Financial tasks, such as predicting stock movements, assessing credit risks, and extracting information from financial reports, present unique challenges that require models with specialized skills. This is why we decided to create the Open FinLLM Leaderboard.</summary><published>2024-10-04</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/chinese-ai-expansion</id><title>A Short Summary of Chinese AI Global Expansion</title><author>Adina Yakefu</author><summary>Who is Expanding Overseas?

Why Expand Overseas?

What are the key success factors ?
Promoting ESG Strategy


Conclusion

In the early 15th century, Zheng He (also known as Chong Ho), a Chinese mariner and explorer during the early Ming Dynasty, led seven major naval expeditions, known as the "Voyages to the Western Oceans". His journey traced a path that went through Southeast Asia, the Middle East and then reached out to Africa. It was a bold move by China to establish diplomatic and trade relations with foreign lands, while exploring overseas opportunities. The word ‚ÄúÂá∫Êµ∑‚Äù (Chu Hai, sailing abroad) has since held a special meaning about going global. 600 years later, China is once again making its mark internationally, evolving from a global manufacturing hub to a leader in ICT, electric vehicles, and AI technologies. By 2024, Chinese companies have accelerated their overseas expansion, particularly in AI. A June report from Feifan Research shows that out of 1,500 active AI companies worldwide, 751 are based in China, with 103 already expanding internationally. Many see this as a sign of China‚Äôs growing strength in tech innovation. Others argue that as domestic markets become saturated and competition intensifies, expanding overseas may have become the only viable option for these companies.</summary><published>2024-10-03</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/benczechmark</id><title>üá®üáø BenCzechMark - Can your LLM Understand Czech?</title><author>Martin Fajƒç√≠k, Hynek Kydlicek, Martin Doƒçekal, Jan Dole≈æal, Jakub ≈†tƒõtina, Alexander Polok, Zuzana Nevƒõ≈ôilov√°, Ales Horak, Michal ≈†tef√°nik, Adam Jirkovsk√Ω, Jan Hula, Jan Sedivy</author><summary>üìã¬†Tasks and Categories

‚öîÔ∏è¬†Model Duels and Average Score

üëë¬†BenCzechMark Leaderboard - Llama-405B Takes the Crown

üá®üáø¬†Think Your Model Can Excel in Czech? Submit It!

üåü Acknowledgements

üìö¬†Citation and references

The üá®üáø¬†BenCzechMark is the first and most comprehensive evaluation suite for assessing the abilities of Large Language Models (LLMs) in the Czech language. It aims to test how well LLMs can: To achieve this, we've sourced 50 tasks spanning 9 categories, with 90% of tasks having native, non-translated content.</summary><published>2024-10-01</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/vertex-colored-to-textured-mesh</id><title>Converting Vertex-Colored Meshes to Textured Meshes</title><author>Dylan Ebert</author><summary>Introduction

The Short Version
Usage


The Long Version

Limitations

Conclusion Convert vertex-colored meshes to UV-mapped, textured meshes.</summary><published>2024-09-30</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/llama32</id><title>Llama can now see and run on your device - welcome Llama 3.2</title><author>Merve Noyan, Philipp Schmid, Omar Sanseviero, Vaibhav Srivastav, Lewis Tunstall, Aritra Roy Gosthipaty, Pedro Cuenca</author><summary>Table of contents

What is Llama 3.2 Vision?

Llama 3.2 license changes. Sorry, EU :(

What is special about Llama 3.2 1B and 3B?

Demo

Using Hugging Face Transformers

Llama 3.2 1B &amp; 3B Language Models

Llama 3.2 Vision

On-device
Llama.cpp &amp; Llama-cpp-python

Transformers.js

MLC.ai Web-LLM


Fine-tuning Llama 3.2

Hugging Face Partner Integrations

Additional Resources

Acknowledgements

Llama 3.2 is out! Today, we welcome the next iteration of the Llama collection to Hugging Face. This time, we‚Äôre excited to collaborate with Meta on the release of multimodal and small models. Ten open-weight models (5 multimodal models and 5 text-only ones) are available on the Hub. Llama 3.2 Vision comes in two sizes: 11B for efficient deployment and development on consumer-size GPU, and 90B for large-scale applications. Both versions come in base and instruction-tuned variants. In addition to the four multimodal models, Meta released a new version of Llama Guard with vision support. Llama Guard 3 is a safeguard model that can classify model inputs and generations, including detecting harmful multimodal prompts or assistant responses.</summary><published>2024-09-25</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/fine-video</id><title>FineVideo: behind the scenes</title><author>Miquel Farr√©, Andres Marafioti, Lewis Tunstall, Leandro von Werra, Pedro Cuenca, Thomas Wolf</author><summary>Table of Contents

About this blog post

Building the Raw dataset
Filtering YouTube-Commons

Downloading the videos


Keeping dynamic content
Word density filtering

Visual dynamism filtering


Video Categorization
Custom built Taxonomy

Content annotation

Feedback loop taxonomy - content annotation


Contributing descriptive metadata
Long videos &amp; Gemini 1.5 Pro

Content selection

Annotating with Gemini 1.5 Pro and Structured Output with GPT4o


Fine Alignment and anomaly filtering

Future Work

Open video datasets are scarce and therefore slowing down the development of open-source video AI. For this reason we built FineVideo, a dataset with 43k videos that span 3.4k hours and are annotated with rich descriptions, narrative details, scene splits, and QA pairs. FineVideo contains a highly diverse collection of videos and metadata which makes it a good ingredient to train models to understand video content, train diffusion models to generate videos from a text description or train computer vision models using its structured data as input.</summary><published>2024-09-23</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/daily-papers</id><title>Exploring the Daily Papers Page on Hugging Face</title><author>Adina Yakefu</author><summary>üìë Claim your Papers

‚è´ Submit Papers

üí¨ Chat with Authors

üîó All You Need in One Page

üó≥ Show Your Support with Upvotes

üôã Recommend Similar Papers

üî† Multilingual Comments and Translation

‚úÖ Subscription

üí° Interactive Features with arXiv

In the fast-paced world of research, staying up-to-date with the latest advancements is crucial. To help developers and researchers keep a pulse on the cutting-edge of AI, Hugging Face introduced the Daily Papers page. Since its launch, Daily Papers has featured high-quality research selected by AK and researchers from the community. Over the past year, more than 3,700 papers have been featured, and the page has grown to over 12k subscribers! However, many people may not have fully explored all of the features Daily Papers offers. This article will guide you through some hidden functionalities to help you make the most of this platform.</summary><published>2024-09-23</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/deploy-with-openvino</id><title>Optimize and deploy models with Optimum-Intel and OpenVINO GenAI</title><author>Alexander, Yury Gorbachev, Ekaterina Aidova, Ilya Lavrenov, Raymond Lo, Helena, Ella Charlaix</author><summary>Table of Contents

Why Use OpenVINO‚Ñ¢ for Edge Deployment

Step 1: Setting Up the Environment

Pre-requisites

Step 2: Exporting Models to OpenVINO IR
Using Python API

Using Command Line Interface (CLI)


Step 3: Model Optimization
Using Python API

Using Command Line Interface (CLI):


Step 4: Deploying with OpenVINO GenAI API
Python API Example

C++ API Example

Customizing Generation Config


Conclusion

Additional Resources

Deploying Transformers models at the edge or client-side requires careful consideration of performance and compatibility. Python, though powerful, is not always ideal for such deployments, especially in environments dominated by C++. This blog will guide you through optimizing and deploying Hugging Face Transformers models using Optimum-Intel and OpenVINO‚Ñ¢ GenAI, ensuring efficient AI inference with minimal dependencies. OpenVINO‚Ñ¢ was originally developed as a C++ AI inference solution, making it ideal for edge and client deployment where minimizing dependencies is crucial. With the introduction of the GenAI API, integrating large language models (LLMs) into C++ or Python applications has become even more straightforward, with features designed to simplify deployment and enhance performance.</summary><published>2024-09-20</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/1_58_llm_extreme_quantization</id><title>Fine-tuning LLMs to 1.58bit: extreme quantization made easy</title><author>Mohamed Mekkouri, Marc Sun, Leandro von Werra, Pedro Cuenca, Omar Sanseviero, Thomas Wolf</author><summary>Table of Contents

TL;DR
How to Use with Transformers


What is BitNet In More Depth?
Training

Inference


Pre-training Results in 1.58b

Fine-tuning in 1.58bit
Scaling to 100B Tokens !

Experiments on Smaller Models

Results &amp; Comparison


Custom Kernels &amp; Benchmarks
Basic GPU Concepts: Threads, Blocks, and Shared Memory

Challenges in Matrix Multiplication

The Idea of Tiling

Triton Kernel

Code Breakdown

Benchmark


Conclusion

Acknowledgements

Additional Resources

As Large Language Models (LLMs) grow in size and complexity, finding ways to reduce their computational and energy costs has become a critical challenge. One popular solution is quantization, where the precision of parameters is reduced from the standard 16-bit floating-point (FP16) or 32-bit floating-point (FP32) to lower-bit formats like 8-bit or 4-bit. While this approach significantly cuts down on memory usage and speeds up computation, it often comes at the expense of accuracy. Reducing the precision too much can cause models to lose crucial information, resulting in degraded performance. BitNet is a special transformers architecture that represents each parameter with only three values: (-1, 0, 1), offering a extreme quantization of just 1.58 ( log2(3) log_2(3) log2‚Äã(3) ) bits per parameter. However, it requires to train a model from scratch. While the results are impressive, not everybody has the budget to pre-train an LLM. To overcome this limitation, we explored a few tricks that allow fine-tuning an existing model to 1.58 bits! Keep reading to find out how !</summary><published>2024-09-18</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/sql-console</id><title>Introducing the SQL Console on Datasets</title><author>Caleb Fahlgren</author><summary>Introducing the SQL Console for Datasets

How it works
Parquet Conversion

DuckDB WASM ü¶Ü

Example: Converting a dataset from Alpaca to conversations

SQL


SQL Snippets

Resources

Datasets use has been exploding and Hugging Face has become the default home for many datasets. Each month, as the amount of datasets uploaded to the Hub increases, so does the need to query, filter and discover them. Datasets created on Hugging Face Hub each month</summary><published>2024-09-17</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/community-tools</id><title>Introducing Community Tools on HuggingChat</title><author>Nathan Sarrazin</author><summary>Turning a community Space into a tool

Creating a custom tool yourself

Enhance your assistants with Community Tools

Create a RAG tool on your own documents
Share your feedback with us


Today we‚Äôre releasing our latest feature on HuggingChat: Community Tools! This lets you turn any Space that you love on HuggingFace into a tool that can be used by models directly from HuggingChat. With this feature, we‚Äôre also expanding the modalities available in HuggingChat. You can now use community tools to understand images, generate videos, or answer with a text-to-speech model. The possibilities are endless and anyone can create tools using Spaces on Hugging Face! Explore existing tools here.</summary><published>2024-09-16</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/accelerate-v1</id><title>Accelerate 1.0.0</title><author>Zachary Mueller, Marc Sun, Benjamin Bossan</author><summary>3.5 years ago, Accelerate was a simple framework aimed at making training on multi-GPU and TPU systems easier
by having a low-level abstraction that simplified a raw PyTorch training loop: Since then, Accelerate has expanded into a multi-faceted library aimed at tackling many common problems with
large-scale training and large models in an age where 405 billion parameters (Llama) are the new language model size. This involves:</summary><published>2024-09-13</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/trufflesecurity-partnership</id><title>Hugging Face partners with TruffleHog to Scan for Secrets</title><author>Luc Georges</author><summary>Enhancing our automated scanning pipeline with TruffleHog

TruffleHog Native Hugging Face Scanner

We're excited to announce our partnership and integration with Truffle Security, bringing TruffleHog's powerful secret scanning features to our platform as part of our ongoing commitment to security. TruffleHog is an open-source tool that detects and verifies secret leaks in code. With a wide range of detectors for popular SaaS and cloud providers, it scans files and repositories for sensitive information like credentials, tokens, and encryption keys.</summary><published>2024-09-04</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/video-encoding</id><title>Scaling robotics datasets with video encoding</title><author>Simon  Alibert, Remi Cadene, Miquel Farr√©</author><summary>What's a dataset in robotics?

Contribution

But what is a codec? And what is video encoding &amp; decoding actually doing?

Criteria

Metrics

Variables
Encoding parameters

Decoding parameters


Results
Sizes

Loading times

Summary

Policies


Future work

Over the past few years, text and image-based models have seen dramatic performance improvements, primarily due to scaling up model weights and dataset sizes. While the internet provides an extensive database of text and images for LLMs and image generation models, robotics lacks such a vast and diverse qualitative data source and efficient data formats. Despite efforts like Open X, we are still far from achieving the scale and diversity seen with Large Language Models. Additionally, we lack the necessary tools for this endeavor, such as dataset formats that are lightweight, fast to load from, easy to share and visualize online. This gap is what ü§ó LeRobot aims to address. In their general form ‚Äî at least the one we are interested in within an end-to-end learning framework ‚Äî robotics datasets typically come in two modalities: the visual modality and the robot's proprioception / goal positions modality (state/action vectors). Here's what this can look like in practice:</summary><published>2024-08-27</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/unsung-heroes</id><title>The 5 Most Under-Rated Tools on Hugging Face</title><author>Derek Thomas</author><summary>Use-Case

ZeroGPU
Application


Multi-process Docker
Application


Gradio API
Application


Webhooks
Application


Nomic Atlas
Application

Features


Ethical Considerations

Conclusion

References

The Hugging Face Hub boasts over 850K public models, with ~50k new ones added every month, and that just seems to be
climbing higher and higher. We also offer an Enterprise Hub subscription that
unlocks compliance, security,
and governance features, along with additional
compute capacities on inference endpoints for production-level inference and more hardware options for doing demos on
Spaces. The Hugging Face Hub allows broad usage since you have diverse hardware, and you can run almost anything you want
in Docker Spaces. I‚Äôve noticed we have a number of features
that are unsung (listed below). In the process of creating a semantic search application on the Hugging Face hub I took
advantage of all of these features to implement various parts of the solution. While I think the final application 
(detailed in this org reddit-tools-HF), is compelling, I'd like to use this example to show how you can apply them to
your own projects.</summary><published>2024-08-22</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/packing-with-FA2</id><title>Improving Hugging Face Training Efficiency Through Packing with Flash Attention</title><author>Rhui Dih Lee, Arthur Zucker, Achintya Kundu, Laura Wynter, Raghu Ganti, Mayank Mishra</author><summary>Training with packed instruction tuning examples (without padding) is now compatible with Flash Attention 2 in Hugging Face, thanks to a recent PR and the new DataCollatorWithFlattening It can provide up to 2x improvement in training throughput while maintaining convergence quality. Read on for the details!</summary><published>2024-08-21</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/llama31-on-vertex-ai</id><title>Deploy Meta Llama 3.1 405B on Google Cloud Vertex AI</title><author>Alvaro Bartolome, Philipp Schmid, Simon Pagezy, Jeff Boudier</author><summary>Introduction to Vertex AI

1. Requirements for Meta Llama 3.1 Models on Google Cloud

2. Setup Google Cloud for Vertex AI

3. Register the Meta Llama 3.1 405B Model on Vertex AI

4. Deploy Meta Llama 3.1 405B on Vertex AI

5. Run online predictions with Meta Llama 3.1 405B
5.1 Via Python

5.2 Via the Vertex AI Online Prediction UI


6. Clean up resources

Conclusion

Meta Llama 3.1 is the latest open LLM from Meta, released in July 2024. Meta Llama 3.1 comes in three sizes: 8B for efficient deployment and development on consumer-size GPU, 70B for large-scale AI native applications, and 405B for synthetic data, LLM as a Judge or distillation; among other use cases. Some of its key features include: a large context length of 128K tokens (vs original 8K), multilingual capabilities, tool usage capabilities, and a more permissive license. In this blog you will learn how to programmatically deploy meta-llama/Meta-Llama-3.1-405B-Instruct-FP8, the FP8 quantized variant of meta-llama/Meta-Llama-3.1-405B-Instruct, in a Google Cloud A3 node with 8 x H100 NVIDIA GPUs on Vertex AI with Text Generation Inference (TGI) using the Hugging Face purpose-built Deep Learning Containers (DLCs) for Google Cloud.</summary><published>2024-08-19</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/infini-attention</id><title>A failed experiment: Infini-Attention, and why we should keep trying?</title><author>neuralink, Leandro von Werra, Thomas Wolf</author><summary>Section 0: Introduction

Section 1: Reproduction Principles

Section 2: How does Infini-attention works

Section 3: First experiments on a small scale

Section 4: Studying convergence?

Section 5: No weight decay on balance factors

Section 6: Conclusion

Acknowledgements

TLDR: Infini-attention's performance gets worse as we increase the number of times we compress the memory, and to the best of our knowledge, ring attention, YaRN and rope scaling are still the best ways for extending a pretrained model to longer context length. The context length of language models is one of the central attributes besides the model‚Äôs performance. Since the emergence of in-context learning, adding relevant information to the model‚Äôs input has become increasingly important. Thus, the context length rapidly increased from paragraphs (512 tokens with BERT/GPT-1) to pages (1024/2048 with GPT-2 and GPT-3 respectively) to books (128k of Claude) all the way to collections of books (1-10M tokens of Gemini). However, extending standard attention to such length remains challenging.</summary><published>2024-08-14</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/introduction-to-ggml</id><title>Introduction to ggml</title><author>Xuan-Son Nguyen, Georgi Gerganov, slaren</author><summary>Getting started

Terminology and concepts

Simple example

Example with a backend

Printing the computational graph

Conclusion

ggml is a machine learning (ML) library written in C and C++ with a focus on Transformer inference. The project is open-source and is being actively developed by a growing community. ggml is similar to ML libraries such as PyTorch and TensorFlow, though it is still in its early stages of development and some of its fundamentals are still changing rapidly. Over time, ggml has gained popularity alongside other projects like llama.cpp and whisper.cpp. Many other projects also use ggml under the hood to enable on-device LLM, including ollama, jan, LM Studio, GPT4All.</summary><published>2024-08-13</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/falconmamba</id><title>Welcome FalconMamba: The first strong attention-free 7B model</title><author>Jingwei Zuo, Maksim Velikanov, Rhaiem, Ilyas Chahed, Younes Belkada, Guillaume Kunsch</author><summary>First general purpose large-scale pure Mamba model

Model training

Evaluations

Processing large sequences

How to use it within Hugging Face transformers?

Acknowledgments

Falcon Mamba is a new model by Technology Innovation Institute (TII) in Abu Dhabi released under the TII Falcon Mamba 7B License 1.0. The model is open access and available within the Hugging Face ecosystem here for anyone to use for their research or application purposes. In this blog, we will go through the design decisions behind the model, how the model is competitive with respect to other existing SoTA models, and how to use it within the Hugging Face ecosystem.</summary><published>2024-08-12</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/unified-tool-use</id><title>Tool Use, Unified</title><author>Matthew Carrigan</author><summary>Introduction

Chat Templating

Passing tools to a chat template

Adding tool calls to the chat

Adding tool responses to the chat

Tool use in action

The regrettable disunity of response formats

Conclusion

There is now a unified tool use API across several popular families of models. This API means the same code is portable - few or no model-specific changes are needed to use tools in chats with Mistral, Cohere, NousResearch or Llama models. In addition, Transformers now includes helper functionality to make tool calling even easier, as well as complete documentation and examples for the entire tool use process. Support for even more models will be added in the near future. Tool use is a curious feature ‚Äì everyone thinks it‚Äôs great, but most people haven‚Äôt tried it themselves. Conceptually, it‚Äôs very straightforward: you give some tools (callable functions) to your LLM, and it can decide to call them to help it respond to user queries. Maybe you give it a calculator so it doesn‚Äôt have to rely on its internal, unreliable arithmetic abilities. Maybe you let it search the web or view your calendar, or you give it (read-only!) access to a company database so it can pull up information or search technical documentation.</summary><published>2024-08-12</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/xethub-joins-hf</id><title>XetHub is joining Hugging Face!</title><author>yuchenglow, Julien Chaumond</author><summary>Our common goal at HF

Example future use cases üî• ‚Äì what this will enable on the Hub

Fun current stats on Hub repos ü§Øü§Ø

A personal word from @ylow

Finally, our Infrastructure team is hiring üëØ

We are super excited to officially announce that Hugging Face acquired XetHub üî• XetHub is a Seattle-based company founded by Yucheng Low, Ajit Banerjee, Rajat Arya who previously worked at Apple where they built and scaled Apple‚Äôs internal ML infrastructure. XetHub‚Äôs mission is to enable software engineering best practices for AI development. XetHub has developed technologies to enable Git to scale to TB repositories and enable teams to explore, understand and work together on large evolving datasets and models. They were soon joined by a talented team of 12 team members. You should give them a follow at their new org page: hf.co/xet-team</summary><published>2024-08-08</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/2024-security-features</id><title>2024 Security Feature Highlights</title><author>Jack Kumar</author><summary>"Default" Hub Security Features
Fine Grained Token

Two Factor Authentication (2FA)

Commit Signing

Organizational Access Controls

Automated Security Scanning


Enterprise Hub Security Features
Single Sign-On (SSO)

Resource Groups

Organization Token Management

Data Residency

Audit Logs

Compliance

Custom Security Features


Conclusion

Security is a top priority at Hugging Face, and we're committed to continually enhancing our defenses to safeguard our users. In our ongoing security efforts, we have developed a range of security features designed to empower users to protect themselves and their  assets. In this blog post, we'll take a look at our current security landscape as of August 6th, 2024, and break down key security features available on the Hugging Face Hub. This post is broken down into two parts: in the first sections, we explore the essential security features available to all users of the Hub. Then in the second section we describe the advanced controls available to Enterprise Hub users.</summary><published>2024-08-06</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/doc_aug_hf_alb</id><title>Introducing TextImage Augmentation for Document Images</title><author>Dana Aubakirova, Pablo Montalvo, Vladimir Iglovikov</author><summary>Motivation

Introduction

Method

Main Features of the TextImage Augmentation

Installation

Visualization

Load data

Random Swap

Random Deletion

Random Insertion

Can we combine with other transformations?

Synthetic Data Generation

Conclusion

References

In this blog post, we provide a tutorial on how to use a new data augmentation technique for document images, developed in collaboration with Albumentations AI. Vision Language Models (VLMs) have an immense range of applications, but they often need to be fine-tuned to specific use-cases, particularly for datasets containing document images, i.e., images with high textual content. In these cases, it is crucial for text and image to interact with each other at all stages of model training, and applying augmentation to both modalities ensures this interaction. Essentially, we want a model to learn to read properly, which is challenging in the most common cases where data is missing.</summary><published>2024-08-06</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/gemma-july-update</id><title>Google releases Gemma 2 2B, ShieldGemma and Gemma Scope</title><author>Joshua, Pedro Cuenca, Vaibhav Srivastav, Joao Gante</author><summary>Gemma 2 2B
Use with Transformers

Use with llama.cpp

Demo

How to prompt Gemma 2

Open LLM Leaderboard v2 Evaluation

Assisted Generation


ShieldGemma
How to prompt ShieldGemma

Use with Transformers

Evaluation


Gemma Scope
Usage

Key links


One month after the release of Gemma 2, Google has expanded their set of Gemma models to include the following new additions: Let‚Äôs take a look at each of these in turn!</summary><published>2024-07-31</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/quanto-diffusers</id><title>Memory-efficient Diffusion Transformers with Quanto and Diffusers</title><author>Sayak Paul, David Corvoysier</author><summary>Preliminaries

Quantizing a DiffusionPipeline with Quanto

Generality of the observations

Misc findings
bfloat16 is usually better on H100

The promise of qint8

How about INT4?


Bonus - saving and loading Diffusers models in Quanto

Tips

Conclusion

Over the past few months, we have seen an emergence in the use of Transformer-based diffusion backbones for high-resolution text-to-image (T2I) generation. These models use the transformer architecture as the building block for the diffusion process, instead of the UNet architecture that was prevalent in many of the initial diffusion models. Thanks to the nature of Transformers, these backbones show good scalability, with models ranging from 0.6B to 8B parameters. As models become larger, memory requirements increase. The problem intensifies because a diffusion pipeline usually consists of several components: a text encoder, a diffusion backbone, and an image decoder. Furthermore, modern diffusion pipelines use multiple text encoders ‚Äì for example, there are three in the case of Stable Diffusion 3. It takes 18.765 GB of GPU memory to run SD3 inference using FP16 precision.</summary><published>2024-07-30</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/inference-dgx-cloud</id><title>Serverless Inference with Hugging Face and NVIDIA NIMs</title><author>Philipp Schmid, Jeff Boudier</author><summary>Serverless Inference powered by NVIDIA NIM

How it works
Create a Fine-Grained Token

Find your NIM

Send your requests


Supported Models and Pricing

Accelerating AI Inference with NVIDIA TensorRT-LLM

Today, we are thrilled to announce the launch of Hugging Face NVIDIA NIM API (serverless), a new service on the Hugging Face Hub, available to Enterprise Hub organizations. This new service makes it easy to use open models with the accelerated compute platform, of NVIDIA DGX Cloud accelerated compute platform for inference serving. We built this solution so that Enterprise Hub users can easily access the latest NVIDIA AI technology in a serverless way to run inference on popular Generative AI models including Llama and Mistral, using standardized APIs and a few lines of code within the Hugging Face Hub. This new experience builds on our collaboration with NVIDIA to simplify the access and use of open Generative AI models on NVIDIA accelerated computing. One of the main challenges developers and organizations face is the upfront cost of infrastructure and the complexity of optimizing inference workloads for LLM. With Hugging Face NVIDIA NIM API (serverless), we offer an easy solution to these challenges, providing instant access to state-of-the-art open Generative AI models optimized for NVIDIA infrastructure with a simple API for running inference. The pay-as-you-go pricing model ensures that you only pay for the request time you use, making it an economical choice for businesses of all sizes.</summary><published>2024-07-29</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/zero-shot-vqa-docmatix</id><title>LAVE: Zero-shot VQA Evaluation on Docmatix with LLMs - Do We Still Need Fine-Tuning?</title><author>Dana Aubakirova, Andres Marafioti</author><summary>Introduction

Method

About LAVE

Qualitative Examples

Are we too strict in evaluating VQA systems and do we need finetuning?

References

While developing Docmatix, we noticed that fine-tuning Florence-2 on it yielded great performance on DocVQA, but resulted in low scores on the benchmark. To enhance performance, we had to fine-tune the model further on DocVQA to learn the syntax required for the benchmark. Interestingly, this additional fine-tuning seemed to perform worse according to human evaluators, which is why we primarily used it for ablation studies and released the model only trained on Docmatix for broader use. Although the generated answers semantically align with the reference answers, as illustrated in Figure 1, they still receive low scores. This raises these questions: Should we fine-tune the models to improve these metrics, or should we develop new metrics that better align with human perception?</summary><published>2024-07-25</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/llama31</id><title>Llama 3.1 - 405B, 70B &amp; 8B with multilinguality and long context</title><author>Philipp Schmid, Omar Sanseviero, Alvaro Bartolome, Leandro von Werra, Daniel Vila, Vaibhav Srivastav, Marc Sun, Pedro Cuenca</author><summary>Table of contents

What‚Äôs new with Llama 3.1?

How much memory does Llama 3.1 need?
Inference Memory Requirements

Training Memory Requirements


Llama 3.1 evaluation

Using Hugging Face Transformers

How to prompt Llama 3.1
Built-in Tool calling


Custom Tool calling

Demo

Llama 3.1 405B quantization with FP8, AWQ, and GPTQ

Inference Integrations
Hugging Face Inference API

Hugging Face Inference Endpoints


Hugging Face Partner Integrations

Fine-tuning with Hugging Face TRL

Synthetic data generation with distilabel

Additional Resources

Acknowledgments

Llama 3.1 is out! Today we welcome the next iteration of the Llama family to Hugging Face. We are excited to collaborate with Meta to ensure the best integration in the Hugging Face ecosystem. Eight open-weight models (3 base models and 5 fine-tuned ones) are available on the Hub. Llama 3.1 comes in three sizes: 8B for efficient deployment and development on consumer-size GPU, 70B for large-scale AI native applications, and 405B for synthetic data, LLM as a Judge or distillation. All three come in base and instruction-tuned variants.</summary><published>2024-07-23</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/mistral-coreml</id><title>WWDC 24: Running Mistral 7B with Core ML</title><author>Pedro Cuenca, Christopher Fleetwood, Vaibhav Srivastav, Omar Sanseviero</author><summary>TL;DR

Best new Core ML features from WWDC‚Äô 24
Swift Tensor

Stateful Buffers

New Quantization Techniques

Multifunction Support


Converting Mistral 7B to Core ML
Tracing &amp; Conversion

State Preparation

Core ML Conversion

Model Compression


Running Mistral 7B with Swift

Running Mistral 7B with Python

What's Next?

How can you help?

WWDC‚Äô 24 is the moment Apple officially unveiled Apple Intelligence and
reiterated their commitment to efficient, private, and on-device AI.
During the keynote and the sessions that followed, they demonstrated
Apple Intelligence, which powers a huge array of AI-enhanced features
that show practical uses for everyday tasks. These are not
*AI-for-the-sake-of-AI* shiny demos. These are time-saving,
appropriate (and fun!) helpers that are deeply integrated with apps and
the OS, that also offer developers a number of ways to include these
features within their own apps. Apple Intelligence features can only work this well
because of the vertically integrated software stack that harnesses
Apple Silicon's capabilities to the fullest. Apple also offers a platform for developers to run models on-device, known as
Core ML. This software stack allows you to run ML models across all 3
compute units (CPU, GPU &amp; Neural Engine) available on Apple Silicon hardware.</summary><published>2024-07-22</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/docmatix</id><title>Docmatix - a huge dataset for Document Visual Question Answering</title><author>Andres Marafioti, Hugo Lauren√ßon</author><summary>Conclusion

Useful Resources

With this blog we are releasing Docmatix - a huge dataset for Document Visual Question Answering (DocVQA) that is 100s of times larger than previously available. Ablations using this dataset for fine-tuning Florence-2 show a 20% increase in performance on DocVQA. We first had the idea to create Docmatix when we developed The Cauldron, an extensive collection of 50 datasets for the fine-tuning of Vision-Language Model (VLM), and Idefics2 in particular. Through this process, we identified a significant gap in the availability of large-scale Document Visual Question Answering (DocVQA) datasets. The primary dataset we relied on for Idefics2 was DocVQA, which contains 10,000 images and 39,000 question-answer (Q/A) pairs. Fine-tuning on this and other datasets, open-sourced models still maintain a large gap in performance to closed-source ones.
To address this limitation, we are excited to introduce Docmatix, a DocVQA dataset featuring 2.4 million images and 9.5 million Q/A pairs derived from 1.3 million PDF documents. A 240X increase in scale compared to previous datasets.</summary><published>2024-07-18</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/multi-lora-serving</id><title>TGI Multi-LoRA: Deploy Once, Serve 30 Models</title><author>Derek Thomas, Diego Maniloff, David Holtz</author><summary>Motivation

Background on LoRA

Multi-LoRA Serving

Gather LoRAs
Low Code Teams


Deploy
TGI

Inference Endpoints GUI

Inference Endpoints Code


Consume

Practical Considerations
Cost


Usage Patterns

Changing the base model

Conclusion

Acknowledgements

References

Are you tired of the complexity and expense of managing multiple AI models? What if you could deploy once and serve 30 models? In today's ML world, organizations looking to leverage the value of their data will likely end up in a fine-tuned world, building a multitude of models, each one highly specialized for a specific task. But how can you keep up with the hassle and cost of deploying a model for each use case? The answer is Multi-LoRA serving. As an organization, building a multitude of models via fine-tuning makes sense for multiple reasons.</summary><published>2024-07-18</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/smollm</id><title>SmolLM - blazingly fast and remarkably powerful</title><author>Loubna Ben Allal, Anton Lozhkov, Elie Bakouch</author><summary>This blog post introduces SmolLM, a family of state-of-the-art small models with 135M, 360M, and 1.7B parameters, trained on a new high-quality dataset.  It covers data curation, model evaluation, and usage. There is increasing interest in small language models that can operate on local devices. This trend involves techniques such as distillation or quantization to compress large models, as well as training small models from scratch on large datasets. These approaches enable novel applications while dramatically reducing inference costs and improving user privacy.</summary><published>2024-07-16</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/argilla-chatbot</id><title>How we leveraged distilabel to create an Argilla 2.0 Chatbot</title><author>Agust√≠n Piqueres Lajar√≠n, Gabriel Mart√≠n Bl√°zquez, Sara Han D√≠az, Omar Sanseviero, Daniel Vila</author><summary>Discover how to build a Chatbot for a tool of your choice (Argilla 2.0 in this case) that can understand technical documentation and chat with users about it. In this article, we'll show you how to leverage distilabel and fine-tune a domain-specific embedding model to create a conversational model that's both accurate and engaging.</summary><published>2024-07-16</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/winning-aimo-progress-prize</id><title>How NuminaMath Won the 1st AIMO Progress Prize</title><author>Yann Fleureau, LI Jia, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul</author><summary>Introducing Numina - an open AI4Maths initiative

The AI Math Olympiad (AIMO) prize

Our winning solution for the 1st progress prize

The training recipe

Good data is all you need
Chain of Thought

Tool-integrated reasoning


Taming the variance with Self-Consistency and Tool Integrated Reasoning (SC-TIR)
Avoiding the curse of overfitting


Other ideas we tried

Numina‚Äôs future - looking for contributors and partners!

Acknowledgements

This year, Numina and Hugging Face collaborated to compete in the 1st Progress Prize of the AI Math Olympiad (AIMO). This competition involved fine-tuning open LLMs to solve difficult math problems that high school students use to train for the International Math Olympiad. We‚Äôre excited to share that our model ‚Äî NuminaMath 7B TIR  ‚Äî was the winner and managed to solve 29 out of 50 problems on the private test set ü•≥! In this blog post, we introduce the Numina initiative and the technical details behind our winning solution. If you want to skip straight to testing out the model with your hardest math problems, check out our demo.</summary><published>2024-07-11</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/keras-hub-integration</id><title>Announcing New Hugging Face and KerasHub integration</title><author>Aritra Roy Gosthipaty</author><summary>Use a wider range of frameworks

How to use it

Under the Hood: How It Works

Common Use Cases
Generation

Changing precision

Using the checkpoint with JAX backend


Gemma 2

PaliGemma

What's Next?

The Hugging Face Hub is a vast repository, currently hosting
750K+ public models,
offering a diverse range of pre-trained models for various machine
learning frameworks. Among these,
346,268
(as of the time of writing) models are built using the popular
Transformers library.
The KerasHub library recently added an
integration with the Hub compatible with a first batch of
33 models. In this first version, users of KerasHub were limited to only the
KerasHub-based models available on the Hugging Face Hub.</summary><published>2024-07-10</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/presidio-pii-detection</id><title>Experimenting with Automatic PII Detection on the Hub using Presidio</title><author>Quentin Lhoest, Margaret Mitchell, Omri M, Omri Mendels</author><summary>Types of Datasets with PII

The Challenges of PII in ML Datasets

A New Experiment on the Dataset Hub: Presidio Reports

An Example of a Presidio Report

Conclusion

At Hugging Face, we've noticed a concerning trend in machine learning (ML) datasets hosted on our Hub: Undocumented private information about individuals. This poses some unique challenges for ML practitioners.
In this blog post, we'll explore different types of datasets containing a type of private information known as Personally Identifying Information (PII), the issues they present, and a new feature we're experimenting with on the Dataset Hub to help address these challenges. We noticed two types of datasets that contain PII:</summary><published>2024-07-10</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/dpo_vlm</id><title>Preference Optimization for Vision Language Models</title><author>Quentin Gallou√©dec, Shengyi Costa Huang, Merve Noyan, Kashif Rasul</author><summary>Preference dataset

Training
How much memory do I need?

Quantization

LoRA

The new memory requirements after quantization and LoRA

What about the batch size?

Summary: complete training script


Results

Evaluation
Inference


Finetuning Llava 1.5, PaliGemma and others

Training models to understand and predict human preferences can be incredibly complex. Traditional methods, like supervised fine-tuning, often require assigning specific labels to data, which is not cost-efficient, especially for nuanced tasks. Preference optimization is an alternative approach that can simplify this process and yield more accurate results. By focusing on comparing and ranking candidate answers rather than assigning fixed labels, preference optimization allows models to capture the subtleties of human judgment more effectively. Preference optimization is widely used for fine-tuning language models, but it can also be applied to vision language models (VLM).
We are excited to announce that the TRL library now supports direct preference optimization (DPO) for VLMs. This article will guide you through the process of training VLMs using TRL and DPO.</summary><published>2024-07-10</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/tpu-inference-endpoints-spaces</id><title>Google Cloud TPUs made available to Hugging Face users</title><author>Simon Pagezy, Michelle Habonneau, Philipp Schmid, Alvaro Moran</author><summary>Hugging Face Inference Endpoints support for TPUs

Hugging Face Spaces support for TPUs We're excited to share some great news! AI builders are now able to accelerate their applications with Google Cloud TPUs on Hugging Face Inference Endpoints and Spaces!</summary><published>2024-07-09</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/sovereign-data-solution-case-study</id><title>Banque des Territoires (CDC Group) x Polyconseil x Hugging Face: Enhancing a Major French Environmental Program with a Sovereign Data Solution</title><author>Anthony Truchet, Jeremy Cailton, RAMAHERISON, Florent Gbelidji, Violette</author><summary>The collaboration initiated last January between Banque des Territoires (part of the Caisse des D√©p√¥ts et Consignations group), Polyconseil, and Hugging Face illustrates the possibility of merging the potential of generative AI with the pressing demands of data sovereignty. As the project's first phase has just finished, the tool developed is ultimately intended to support the national strategy for schools' environmental renovation. Specifically, the solution aims to optimize the support framework of Banque des Territoires‚Äô EduR√©nov program, which is dedicated to the ecological renovation of 10,000 public school facilities (nurseries, grade/middle/high schools, and universities).</summary><published>2024-07-09</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/datasets-filters</id><title>Announcing New Dataset Search Features</title><author>Quentin Lhoest, Sylvain Lesage, Bertrand Chevrier</author><summary>Search by Modality

Search by Size

Search by Format

Search by Library

Combine filters

The AI and ML community has shared more than 180,000 public datasets on The Hugging Face Dataset Hub.
Researchers and engineers are using these datasets for various tasks, from training LLMs to chat with users to evaluating automatic speech recognition or computer vision systems.
Dataset discoverability and visualization are key challenges to letting AI builders find, explore, and transform datasets to fit their use cases. At Hugging Face, we are building the Dataset Hub as the place for the community to collaborate on open datasets.
So we built tools like Dataset Search and the Dataset Viewer, as well as a rich open source ecosystem of tools.
Today we are announcing four new features that will take Dataset Search on the Hub to the next level.</summary><published>2024-07-08</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/intel-protein-language-model-protst</id><title>Accelerating Protein Language Model ProtST on Intel Gaudi 2</title><author>Julien Simon, Jiqing.Feng, Xinyu Yuan, Yi Wang, Matrix Yao, Minghao Xu, Ke Ding</author><summary>Protein Language Models (PLMs) have emerged as potent tools for predicting and designing protein structure and function. At the International Conference on Machine Learning 2023 (ICML), MILA and Intel Labs released ProtST, a pioneering multi-modal language model for protein design based on text prompts. Since then, ProtST has been well-received in the research community, accumulating more than 40 citations in less than a year, showing the scientific strength of the work. One of PLM's most popular tasks is predicting the subcellular location of an amino acid sequence. In this task, users feed an amino acid sequence into the model, and the model outputs a label indicating the subcellular location of this sequence. Out of the box, zero-shot ProtST-ESM-1b outperforms state-of-the-art few-shot classifiers.</summary><published>2024-07-03</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/beating-gaia</id><title>Our Transformers Code Agent beats the GAIA benchmark!</title><author>Aymeric Roucher, Sergei Petrov</author><summary>After some experiments, we were impressed by the performance of Transformers Agents to build agentic systems, so we wanted to see how good it was! We tested using a Code Agent built with the library on the GAIA benchmark, arguably the most difficult and comprehensive agent benchmark‚Ä¶ and ended up on top! The framework transformers.agents used in this blog post has now been upgraded to the stand-alone library smolagents! The two libraries have very similar APIs, so switching is easy.
Go checkout the smolagents introduction blog here.</summary><published>2024-07-01</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/gemma2</id><title>Welcome Gemma 2 - Google's new open LLM</title><author>Philipp Schmid, Omar Sanseviero, Pedro Cuenca, Lewis Tunstall, Tom Aarsen, Vaibhav Srivastav</author><summary>Table of contents

What is Gemma 2?

Technical advances in Gemma 2
Sliding window attention

Soft-capping and attention implementations

Knowledge Distillation

Model Merging


Gemma 2 evaluation
Technical Report results

Open LLM Leaderboard results


How to prompt Gemma 2

Demo

Using Hugging Face¬†Transformers

Integration with Google Cloud

Fine-tuning with ü§ó¬†TRL

Integration with Inference Endpoints

Additional Resources

Acknowledgments

Google released Gemma 2, the latest addition to its family of state-of-the-art open LLMs, and we are excited to collaborate with Google to ensure the best integration in the Hugging Face ecosystem. You can find the 4 open-weight models (2 base models &amp; 2 fine-tuned ones) on the Hub. Among the features and integrations being released, we have: Gemma 2 is Google's latest iteration of open LLMs. It comes in two sizes, 9 billion and 27 billion parameters with base (pre-trained) and instruction-tuned versions. Gemma is based on Google Deepmind Gemini and has a context length of 8K tokens:</summary><published>2024-06-27</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/xlscout-case-study</id><title>XLSCOUT Unveils ParaEmbed 2.0: a Powerful Embedding Model Tailored for Patents and IP with Expert Support from Hugging Face</title><author>Andrew Reed, Khushwant Rai</author><summary>The journey towards enhanced patent analysis

Collaborating with Hugging Face via the Expert Support Program

XLSCOUT's AI-based IP Solutions

A partnership for innovation

Commitment to innovation and future plans

This is a guest blog post by the XLSCOUT team. XLSCOUT, a Toronto-based leader in the use of AI in intellectual property (IP), has developed a powerful proprietary embedding model called ParaEmbed 2.0 stemming from an ambitious collaboration with Hugging Face‚Äôs Expert Support Program. The collaboration focuses on applying state-of-the-art AI technologies and open-source models to enhance the understanding and analysis of complex patent documents including patent-specific terminology, context, and relationships. This allows XLSCOUT‚Äôs products to offer the best performance for drafting patent applications, patent invalidation searches, and ensuring ideas are novel compared to previously available patents and literature.</summary><published>2024-06-25</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/finetune-florence2</id><title>Fine-tuning Florence-2 - Microsoft's Cutting-edge Vision Language Models</title><author>Andres Marafioti, Merve Noyan, Piotr Skalski</author><summary>Pre-training Details and Architecture

Original performance on VQA

Performance on DocVQA after fine-tuning

Fine-tuning Details

Code Walkthrough

Conclusions

Useful Resources

Florence-2, released by Microsoft in June 2024, is a foundation vision-language model. This model is very attractive because of its small size (0.2B and 0.7B) and strong performance on a variety of computer vision and vision-language tasks. Florence supports many tasks out of the box: captioning, object detection, OCR, and more. However, your task or domain might not be supported, or you may want to better control the model's output for your task. That's when you will need to fine-tune.</summary><published>2024-06-24</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/ethics-soc-6</id><title>Ethics and Society Newsletter #6: Building Better AI: The Importance of Data Quality</title><author>Avijit Ghosh, Lucie-Aim√©e Kaffee, Yacine Jernite, Margaret Mitchell, Irene Solaiman, Daniel Vila, Florent Daudens, Brigitte Tousignant, Giada Pistilli, Sasha Luccioni</author><summary>What is Good, High-Quality Data?

Why Data Quality?

What is the Process toward Data Quality?

Data Quality for Improving Model Performance

Data Quality for Improving Representation

Data Quality for Governance and Accountability

Data Quality for Adaptability and Generalizability

Data Quality for Scientific Reproducibility and Replicability

High-Quality Data needs High-Quality Documentation

A Note on Synthetic Data

Data Quality Practices at Hugging Face

Are you working on data quality? Share your tools and methods on the Hugging Face Hub!

In February, Reddit announced a new content partnership with Google where they would provide data that would power the new Generative AI based search engine using Retrieval Augmented Generation (RAG). That attempt did not go as planned, and soon, people were seeing recommendations like adding glue to pizza: In the age of artificial intelligence, massive amounts of data fuel the growth and sophistication of machine learning models. But not all data is created equal; AI systems require high-quality data to produce high-quality outputs.</summary><published>2024-06-24</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/dibt</id><title>Data Is Better Together: A Look Back and Forward</title><author>Daniel van Strien, David Berenstein, Sara Han D√≠az</author><summary>Community efforts

Cookbook efforts

What have we learnt?

How can you get involved?

For the past few months, we have been working on the Data Is Better Together initiative. With this collaboration between Hugging Face and Argilla and the support of the open-source ML community, our goal has been to empower the open-source community to create impactful datasets collectively. Now, we have decided to move forward with the same goal. To provide an overview of our achievements and tasks where everyone can contribute, we organized it into two sections: community efforts and cookbook efforts.</summary><published>2024-06-20</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/prezi-case-study</id><title>Going multimodal: How Prezi is leveraging the Hub and the Expert Support Program to accelerate their ML roadmap</title><author>Violette, Jeff Boudier, Moritz Laurer, M√°t√© B√∂rcs√∂k</author><summary>Transcript with additional details:
Introduction

How does the HF Expert Support Program help you build AI?

What‚Äôs your favorite feature of Inference Endpoints?

What teams would benefit most from Expert Support?


Everybody knows that a great visual is worth a thousand words. The team at Prezi, a visual communications software company, is putting this insight into practice with their Prezi presentations that combine images and text in highly dynamic presentations. Prezi has joined the Hugging Face Expert Support Program to fully leverage modern machine learning's potential. Over the past months, Hugging Face has supported Prezi in integrating smaller, more efficient open-source models into their ML workflows. This cooperation started at a perfect time, as multimodal models are becoming increasingly capable.</summary><published>2024-06-19</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/leaderboard-bigcodebench</id><title>BigCodeBench: Benchmarking Large Language Models on Solving Practical and Challenging Programming Tasks</title><author>Terry Yue Zhuo, Jiawei Liu, Qian Liu, Binyuan Hui, Niklas Muennighoff, Daniel Fried, Harm de Vries, Leandro von Werra, Cl√©mentine Fourrier</author><summary>What do the tasks in BigCodeBench look like? üïµÔ∏è‚Äç‚ôÇÔ∏è

Where do the tasks come from? ü§î

How well do LLMs perform on BigCodeBench? üìä

Great! So, how can I evaluate my model on BigCodeBench? üõ†Ô∏è
Setup

Code Generation

Code Post-processing

Code Evaluation


What's next?

Resources

Citation

HumanEval is a reference benchmark for evaluating large language models (LLMs) on code generation tasks, as it makes the evaluation of compact function-level code snippets easy. However, there are growing concerns about its effectiveness in evaluating the programming capabilities of LLMs, and the main concern is that tasks in HumanEval are too simple and may not be representative of real-world programming tasks. Compared to the algorithm-oriented tasks in HumanEval, real-world software development often involves diverse libraries and function calls. Furthermore, LLMs' performance on HumanEval is subject to contamination and overfitting issues, making it less reliable for evaluating the generalization of LLMs. While there have been some efforts to address these issues, they are either domain-specific, deterministic, or agent-centric (sorry DS-1000, ODEX, and SWE-bench üíî). We feel that the community still lacks an easy-to-use benchmark that can broadly evaluate the programming capabilities of LLMs, and that's what we focused on.</summary><published>2024-06-18</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/deepspeed-to-fsdp-and-back</id><title>From DeepSpeed to FSDP and Back Again with Hugging Face Accelerate</title><author>Yu Chin Fabian Lim, aldo pareja, Zachary Mueller, Stas Bekman</author><summary>Are FSDP and DeepSpeed Interchangeable?

Precision Matters

Harmonizing DeepSpeed and FSDP in ü§ó Accelerate

Throughput results

Closing thoughts

Acknowledgements

There are two popular implementations of the ZeRO Redundancy Optimizer (Zero) algorithm in the community, one from DeepSpeed and the other from PyTorch. Hugging Face Accelerate exposes both these frameworks for the end users to train/tune their models. This blog highlights the differences between how these backends are exposed through Accelerate. To enable users to seamlessly switch between these backends, we upstreamed a precision-related change and a concept guide. Recently, we tried running a training pipeline with DeepSpeed and PyTorch FSDP. We noticed that the results obtained differed. The specific model was Mistral-7B base and it was loaded in half-precision (bfloat16). While the DeepSpeed (blue) loss had converged well, the FSDP (orange) loss was not decreasing, as can be seen in Figure 1.</summary><published>2024-06-13</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/sd3</id><title>üß® Diffusers welcomes Stable Diffusion 3</title><author>Dhruv Nair, YiYi Xu, Sayak Paul, Alvaro Somoza, Kashif Rasul, Apolin√°rio from multimodal AI art</author><summary>Table Of Contents

What‚Äôs New With SD3?
Model

Training with Rectified Flow Matching


Using SD3 with Diffusers
Text-To-Image

Image-To-Image


Memory Optimizations for SD3
Running Inference with Model Offloading

Dropping the T5 Text Encoder during Inference


Using A Quantized Version of the T5-XXL Model
Summary of Memory Optimizations


Performance Optimizations for SD3

Dreambooth and LoRA fine-tuning

Acknowledgements

Stable Diffusion 3 (SD3), Stability AI‚Äôs latest iteration of the Stable Diffusion family of models, is now available on the Hugging Face Hub and can be used with üß®¬†Diffusers. The model released today is Stable Diffusion 3 Medium, with 2B parameters.</summary><published>2024-06-12</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/putting_rl_back_in_rlhf_with_rloo</id><title>Putting RL back in RLHF</title><author>Shengyi Costa Huang, Arash Ahmadian</author><summary>We are excited to introduce the RLOO (REINFORCE Leave One-Out) Trainer in TRL. As an alternative to PPO, RLOO is a new online RLHF training algorithm designed to be more accessible and easier to implement. In particular, RLOO requires less GPU memory and takes less wall time to converge. As shown in the figures below: With RLOO, we bring Reinforcement Learning back into RLHF, enabling the community to explore online RL methods more easily. This is exciting because more and more studies have shown that online RL is more effective than offline methods such as DPO (https://arxiv.org/abs/2402.04792, https://arxiv.org/abs/2405.08448).</summary><published>2024-06-12</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/transformers-docs-redesign</id><title>Making sense of this mess</title><author>Steven Liu</author><summary>A new audience

Toward a more organic structure

Integration versus amendment

Next steps

When I joined Hugging Face nearly 3 years ago, the Transformers documentation was very different from its current form today. It focused on text models and how to train or use them for inference on natural language tasks (text classification, summarization, language modeling, etc.). The main version of the Transformers documentation today compared to version 4.10.0 from nearly 3 years ago.</summary><published>2024-06-07</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/sagemaker-huggingface-embedding</id><title>Introducing the Hugging Face Embedding Container for Amazon SageMaker</title><author>Philipp Schmid, Jeff Boudier</author><summary>What is the Hugging Face Embedding Container?

1. Setup development environment

2. Retrieve the new Hugging Face Embedding Container

3. Deploy Snowflake Arctic to Amazon SageMaker

4. Run and evaluate Inference performance

5. Delete model and endpoint

Conclusion

We are excited to announce that the new Hugging Face Embedding Container for Amazon SageMaker is now generally available (GA). AWS customers can now efficiently deploy embedding models on SageMaker to build Generative AI applications, including Retrieval-Augmented Generation (RAG) applications. In this Blog we will show you how to deploy open Embedding Models, like Snowflake/snowflake-arctic-embed-l, BAAI/bge-large-en-v1.5 or sentence-transformers/all-MiniLM-L6-v2 to Amazon SageMaker for inference using the new Hugging Face Embedding Container. We will deploy the Snowflake/snowflake-arctic-embed-m-v1.5 one of the best open Embedding Models for retrieval - you can check its rankings on the MTEB Leaderboard.</summary><published>2024-06-07</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/leaderboard-artificial-analysis2</id><title>Launching the Artificial Analysis Text to Image Leaderboard &amp; Arena</title><author>Micah Hill-Smith, George Cameron</author><summary>Methodology

Early Insights From the Results üëÄ

How to contribute or get in touch

Other Image Model Quality Initiatives

In two short years since the advent of diffusion-based image generators, AI image models have achieved near-photographic quality. How do these models compare? Are the open-source alternatives on par with their proprietary counterparts? The Artificial Analysis Text to Image Leaderboard aims to answer these questions with human preference based rankings. The ELO score is informed by over 45,000 human image preferences collected in the Artificial Analysis Image Arena. The leaderboard features the leading open-source and proprietary image models : the latest versions of Midjourney, OpenAI's DALL¬∑E, Stable Diffusion, Playground and more.</summary><published>2024-06-06</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/npc-gigax-cubzh</id><title>Introducing NPC-Playground, a 3D playground to interact with LLM-powered NPCs</title><author>Tristan Deborde, Duermael, Gaetan de Villele, Corentin CAILLEAUD, Thomas Simonini</author><summary>The Tech Stack

What is Cubzh?

What is Gigax?

The NPC-Playground Demo

Make your own demo üî•

AI-powered NPCs (Non-Playable Characters) are one of the most important breakthroughs brought about by the use of LLMs in games. LLMs, or Large Language Models, make it possible to design "intelligent" in-game characters that can engage in realistic conversations with the player, perform complex actions and follow instructions, dramatically enhancing the player's experience. AI-powered NPCs represent a huge advancement vs rule-based and heuristics systems.</summary><published>2024-06-05</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/assisted-generation-support-gaudi</id><title>Faster assisted generation support for Intel Gaudi</title><author>Haim Barad, Neha Raste, Tien Pei Chou</author><summary>Speculative Sampling - Assisted Decoding

Usage &amp; Experiments

Conclusion

As model sizes grow, Generative AI implementations require significant inference resources. This not only increases the cost per generation, but also increases the power consumption used to serve such requests. Inference optimizations for text generation are essential for reducing latency, infrastructure costs, and power consumption. This can lead to an improved user experience and increased efficiency in text generation tasks.</summary><published>2024-06-04</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/space-secrets-disclosure</id><title>Space secrets security update</title><author>Hugging Face</author><summary>Earlier this week our team detected unauthorized access to our Spaces platform, specifically related to Spaces secrets. As a consequence, we have suspicions that a subset of Spaces‚Äô secrets could have been accessed without authorization. As a first step of remediation, we have revoked a number of HF tokens present in those secrets. Users whose tokens have been revoked already received an email notice. We recommend you refresh any key or token and consider switching your HF tokens to fine-grained access tokens which are the new default.</summary><published>2024-05-31</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/tgi-benchmarking</id><title>Benchmarking Text Generation Inference</title><author>Derek Thomas</author><summary>Motivation

Pre-requisites
Latency vs Throughput

Pre-filling and Decoding


Benchmarking Tool
Motivation

Setup

Getting Started

Main Components

Understanding the Benchmarking tool

Winding Down


Conclusion

References

In this blog we will be exploring Text Generation Inference‚Äôs (TGI) little brother, the TGI Benchmarking tool. It will help us understand how to profile TGI beyond simple throughput to better understand the tradeoffs to make decisions on how to tune your deployment for your needs. If you have ever felt like LLM deployments cost too much or if you want to tune your deployment to improve performance this blog is for you! I‚Äôll show you how to do this in a convenient Hugging Face Space. You can take the results and use it on an Inference Endpoint or other copy of the same hardware.</summary><published>2024-05-29</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/train-sentence-transformers</id><title>Training and Finetuning Embedding Models with Sentence Transformers v3</title><author>Tom Aarsen</author><summary>Table of Contents

Why Finetune?

Training Components

Dataset
Data on Hugging Face Hub

Local Data (CSV, JSON, Parquet, Arrow, SQL)

Local Data that requires pre-processing

Dataset Format


Loss Function

Training Arguments

Evaluator
EmbeddingSimilarityEvaluator with STSb

TripletEvaluator with AllNLI


Trainer
Callbacks


Multi-Dataset Training

Deprecation

Additional Resources
Training Examples

Documentation


Sentence Transformers is a Python library for using and training embedding models for a wide range of applications, such as retrieval augmented generation, semantic search, semantic textual similarity, paraphrase mining, and more. Its v3.0 update is the largest since the project's inception, introducing a new training approach. In this blogpost, I'll show you how to use it to finetune Sentence Transformer models to improve their performance on specific tasks. You can also use this method to train new Sentence Transformer models from scratch. Finetuning Sentence Transformers now involves several components, including datasets, loss functions, training arguments, evaluators, and the new trainer itself. I'll go through each of these components in detail and provide examples of how to use them to train effective models.</summary><published>2024-05-28</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/falcon2-11b</id><title>Falcon 2: An 11B parameter pretrained language model and VLM, trained on over 5000B tokens tokens and 11 languages</title><author>Quentin Malartic, Nilabhra Roy Chowdhury, Ruxandra Cojocaru, Mughaira, Giulia Campesan, Yasser Dahou, Sanath Narayan, Ankit Singh, Cl√©mentine Fourrier</author><summary>The Falcon 2 Models

Table of Contents

Falcon2-11B LLM
Training Data

Model Architecture

Training Procedure

Training Hyperparameters


Falcon2-11B Evaluation
English performance

Multilingual capabilities

Code generation capabilities


Using Falcon2-11B

Falcon2-11B VLM
Training


Falcon2-11B VLM Evaluation

Using Falcon2-11B-FalconVLM

License information TII is launching a new generation of models, Falcon 2, focused on providing the open-source community with a series of smaller models with enhanced performance and multi-modal support. Our goal is to enable cheaper inference and encourage the development of more downstream applications with improved usability.</summary><published>2024-05-24</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/leaderboard-llamaguard</id><title>CyberSecEval 2 - A Comprehensive Evaluation Framework for Cybersecurity Risks and Capabilities of Large Language Models</title><author>r34p3r, Sahana C, Yue, Cyrus Nikolaidis, Daniel Song, Simon Wan, Faizan Ahmad, cornelius aschermann, Yaohui Chen, Dhaval Kapil, David Molnar, Spencer Whitman, Joshua Saxe, Varun Vontimitta, Carl Parker, Cl√©mentine Fourrier</author><summary>Benchmarks

Key Insights
Industry Improvement

Model Comparison

Prompt Injection Risks

Code Exploitation Limitations

Interpreter Abuse Risks


How to contribute?

Other Resources

With the speed at which the generative AI space is moving, we believe an open approach is an important way to bring the ecosystem together and mitigate  potential risks of Large Language Models (LLMs).  Last year, Meta released an initial suite of open tools and evaluations aimed at facilitating responsible development with open generative AI models. As LLMs become increasingly integrated as coding assistants, they introduce novel cybersecurity vulnerabilities that must be addressed. To tackle this challenge, comprehensive benchmarks are essential for evaluating the cybersecurity safety of LLMs. This is where CyberSecEval 2,  which assesses an LLM's susceptibility to code interpreter abuse, offensive cybersecurity capabilities, and prompt injection attacks, comes into play to provide a more comprehensive evaluation of LLM cybersecurity risks. You can view the CyberSecEval 2 leaderboard here. CyberSecEval 2 benchmarks help evaluate LLMs‚Äô propensity to generate insecure code and comply with requests to aid cyber attackers:</summary><published>2024-05-24</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/kv-cache-quantization</id><title>Unlocking Longer Generation with Key-Value Cache Quantization</title><author>Raushan Turganbay</author><summary>Implementation Details

Comparing performance of fp16 and quantized cache

How to use quantized kv cache in ü§ó Transformers?

Conclusion

Acknowledgment

Additional Resources

At Hugging Face, we are excited to share with you a new feature that's going to take your language models to the next level: KV Cache Quantization. TL;DR: KV Cache Quantization reduces memory usage for long-context text generation in LLMs with minimal impact on quality, offering customizable trade-offs between memory efficiency and generation speed.</summary><published>2024-05-16</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/inferentia-inference-endpoints</id><title>Deploy models on AWS Inferentia2 from Hugging Face</title><author>Jeff Boudier, Philipp Schmid</author><summary>Enabling over 100,000 models on AWS Inferentia2 with Amazon SageMaker

Hugging Face Inference Endpoints introduces support for AWS Inferentia2

Whats Next AWS Inferentia2 is the latest AWS machine learning chip available through the Amazon EC2 Inf2 instances on Amazon Web Services. Designed from the ground up for AI workloads, Inf2 instances offer great performance and cost/performance for production workloads.</summary><published>2024-05-22</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/spaces-dev-mode</id><title>Introducing Spaces Dev Mode for a seamless developer experience</title><author>Simon Pagezy</author><summary>Enable Dev Mode

Connect to VS Code

Hugging Face Spaces makes it easy for you to create and deploy AI-powered demos in minutes. Over 500,000 Spaces have been created by the Hugging Face community and it keeps growing! As part of Hugging Face Spaces, we recently released support for ‚ÄúDev Mode‚Äù, to make your experience of building Spaces even more seamless. Spaces Dev Mode lets you connect with VS Code or SSH directly to your Space. In a click, you can connect to your Space, and start editing your code, removing the need to push your local changes to the Space repository using git.
Let's see how to setup this feature in your Space‚Äôs settings üî•</summary><published>2024-05-21</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/dell-enterprise-hub</id><title>Build AI on premise with Dell Enterprise Hub</title><author>Jeff Boudier, Philipp Schmid, Balachandran Rajendran, Ian Roche</author><summary>Enterprises need to build AI with open models

Dell Enterprise Hub: On-Premise LLMs made easy

Deploy open models with Dell Enterprise Hub

Train open models with Dell Enterprise Hub

Bring your Own Model with Dell Enterprise Hub

We‚Äôre just getting started Today we announce the Dell Enterprise Hub, a new experience on Hugging Face to easily train and deploy open models on-premise using Dell platforms.</summary><published>2024-05-21</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/huggingface-amd-mi300</id><title>Hugging Face on AMD Instinct MI300 GPU</title><author>F√©lix Marty, Mohit Sharma, seungrok jung, Morgan Funtowicz</author><summary>Introduction

Open-Source and production enablement
Maintaining support for AMD Instinct GPUs in Transformers and text-generation-inference


Improving performances for production AI workloads
Inferencing performance

Model fine-tuning performances


What's next?

Conclusion

Join the next Hugging Cast on June 6th to ask questions to the post authors, watch a live demo deploying Llama 3 on MI300X on Azure, plus a bonus demo deploying models locally on Ryzen AI PC! Register at https://streamyard.com/watch/iMZUvJnmz8BV</summary><published>2024-05-21</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/microsoft-collaboration</id><title>From cloud to developers: Hugging Face and Microsoft Deepen Collaboration</title><author>Jeff Boudier, Philipp Schmid</author><summary>A collaboration for Cloud AI Builders
Expanded HF Collection in Azure Model Catalog

Build AI with the new AMD MI300X on Azure


A Collaboration for Open Science

A Collaboration for Open Source

A Collaboration for Developers

What‚Äôs Next

Today at Microsoft Build we are happy to announce a broad set of new features and collaborations as Microsoft and Hugging Face deepen their strategic collaboration to make open models and open source AI easier to use everywhere. Together, we will work to enable AI builders across open science, open source, cloud, hardware and developer experiences - read on for announcements today on all fronts! we are excited to announce two major new experiences to build AI with open models on Microsoft Azure.</summary><published>2024-05-21</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/paligemma</id><title>PaliGemma ‚Äì Google's Cutting-Edge Open Vision Language Model</title><author>Merve Noyan, Andreas P. Steiner, Pedro Cuenca</author><summary>What is PaliGemma?

Model Capabilities
Image Captioning

Visual Question Answering

Detection

Referring Expression Segmentation

Document Understanding

Mix Benchmarks


Fine-tuned Checkpoints

Demo

How to Run Inference
Using Transformers


Detailed Inference Process

Fine-tuning
Using big_vision

Using transformers


Additional Resources

Updated on 23-05-2024: We have introduced a few changes to the transformers PaliGemma implementation around fine-tuning, which you can find in this notebook. PaliGemma is a new family of vision language models from Google. PaliGemma can take in an image and a text and output text.</summary><published>2024-05-14</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/langchain</id><title>Hugging Face x LangChain : A new partner package in LangChain</title><author>Joffrey THOMAS, Cyril KONDRATENKO, Erick Friis</author><summary>Getting Started

The LLMs
HuggingFacePipeline

HuggingFaceEndpoint

ChatHuggingFace


The Embeddings
HuggingFaceEmbeddings

HuggingFaceEndpointEmbeddings


Conclusion

We are thrilled to announce the launch of langchain_huggingface, a partner package in LangChain jointly maintained by Hugging Face and LangChain. This new Python package is designed to bring the power of the latest development of Hugging Face into LangChain and keep it up to date. All Hugging Face-related classes in LangChain were coded by the community, and while we thrived on this, over time, some of them became deprecated because of the lack of an insider‚Äôs perspective.</summary><published>2024-05-14</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/leaderboard-arabic</id><title>Introducing the Open Arabic LLM Leaderboard</title><author>Ali El Filali, Hamza Alobeidli, Ruxandra Cojocaru, Basma Boussaha, Cl√©mentine Fourrier</author><summary>Benchmarks, Metrics &amp; Technical setup
Benchmark Datasets

Evaluation Metrics

Technical setup


Future Directions

Submit Your Model !
Model Submission Process

In Case of Model Failure


Acknowledgements

Citations and References

The Open Arabic LLM Leaderboard (OALL) is designed to address the growing need for specialized benchmarks in the Arabic language processing domain. As the field of Natural Language Processing (NLP) progresses, the focus often remains heavily skewed towards English, leaving a significant gap in resources for other languages. The OALL aims to balance this by providing a platform specifically for evaluating and comparing the performance of Arabic Large Language Models (LLMs), thus promoting research and development in Arabic NLP. This initiative is particularly significant given that it directly serves over 380 million Arabic speakers worldwide. By enhancing the ability to accurately evaluate and improve Arabic LLMs, we hope the OALL will play a crucial role in developing models and applications that are finely tuned to the nuances of the Arabic language, culture and heritage.</summary><published>2024-05-14</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/agents</id><title>License to Call: Introducing Transformers Agents 2.0</title><author>Aymeric Roucher, Lysandre, Pedro Cuenca</author><summary>We are releasing Transformers Agents 2.0! ‚áí üéÅ On top of our existing agent type, we introduce two new agents that can iterate based on past observations to solve complex tasks.</summary><published>2024-05-13</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/enterprise-hub-aws-marketplace</id><title>Subscribe to Enterprise Hub with your AWS Account</title><author>Violette, Simon Brandeis, Jeff Boudier</author><summary>What is Enterprise Hub?
1. Getting Started

2. Connect your Hugging Face Account with your AWS Account

3. Activate the Enterprise Hub for your team and unlock new features

Congratulations! ü•≥


You can now upgrade your Hugging Face Organization to Enterprise using your AWS account - get started on the AWS Marketplace. Enterprise Hub is a premium subscription to upgrade a free Hugging Face organization with advanced security features, access controls, collaboration tools and compute options. With Enterprise Hub, companies can build AI privately and securely within our GDPR compliant and SOC2 Type 2 certified platform. Exclusive features include:</summary><published>2024-05-09</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/cost-efficient-rag-applications-with-intel</id><title>Building Cost-Efficient Enterprise RAG applications with Intel Gaudi 2 and Intel Xeon</title><author>Julien Simon, Haihao Shen, Antony Vance Jeyaraj, Matrix Yao, Leon Lv, Greg Serochi, Deb Bharadwaj, Ke Ding</author><summary>Retrieval-augmented generation (RAG) enhances text generation with a large language model by incorporating fresh domain knowledge stored in an external datastore. Separating your company data from the knowledge learned by language models during training is essential to balance performance, accuracy, and security privacy goals. In this blog, you will learn how Intel can help you develop and deploy RAG applications as part of OPEA, the Open Platform for Enterprise AI. You will also discover how Intel Gaudi 2 AI accelerators and Xeon CPUs can significantly enhance enterprise performance through a real-world RAG use case.</summary><published>2024-05-09</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/leaderboard-hebrew</id><title>Introducing the Open Leaderboard for Hebrew LLMs!</title><author>Shaltiel Shmidman, Tal Geva, Omer Koren, Cl√©mentine Fourrier</author><summary>Leaderboard Metrics and Tasks

Technical Setup

Engage with Us

Sponsorship

This project addresses the critical need for advancement in Hebrew NLP.  As Hebrew is considered a low-resource language, existing LLM leaderboards often lack benchmarks that accurately reflect its unique characteristics. Today, we are excited to introduce a pioneering effort to change this narrative ‚Äî our new open LLM leaderboard, specifically designed to evaluate and enhance language models in Hebrew. Hebrew is a morphologically rich language with a complex system of roots and patterns. Words are built from roots with prefixes, suffixes, and infixes used to modify meaning, tense, or form plurals (among other functions). This complexity can lead to the existence of multiple valid word forms derived from a single root, making traditional tokenization strategies, designed for morphologically simpler languages, ineffective. As a result, existing language models may struggle to accurately process and understand the nuances of Hebrew, highlighting the need for benchmarks that cater to these unique linguistic properties.</summary><published>2024-05-05</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/leaderboard-artificial-analysis</id><title>Bringing the Artificial Analysis LLM Performance Leaderboard to Hugging Face</title><author>Micah Hill-Smith, George Cameron, Cl√©mentine Fourrier</author><summary>The LLM Performance Leaderboard
Metric coverage

Test Workloads

Methodology


Highlights (May 2024, see the leaderboard for the latest)

Use Case Example: Speed and Price can be as important as Quality

Get in touch

Building applications with LLMs requires considering more than just quality: for many use-cases, speed and price are equally or more important. For consumer applications and chat experiences, speed and responsiveness are critical to user engagement. Users expect near-instant responses, and delays can directly lead to reduced engagement. When building more complex applications involving tool use or agentic systems, speed and cost become even more important, and can become the limiting factor on overall system capability. The time taken by sequential requests to LLMs can quickly stack up for each user request adding to the cost.</summary><published>2024-05-03</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/asr-diarization</id><title>Powerful ASR + diarization + speculative decoding with Hugging Face Inference Endpoints</title><author>Sergei Petrov, Vaibhav Srivastav, Pedro Cuenca, Philipp Schmid</author><summary>The main modules

Set up your own endpoint
Deploy on Inference Endpoints

When to use an assistant model

Inference parameters

Payload


Recap

Whisper is one of the best open source speech recognition models and definitely the one most widely used. Hugging Face Inference Endpoints make it very easy to deploy any Whisper model out of the box. However, if you‚Äôd like to
introduce additional features, like a diarization pipeline to identify speakers, or assisted generation for speculative decoding, things get trickier. The reason is that you need to combine Whisper with additional models, while still exposing a single API endpoint. We'll solve this challenge using a custom inference handler, which will implement the Automatic Speech Recogniton (ASR) and Diarization pipeline on Inference Endpoints, as well as supporting speculative decoding. The implementation of the diarization pipeline is inspired by the famous Insanely Fast Whisper, and it uses a Pyannote model for diarization.</summary><published>2024-05-01</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/evaluation-structured-outputs</id><title>Improving Prompt Consistency with Structured Generations</title><author>Will Kurt, Remi Louf, Cl√©mentine Fourrier</author><summary>Context: Evaluation Sensitivity to Format Changes

What if we focused on the output, not the input, to make results more consistent across these small changes to format?
Note on the experimental setup: Focusing on n-shot and shot order


Initial Exploration: GSM8K 1-8 shot prompting

Diving Deeper: GPQA n-shot and shot order variations

Conclusion and Future Work

Recently, the Leaderboards and Evals research team at Hugging Face did small experiments, which highlighted how fickle evaluation can be. For a given task, results are extremely sensitive to minuscule changes in prompt format! However, this is not what we want: a model prompted with the same amount of information as input should output similar results. We discussed this with our friends at Dottxt, who had an idea - what if there was a way to increase consistency across prompt formats?</summary><published>2024-04-30</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/sc2-instruct</id><title>StarCoder2-Instruct: Fully Transparent and Permissive Self-Alignment for Code Generation</title><author>Yuxiang Wei, Federico Cassano, Jiawei Liu, Yifeng Ding, Naman Jain, Harm de Vries, Leandro von Werra, Arjun Guha, Lingming Zhang</author><summary>Method
Collecting seed code snippets

Self-OSS-Instruct

Response self-validation


Evaluation

Conclusion
Resources

Citation


Instruction tuning is an approach of fine-tuning that gives large language models (LLMs) the capability to follow natural and human-written instructions. However, for programming tasks, most models are tuned on either human-written instructions (which are very expensive) or instructions generated by huge and proprietary LLMs (which may not be permitted). We introduce StarCoder2-15B-Instruct-v0.1, the very first entirely self-aligned code LLM trained with a fully permissive and transparent pipeline. Our open-source pipeline uses StarCoder2-15B to generate thousands of instruction-response pairs, which are then used to fine-tune StarCoder-15B itself without any human annotations or distilled data from huge and proprietary LLMs. StarCoder2-15B-Instruct achieves a 72.6 HumanEval score, even surpassing the 72.0 score of CodeLlama-70B-Instruct! Further evaluation on LiveCodeBench shows that the self-aligned model is even better than the same model trained on data distilled from GPT-4, implying that an LLM could learn more effectively from data within its own distribution than a shifted distribution from a teacher LLM.</summary><published>2024-04-29</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/leaderboard-cot</id><title>Introducing the Open Chain of Thought Leaderboard</title><author>Gregor Betz, Sebastian Cacean, Cl√©mentine Fourrier, Kyle Richardson</author><summary>What‚Äôs the motivation behind such a leaderboard for chain-of-thought?

Which tasks are used?

How are chain-of-thought traces generated?

What are the main take-aways so far?

What are the next steps? ‚Äì And how to contribute.

Chain-of-thought prompting is emerging as a powerful and effective design pattern for LLM-based apps and agents. The basic idea of chain-of-thought prompting is to let a model generate a step-by-step solution (‚Äúreasoning trace‚Äù) before answering a question or taking a decision. With the Open CoT Leaderboard we‚Äôre tracking LLMs‚Äô ability to generate effective chain-of-thought traces for challenging reasoning tasks. Unlike most performance based leaderboards, we‚Äôre not scoring the absolute accuracy a model achieves on a given task, but the difference between the accuracy with and without chain-of-thought prompting:</summary><published>2024-04-23</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/jat</id><title>Jack of All Trades, Master of Some, a Multi-Purpose Transformer Agent</title><author>Quentin Gallou√©dec, Edward Beeching, Cl√©ment ROMAC, Thomas Wolf</author><summary>We're excited to share Jack of All Trades (JAT), a project that aims to move in the direction of a generalist agent. The project started as an open reproduction of the Gato (Reed et al., 2022) work, which proposed to train a Transformer able to perform both vision-and-language and decision-making tasks. We thus started by building an open version of Gato‚Äôs dataset. We then trained multi-modal Transformer models on it, introducing several improvements over Gato for handling sequential data and continuous values. Overall, the project has resulted in:</summary><published>2024-04-22</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/llama3</id><title>Welcome Llama 3 - Meta's new open LLM</title><author>Philipp Schmid, Omar Sanseviero, Pedro Cuenca, Younes Belkada, Leandro von Werra</author><summary>Meta‚Äôs Llama 3, the next iteration of the open-access Llama family, is now released and available at Hugging Face. It's great to see Meta continuing its commitment to open AI, and we‚Äôre excited to fully support the launch with comprehensive integration in the Hugging Face ecosystem. Llama 3 comes in two sizes: 8B for efficient deployment and development on consumer-size GPU, and 70B for large-scale AI native applications. Both come in base and instruction-tuned variants. In addition to the 4 models, a new version of Llama Guard was fine-tuned on Llama 3 8B and is released as Llama Guard 2 (safety fine-tune).</summary><published>2024-04-18</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/leaderboard-medicalllm</id><title>The Open Medical-LLM Leaderboard: Benchmarking Large Language Models in Healthcare</title><author>Aaditya Ura, Pasquale Minervini, Cl√©mentine Fourrier</author><summary>Datasets, Tasks, and Evaluation Setup
MedQA

MedMCQA

PubMedQA

MMLU Subsets (Medicine and Biology)


Insights and Analysis

Submitting Your Model for Evaluation

What's next? Expanding the Open Medical-LLM Leaderboard

Credits and Acknowledgments

About Open Life Science AI

Citation Over the years, Large Language Models (LLMs) have emerged as a groundbreaking technology with immense potential to revolutionize various aspects of healthcare. These models, such as GPT-3, GPT-4 and Med-PaLM 2 have demonstrated remarkable capabilities in understanding and generating human-like text, making them valuable tools for tackling complex medical tasks and improving patient care. They have notably shown promise in various medical applications, such as medical question-answering (QA), dialogue systems, and text generation. Moreover, with the exponential growth of electronic health records (EHRs), medical literature, and patient-generated data, LLMs could help healthcare professionals extract valuable insights and make informed decisions.</summary><published>2024-04-19</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/gradio-reload</id><title>AI Apps in a Flash with Gradio's Reload Mode</title><author>Freddy Boulton</author><summary>What Does Reload Mode Do?

Why Did Gradio Build Its Own Reloader?

Building a Document Analyzer Application

Conclusion

In this post, I will show you how you can build a functional AI application quickly with Gradio's reload mode. But before we get to that, I want to explain what reload mode does and why Gradio implements its own auto-reloading logic. If you are already familiar with Gradio and want to get to building, please skip to the third section. To put it simply, it pulls in the latest changes from your source files without restarting the Gradio server. If that does not make sense yet, please continue reading.</summary><published>2024-04-16</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/leaderboard-livecodebench</id><title>Introducing the LiveCodeBench Leaderboard - Holistic and Contamination-Free Evaluation of Code LLMs</title><author>Naman Jain, Alex Gu, Tianjun Zhang, Wen-Ding Li, King Han, Fanjia Yan, Cl√©mentine Fourrier</author><summary>LiveCodeBench Scenarios and Evaluation

Preventing Benchmark Contamination

Findings

How to Submit?

How to contribute

We are excited to introduce the LiveCodeBench leaderboard, based on LiveCodeBench, a new benchmark developed by researchers from UC Berkeley, MIT, and Cornell for measuring LLMs‚Äô code generation capabilities. LiveCodeBench collects coding problems over time from various coding contest platforms, annotating problems with their release dates. Annotations are used to evaluate models on problem sets released in different time windows, allowing an ‚Äúevaluation over time‚Äù strategy that helps detect and prevent contamination. In addition to the usual code generation task, LiveCodeBench also assesses self-repair, test output prediction, and code execution, thus providing a more holistic view of coding capabilities required for the next generation of AI programming agents.</summary><published>2024-04-16</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/fhe-endpoints</id><title>Running Privacy-Preserving Inference on Hugging Face Endpoints</title><author>Benoit Chevallier-Mames</author><summary>Deploying a pre-compiled model

Using the Endpoint
Installing the client side

Running inferences

Adapting to your application or needs

Under the hood

Limits


Preparing your pre-compiled model

Pre-compiled models available today

Additional resources

Conclusion and next steps

This is a guest blog post by the Zama team. Zama is an open source cryptography company building state-of-the-art FHE solutions for blockchain and AI. Eighteen months ago, Zama started Concrete ML, a privacy-preserving ML framework with bindings to traditional ML frameworks such as scikit-learn, ONNX, PyTorch, and TensorFlow. To ensure privacy for users' data, Zama uses Fully Homomorphic Encryption (FHE), a cryptographic tool that allows to make direct computations over encrypted data, without ever knowing the private key.</summary><published>2024-04-16</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/ryght-case-study</id><title>Ryght‚Äôs Journey to Empower Healthcare and Life Sciences with Expert Support from Hugging Face</title><author>Andrew Reed, Johnny Crupi</author><summary>Who is Ryght?

Overcoming challenges, together
1. The need to quickly upskill a team and stay informed in a highly dynamic environment

2. Identifying the most [cost] effective ML approaches amidst the noisy sea of options

3. Requirement to develop performant solutions that emphasize security, privacy, and flexibility


Conclusion

This is a guest blog post by the Ryght team. Ryght is building an enterprise-grade generative AI platform tailored for the healthcare and life sciences sectors. Today is their official launch of Ryght Preview, now publicly available for all.</summary><published>2024-04-16</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/idefics2</id><title>Introducing Idefics2: A Powerful 8B Vision-Language Model for the community</title><author>Leo Tronchon, Hugo Lauren√ßon, Victor Sanh</author><summary>Training Data

Improvements over Idefics1

Getting Started with Idefics2

Resources

License

Acknowledgments

We are excited to release Idefics2, a general multimodal model that takes as input arbitrary sequences of texts and images, and generates text responses. It can answer questions about images, describe visual content, create stories grounded in multiple images, extract information from documents, and perform basic arithmetic operations. Idefics2 improves upon Idefics1: with 8B parameters, an open license (Apache 2.0), and enhanced OCR (Optical Character Recognition) capabilities, Idefics2 is a strong foundation for the community working on multimodality. Its performance on Visual Question Answering benchmarks is top of its class size, and competes with much larger models such as LLava-Next-34B and MM1-30B-chat. Idefics2 is also integrated in ü§ó Transformers from the get-go and therefore is straightforward to finetune for many multimodal applications. You can try out the models on the Hub right now! * w/ im. split: Following the strategy from SPHINX and LLaVa-NeXT, we allow for an optional sub-image splitting in 4.</summary><published>2024-04-15</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/vlms</id><title>Vision Language Models Explained</title><author>Merve Noyan, Edward Beeching</author><summary>What is a Vision Language Model?

Overview of Open-source Vision Language Models

Finding the right Vision Language Model
MMMU

MMBench


Technical Details

Using Vision Language Models with transformers

Fine-tuning Vision Language Models with TRL

Vision language models are models that can learn simultaneously from images and texts to tackle many tasks, from visual question answering to image captioning. In this post, we go through the main building blocks of vision language models: have an overview, grasp how they work, figure out how to find the right model, how to use them for inference and how to easily fine-tune them with the new version of trl released today! Vision language models are broadly defined as multimodal models that can learn from images and text. They are a type of generative models that take image and text inputs, and generate text outputs. Large vision language models have good zero-shot capabilities, generalize well, and can work with many types of images, including documents, web pages, and more. The use cases include chatting about images, image recognition via instructions, visual question answering, document understanding, image captioning, and others. Some vision language models can also capture spatial properties in an image. These models can output bounding boxes or segmentation masks when prompted to detect or segment a particular subject, or they can localize different entities or answer questions about their relative or absolute positions. There‚Äôs a lot of diversity within the existing set of large vision language models, the data they were trained on, how they encode images, and, thus, their capabilities.</summary><published>2024-04-11</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/google-cloud-model-garden</id><title>Making thousands of open LLMs bloom in the Vertex AI Model Garden</title><author>Philipp Schmid, Jeff Boudier</author><summary>A Collaboration for AI Builders

How it works - from the Hub

How it works - from Vertex Model Garden

We‚Äôre just getting started

Today, we are thrilled to announce the launch of Deploy on Google Cloud, a new integration on the Hugging Face Hub to deploy thousands of foundation models easily to Google Cloud using Vertex AI or Google Kubernetes Engine (GKE). Deploy on Google Cloud makes it easy to deploy open models as API Endpoints within your own Google Cloud account, either directly through Hugging Face model cards or within Vertex Model Garden, Google Cloud‚Äôs single place to discover, customize, and deploy a wide variety of models from Google and Google partners. Starting today, we are enabling the most popular open models on Hugging Face for inference powered by our production solution, Text Generation Inference. With Deploy on Google Cloud, developers can build production-ready Generative AI applications without managing infrastructure and servers, directly within their secure Google Cloud environment.</summary><published>2024-04-10</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/codegemma</id><title>CodeGemma - an official Google release for code LLMs</title><author>Pedro Cuenca, Omar Sanseviero, Vaibhav Srivastav, Philipp Schmid, Mishig Davaadorj, Loubna Ben Allal</author><summary>Table of contents

What is CodeGemma?
Evaluation Results

Prompt format


Using CodeGemma
Demo

Using¬†Transformers

A note on precision

Integration with Google Cloud


Integration with Inference Endpoints

Additional Resources

CodeGemma is a family of open-access versions of Gemma specialized in code, and we‚Äôre excited to collaborate with Google on its release to make it as accessible as possible.ü§ó CodeGemma comes in three flavors:</summary><published>2024-04-09</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/hugging-face-wiz-security-blog</id><title>Hugging Face partners with Wiz Research to Improve AI Security</title><author>Josef Fukano, Guillaume Salou, Michelle Habonneau, Adrien, Luc Georges, Nicolas Patry, Julien Chaumond</author><summary>Hugging Face Security

Open Source Security Collaboration and Tools for the Community

Security Best Practices for Open Source AI/ML users

Pickle Files - The Insecure Elephant in the Room

Closing remarks

We are pleased to announce that we are partnering with Wiz with the goal of improving security across our platform and the AI/ML ecosystem at large. Wiz researchers collaborated with Hugging Face on the security of our platform and shared their findings. Wiz is a cloud security company that helps their customers build and maintain software in a secure manner. Along with the publication of this research, we are taking the opportunity to highlight some related Hugging Face security improvements.</summary><published>2024-04-04</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/duckdb-nsql-7b</id><title>Text2SQL using Hugging Face Dataset Viewer API and Motherduck DuckDB-NSQL-7B</title><author>Andrea Soria, Till D√∂hmen, Sen Wu, Laurel Orr</author><summary>How to use the model

Hugging Face Dataset Viewer API for more than 120K datasets

Generate SQL queries from text instructions

Today, integrating AI-powered features, particularly leveraging Large Language Models (LLMs), has become increasingly prevalent across various tasks such as text generation, classification, image-to-text, image-to-image transformations, etc. Developers are increasingly recognizing these applications' potential benefits, particularly in enhancing core tasks such as scriptwriting, web development, and, now, interfacing with data. Historically, crafting insightful SQL queries for data analysis was primarily the domain of data analysts, SQL developers, data engineers, or professionals in related fields, all navigating the nuances of SQL dialect syntax. However, with the advent of AI-powered solutions, the landscape is evolving. These advanced models offer new avenues for interacting with data, potentially streamlining processes and uncovering insights with greater efficiency and depth.</summary><published>2024-04-04</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/setfit-optimum-intel</id><title>Blazing Fast SetFit Inference with ü§ó Optimum Intel on Xeon</title><author>Daniel Korat, Tom Aarsen, Oren Pereg, Moshe Wasserblat, Ella Charlaix, Abirami Prabhakaran</author><summary>Faster!

Step 1: Quantize the SetFit Model using ü§ó Optimum Intel
Prepare a Calibration Dataset

Run Quantization


Step 2: Benchmark Inference

Results

Summary

References

SetFit is a promising solution for a common modeling problem: how to deal with lack of labeled data for training. Developed with Hugging Face‚Äôs research partners at Intel Labs and the UKP Lab, SetFit is an efficient framework for few-shot fine-tuning of Sentence Transformers models. SetFit achieves high accuracy with little labeled data - for example, SetFit outperforms GPT-3.5 in 3-shot prompting and with 5 shot it also outperforms 3-shot GPT-4 on the Banking 77 financial intent dataset.</summary><published>2024-04-03</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/policy-blog</id><title>Public Policy at Hugging Face</title><author>Irene Solaiman, Yacine Jernite, Margaret Mitchell, Avijit Ghosh, Lucie-Aim√©e Kaffee</author><summary>Policy Materials

AI Policy at Hugging Face is a multidisciplinary and cross-organizational workstream. Instead of being part of a vertical communications or global affairs organization, our policy work is rooted in the expertise of our many researchers and developers, from Ethics and Society Regulars and the legal team to machine learning engineers working on healthcare, art, and evaluations. What we work on is informed by our Hugging Face community needs and experiences on the Hub. We champion responsible openness, investing heavily in ethics-forward research, transparency mechanisms, platform safeguards, and translate our lessons to policy.</summary><published>2024-04-08</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/cloudflare-workers-ai</id><title>Bringing serverless GPU inference to Hugging Face users</title><author>Philipp Schmid, Jeff Boudier, Rita Kozlov, Nikhil Kothari</author><summary>Generative AI for Developers

How it works

We‚Äôre just getting started

Today, we are thrilled to announce the launch of Deploy on Cloudflare Workers AI, a new integration on the Hugging Face Hub. Deploy on Cloudflare Workers AI makes using open models as a serverless API easy, powered by state-of-the-art GPUs deployed in Cloudflare edge data centers. Starting today, we are integrating some of the most popular open models on Hugging Face into Cloudflare Workers AI, powered by our production solutions, like Text Generation Inference. With Deploy on Cloudflare Workers AI, developers can build robust Generative AI applications without managing GPU infrastructure and servers and at a very low operating cost: only pay for the compute you use, not for idle capacity.</summary><published>2024-04-02</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/pollen-vision</id><title>Pollen-Vision: Unified interface for Zero-Shot vision models in robotics</title><author>Antoine Pirrone, Simon Le Goff, Rouanet</author><summary>The Core Models of Pollen-Vision

Get started in very few lines of code!

A robotics use case: grasping unknown objects in unconstrained environments

What‚Äôs next?

Try pollen-vision

This is a guest blog post by the Pollen Robotics team. We are the creators of Reachy, an open-source humanoid robot designed for manipulation in the real world. In the context of autonomous behaviors, the essence of a robot's usability lies in its ability to understand and interact with its environment. This understanding primarily comes from visual perception, which enables robots to identify objects, recognize people, navigate spaces, and much more.</summary><published>2024-03-25</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/noob_intro_transformers</id><title>Total noob‚Äôs intro to Hugging Face Transformers</title><author>Andrew Jardine</author><summary>What is Hugging Face Transformers?

What is a library?

What is the Hugging Face Hub?

What are Hugging Face Spaces?

What is a notebook?

Why do we need your credit card?

What is a docker template?

Why do I need to select a GPU Space Hardware?

What is !pip install?

If we are using Transformers, why do we need Pytorch too?

What is a Class?

Why do I need to import the Class again after installing Transformers?

What is an instruction tuned model?

What is an argument?

What is a Method?

What is a tokenizer?

Why do I need to decode?

Why does the output read like a story?

Welcome to "A Total Noob‚Äôs Introduction to Hugging Face Transformers," a guide designed specifically for those looking to understand the bare basics of using open-source ML. Our goal is to demystify what Hugging Face Transformers is and how it works, not to turn you into a machine learning practitioner, but to enable better understanding of and collaboration with those who are. That being said, the best way to learn is by doing, so we'll walk through a simple worked example of running Microsoft‚Äôs Phi-2 LLM in a notebook on a Hugging Face space. You might wonder, with the abundance of tutorials on Hugging Face already available, why create another? The answer lies in accessibility: most existing resources assume some technical background, including Python proficiency, which can prevent non-technical individuals from grasping ML fundamentals. As someone who came from the business side of AI, I recognize that the learning curve presents a barrier and wanted to offer a more approachable path for like-minded learners.</summary><published>2024-03-22</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/embedding-quantization</id><title>Binary and Scalar Embedding Quantization for Significantly Faster &amp; Cheaper Retrieval</title><author>Aamir Shakir, Tom Aarsen, SeanLee</author><summary>Table of Contents

Why Embeddings?
Embeddings may struggle to scale


Improving scalability
Binary Quantization

Scalar (int8) Quantization

Combining Binary and Scalar Quantization

Quantization Experiments

Influence of Rescoring

Performance Summarization

Demo

Try it yourself

Future work

Acknowledgments

Citation

Resources


We introduce the concept of embedding quantization and showcase their impact on retrieval speed, memory usage, disk space, and cost. We'll discuss how embeddings can be quantized in theory and in practice, after which we introduce a demo showing a real-life retrieval scenario of 41 million Wikipedia texts. Embeddings are one of the most versatile tools in natural language processing, supporting a wide variety of settings and use cases. In essence, embeddings are numerical representations of more complex objects, like text, images, audio, etc. Specifically, the objects are represented as n-dimensional vectors.</summary><published>2024-03-22</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/arena-lighthouz</id><title>Introducing the Chatbot Guardrails Arena</title><author>Sonali Pattnaik, Rohan Karan, Srijan Kumar, Cl√©mentine Fourrier</author><summary>Why Stress Test Privacy Guardrails?

The Arena

The Leaderboard

How is the Chatbot Guardrails Arena different from other Chatbot Arenas?

Taking Part in the Next Steps

With the recent advancements in augmented LLM capabilities, deployment of enterprise AI assistants (such as chatbots and agents) with access to internal databases is likely to increase; this trend could help with many tasks, from internal document summarization to personalized customer and employee support. However, data privacy of said databases can be a serious concern (see 1, 2 and 3) when deploying these models in production. So far, guardrails have emerged as the widely accepted technique to ensure the quality, security, and privacy of AI chatbots, but anecdotal evidence suggests that even the best guardrails can be circumvented with relative ease. Lighthouz AI is therefore launching the Chatbot Guardrails Arena in collaboration with Hugging Face, to stress test LLMs and privacy guardrails in leaking sensitive data.</summary><published>2024-03-21</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/phi2-intel-meteor-lake</id><title>A Chatbot on your Laptop: Phi-2 on Intel Meteor Lake</title><author>Julien Simon, Ella Charlaix, Ofir Zafrir, Igor Margulis, Guy Boudoukh, Moshe Wasserblat</author><summary>Why local LLM inference is desirable

Why local LLM inference is now possible

Intel Meteor Lake

The Microsoft Phi-2 model

Quantization with Intel OpenVINO and Optimum Intel

Conclusion

Because of their impressive abilities, large language models (LLMs) require significant computing power, which is seldom available on personal computers. Consequently, we have no choice but to deploy them on powerful bespoke AI servers hosted on-premises or in the cloud. What if we could run state-of-the-art open-source LLMs on a typical personal computer? Wouldn't we enjoy benefits like:</summary><published>2024-03-20</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/cosmopedia</id><title>Cosmopedia: how to create large-scale synthetic data for pre-training Large Language Models</title><author>Loubna Ben Allal, Anton Lozhkov, Daniel van Strien</author><summary>Why Cosmopedia?

Behind the scenes of Cosmopedia‚Äôs creation
Prompts curation

Technical stack


Conclusion &amp; next steps

References

In this blog post, we outline the challenges and solutions involved in generating a synthetic dataset with billions of tokens to replicate Phi-1.5, leading to the creation of Cosmopedia. Synthetic data has become a central topic in Machine Learning.  It refers to artificially generated data, for instance by large language models (LLMs), to mimic real-world data. Traditionally, creating datasets for supervised fine-tuning and instruction-tuning required the costly and time-consuming process of hiring human annotators. This practice entailed significant resources, limiting the development of such datasets to a few key players in the field. However, the landscape has recently changed. We've seen hundreds of high-quality synthetic fine-tuning datasets developed, primarily using GPT-3.5 and GPT-4. The community has also supported this development with numerous publications that guide the process for various domains, and address the associated challenges [1][2][3][4][5].</summary><published>2024-03-20</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/galore</id><title>GaLore: Advancing Large Model Training on Consumer-grade Hardware</title><author>Titus von Koeller, Jiawei Zhao, Matthew Douglas, Yaowei Zheng, Younes Belkada, Zachary Mueller, Amy Roberts, Sourab Mangrulkar, Benjamin Bossan</author><summary>Scaling LLMs with Consumer-Grade Hardware

Memory Efficiency in Optimizer States

Subspace Switching and Advanced Projection Techniques

Combining GaLore with 8-bit Optimizers

Implementation Details
Algorithmic Overview of 8-bit Optimization with GaLore


Use it with Hugging Face Transformers
Layer-wise Updates


Conclusion

Resources

The integration of GaLore into the training of large language models (LLMs) marks a significant advancement in the field of deep learning, particularly in terms of memory efficiency and the democratization of AI research. By allowing for the training of billion-parameter models on consumer-grade hardware, reducing memory footprint in optimizer states, and leveraging advanced projection matrix techniques, GaLore opens new horizons for researchers and practitioners with limited access to high-end computational resources. The capability of GaLore to facilitate the training of models with up to 7 billion parameters, such as those based on the Llama architecture, on consumer GPUs like the NVIDIA RTX 4090, is groundbreaking. This is achieved by significantly reducing the memory requirements traditionally associated with optimizer states and gradients during the training process. The approach leverages the inherent low-rank structure of gradients in deep neural networks, applying a projection that reduces the dimensionality of the data that needs to be stored and manipulated.</summary><published>2024-03-20</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/train-dgx-cloud</id><title>Easily Train Models with H100 GPUs on NVIDIA DGX Cloud</title><author>Philipp Schmid, Jeff Boudier, Rafael Pierre, Abhishek Thakur</author><summary>GPU Poor No More

How it works

Pricing for Train on DGX Cloud

We‚Äôre just getting started

Today, we are thrilled to announce the launch of Train on DGX Cloud, a new service on the Hugging Face Hub, available to Enterprise Hub organizations. Train on DGX Cloud makes it easy to use open models with the accelerated compute infrastructure of NVIDIA DGX Cloud. Together, we built Train on DGX Cloud so that Enterprise Hub users can easily access the latest NVIDIA H100 Tensor Core GPUs, to fine-tune popular Generative AI models like Llama, Mistral, and Stable Diffusion, in just a few clicks within the Hugging Face Hub. This new experience expands upon the strategic partnership we announced last year to simplify the training and deployment of open Generative AI models on NVIDIA accelerated computing. One of the main problems developers and organizations face is the scarcity of GPU availability, and the time-consuming work of writing, testing, and debugging training scripts for AI models. Train with DGX Cloud offers an easy solution to these challenges, providing instant access to NVIDIA GPUs, starting with H100 on NVIDIA DGX Cloud.  In addition, Train with DGX Cloud offers a simple no-code training job creation experience powered by Hugging Face AutoTrain and Hugging Face Spaces.</summary><published>2024-03-18</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/quanto-introduction</id><title>quanto: a pytorch quantization toolkit</title><author>David Corvoysier, Younes Belkada, Marc Sun</author><summary>Quantization workflow

Performance

Integration in transformers

Contributing to quanto

Quantization is a technique to reduce the computational and memory costs of evaluating Deep Learning Models by representing their weights and activations with low-precision data types like 8-bit integer (int8) instead of the usual 32-bit floating point (float32). Reducing the number of bits means the resulting model requires less memory storage, which is crucial for deploying Large Language Models on consumer devices.
It also enables specific optimizations for lower bitwidth datatypes, such as int8 or float8 matrix multiplications on CUDA devices.</summary><published>2024-03-18</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/intel-fast-embedding</id><title>CPU Optimized Embeddings with ü§ó Optimum Intel and fastRAG</title><author>Peter Izsak, Moshe Berchansky, Daniel Fleischer, Ella Charlaix, Morgan Funtowicz, Moshe Wasserblat</author><summary>Information Retrieval with Embedding Models
Embedding models and RAG


Optimizing Embedding Models with Optimum Intel and IPEX

Example: Optimizing BGE Embedding Models
BGE Technical Details

Step-by-step: Optimization by Quantization

Model Evaluation with MTEB

Speed and Latency

How did we run the evaluation?

Latency performance

Throughput Performance


Optimized Embedding Models with fastRAG
Fast indexing using the optimized Retriever

Reranking using the Optimized Ranker


Embedding models are useful for many applications such as retrieval, reranking, clustering, and classification. The research community has witnessed significant advancements in recent years in embedding models, leading to substantial enhancements in all applications building on semantic representation. Models such as BGE, GTE, and E5 are placed at the top of the MTEB benchmark and in some cases outperform proprietary embedding services. There are a variety of model sizes found in Hugging Face's Model hub, from lightweight (100-350M parameters) to 7B models (such as Salesforce/SFR-Embedding-Mistral). The lightweight models based on an encoder architecture are ideal candidates for optimization and utilization on CPU backends running semantic search-based applications, such as Retrieval Augmented Generation (RAG). In this blog, we will show how to unlock significant performance boost on Xeon based CPUs, and show how easy it is to integrate optimized models into existing RAG pipelines using fastRAG.</summary><published>2024-03-15</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/websight</id><title>Unlocking the conversion of Web Screenshots into HTML Code with the WebSight Dataset</title><author>Hugo Lauren√ßon, Leo Tronchon, Victor Sanh</author><summary>The challenge

WebSight: A large synthetic dataset of screenshot/HTML code pairs

Sightseer: A model fine-tuned on WebSight

Towards more powerful tools unlocked by visual language models

Resources

In the world of web development, turning designs into functional websites usually involves a lot of coding and careful testing. What if we could simplify this process, making it possible to convert web designs into working websites more easily and quickly? WebSight is a new dataset that aims at building AI systems capable of transforming screenshots to HTML code. Turning a website design or screenshot into HTML code usually needs an experienced developer. But what if this could be more efficient? Motivated by this question, we investigated how vision-language models (VLMs) could be used in web development to create low-code solutions that improve efficiency.</summary><published>2024-03-15</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/leaderboard-contextual</id><title>Introducing ConTextual: How well can your Multimodal model jointly reason over text and image in text-rich scenes?</title><author>Rohan Wadhawan, Hritik Bansal, Kai-Wei Chang, NANYUN (Violet) PENG, Cl√©mentine Fourrier</author><summary>What is ConTextual

Experiments

Key Takeaways!

What‚Äôs next?

How to Submit?
Validation Set Submission

Test Set Submission


Models are becoming quite good at understanding text on its own, but what about text in images, which gives important contextual information? For example, navigating a map, or understanding a meme? The ability to reason about the interactions between the text and visual context in images can power many real-world applications, such as AI assistants, or tools to assist the visually impaired. We refer to these tasks as "context-sensitive text-rich visual reasoning tasks".</summary><published>2024-03-05</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/community-datasets</id><title>Data is better together</title><author>Daniel van Strien, Daniel Vila</author><summary>Data remains essential for better models

Why build datasets collectively?
Making it easy for people to contribute


Join our first cohort of communities who want to build better datasets together!

What types of projects are we looking for?

Recently, Argilla and Hugging Face launched Data is Better Together, an experiment to collectively build a preference dataset of prompt rankings. In a few days, we had: See the progress dashboard for the latest stats!</summary><published>2024-03-04</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/textgen-pipe-gaudi</id><title>Text-Generation Pipeline on Intel¬Æ Gaudi¬Æ 2 AI Accelerator</title><author>Siddhant Jagtap</author><summary>Prerequisites

Using the Pipeline

Usage in Python Scripts

LangChain Compatibility

Conclusion

With the Generative AI (GenAI) revolution in full swing, text-generation with open-source transformer models like Llama 2 has become the talk of the town. AI enthusiasts as well as developers are looking to leverage the generative abilities of such models for their own use cases and applications. This article shows how easy it is to generate text with the Llama 2 family of models (7b, 13b and 70b) using Optimum Habana and a custom pipeline class ‚Äì you'll be able to run the models with just a few lines of code! This custom pipeline class has been designed to offer great flexibility and ease of use. Moreover, it provides a high level of abstraction and performs end-to-end text-generation which involves pre-processing and post-processing. There are multiple ways to use the pipeline - you can run the run_pipeline.py script from the Optimum Habana repository, add the pipeline class to your own python scripts, or initialize LangChain classes with it.</summary><published>2024-02-29</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/starcoder2</id><title>StarCoder2 and The Stack v2</title><author>Leandro von Werra, Loubna Ben Allal, Anton Lozhkov, Nouamane Tazi</author><summary>What is StarCoder2?

What is The Stack v2?

About BigCode

Links
Models

Data &amp; Governance

Others


BigCode is releasing StarCoder2, the next generation of transparently trained open code LLMs. All StarCoder2 variants were trained on The Stack v2, a new large and high-quality code dataset. We release all models, datasets, and the processing as well as the training code. Check out the paper for details. StarCoder2 is a family of open LLMs for code and comes in 3 different sizes with 3B, 7B and 15B parameters. The flagship StarCoder2-15B model is trained on over 4 trillion tokens and 600+ programming languages from The Stack v2. All models use Grouped Query Attention, a context window of 16,384 tokens with a sliding window attention of 4,096 tokens, and were trained using the Fill-in-the-Middle objective.</summary><published>2024-02-28</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/arena-tts</id><title>TTS Arena: Benchmarking Text-to-Speech Models in the Wild</title><author>mrfakename, Vaibhav Srivastav, Cl√©mentine Fourrier, Lucain Pouget, Yoach Lacombe, Main Horse, Sanchit Gandhi</author><summary>Motivation

The TTS Arena

Selected Models

The TTS Leaderboard

Conclusion

Credits

Automated measurement of the quality of text-to-speech (TTS) models is very difficult. Assessing the naturalness and inflection of a voice is a trivial task for humans, but it is much more difficult for AI. This is why today, we‚Äôre thrilled to announce the TTS Arena. Inspired by LMSys's Chatbot Arena for LLMs, we developed a tool that allows anyone to easily compare TTS models side-by-side. Just submit some text, listen to two different models speak it out, and vote on which model you think is the best. The results will be organized into a leaderboard that displays the community‚Äôs highest-rated models. The field of speech synthesis has long lacked an accurate method to measure the quality of different models. Objective metrics like WER (word error rate) are unreliable measures of model quality, and subjective measures such as MOS (mean opinion score) are typically small-scale experiments conducted with few listeners. As a result, these measurements are generally not useful for comparing two models of roughly similar quality. To address these drawbacks, we are inviting the community to rank models in an easy-to-use interface. By opening this tool and disseminating results to the public, we aim to democratize how models are ranked and to make model comparison and selection accessible to everyone.</summary><published>2024-02-27</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/watermarking</id><title>AI Watermarking 101: Tools and Techniques</title><author>Sasha Luccioni, Yacine Jernite, Derek Thomas, Emily Witko, Ezi Ozoani, Josef Fukano, Vaibhav Srivastav, Brigitte Tousignant, Margaret Mitchell</author><summary>What is watermarking and how does it work?
Data Poisoning and Signing Techniques

Open vs Closed Watermarks


Watermarking Different Types of Data
Watermarking Images

Watermarking Text

Watermarking Audio


Conclusion

Relevant press stories

In recent months, we've seen multiple news stories involving ‚Äòdeepfakes‚Äô, or AI-generated content: from images of Taylor Swift to videos of Tom Hanks and recordings of US President Joe Biden. Whether they are selling products, manipulating images of people without their consent, supporting phishing for private information, or creating misinformation materials intended to mislead voters, deepfakes are increasingly being shared on social media platforms. This enables them to be quickly propagated and have a wider reach and therefore, the potential to cause long-lasting damage. In this blog post, we will describe approaches to carry out watermarking of AI-generated content, discuss their pros and cons, and present some of the tools available on the Hugging Face Hub for adding/detecting watermarks.</summary><published>2024-02-26</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/gemma-peft</id><title>Fine-Tuning Gemma Models in Hugging Face</title><author>Vaibhav Singh, Jiewen Tan, Younes Belkada, Arthur Zucker</author><summary>Why PEFT?

PyTorch on GPU and TPU

Low-Rank Adaptation for Large Language Models

Before we begin

Learning to quote

Accelerate with FSDP via SPMD on TPU

Next Steps

We recently announced that Gemma, the open weights language model from Google Deepmind, is available for the broader open-source community via Hugging Face. It‚Äôs available in 2 billion and 7 billion parameter sizes with pretrained and instruction-tuned flavors. It‚Äôs available on Hugging Face, supported in TGI, and easily accessible for deployment and fine-tuning in the Vertex Model Garden and Google Kubernetes Engine. The Gemma family of models also happens to be well suited for prototyping and experimentation using the free GPU resource available via Colab. In this post we will briefly review how you can do Parameter Efficient FineTuning (PEFT) for Gemma models, using the Hugging Face Transformers and PEFT libraries on GPUs and Cloud TPUs for anyone who wants to fine-tune Gemma models on their own dataset.</summary><published>2024-02-23</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/leaderboard-haizelab</id><title>Introducing the Red-Teaming Resistance Leaderboard</title><author>Steve Li, Richard, Leonard Tang, Cl√©mentine Fourrier</author><summary>Measuring Robustness to Realistic, Human-Like Attacks

Red-Teaming Resistance Datasets

Robustness by Violation Category
Harm and Violence

Criminal Conduct

Unsolicited Counsel

NSFW


Insights from the RTR Leaderboard

Content warning: since this blog post is about a red-teaming leaderboard (testing elicitation of harmful behavior in LLMs), some users might find the content of the related datasets or examples unsettling. LLM research is moving fast. Indeed, some might say too fast.</summary><published>2024-02-23</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/matryoshka</id><title>ü™Ü Introduction to Matryoshka Embedding Models</title><author>Tom Aarsen, Joshua, Omar Sanseviero</author><summary>Table of Contents

Understanding Embeddings

ü™Ü Matryoshka Embeddings

ü™Ü Matryoshka Dolls

Why would you use ü™Ü Matryoshka Embedding models?

How are ü™Ü Matryoshka Embedding models trained?
Theoretically

In Sentence Transformers


How do I use ü™Ü Matryoshka Embedding models?
Theoretically

In Sentence Transformers


Results

Demo

References

In this blogpost, we will introduce you to the concept of Matryoshka Embeddings and explain why they are useful. We will discuss how these models are theoretically trained and how you can train them using Sentence Transformers. Additionally, we will provide practical guidance on how to use Matryoshka Embedding models and share a comparison between a Matryoshka embedding model and a regular embedding model. Finally, we invite you to check out our interactive demo that showcases the power of these models.</summary><published>2024-02-23</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/fetch-eap-case-study</id><title>Fetch Consolidates AI Tools and Saves 30% Development Time with Hugging Face on AWS</title><author>Violette</author><summary>Executive Summary

Fetch Needed a Scalable Way to Train AI Faster

Hugging Face Opens Up the Black Box

Fetch Grows AI Expertise, Cuts Latency by 50%, and Saves Costs

If you need support in using Hugging Face and AWS, please get in touch with us here - our team will contact you to discuss your requirements! Fetch, a consumer rewards company, developed about 15 different AI tools to help it receive, route, read, process, analyze, and store receipts uploaded by users. The company has more than 18 million active monthly users for its shopping rewards app. Fetch wanted to rebuild its AI-powered platform and, using Amazon Web Services (AWS) and with the support of AWS Partner Hugging Face, moved from using third-party applications to developing its own tools to gain better insights about customers. Consumers scan receipts ‚Äîor forward electronic receipts‚Äî to receive rewards points for their purchases. Businesses can offer special rewards to users, such as extra points for purchasing a particular product. The company can now process more than 11 million receipts per day faster and gets better data.</summary><published>2023-02-23</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/gemma</id><title>Welcome Gemma - Google's new open LLM</title><author>Philipp Schmid, Omar Sanseviero, Pedro Cuenca</author><summary>Table of contents

What is Gemma?
Prompt format

Exploring the Unknowns


Demo
Using ü§ó¬†Transformers

JAX Weights


Integration with Google Cloud

Integration with Inference Endpoints

Fine-tuning with ü§ó¬†TRL

Additional Resources

Acknowledgments

An update to the Gemma models was released two months after this post, see the latest versions in this collection. Gemma, a new family of state-of-the-art open LLMs, was released today by Google! It's great to see Google reinforcing its commitment to open-source AI, and we‚Äôre excited to fully support the launch with comprehensive integration in Hugging Face.</summary><published>2024-02-21</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/leaderboard-upstage</id><title>Introducing the Open Ko-LLM Leaderboard: Leading the Korean LLM Evaluation Ecosystem</title><author>Chanjun Park, Sung Kim, Cl√©mentine Fourrier</author><summary>Leaderboard design choices: creating a new private test set for fairness

Evaluation Tasks

A leaderboard in action: the barometer of Ko-LLM

Our vision and next steps

Many thanks to our partners

In the fast-evolving landscape of Large Language Models (LLMs), building an ‚Äúecosystem‚Äù has never been more important. This trend is evident in several major developments like Hugging Face's democratizing NLP and Upstage building a Generative AI ecosystem. Inspired by these industry milestones, in September of 2023, at Upstage we initiated the Open Ko-LLM Leaderboard. Our goal was to quickly develop and introduce an evaluation ecosystem for Korean LLM data, aligning with the global movement towards open and collaborative AI development.</summary><published>2024-02-20</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/peft_merging</id><title>ü§ó PEFT welcomes new merging methods</title><author>Sourab Mangrulkar, Sayak Paul</author><summary>Methods for combining/merging LoRA adapters
Concatenation (cat)

Linear/Task Arithmetic (linear)

SVD (svd)

TIES (ties , ties_svd )

DARE (dare_linear , dare_ties , dare_linear_svd , dare_ties_svd )

Magnitude Prune (magnitude_prune , magnitude_prune_svd )


How do I merge my LoRA adapters?

Extending to text-to-image generation

Observations

Acknowledgements

Useful links

Citations

Model merging has quickly become the de-facto standard of pushing the performance limits of large language models. On the Open LLM Leaderboard, we continue to notice merged models topping up the charts. Our very own Omar Sanseviero, made a little sprint on model merging and discovered interesting findings. The typical way of model merging, so far, has been to take a set of models and merge them. This post gives a nice primer on this topic. Generally, for merging multiple models, we first download their checkpoints and then perform merging. Depending on the merge algorithm and the sizes of the underlying model, this process can be quite memory-intensive. The mergekit library provides optimized ways for handling this, making the process manageable on limited memory.</summary><published>2024-02-19</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/synthetic-data-save-costs</id><title>Synthetic data: save money, time and carbon with open source</title><author>Moritz Laurer</author><summary>Should you fine-tune your own model or use an LLM API? Creating your own model puts you in full control but requires expertise in data collection, training, and deployment. LLM APIs are much easier to use but force you to send your data to a third party and create costly dependencies on LLM providers. This blog post shows how you can combine the convenience of LLMs with the control and efficiency of customized models. In a case study on identifying investor sentiment in the news, we show how to use an open-source LLM to create synthetic data to train your customized model in a few steps. Our resulting custom RoBERTa model can analyze a large news corpus for around $2.7 compared to $3061 with GPT4; emits around 0.12 kg CO2 compared to very roughly 735 to 1100 kg CO2 with GPT4; with a latency of 0.13 seconds compared to often multiple seconds with GPT4; while performing on par with GPT4 at identifying investor sentiment (both 94% accuracy and 0.94 F1 macro). We provide reusable notebooks, which you can apply to your own use cases.</summary><published>2024-02-16</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/amd_pervasive_developer_ai_contest</id><title>AMD Pervasive AI Developer Contest!</title><author>Guruprasad MP</author><summary>AMD + Hugging Face Collaboration

Sign Up Today

AMD and Hugging Face are actively engaged in helping developers seamlessly deploy cutting-edge AI models on AMD hardware. 
This year, AMD takes their commitment one step further by providing developers free, hands-on access to state-of-the-art AMD hardware through their recently announced Pervasive AI Developer Contest. 
This global competition is an incubator of AI innovation, beckoning developers worldwide to create unique AI applications.
Developers can choose from three exciting categories: Generative AI, Robotics AI, and PC AI, each of them entitled to cash prices up to $10,000 USD for winners, with a total of $160,000 USD being given away.
700 AMD platforms are up for grabs to eligible participants. 
Don‚Äôt miss your chance to receive an AMD Radeon ‚Ñ¢ PRO W7900, AMD Kria ‚Ñ¢ KR260 Robotics Starter Kit, Ryzen ‚Ñ¢ AI powered PC or cloud access to an AMD Instinct ‚Ñ¢ MI210 accelerator card. For those focusing on large language model development, Hugging Face and AMD have made significant strides to provide out-of-the-box support on AMD GPUs. 
Our combined efforts include the ability to run HF transformer models without the need for code modifications allowing for seamless operation. 
On top of native support, additional acceleration tools like ONNX models execution on ROCm-powered GPU, Optimum-Benchmark, DeepSpeed for ROCm-powered GPUs using Transformers, GPTQ, TGI and more are supported.
Additionally, for those applying for the PC AI contest category to develop on AMD Ryzen AI Powered PCs, we are continuously growing our pre-trained model zoo to support a wide variety of models enabling developers to get started in building AI applications swiftly.</summary><published>2024-02-14</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/tgi-messages-api</id><title>From OpenAI to Open LLMs with Messages API</title><author>Andrew Reed, Philipp Schmid, Joffrey THOMAS, David Holtz</author><summary>Create an Inference Endpoint

Using Inference Endpoints with OpenAI client libraries
With the Python client

With the JavaScript client


Integrate with LangChain and LlamaIndex
How to use with LangChain

How to use with LlamaIndex


Cleaning up

Conclusion

We are excited to introduce the Messages API to provide OpenAI compatibility with Text Generation Inference (TGI) and Inference Endpoints. Starting with version 1.4.0, TGI offers an API compatible with the OpenAI Chat Completion API. The new Messages API allows customers and users to transition seamlessly from OpenAI models to open LLMs. The API can be directly used with OpenAI's client libraries or third-party tools, like LangChain or LlamaIndex.</summary><published>2024-02-08</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/segmoe</id><title>SegMoE: Segmind Mixture of Diffusion Experts</title><author>Yatharth Gupta, Vishnu V Jaddipal, Harish Prabhala</author><summary>Table of Contents

What is SegMoE?
About the name


Inference
Samples

Using ü§ó Diffusers

Using a Local Model


Comparison

Creating your Own SegMoE
Push to Hub


Disclaimers and ongoing work

Conclusion

Additional Resources

SegMoE is an exciting framework for creating Mixture-of-Experts Diffusion models from scratch! SegMoE is comprehensively integrated within the Hugging Face ecosystem and comes supported with diffusers üî•! Among the features and integrations being released today:</summary><published>2024-02-03</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/leaderboard-nphardeval</id><title>NPHardEval Leaderboard: Unveiling the Reasoning Abilities of Large Language Models through Complexity Classes and Dynamic Updates</title><author>Lizhou Fan, Wenyue Hua, Haoyang Ling, Cl√©mentine Fourrier</author><summary>A Unique Approach to LLM Evaluation

Data Synthesis

Evaluation Metrics
Weighted Accuracy (WA)

Failure Rate (FR)


Experimentation and Insights

Reproducing NPHardEval Benchmark results on your machine

Join the Conversation

We're happy to introduce the NPHardEval leaderboard, using NPHardEval, a cutting-edge benchmark developed by researchers from the University of Michigan and Rutgers University. NPHardEval introduces a dynamic, complexity-based framework for assessing Large Language Models' (LLMs) reasoning abilities. It poses 900 algorithmic questions spanning the NP-Hard complexity class and lower, designed to rigorously test LLMs, and is updated on a monthly basis to prevent overfitting!</summary><published>2024-02-02</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/constitutional_ai</id><title>Constitutional AI with Open LLMs</title><author>Shengyi Costa Huang, Lewis Tunstall, Edward Beeching, Leandro von Werra, Omar Sanseviero, Kashif Rasul, Thomas Wolf</author><summary>Constitutional AI: learn to self-align

Mistral 7B Instruct: an amazingly helpful model
Gathering prompts to generate AI preference data


Enter llm-swarm: scalable text generation on a Slurm cluster

Generating a CAI dataset
Training a Constitutional AI chat model


Oh, honey, let's not go down that road ‚Äî a different safety style

Conclusion

Acknowledgement

Bibtex

Since the launch of ChatGPT in 2022, we have seen tremendous progress in LLMs, ranging from the release of powerful pretrained models like Llama 2 and Mixtral, to the development of new alignment techniques like Direct Preference Optimization. However, deploying LLMs in consumer applications poses several challenges, including the need to add guardrails that prevent the model from generating undesirable responses. For example, if you are building an AI tutor for children, then you don‚Äôt want it to generate toxic answers or teach them to write scam emails! To align these LLMs according to a set of values, researchers at Anthropic have proposed a technique called Constitutional AI (CAI), which asks the models to critique their outputs and self-improve according to a set of user-defined principles. This is exciting because the practitioners only need to define the principles instead of having to collect expensive human feedback to improve the model.</summary><published>2024-02-01</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/text-generation-inference-on-inferentia2</id><title>Hugging Face Text Generation Inference available for AWS Inferentia2</title><author>Philipp Schmid, David Corvoysier</author><summary>Deploy Zephyr 7B on AWS Inferentia2 using Amazon SageMaker
1. Setup development environment

2. Retrieve TGI Neuronx Image

4. Deploy Zephyr 7B to Amazon SageMaker

5. Run inference and chat with the model

6. Clean up


Conclusion

We are excited to announce the general availability of Hugging Face Text Generation Inference (TGI) on AWS Inferentia2 and Amazon SageMaker. Text Generation Inference (TGI), is a purpose-built solution for deploying and serving Large Language Models (LLMs) for production workloads at scale. TGI enables high-performance text generation using Tensor Parallelism and continuous batching for the most popular open LLMs, including Llama, Mistral, and more. Text Generation Inference is used in production by companies such as Grammarly, Uber, Deutsche Telekom, and many more.</summary><published>2024-02-01</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/patchtst</id><title>Patch Time Series Transformer in Hugging Face</title><author>Nam Nguyen, Wesley M. Gifford, Arindam Jati, Vijay Ekambaram, Kashif Rasul</author><summary>Quick overview of PatchTST

Installation

Part 1: Forecasting on the Electricity dataset
Set seed

Load and prepare datasets

Configure the PatchTST model

Train model

Evaluate the model on the test set of the source domain

Save model


Part 2: Transfer Learning from Electricity to ETTh1
Transfer learning on ETTh1 data.

Load ETTh dataset

Zero-shot forecasting on ETTH

Linear probing on ETTh1

Full fine-tune on ETTh1


Summary

In this blog, we provide examples of how to get started with PatchTST. We first demonstrate the forecasting capability of PatchTST on the Electricity data. We will then demonstrate the transfer learning capability of PatchTST by using the previously trained model to do zero-shot forecasting on the electrical transformer (ETTh1) dataset. The zero-shot forecasting
 performance will denote the test performance of the model in the target domain, without any
 training on the target domain. Subsequently, we will do linear probing and (then) finetuning of
 the pretrained model on the train part of the target data, and will validate the forecasting
 performance on the test part of the target data. The PatchTST model was proposed in A Time Series is Worth 64 Words: Long-term Forecasting with Transformers by Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, Jayant Kalagnanam and presented at ICLR 2023.</summary><published>2024-02-01</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/leaderboard-patronus</id><title>Introducing the Enterprise Scenarios Leaderboard: a Leaderboard for Real World Use Cases</title><author>Selvan Sunitha Ravi, Rebecca Qian, Anand Kannappan, Cl√©mentine Fourrier</author><summary>Why do we need a leaderboard for real world use cases?

Our Tasks

Submitting to the Leaderboard

How to view your results on the validation set

Today, the Patronus team is excited to announce the new Enterprise Scenarios Leaderboard, built using the Hugging Face Leaderboard Template in collaboration with their teams. The leaderboard aims to evaluate the performance of language models on real-world enterprise use cases. We currently support 6 diverse tasks - FinanceBench, Legal Confidentiality, Creative Writing, Customer Support Dialogue, Toxicity, and Enterprise PII.</summary><published>2024-01-31</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/intel-starcoder-quantization</id><title>Accelerate StarCoder with ü§ó Optimum Intel on Xeon: Q8/Q4 and Speculative Decoding</title><author>Ofir Zafrir, Ella Charlaix, Igor Margulis, Daniel Korat, Jonathan Mamou, Guy Boudoukh, Oren Pereg, Moshe Wasserblat, Haihao Shen, Ahmad Yasin, FanZhao</author><summary>Recently, code generation models have become very popular, especially with the release of state-of-the-art open-source models such as BigCode‚Äôs StarCoder and Meta AI‚Äôs Code Llama. A growing number of works focuses on making Large Language Models (LLMs) more optimized and accessible. In this blog, we are happy to share the latest results of LLM optimization on Intel Xeon focusing on the popular code generation LLM, StarCoder. The StarCoder Model is a cutting-edge LLM specifically designed for assisting the user with various coding tasks such as code completion, bug fixing, code summarization, and even generating code snippets from natural language descriptions. The StarCoder model is a member of the StarCoder family which includes the StarCoderBase variant as well. These Large Language Models for Code (Code LLMs) are trained on permissively licensed data from GitHub, including over 80 programming languages, Git commits, GitHub issues, and Jupyter notebooks. In this work we show more than 7x inference acceleration of StarCoder-15B model on Intel 4th generation Xeon by integrating 8bit and 4bit quantization with assisted generation.</summary><published>2024-01-30</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/leaderboard-hallucinations</id><title>The Hallucinations Leaderboard, an Open Effort to Measure Hallucinations in Large Language Models</title><author>Pasquale Minervini, Ping Nie, Cl√©mentine Fourrier, Rohit Saxena, Aryo Pradipta Gema, Xuanli He</author><summary>What are Hallucinations?

The Hallucinations Leaderboard

A glance at the results so far
Closed-book Open-Domain Question Answering

Instruction Following

Summarisation

Reading Comprehension

Hallucination Detection


Wrapping up
Citing


In the rapidly evolving field of Natural Language Processing (NLP), Large Language Models (LLMs) have become central to AI's ability to understand and generate human language. However, a significant challenge that persists is their tendency to hallucinate ‚Äî i.e., producing content that may not align with real-world facts or the user's input. With the constant release of new open-source models, identifying the most reliable ones, particularly in terms of their propensity to generate hallucinated content, becomes crucial. The Hallucinations Leaderboard aims to address this problem: it is a comprehensive platform that evaluates a wide array of LLMs against benchmarks specifically designed to assess hallucination-related issues via in-context learning.</summary><published>2024-01-29</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/leaderboard-decodingtrust</id><title>An Introduction to AI Secure LLM Safety Leaderboard</title><author>Chenhui Zhang, Chulin Xie, Mintong Kang, Chejian Xu, Bo Li</author><summary>Red-teaming Evaluation

Some key findings from our paper

How to submit your model for evaluation

Citation

Given the widespread adoption of LLMs, it is critical to understand their safety and risks in different scenarios before extensive deployments in the real world. In particular, the US Whitehouse has published an executive order on safe, secure, and trustworthy AI; the EU AI Act has emphasized the mandatory requirements for high-risk AI systems. Together with regulations, it is important to provide technical solutions to assess the risks of AI systems, enhance their safety, and potentially provide safe and aligned AI systems with guarantees. Thus, in 2023, at Secure Learning Lab, we introduced DecodingTrust, the first comprehensive and unified evaluation platform dedicated to assessing the trustworthiness of LLMs. (This work won the Outstanding Paper Award at NeurIPS 2023.)</summary><published>2024-01-26</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/gcp-partnership</id><title>Hugging Face and Google partner for open AI collaboration</title><author>Jeff Boudier, Philipp Schmid</author><summary>A collaboration for open science

A collaboration for open source

A collaboration for Google Cloud customers

A collaboration for Hugging Face Hub users

What‚Äôs next At Hugging Face, we want to enable all companies to build their own AI, leveraging open models and open source technologies. Our goal is to build an open platform, making it easy for data scientists, machine learning engineers and developers to access the latest models from the community, and use them within the platform of their choice.</summary><published>2024-01-25</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/open-source-llms-as-agents</id><title>Open-source LLMs as LangChain Agents</title><author>Aymeric Roucher, Joffrey THOMAS, Andrew Reed</author><summary>Open-source LLMs have now reached a performance level that makes them suitable reasoning engines for powering agent workflows: Mixtral even surpasses GPT-3.5 on our benchmark, and its performance could easily be further enhanced with fine-tuning. We've released the simplest agentic library out there: smolagents!
Go checkout the smolagents introduction blog here.</summary><published>2024-01-24</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/fine-tune-w2v2-bert</id><title>Fine-Tune W2V2-Bert for low-resource ASR with ü§ó Transformers</title><author>Yoach Lacombe</author><summary>Introduction

Motivation

Notebook Setup

Prepare Data, Tokenizer, Feature Extractor
Create Wav2Vec2CTCTokenizer

Create SeamlessM4TFeatureExtractor

Preprocess Data


Training
Set-up Trainer

Training

Evaluation


Scaling-up the training
Datasets-related tips

Training-related tips


New (01/2024): This blog post is strongly inspired by "Fine-tuning XLS-R on Multi-Lingual ASR" and "Fine-tuning MMS Adapter Models for Multi-Lingual ASR". Last month, MetaAI released Wav2Vec2-BERT, as a building block of their Seamless Communication, a family of AI translation models.</summary><published>2024-01-19</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/patchtsmixer</id><title>PatchTSMixer in HuggingFace</title><author>Arindam Jati, Vijay Ekambaram, Nam Nguyen, Wesley M. Gifford, Kashif Rasul, Niels Rogge</author><summary>PatchTSMixer Quick Overview

Installation

Part 1: Forecasting on Electricity dataset
Set seed

Load and prepare datasets


Configure the PatchTSMixer model

Train model

Evaluate the model on the test set

Save model

Part 2: Transfer Learning from Electricity to ETTh2
Transfer Learning on ETTh2 data

Zero-shot forecasting on ETTh2

Linear probing on ETTh2

Full finetuning on ETTh2


Summary

PatchTSMixer is a lightweight time-series modeling approach based on the MLP-Mixer architecture. It is proposed in TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting by IBM Research authors Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong and Jayant Kalagnanam. For effective mindshare and to promote open-sourcing - IBM Research joins hands with the HuggingFace team to release this model in the Transformers library.</summary><published>2024-01-19</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/pref-tuning</id><title>Preference Tuning LLMs with Direct Preference Optimization Methods</title><author>Kashif Rasul, Edward Beeching, Lewis Tunstall, Leandro von Werra, Omar Sanseviero</author><summary>Introduction

Alignment without Reinforcement Learning

Links

Experimental Setup

Configuring the experiments

Hyperparameter Sweep

Results
Zephyr-7b-beta-SFT

OpenHermes-7b-2.5


Summary &amp; Insights

What‚Äôs next?

Addendum After consulting with the authors of the IPO paper, we discovered that the implementation of IPO in TRL was incorrect; in particular, the loss over the log-likelihoods of the completions needs to be averaged instead of summed. We have added a fix in this PR and re-run the experiments. The results are now consistent with the paper, with IPO on par with DPO and performing better than KTO in the paired preference setting. We have updated the post to reflect these new results.</summary><published>2024-01-18</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/sdxl_ort_inference</id><title>Accelerating SD Turbo and SDXL Turbo Inference with ONNX Runtime and Olive</title><author>Sophie Schoenmeyer, Tianlei Wu, Morgan Funtowicz</author><summary>SD Turbo and SDXL Turbo are two fast generative text-to-image models capable of generating viable images in as little as one step, a significant improvement over the 30+ steps often required with previous Stable Diffusion models. SD Turbo is a distilled version of Stable Diffusion 2.1, and SDXL Turbo is a distilled version of SDXL 1.0. We‚Äôve previously shown how to accelerate Stable Diffusion inference with ONNX Runtime. Not only does ONNX Runtime provide performance benefits when used with SD Turbo and SDXL Turbo, but it also makes the models accessible in languages other than Python, like C# and Java. In this post, we will introduce optimizations in the ONNX Runtime CUDA and TensorRT execution providers that speed up inference of SD Turbo and SDXL Turbo on NVIDIA GPUs significantly.</summary><published>2024-01-15</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/leaderboard-vectara</id><title>A guide to setting up your own Hugging Face leaderboard: an end-to-end example with Vectara's hallucination leaderboard</title><author>Ofer Mendelevitch, Minseok Bae, Cl√©mentine Fourrier</author><summary>Vectara‚Äôs Hughes Hallucination Evaluation Model (HHEM)

Setting up HHEM with the LLM leaderboard template

Summary

Hugging Face‚Äôs Open LLM Leaderboard (originally created by Ed Beeching and Lewis Tunstall, and maintained by Nathan Habib and Cl√©mentine Fourrier) is well known for tracking the performance of open source LLMs, comparing their performance in a variety of tasks, such as TruthfulQA or HellaSwag. This has been of tremendous value to the open-source community, as it provides a way for practitioners to keep track of the best open-source models.</summary><published>2024-01-12</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/unsloth-trl</id><title>Faster fine-tuning using TRL &amp; Unsloth</title><author>Daniel Han-Chen</author><summary>Unsloth - 2x faster, -40% memory usage, 0% accuracy degradation

Benchmarking

How do I use Unsloth?

Unsloth + TRL integration

Reproducible notebooks

Pulling your hair out because LLM fine-tuning is taking forever? In this post, we introduce a lightweight tool developed by the community to make LLM fine-tuning go super fast! Before diving into Unsloth, it may be helpful to read our QLoRA blog post, or be familiar with LLM fine-tuning using the ü§ó PEFT library.</summary><published>2024-01-10</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/amused</id><title>Welcome aMUSEd: Efficient Text-to-Image Generation</title><author>Isamu Isozaki, Suraj Patil, Will Berman, Sayak Paul</author><summary>Table of contents

How does it work?

Using aMUSEd in üß®¬†diffusers

Fine-tuning aMUSEd

Limitations

Resources

Acknowledgements We‚Äôre excited to present an efficient non-diffusion text-to-image model named aMUSEd. It‚Äôs called so because it‚Äôs a open reproduction of Google's MUSE. aMUSEd‚Äôs generation quality is not the best and we‚Äôre releasing a research preview with a permissive license.</summary><published>2024-01-04</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/sdxl_lora_advanced_script</id><title>LoRA training scripts of the world, unite!</title><author>Linoy Tsaban, Apolin√°rio from multimodal AI art</author><summary>Overview

Pivotal Tuning

Adaptive Optimizers

Additional Good Practices
Independent learning rates for text encoder and UNet

Custom Captioning

Min-SNR Gamma weighting

Repeats

Training Set Creation

Experiments Settings and Results


Inference

Comfy UI / AUTOMATIC1111 Inference
What‚Äôs next?


A community derived guide to some of the SOTA practices for SD-XL Dreambooth LoRA fine tuning We combined the Pivotal Tuning technique used on Replicate's SDXL Cog trainer with the Prodigy optimizer used in the
Kohya trainer (plus a bunch of other optimizations) to achieve very good results on training Dreambooth LoRAs for SDXL.
Check out the training script on diffusersüß®. Try it out on Colab.</summary><published>2024-01-02</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/whisper-speculative-decoding</id><title>Speculative Decoding for 2x Faster Whisper Inference</title><author>Sanchit Gandhi</author><summary>Speculative Decoding

English Speech Transcription
Baseline Implementation

Speculative Decoding


Multilingual Speech Transcription

Strategies for Efficient Speculative Decoding

Conclusion

Acknowledgements

Open AI's Whisper is a general 
purpose speech transcription model that achieves state-of-the-art results across a range of different benchmarks and 
audio conditions. The latest large-v3 model tops the 
OpenASR Leaderboard, ranking as the best open-source 
speech transcription model for English. The model also demonstrates strong multilingual performance, achieving less than 
30% word error rate (WER) on 42 of the 58 languages tested in the Common Voice 15 dataset. While the transcription accuracy is exceptional, the inference time is very slow. A 1 hour audio clip takes upwards of 
6 minutes to transcribe on a 16GB T4 GPU, even after leveraging inference optimisations like flash attention, 
half-precision, and chunking.</summary><published>2023-12-20</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/2023-in-llms</id><title>2023, year of open LLMs</title><author>Cl√©mentine Fourrier</author><summary>üçú Recipe for a pretrained Large Language Model

üóùÔ∏è 2022, from a race for size to a race for data

üåä 2023, a year of open releases
The rise of small Large Language Models

Dialog models everywhere

What about the community?


Democratizing access
Merging: Extreme customization

PEFT: Personalization at the tip of your fingers

Quantization: Models running everywhere


What's next?

Takeaways

2023 has seen a surge of public interest in Large Language Models (LLMs), and now that most people have an idea of what they are and can do, the public debates around open versus closed source have reached a wide audience as well. At Hugging Face, we follow open models with great interest, as they allow research to be reproducible, empower the community to participate in the development of AI models, permit the easier scrutiny of model biases and limitations, and lower the overall carbon impact of our field by favoring checkpoint reuse (among many other benefits). So let's do a retrospective of the year in open LLMs!</summary><published>2023-12-18</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/mixtral</id><title>Welcome Mixtral - a SOTA Mixture of Experts on Hugging Face</title><author>Lewis Tunstall, Philipp Schmid, Omar Sanseviero, Pedro Cuenca, Olivier Dehaene, Leandro von Werra, Younes Belkada</author><summary>Table of Contents

What is Mixtral 8x7b?
About the name

Prompt format

What we don't know


Demo

Inference
Using ü§ó¬†Transformers

Using Text Generation Inference


Fine-tuning with ü§ó¬†TRL

Quantizing Mixtral
Load Mixtral with 4-bit quantization

Load Mixtral with GPTQ


Disclaimers and ongoing work

Additional Resources

Conclusion

Mixtral 8x7b is an exciting large language model released by Mistral today, which sets a new state-of-the-art for open-access models and outperforms GPT-3.5 across many benchmarks. We‚Äôre excited to support the launch with a comprehensive integration of Mixtral in the Hugging Face ecosystem üî•! Among the features and integrations being released today, we have:</summary><published>2023-12-11</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/moe</id><title>Mixture of Experts Explained</title><author>Omar Sanseviero, Lewis Tunstall, Philipp Schmid, Sourab Mangrulkar, Younes Belkada, Pedro Cuenca</author><summary>Table of Contents

TL;DR

What is a Mixture of Experts (MoE)?

A Brief History of MoEs

What is Sparsity?

Load balancing tokens for MoEs

MoEs and Transformers

Switch Transformers

Stabilizing training with router Z-loss

What does an expert learn?

How does scaling the number of experts impact pretraining?

Fine-tuning MoEs

When to use sparse MoEs vs dense models?

Making MoEs go brrr
Parallelism

Capacity Factor and communication costs

Serving techniques

More on efficient training


Open Source MoEs

Exciting directions of work

Some resources

Citation

With the release of Mixtral 8x7B (announcement, model card), a class of transformer has become the hottest topic in the open AI community: Mixture of Experts, or MoEs for short. In this blog post, we take a look at the building blocks of MoEs, how they‚Äôre trained, and the tradeoffs to consider when serving them for inference. The scale of a model is one of the most important axes for better model quality. Given a fixed computing budget, training a larger model for fewer steps is better than training a smaller model for more steps.</summary><published>2023-12-11</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/huggingface-and-optimum-amd</id><title>AMD + ü§ó: Large Language Models Out-of-the-Box Acceleration with AMD GPU</title><author>F√©lix Marty, Ilyas Moutawwakil, Mohit Sharma, Ella Charlaix, seungrok jung, Morgan Funtowicz</author><summary>Out-of-the-box Acceleration

Production Solutions

What's next?

Earlier this year, AMD and Hugging Face announced a partnership to accelerate AI models during the AMD's  AI Day event. We have been hard at work to bring this vision to reality, and make it easy for the Hugging Face community to run the latest AI models on AMD hardware with the best possible performance. AMD is powering some of the most powerful supercomputers in the World, including the fastest European one, LUMI, which operates over 10,000 MI250X AMD GPUs. At this event, AMD revealed their latest generation of server GPUs, the AMD Instinct‚Ñ¢  MI300 series accelerators, which will soon become generally available.</summary><published>2023-12-05</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/setfit-absa</id><title>SetFitABSA: Few-Shot Aspect Based Sentiment Analysis using SetFit</title><author>Ronen Laperdon, Tom Aarsen, Lewis Tunstall, Daniel Korat, Oren Pereg, Moshe Wasserblat</author><summary>SetFitABSA is an efficient technique to detect the sentiment towards specific aspects within the text. How does it work?
Training


Running inference

Benchmarking
Model size comparison

Performance comparison


Training your own model

References

Aspect-Based Sentiment Analysis (ABSA) is the task of detecting the sentiment towards specific aspects within the text. For example, in the sentence, "This phone has a great screen, but its battery is too small", the aspect terms are "screen" and "battery" and the sentiment polarities towards them are Positive and Negative, respectively.</summary><published>2023-12-06</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/optimum-nvidia</id><title>Optimum-NVIDIA - Unlock blazingly fast LLM inference in just 1 line of code</title><author>Laikh Tewari, Morgan Funtowicz</author><summary>How to Run

Performance Evaluation

Next steps

Large Language Models (LLMs) have revolutionized natural language processing and are increasingly deployed to solve complex problems at scale. Achieving optimal performance with these models is notoriously challenging due to their unique and intense computational demands. Optimized performance of LLMs is incredibly valuable for end users looking for a snappy and responsive experience, as well as for scaled deployments where improved throughput translates to dollars saved. That's where the Optimum-NVIDIA inference library comes in. Available on Hugging Face, Optimum-NVIDIA dramatically accelerates LLM inference on the NVIDIA platform through an extremely simple API. 
By changing just a single line of code, you can unlock up to 28x faster inference and 1,200 tokens/second on the NVIDIA platform.</summary><published>2023-12-05</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/lora-adapters-dynamic-loading</id><title>Goodbye cold boot - how we made LoRA inference 300% faster</title><author>raphael g</author><summary>LoRA

Benefits

Implementation
LoRA structure

Loading/Offloading LoRA for Diffusers üß®


Loading figures
Serving requests

What about batching ?


Conclusion: Time!

tl;dr: We swap the Stable Diffusion LoRA adapters per user request, while keeping the base model warm allowing fast LoRA inference across multiple users. You can experience this by browsing our LoRA catalogue and playing with the inference widget. In this blog we will go in detail over how we achieved that.</summary><published>2023-12-05</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/open-llm-leaderboard-drop</id><title>Open LLM Leaderboard: DROP deep dive</title><author>Cl√©mentine Fourrier, Alex Cabrera, Stella Biderman, Nathan Habib, Thomas Wolf</author><summary>Initial observations

Normalization interrogations

Diving into the results

Changing the end of generation token

So what's next?

Recently, three new benchmarks were added to the Open LLM Leaderboard: Winogrande, GSM8k and DROP, using the original implementations reproduced in the EleutherAI Harness. A cursory look at the scores for DROP revealed something strange was going on, with the overwhelming majority of models scoring less than 10 out of 100 on their f1-score! We did a deep dive to understand what was going on, come with us to see what we found out! DROP (Discrete Reasoning Over Paragraphs) is an evaluation where models must extract relevant information from English-text paragraphs before executing discrete reasoning steps on them (for example, sorting or counting items to arrive at the correct answer, see the table below for examples). The metrics used are custom f1 and exact match scores.</summary><published>2023-12-01</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/lcm_lora</id><title>SDXL in 4 steps with Latent Consistency LoRAs</title><author>Pedro Cuenca, Suraj Patil, Simian Luo, Daniel Gu, Yiqin Tan, Sayak Paul, Apolin√°rio from multimodal AI art</author><summary>Contents

Method Overview

Why does this matter?

Fast Inference with SDXL LCM LoRAs
Quality Comparison

Guidance Scale and Negative Prompts

Quality vs base SDXL

LCM LoRAs with other models

Full Diffusers Integration


Benchmarks

LCM LoRAs and Models Released Today

Bonus: Combine LCM LoRAs with regular SDXL LoRAs

How to Train LCM Models and LoRAs

Resources

Credits

Latent Consistency Models (LCM) are a way to decrease the number of steps required to generate an image with Stable Diffusion (or SDXL) by distilling the original model into another version that requires fewer steps (4 to 8 instead of the original 25 to 50). Distillation is a type of training procedure that attempts to replicate the outputs from a source model using a new one. The distilled model may be designed to be smaller (that‚Äôs the case of DistilBERT or the recently-released Distil-Whisper) or, in this case, require fewer steps to run. It‚Äôs usually a lengthy and costly process that requires huge amounts of data, patience, and a few GPUs. Well, that was the status quo before today!</summary><published>2023-11-09</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/inferentia-llama2</id><title>Make your llama generation time fly with AWS Inferentia2</title><author>David Corvoysier</author><summary>Setup ü§ó optimum-neuron on your Inferentia2 instance

Export the Llama 2 model to Neuron

Generate Text using Llama 2 on AWS Inferentia2

All-in-one with optimum-neuron pipelines

Benchmarks
Encoding time

End-to-end Latency

Throughput


Conclusion

Update (02/2024): Performance has improved even more! Check our updated benchmarks. In a previous post on the Hugging Face blog, we introduced AWS Inferentia2, the second-generation AWS Inferentia accelerator, and explained how you could use optimum-neuron to quickly deploy Hugging Face models for standard text and vision tasks on AWS Inferencia 2 instances.</summary><published>2023-11-07</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/prodigy-hf</id><title>Introducing Prodigy-HF: a direct integration with Hugging Face</title><author>Vincent D. Warmerdam</author><summary>Features
Upload


More to come

Prodigy is an annotation tool made by Explosion, a company well known as the creators of spaCy. It's a fully scriptable product with a large community around it. The product has many features, including tight integration with spaCy and active learning capabilities. But the main feature of the product is that it is programmatically customizable with Python. To foster this customisability, Explosion has started releasing plugins. These plugins integrate with third-party tools in an open way that encourages users to work on bespoke annotation workflows. However, one customization specifically deserves to be celebrated explicitly. Last week, Explosion introduced Prodigy-HF, which offers code recipes that directly integrate with the Hugging Face stack. It's been a much-requested feature on the Prodigy support forum, so we're super excited to have it out there.</summary><published>2023-11-07</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/Lora-for-sequence-classification-with-Roberta-Llama-Mistral</id><title>Comparing the Performance of LLMs: A Deep Dive into Roberta, Llama 2, and Mistral for Disaster Tweets Analysis with Lora</title><author>mehdi iraqi</author><summary>In the fast-moving world of Natural Language Processing (NLP), we often find ourselves comparing different language models to see which one works best for specific tasks. This blog post is all about comparing three models: RoBERTa, Mistral-7b, and Llama-2-7b. We used them to tackle a common problem - classifying tweets about disasters. It is important to note that Mistral and Llama 2 are large models with 7 billion parameters. In contrast, RoBERTa-large (355M parameters) is a relatively smaller model used as a baseline for the comparison study. In this blog, we used PEFT (Parameter-Efficient Fine-Tuning) technique: LoRA (Low-Rank Adaptation of Large Language Models) for fine-tuning the pre-trained model on the sequence classification task. LoRa is designed to significantly reduce the number of trainable parameters while maintaining strong downstream task performance.</summary><published>2023-11-07</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/regions</id><title>Introducing Storage Regions on the HF Hub</title><author>Eliott Coyac, Remy, Adrien, Michelle Habonneau, Violette, Julien Chaumond</author><summary>Org settings

Repository Tag

Regulatory and legal compliance

Performance

As part of our Enterprise Hub plan, we recently released support for Storage Regions. Regions let you decide where your org's models and datasets will be stored. This has two main benefits, which we'll briefly go over in this blog post:</summary><published>2023-11-03</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/personal-copilot</id><title>Personal Copilot: Train Your Own Coding Assistant</title><author>Sourab Mangrulkar, Sayak Paul</author><summary>Data Collection Workflow

Finetuning your own Personal Co-Pilot

Full Finetuning

PEFT

Comparison

How do I use it in VS Code?
Setting an Inference Endpoint

Setting up the VS Code Extension


Finetuning your own Code Chat Assistant

Dance of LoRAs
Mix-and-Match LoRAs

Transfer LoRAs to different base models


How do I run it locally?
Conclusion

Acknowledgements


In the ever-evolving landscape of programming and software development, the quest for efficiency and productivity has led to remarkable innovations. One such innovation is the emergence of code generation models such as Codex, StarCoder and Code Llama. These models have demonstrated remarkable capabilities in generating human-like code snippets, thereby showing immense potential as coding assistants. However, while these pre-trained models can perform impressively across a range of tasks, there's an exciting possibility lying just beyond the horizon: the ability to tailor a code generation model to your specific needs. Think of personalized coding assistants which could be leveraged at an enterprise scale.</summary><published>2023-10-27</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/scalable-data-inspection</id><title>Interactively explore your Huggingface dataset with one line of code</title><author>Stefan Suwelack, Alexander Druz, Dominik H, Markus Stoll</author><summary>Spotlight ü§ù Hugging Face datasets

Leveraging model results for data inspection

Customizing data inspection workflows

Using Spotlight on the Hugging Face hub

What‚Äôs next?

The Hugging Face datasets library not only provides access to more than 70k publicly available datasets, but also offers very convenient data preparation pipelines for custom datasets. Renumics Spotlight allows you to create interactive visualizations to identify critical clusters in your data. Because Spotlight understands the data semantics within Hugging Face datasets, you can get started with just one line of code:</summary><published>2023-10-25</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/inference-endpoints-embeddings</id><title>Deploy Embedding Models with Hugging Face Inference Endpoints</title><author>Philipp Schmid</author><summary>1. What is Hugging Face Inference Endpoints?

2. What is Text Embeddings Inference?

3. Deploy Embedding Model as Inference Endpoint

4. Send request to endpoint and create embeddings

Conclusion

The rise of Generative AI and LLMs like ChatGPT has increased the interest and importance of embedding models for a variety of tasks especially for retrievel augemented generation, like search or chat with your data. Embeddings are helpful since they represent sentences, images, words, etc. as numeric vector representations, which allows us to map semantically related items and retrieve helpful information. This helps us to provide relevant context for our prompt to improve the quality and specificity of generation. Compared to LLMs are Embedding Models smaller in size and faster for inference. That is very important since you need to recreate your embeddings after you changed your model or improved your model fine-tuning. Additionally, is it important that the whole retrieval augmentation process is as fast as possible to provide a good user experience.</summary><published>2023-10-24</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/the_n_implementation_details_of_rlhf_with_ppo</id><title>The N Implementation Details of RLHF with PPO</title><author>Shengyi Costa Huang, Tianlin Liu, Leandro von Werra</author><summary>Matching Learning Curves
A note on running openai/lm-human-preferences


General Implementation Details

Reward Model Implementation Details

Policy Training Implementation Details
PyTorch Adam optimizer numerical issues w.r.t RLHF


Limitations

Conclusion

Acknowledgement

Bibtex

RLHF / ChatGPT has been a popular research topic these days. In our quest to research more on RLHF, this blog post attempts to do a reproduction of OpenAI‚Äôs 2019 original RLHF codebase at openai/lm-human-preferences. Despite its ‚Äútensorflow-1.x-ness,‚Äù OpenAI‚Äôs original codebase is very well-evaluated and benchmarked, making it a good place to study RLHF implementation engineering details. This work is just for educational / learning purposes. For advanced users requiring more features, such as running larger models with PEFT, huggingface/trl would be a great choice.</summary><published>2023-10-24</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/simple_sdxl_optimizations</id><title>Exploring simple optimizations for SDXL</title><author>Sayak Paul, Steven Liu</author><summary>Inference speed
Lower precision

Memory-efficient attention

torch.compile


Model memory footprint
Model CPU offloading

Sequential CPU offloading


Slicing

Caching computations

Tiny Autoencoder

Conclusion

Stable Diffusion XL (SDXL) is the latest latent diffusion model by Stability AI for generating high-quality super realistic images. It overcomes challenges of previous Stable Diffusion models like getting hands and text right as well as spatially correct compositions. In addition, SDXL is also more context aware and requires fewer words in its prompt to generate better looking images. However, all of these improvements come at the expense of a significantly larger model. How much larger? The base SDXL model has 3.5B parameters (the UNet, in particular), which is approximately 3x larger than the previous Stable Diffusion model.</summary><published>2023-10-24</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/gradio-lite</id><title>Gradio-Lite: Serverless Gradio Running Entirely in Your Browser</title><author>Abubakar Abid, Yuichiro Tachibana, Ali Abdalla</author><summary>What is @gradio/lite?

Getting Started
1. Import JS and CSS

2. Create the   tags

3. Write your Gradio app inside of the tags


More Examples: Adding Additional Files and Requirements
Multiple Files

Additional Requirements


Benefits of Using @gradio/lite
1. Serverless Deployment

2. Low Latency

3. Privacy and Security

Limitations


Try it out!

Gradio is a popular Python library for creating interactive machine learning apps. Traditionally, Gradio applications have relied on server-side infrastructure to run, which can be a hurdle for developers who need to host their applications. Enter Gradio-lite (@gradio/lite): a library that leverages Pyodide to bring Gradio directly to your browser. In this blog post, we'll explore what @gradio/lite is, go over example code, and discuss the benefits it offers for running Gradio applications.</summary><published>2023-10-19</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/ort-accelerating-hf-models</id><title>Accelerating over 130,000 Hugging Face models with ONNX Runtime</title><author>Sophie Schoenmeyer, Morgan Funtowicz</author><summary>ONNX Runtime is a cross-platform machine learning tool that can be used to accelerate a wide variety of models, particularly those with ONNX support. There are over 130,000 ONNX-supported models on Hugging Face, an open source community that allows users to build, train, and deploy hundreds of thousands of publicly available machine learning models.
These ONNX-supported models, which include many increasingly popular large language models (LLMs) and cloud models, can leverage ONNX Runtime to improve performance, along with other benefits.
For example, using ONNX Runtime to accelerate the whisper-tiny model can improve average latency per inference, with an up to 74.30% gain over PyTorch.
ONNX Runtime works closely with Hugging Face to ensure that the most popular models on the site are supported.
In total, over 90 Hugging Face model architectures are supported by ONNX Runtime, including the 11 most popular architectures (where popularity is determined by the corresponding number of models uploaded to the Hugging Face Hub):</summary><published>2023-10-04</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/sdxl_jax</id><title>Accelerating Stable Diffusion XL Inference with JAX on Cloud TPU v5e</title><author>Pedro Cuenca, Juan Acevedo, Alex Spiridonov, Pate Motter, Yavuz Yetim, Vaibhav Singh, Vijaya Singh, Patrick von Platen</author><summary>Why JAX + TPU v5e for SDXL?

How to write an image generation pipeline in JAX

Benchmark

How does the demo work?

Generative AI models, such as Stable Diffusion XL (SDXL), enable the creation of high-quality, realistic content with wide-ranging applications. However, harnessing the power of such models presents significant challenges and computational costs. SDXL is a large image generation model whose UNet component is about three times as large as the one in the previous version of the model. Deploying a model like this in production is challenging due to the increased memory requirements, as well as increased inference times. Today, we are thrilled to announce that Hugging Face Diffusers now supports serving SDXL using JAX on Cloud TPUs, enabling high-performance, cost-efficient inference. Google Cloud TPUs are custom-designed AI accelerators, which are optimized for training and inference of large AI models, including state-of-the-art LLMs and generative AI models such as SDXL. The new Cloud TPU v5e is purpose-built to bring the cost-efficiency and performance required for large-scale AI training and inference. At less than half the cost of TPU v4, TPU v5e makes it possible for more organizations to train and deploy AI models.</summary><published>2023-10-03</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/chat-templates</id><title>Chat Templates: An End to the Silent Performance Killer</title><author>Matthew Carrigan</author><summary>tl;dr

Introduction

Templates: A way to save format information

Why templates?

Why bother doing this? Why not just pick a standard format?

How do templates work?

How do I get started with templates?

Conclusion: Template philosophy

A spectre is haunting chat models - the spectre of incorrect formatting! Chat models have been trained with very different formats for converting conversations into a single tokenizable string. Using a format different from the format a model was trained with will usually cause severe, silent performance degradation, so matching the format used during training is extremely important! Hugging Face tokenizers now have a chat_template attribute that can be used to save the chat format the model was trained with. This attribute contains a Jinja template that converts conversation histories into a correctly formatted string. Please see the technical documentation for information on how to write and apply chat templates in your code.</summary><published>2023-10-03</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/ai-comic-factory</id><title>Deploying the AI Comic Factory using the Inference API</title><author>Julian Bilcke</author><summary>Getting started

How the AI Comic Factory works

Duplicating the Space

Selecting the LLM and SD engines

Configuring the models

Going further

We recently announced Inference for PROs, our new offering that makes larger models accessible to a broader audience. This opportunity opens up new possibilities for running end-user applications using Hugging Face as a platform. An example of such an application is the AI Comic Factory - a Space that has proved incredibly popular. Thousands of users have tried it to create their own AI comic panels, fostering its own community of regular users. They share their creations, with some even opening pull requests.</summary><published>2023-10-02</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/ethics-soc-5</id><title>Ethics and Society Newsletter #5: Hugging Face Goes To Washington and Other Summer 2023 Musings</title><author>Margaret Mitchell</author><summary>One of the most important things to know about ‚Äúethics‚Äù in AI is that it has to do with values. Ethics doesn‚Äôt tell you what‚Äôs right or wrong, it provides a vocabulary of values ‚Äì transparency, safety, justice ‚Äì and frameworks to prioritize among them. This summer, we were able to take our understanding of values in AI to legislators in the E.U., U.K., and U.S., to help shape the future of AI regulation. This is where ethics shines: helping carve out a path forward when laws are not yet in place. In keeping with Hugging Face‚Äôs core values of openness and accountability, we are sharing a collection of what we‚Äôve said and done here.  This includes our CEO Clem‚Äôs testimony to U.S. Congress and statements at the U.S. Senate AI Insight Forum; our advice on the E.U. AI Act; our comments to the NTIA on AI Accountability; and our Chief Ethics Scientist Meg‚Äôs comments to the Democratic Caucus. Common to many of these discussions were questions about why openness in AI can be beneficial, and we share a collection of our answers to this question here.</summary><published>2023-09-29</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/trl-ddpo</id><title>Finetune Stable Diffusion Models with DDPO via TRL</title><author>luke meyers, Sayak Paul, Kashif Rasul, Leandro von Werra</author><summary>Diffusion models (e.g., DALL-E 2, Stable Diffusion) are a class of generative models that are widely successful at generating images most notably of the photorealistic kind. However, the images generated by these models may not always be on par with human preference or human intention. Thus arises the alignment problem i.e. how does one go about making sure that the outputs of a model are aligned with human preferences like ‚Äúquality‚Äù or that outputs are aligned with intent that is hard to express via prompts? This is where Reinforcement Learning comes into the picture. In the world of Large Language Models (LLMs), Reinforcement learning (RL) has proven to become a very effective tool for aligning said models to human preferences. It‚Äôs one of the main recipes behind the superior performance of systems like ChatGPT. More precisely, RL is the critical ingredient of Reinforcement Learning from Human Feedback (RLHF), which makes ChatGPT chat like human beings.</summary><published>2023-09-29</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/Llama2-for-non-engineers</id><title>Non-engineers guide: Train a LLaMA 2 chatbot</title><author>Andrew Jardine, Abhishek Thakur</author><summary>In this tutorial we will show you how anyone can build their own open-source ChatGPT without ever writing a single line of code! We‚Äôll use the LLaMA 2 base model, fine tune it for chat with an open-source instruction dataset and then deploy the model to a chat app you can share with your friends. All by just clicking our way to greatness. üòÄ Why is this important? Well, machine learning, especially LLMs (Large Language Models), has witnessed an unprecedented surge in popularity, becoming a critical tool in our personal and business lives. Yet, for most outside the specialized niche of ML engineering, the intricacies of training and deploying these models appears beyond reach. If the anticipated future of machine learning is to be one filled with ubiquitous personalized models, then there's an impending challenge ahead: How do we empower those with non-technical backgrounds to harness this technology independently?</summary><published>2023-09-28</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/llama-sagemaker-benchmark</id><title>Llama 2 on Amazon SageMaker a Benchmark</title><author>Philipp Schmid</author><summary>Benchmark

Recommendations &amp; Insights
Most Cost-Effective Deployment

Best Throughput Deployment

Best Latency Deployment


Conclusions Deploying large language models (LLMs) and other generative AI models can be challenging due to their computational requirements and latency needs. To provide useful recommendations to companies looking to deploy Llama 2 on Amazon SageMaker with the Hugging Face LLM Inference Container, we created a comprehensive benchmark analyzing over 60 different deployment configurations for Llama 2.</summary><published>2023-09-26</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/inference-pro</id><title>Inference for PROs</title><author>Omar Sanseviero, Pedro Cuenca, Victor Mustar</author><summary>Contents

Supported Models

Getting started with Inference For PROs

Applications
Chat with Llama 2 and Code Llama 34B

Code infilling with Code Llama

Stable Diffusion XL


Messages API

Generation Parameters
Controlling Text Generation

Controlling Image Generation

Caching

Streaming


Subscribe to PRO

FAQ Today, we're introducing Inference for PRO users - a community offering that gives you access to APIs of curated endpoints for some of the most exciting models available, as well as improved rate limits for the usage of free Inference API. Use the following page to subscribe to PRO.</summary><published>2023-09-22</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/rocketmoney-case-study</id><title>Rocket Money x Hugging Face: Scaling Volatile ML Models in Production</title><author>Nico Kuzak, Chris Poirier</author><summary>We created Rocket Money (a personal finance app formerly known as Truebill) to help users improve their financial wellbeing. Users link their bank accounts to the app which then classifies and categorizes their transactions, identifying recurring patterns to provide a consolidated, comprehensive view of their personal financial life. A critical stage of transaction processing is detecting known merchants and services, some of which Rocket Money can cancel and negotiate the cost of for members. This detection starts with the transformation of short, often truncated and cryptically formatted transaction strings into classes we can use to enrich our product experience. We first extracted brands and products from transactions using regular expression-based normalizers. These were used in tandem with an increasingly intricate decision table that mapped strings to corresponding brands. This system proved effective for the first four years of the company when classes were tied only to the products we supported for cancellations and negotiations. However, as our user base grew, the subscription economy boomed and the scope of our product increased, we needed to keep up with the rate of new classes while simultaneously tuning regexes and preventing collisions and overlaps. To address this, we explored various traditional machine learning (ML) solutions, including a bag of words model with a model-per-class architecture. This system struggled with maintenance and performance and was mothballed.</summary><published>2023-09-19</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/gaussian-splatting</id><title>Introduction to 3D Gaussian Splatting</title><author>Dylan Ebert</author><summary>What is 3D Gaussian Splatting?

How it works
1. Structure from Motion

2. Convert to Gaussians

3. Training

4. Differentiable Gaussian Rasterization


Who cares?

The future of graphics

3D Gaussian Splatting is a rasterization technique described in 3D Gaussian Splatting for Real-Time Radiance Field Rendering that allows real-time rendering of photorealistic scenes learned from small samples of images. This article will break down how it works and what it means for the future of graphics. 3D Gaussian Splatting is, at its core, a rasterization technique. That means:</summary><published>2023-09-18</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/object-detection-leaderboard</id><title>Object Detection Leaderboard</title><author>Padilla, Amy Roberts</author><summary>Table of Contents

What's Object Detection?

Metrics
What's Average Precision and how to compute it?

What's Average Recall and how to compute it?

What are the variants of Average Precision and Average Recall?


Object Detection Leaderboard
How to pick the best model based on the metrics?

Which parameters can impact the Average Precision results?


Conclusions

Additional Resources

Welcome to our latest dive into the world of leaderboards and models evaluation. In a previous post, we navigated the waters of evaluating Large Language Models. Today, we set sail to a different, yet equally challenging domain ‚Äì Object Detection. Recently, we released our Object Detection Leaderboard, ranking object detection models available in the Hub according to some metrics. In this blog, we will demonstrate how the models were evaluated and demystify the popular metrics used in Object Detection, from Intersection over Union (IoU) to Average Precision (AP) and Average Recall (AR). More importantly, we will spotlight the inherent divergences and pitfalls that can occur during evaluation, ensuring that you're equipped with the knowledge not just to understand but to assess model performance critically.</summary><published>2023-09-18</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/optimize-llm</id><title>Optimizing your LLM in production</title><author>Patrick von Platen</author><summary>2. Flash Attention: A Leap Forward

3. The Science Behind LLM Architectures: Strategic Selection for Long Text Inputs and Chat
3.1 Improving positional embeddings of LLMs

3.2 The key-value cache


Conclusion

Note: This blog post is also available as a documentation page on Transformers. Large Language Models (LLMs) such as GPT3/4, Falcon, and LLama are rapidly advancing in their ability to tackle human-centric tasks, establishing themselves as essential tools in modern knowledge-based industries.
Deploying these models in real-world tasks remains challenging, however:</summary><published>2023-09-15</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/wuerstchen</id><title>Introducing W√ºrstchen: Fast Diffusion for Image Generation</title><author>Dominic Rampas, Pablo Pern√≠as, Kashif Rasul, Sayak Paul, Pedro Cuenca</author><summary>What is W√ºrstchen?

Why another text-to-image model?

How to use W√ºrstchen?
What image sizes does W√ºrstchen work on?

Models on the Hub

Diffusers integration


Optimisation Technique 1: Flash Attention

Optimisation Technique 2: Torch Compile

How was the model trained?

Resources W√ºrstchen is a diffusion model, whose text-conditional component works in a highly compressed latent space of images. Why is this important? Compressing data can reduce computational costs for both training and inference by orders of magnitude. Training on 1024√ó1024 images is way more expensive than training on 32√ó32. Usually, other works make use of a relatively small compression, in the range of 4x - 8x spatial compression. W√ºrstchen takes this to an extreme. Through its novel design, it achieves a 42x spatial compression! This had never been seen before, because common methods fail to faithfully reconstruct detailed images after 16x spatial compression. W√ºrstchen employs a two-stage compression, what we call Stage A and Stage B. Stage A is a VQGAN, and Stage B is a Diffusion Autoencoder (more details can be found in the¬†¬†paper). Together Stage A and B are called the Decoder, because they decode the compressed images back into pixel space. A third model, Stage C, is learned in that highly compressed latent space. This training requires fractions of the compute used for current top-performing models, while also allowing cheaper and faster inference. We refer to Stage C as the Prior.</summary><published>2023-09-13</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/ram-efficient-pytorch-fsdp</id><title>Fine-tuning Llama 2 70B using PyTorch FSDP</title><author>Sourab Mangrulkar, Sylvain Gugger, Lewis Tunstall, Philipp Schmid</author><summary>In this blog post, we will look at how to fine-tune Llama 2 70B using PyTorch FSDP and related best practices. We will be leveraging Hugging Face Transformers, Accelerate and TRL. We will also learn how to use Accelerate with SLURM. Fully Sharded Data Parallelism (FSDP) is a paradigm in which the optimizer states, gradients and parameters are sharded across devices. During the forward pass, each FSDP unit performs an all-gather operation to get the complete weights, computation is performed followed by discarding the shards from other devices. After the forward pass, the loss is computed followed by the backward pass. In the backward pass, each FSDP unit performs an all-gather operation to get the complete weights, with computation performed to get the local gradients. These local gradients are averaged and sharded across the devices via a reduce-scatter operation so that each device can update the parameters of its shard. For more information on what PyTorch FSDP is, please refer to this blog post: Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel.</summary><published>2023-09-13</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/overview-quantization-transformers</id><title>Overview of natively supported quantization schemes in ü§ó Transformers</title><author>Younes Belkada, Marc Sun, Ilyas Moutawwakil, Cl√©mentine Fourrier, F√©lix Marty</author><summary>Table of contents

Resources

Comparing bitsandbytes and auto-gptq
What are the benefits of bitsandbytes?

What are the benefits of autoGPTQ?

What are the potential rooms of improvements of bitsandbytes?

What are the potential rooms of improvements of autoGPTQ?


Diving into speed benchmarks
Inference speed (forward pass only)

Generate speed

Adapter fine-tuning (forward + backward)

Performance degradation


Conclusion and final words

Acknowledgements

We aim to give a clear overview of the pros and cons of each quantization scheme supported in transformers to help you decide which one you should go for. Currently, quantizing models are used for two main purposes:</summary><published>2023-09-12</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/safecoder-vs-closed-source-code-assistants</id><title>SafeCoder vs. Closed-source Code Assistants</title><author>Julien Simon</author><summary>State-of-the-art models

Transparency

Customization

IT flexibility

Security and privacy

Conclusion

For decades, software developers have designed methodologies, processes, and tools that help them improve code quality and increase productivity. For instance, agile, test-driven development, code reviews, and CI/CD are now staples in the software industry. In "How Google Tests Software" (Addison-Wesley, 2012), Google reports that fixing a bug during system tests - the final testing stage - is 1000x more expensive than fixing it at the unit testing stage. This puts much pressure on developers - the first link in the chain - to write quality code from the get-go.</summary><published>2023-09-11</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/t2i-sdxl-adapters</id><title>Efficient Controllable Generation for SDXL with T2I-Adapters</title><author>ChongMou, Suraj Patil, Sayak Paul, Xintao Wang, hysts</author><summary>Training T2I-Adapter-SDXL with diffusers

Using T2I-Adapter-SDXL in diffusers

Try out the Demo

More Results
Lineart Guided

Sketch Guided

Canny Guided

Depth Guided

OpenPose Guided


T2I-Adapter is an efficient plug-and-play model that provides extra guidance to pre-trained text-to-image models while¬†freezing¬†the original large text-to-image models. T2I-Adapter aligns internal knowledge in T2I models with external control signals. We can train various adapters according to different conditions and achieve rich control and editing effects. As a contemporaneous work, ControlNet has a similar function and is widely used. However, it can be computationally expensive to run. This is because, during each denoising step of the reverse diffusion process, both the ControlNet and UNet need to be run. In addition, ControlNet emphasizes the importance of copying the UNet encoder as a control model, resulting in a larger parameter number. Thus, the generation is bottlenecked by the size of the ControlNet (the larger, the slower the process becomes).</summary><published>2023-09-08</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/falcon-180b</id><title>Spread Your Wings: Falcon 180B is here</title><author>Philipp Schmid, Omar Sanseviero, Pedro Cuenca, Leandro von Werra, Julien Launay</author><summary>Today, we're excited to welcome TII's Falcon 180B to HuggingFace! Falcon 180B sets a new state-of-the-art for open models. It is the largest openly available language model, with 180 billion parameters, and was trained on a massive 3.5 trillion tokens using TII's RefinedWeb dataset. This represents the longest single-epoch pretraining for an open model. You can find the model on the Hugging Face Hub (base and chat model) and interact with the model on the Falcon Chat Demo Space.</summary><published>2023-09-06</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/fetch-case-study</id><title>Fetch Cuts ML Processing Latency by 50% Using Amazon SageMaker &amp; Hugging Face</title><author>Violette</author><summary>Overview

Opportunity | Using Amazon SageMaker to Accelerate an ML Pipeline in 12 Months for Fetch

Solution | Cutting Latency by 50% Using ML &amp; Hugging Face on Amazon SageMaker GPU Instances

Outcome | Expanding ML to New Use Cases

About Fetch

This article is a cross-post from an originally published post on September 2023 on AWS's website. Consumer engagement and rewards company Fetch offers an application that lets users earn rewards on their purchases by scanning their receipts. The company also parses these receipts to generate insights into consumer behavior and provides those insights to brand partners. As weekly scans rapidly grew, Fetch needed to improve its speed and precision.</summary><published>2023-09-01</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/audioldm2</id><title>AudioLDM 2, but faster ‚ö°Ô∏è</title><author>Sanchit Gandhi</author><summary>Model overview

Load the pipeline

Optimisation 1: Flash Attention

Optimisation 2: Half-Precision

Optimisation 3: Torch Compile

Optimisation 4: Scheduler

What about memory?

Output:
```

Conclusion

AudioLDM 2 was proposed in AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining
by Haohe Liu et al. AudioLDM 2 takes a text prompt as input and predicts the corresponding audio. It can generate realistic 
sound effects, human speech and music. While the generated audios are of high quality, running inference with the original implementation is very slow: a 10 
second audio sample takes upwards of 30 seconds to generate. This is due to a combination of factors, including a deep 
multi-stage modelling approach, large checkpoint sizes, and un-optimised code.</summary><published>2023-08-30</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/codellama</id><title>Code Llama: Llama 2 learns to code</title><author>Philipp Schmid, Omar Sanseviero, Pedro Cuenca, Lewis Tunstall, Leandro von Werra, Loubna Ben Allal, Arthur Zucker, Joao Gante</author><summary>Code Llama is a family of state-of-the-art, open-access versions of Llama 2 specialized on code tasks, and we‚Äôre excited to release integration in the Hugging Face ecosystem! Code Llama has been released with the same permissive community license as Llama 2 and is available for commercial use. Today, we‚Äôre excited to release:</summary><published>2023-08-25</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/password-git-deprecation</id><title>Deprecation of Git Authentication using password</title><author>Sylvestre Bcht, Pierric Cistac, Simon Brandeis</author><summary>Background

Action Required Today
Switching to personal access token

Switching to SSH keys


Timeline

Because we are committed to improving the security of our services, we are making changes to the way you authenticate when interacting with the Hugging Face Hub through Git.
Starting from October 1st, 2023, we will no longer accept passwords as a way to authenticate your command-line Git operations. Instead, we recommend using more secure authentication methods, such as replacing the password with a personal access token or using an SSH key. In recent months, we have implemented various security enhancements, including sign-in alerts and support for SSH keys in Git. However, users have still been able to authenticate Git operations using their username and password. To further improve security, we are now transitioning to token-based or SSH key authentication.
Token-based and SSH key authentication offer several advantages over traditional password authentication, including unique, revocable, and random features that enhance security and control.</summary><published>2023-08-25</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/gptq-integration</id><title>Making LLMs lighter with AutoGPTQ and transformers</title><author>Marc Sun, F√©lix Marty, ÊΩòÂÖ∂Â®Å, Junjae Lee, Younes Belkada, Tom Jobbins</author><summary>Table of contents

Resources

A gentle summary of the GPTQ paper

AutoGPTQ library ‚Äì the one-stop library for efficiently leveraging GPTQ for LLMs

Native support of GPTQ models in ü§ó Transformers

Quantizing models with the Optimum library

Running GPTQ models through Text-Generation-Inference

Fine-tune quantized models with PEFT

Room for improvement
Supported models


Conclusion and final words

Acknowledgements

Large language models have demonstrated remarkable capabilities in understanding and generating human-like text, revolutionizing applications across various domains. However, the demands they place on consumer hardware for training and deployment have become increasingly challenging to meet. ü§ó Hugging Face's core mission is to democratize good machine learning, and this includes making large models as accessible as possible for everyone. In the same spirit as our bitsandbytes collaboration, we have just integrated the AutoGPTQ library in Transformers, making it possible for users to quantize and run models in 8, 4, 3, or even 2-bit precision using the GPTQ algorithm (Frantar et al. 2023). There is negligible accuracy degradation with 4-bit quantization, with inference speed comparable to the fp16 baseline for small batch sizes. Note that GPTQ method slightly differs from post-training quantization methods proposed by bitsandbytes as it requires to pass a calibration dataset.</summary><published>2023-08-23</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/safecoder</id><title>Introducing SafeCoder</title><author>Jeff Boudier, Philipp Schmid</author><summary>Why SafeCoder?

From StarCoder to SafeCoder

Privacy and Security as a Core Principle

Compliance as a Core Principle

How does it work?
Training your own SafeCoder model

Deploying SafeCoder

Using SafeCoder


How can I get SafeCoder?

Today we are excited to announce SafeCoder - a code assistant solution built for the enterprise. The goal of SafeCoder is to unlock software development productivity for the enterprise, with a fully compliant and self-hosted pair programmer. In marketing speak: ‚Äúyour own on-prem GitHub copilot‚Äù.</summary><published>2023-08-22</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/idefics</id><title>Introducing IDEFICS: An Open Reproduction of State-of-the-art Visual Language Model</title><author>Hugo Lauren√ßon, Daniel van Strien, Stas Bekman, Leo Tronchon, Lucile Saulnier, Thomas Wang, Siddharth Karamcheti, Amanpreet Singh, Giada Pistilli, Yacine Jernite, Victor Sanh</author><summary>What is IDEFICS?

Training Data

Ethical evaluation

License

Getting Started with IDEFICS

We are excited to release IDEFICS (Image-aware Decoder Enhanced √† la Flamingo with Interleaved Cross-attentionS), an open-access visual language model. IDEFICS is based on Flamingo, a state-of-the-art visual language model initially developed by DeepMind, which has not been released publicly. Similarly to GPT-4, the model accepts arbitrary sequences of image and text inputs and produces text outputs. IDEFICS is built solely on publicly available data and models (LLaMA v1 and OpenCLIP) and comes in two variants‚Äîthe base version and the instructed version. Each variant is available at the 9 billion and 80 billion parameter sizes. The development of state-of-the-art AI models should be more transparent. Our goal with IDEFICS is to reproduce and provide the AI community with systems that match the capabilities of large proprietary models like Flamingo. As such, we took important steps contributing to bringing transparency to these AI systems: we used only publicly available data, we provided tooling to explore training datasets, we shared technical lessons and mistakes of building such artifacts and assessed the model‚Äôs harmfulness by adversarially prompting it before releasing it. We are hopeful that IDEFICS will serve as a solid foundation for more open research in multimodal AI systems, alongside models like OpenFlamingo-another open reproduction of Flamingo at the 9 billion parameter scale.</summary><published>2023-08-22</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/aws-marketplace</id><title>Hugging Face Platform on the AWS Marketplace: Pay with your AWS Account</title><author>Philipp Schmid, Simon Brandeis, Jeff Boudier</author><summary>Getting Started
1. Subscribe to the Hugging Face Hub


The Hugging Face Hub has landed on the AWS Marketplace. Starting today, you can subscribe to the Hugging Face Hub through AWS Marketplace to pay for your Hugging Face usage directly with your AWS account. This new integrated billing method makes it easy to manage payment for usage of all our managed services by all members of your organization, including Inference Endpoints, Spaces Hardware Upgrades, and AutoTrain to easily train, test and deploy the most popular machine learning models like Llama 2, StarCoder, or BERT. By making Hugging Face available on AWS Marketplace, we are removing barriers to adopting AI and making it easier for companies to leverage large language models. Now with just a few clicks, AWS customers can subscribe and connect their Hugging Face Account with their AWS account.</summary><published>2023-08-10</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/optimizing-bark</id><title>Optimizing Bark using ü§ó Transformers</title><author>Yoach Lacombe</author><summary>Table of Contents

Bark Architecture
Load the Model and its Processor


Optimization techniques
Some set-ups

Base case

1. ü§ó Better Transformer

2. Half-precision

3. CPU offload

4. Combine

Using batching


Benchmark results
How to read the results?

No batching

Batch size set to 8


Concluding remarks

ü§ó Transformers provides many of the latest state-of-the-art (SoTA) models across domains and tasks. To get the best performance from these models, they need to be optimized for inference speed and memory usage. The ü§ó Hugging Face ecosystem offers precisely such ready &amp; easy to use optimization tools that can be applied across the board to all the models in the library. This makes it easy to reduce memory footprint and improve inference with just a few extra lines of code.</summary><published>2023-08-09</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/deploy-deepfloydif-using-bentoml</id><title>Deploying Hugging Face Models with BentoML: DeepFloyd IF in Action</title><author>Sherlock Xu, Zhao Shenyang</author><summary>Table of contents

A brief introduction to DeepFloyd IF

Preparing the environment

Downloading the model to the BentoML Model Store

Starting a BentoML Service

Testing the server

Building and serving a Bento

What‚Äôs next?

Hugging Face provides a Hub platform that allows you to upload, share, and deploy your models with ease. It saves developers the time and computational resources required to train models from scratch. However, deploying models in a real-world production environment or in a cloud-native way can still present challenges. This is where BentoML comes into the picture. BentoML is an open-source platform for machine learning model serving and deployment. It is a unified framework for building, shipping, and scaling production-ready AI applications incorporating traditional, pre-trained, and generative models as well as Large Language Models. Here is how you use the BentoML framework from a high-level perspective:</summary><published>2023-08-09</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/dpo-trl</id><title>Fine-tune Llama 2 with DPO</title><author>Kashif Rasul, Younes Belkada, Leandro von Werra</author><summary>Reinforcement Learning from Human Feedback (RLHF) has become the de facto last training step of LLMs such as GPT-4 or Claude to ensure that the language model's outputs are aligned with human expectations such as chattiness or safety features. However, it brings some of the complexity of RL into NLP: we need to build a good reward function, train the model to estimate the value of a state, and at the same time be careful not to strive too far from the original model and produce gibberish instead of sensible text. Such a process is quite involved requiring a number of complex moving parts where it is not always easy to get things right. The recent paper Direct Preference Optimization by Rafailov, Sharma, Mitchell et al. proposes to cast the RL-based objective used by existing methods to an objective which can be directly optimized via a simple binary cross-entropy loss which simplifies this process of refining LLMs greatly.</summary><published>2023-08-08</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/swift-coreml-llm</id><title>Releasing Swift Transformers: Run On-Device LLMs in Apple Devices</title><author>Pedro Cuenca</author><summary>Released Today

Tasks Overview

Conversion to Core ML
Important lessons learned


Optimization

swift-transformers
Tokenizers

Model and Hub wrappers

Generation Algorithms

Supported Models


swift-chat

Missing Parts / Coming Next

Conclusion
Appendix: Converting Llama 2 the Hard Way


Resources

I have a lot of respect for iOS/Mac developers. I started writing apps for iPhones in 2007, when not even APIs or documentation existed. The new devices adopted some unfamiliar decisions in the constraint space, with a combination of power, screen real estate, UI idioms, network access, persistence, and latency that was different to what we were used to before. Yet, this community soon managed to create top-notch applications that felt at home with the new paradigm. I believe that ML is a new way to build software, and I know that many Swift developers want to incorporate AI features in their apps. The ML ecosystem has matured a lot, with thousands of models that solve a wide variety of problems. Moreover, LLMs have recently emerged as almost general-purpose tools ‚Äì they can be adapted to new domains as long as we can model our task to work on text or text-like data. We are witnessing a defining moment in computing history, where LLMs are going out of research labs and becoming computing tools for everybody.</summary><published>2023-08-08</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/run-musicgen-as-an-api</id><title>Deploy MusicGen in no time with Inference Endpoints</title><author>Vaibhav Srivastav, Merve Noyan</author><summary>Conclusion
Read More


MusicGen is a powerful music generation model that takes in text prompt and an optional melody to output music. This blog post will guide you through generating music with MusicGen using Inference Endpoints. Inference Endpoints allow us to write custom inference functions called custom handlers. These are particularly useful when a model is not supported out-of-the-box by the transformers high-level abstraction pipeline.</summary><published>2023-08-04</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/huggy-lingo</id><title>Huggy Lingo: Using Machine Learning to Improve Language Metadata on the Hugging Face Hub</title><author>Daniel van Strien</author><summary>tl;dr: We're using machine learning to detect the language of Hub datasets with no language metadata, and librarian-bots to make pull requests to add this metadata. The Hugging Face Hub has become the repository where the community shares machine learning models, datasets, and applications. As the number of datasets grows, metadata becomes increasingly important as a tool for finding the right resource for your use case.</summary><published>2023-08-02</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/encrypted-llm</id><title>Towards Encrypted Large Language Models with FHE</title><author>Jordan Frery</author><summary>The Impact of Large Language Models on Users' Privacy

Fully Homomorphic Encryption (FHE) Can Solve LLM Privacy Challenges

Implementation of a LLM layer with FHE
Quantization

Applying FHE to the Hugging Face GPT2 model

Compilation to FHE

Complexity


Conclusion

Large Language Models (LLM) have recently been proven as reliable tools for improving productivity in many areas such as programming, content creation, text analysis, web search, and distance learning. Despite the appeal of LLMs, privacy concerns persist surrounding user queries that are processed by these models. On the one hand, leveraging the power of LLMs is desirable, but on the other hand, there is a risk of leaking sensitive information to the LLM service provider. In some areas, such as healthcare, finance, or law, this privacy risk is a showstopper.</summary><published>2023-08-02</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/3d-assets</id><title>Practical 3D Asset Generation: A Step-by-Step Guide</title><author>Dylan Ebert</author><summary>Generative AI has become an instrumental part of artistic workflows for game development. However, as detailed in my earlier post, text-to-3D lags behind 2D in terms of practical applicability. This is beginning to change. Today, we'll be revisiting practical workflows for 3D Asset Generation and taking a step-by-step look at how to integrate Generative AI in a PS1-style 3D workflow. Why the PS1 style? Because it's much more forgiving to the low fidelity of current text-to-3D models, and allows us to go from text to usable 3D asset with as little effort as possible.</summary><published>2023-08-01</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/sd_distillation</id><title>Open-sourcing Knowledge Distillation Code and Weights of SD-Small and SD-Tiny</title><author>Yatharth Gupta</author><summary>Knowledge Distillation

Model Usage

Speed in terms of inference latency

Potential Limitations

Fine-tuning SD-tiny model on portrait dataset

LoRA Training

Conclusion

In recent times, the AI community has witnessed a remarkable surge in the development of larger and more performant language models, such as Falcon 40B, LLaMa-2 70B, Falcon 40B, MPT 30B, and in the imaging domain with models like SD2.1 and SDXL. These advancements have undoubtedly pushed the boundaries of what AI can achieve, enabling highly versatile and state-of-the-art image generation and language understanding capabilities. However, as we marvel at the power and complexity of these models, it is essential to recognize a growing need to make AI models smaller, efficient, and more accessible, particularly by open-sourcing them. At Segmind, we have been working on how to make generative AI models faster and cheaper. Last year, we have open-sourced our accelerated SD-WebUI library called voltaML, which is a AITemplate/TensorRT based inference acceleration library that has delivered between 4-6X increase in the inference speed. To continue towards the goal of making generative models faster, smaller and cheaper, we are open-sourcing the weights and training code of our compressed SD models; SD-Small and SD-Tiny. The pretrained checkpoints are available on Huggingface ü§ó</summary><published>2023-08-01</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/stable-diffusion-xl-coreml</id><title>Stable Diffusion XL on Mac with Advanced Core ML Quantization</title><author>Pedro Cuenca, Orhon</author><summary>Contents

Using SD XL Models from the Hugging Face Hub

What is Mixed-Bit Palettization?

How are Mixed-Bit Recipes Created?

Converting Fine-Tuned Models
Running Mixed-Bit Palettization

Published Resources


Stable Diffusion XL was released yesterday and it‚Äôs awesome. It can generate large (1024x1024) high quality images; adherence to prompts has been improved with some new tricks; it can effortlessly produce very dark or very bright images thanks to the latest research on noise schedulers; and it‚Äôs open source! The downside is that the model is much bigger, and therefore slower and more difficult to run on consumer hardware. Using the latest release of the Hugging Face diffusers library, you can run Stable Diffusion XL on CUDA hardware in 16 GB of GPU RAM, making it possible to use it on Colab‚Äôs free tier.</summary><published>2023-07-27</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/eu-ai-act-oss</id><title>AI Policy @ü§ó: Open ML Considerations in the EU AI Act</title><author>Yacine Jernite</author><summary>Like everyone else in Machine Learning, we‚Äôve been following the EU AI Act closely at Hugging Face.
It‚Äôs a ground-breaking piece of legislation that is poised to shape how democratic inputs interact with AI technology development around the world.
It‚Äôs also the outcome of extensive work and negotiations between organizations representing many different components of society ‚Äì
a process we‚Äôre particularly sensitive to as a community-driven company.
In the present position paper written in coalition with Creative Commons,
Eleuther AI, GitHub, LAION, and Open Future,
we aim to contribute to this process by sharing our experience of the necessary role of open ML development
in supporting the goals of the Act ‚Äì and conversely, by outlining specific ways in which the regulation
can better account for the needs of open, modular, and collaborative ML development. Hugging Face is where it is today thanks to its community of developers, so we‚Äôve seen firsthand what open development brings to the table
to support more robust innovation for more diverse and context-specific use cases;
where developers can easily share innovative new techniques, mix and match ML components to suit their own needs,
and reliably work with full visibility into their entire stack.
We‚Äôre also acutely aware of the necessary role of transparency in supporting more accountability and inclusivity of the technology ‚Äì
which we‚Äôve worked on fostering through better documentation and accessibility of ML artifacts, education efforts,
and hosting large-scale multidisciplinary collaborations, among others.
Thus, as the EU AI Act moves toward its final phase, we believe accounting for the specific needs and strengths of open and open-source development of ML systems will be instrumental in supporting its long-term goals.
Along with our co-signed partner organizations, we make the following five recommendations to that end:</summary><published>2023-07-24</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/agents-js</id><title>Introducing Agents.js: Give tools to your LLMs using JavaScript</title><author>Nathan Sarrazin</author><summary>Installation

Usage
Usage warning


Custom LLMs üí¨

Custom Tools üõ†Ô∏è

Passing input files to the agent üñºÔ∏è

Demo üéâ

We have recently been working on Agents.js at huggingface.js. It's a new library for giving tool access to LLMs from JavaScript in either the browser or the server. It ships with a few multi-modal tools out of the box and can easily be extended with your own tools and language models. Getting started is very easy, you can grab the library from npm with the following:</summary><published>2023-07-24</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/game-jam-first-edition-results</id><title>Results of the Open Source AI Game Jam</title><author>Thomas Simonini, Dylan Ebert, Omar Sanseviero</author><summary>The Theme: Expanding

The Winner üèÜü•á

Participants Selection: Top 10 ü•àü•âüèÖ
#1: Snip It

#2: Yabbit Attack

#3: Fish Dang Bot Rolling Land

#4: Everchanging Quest

#5: Word Conquest

#6: Expanding Universe

#7: Hexagon Tactics: The Expanding Arena

#8: Galactic Domination

#9: Apocalypse Expansion

#10: Galactic Bride: Bullet Ballet

#10: Singularity


From July 7th to July 11th, we hosted our first Open Source AI Game Jam, an exciting event that challenged game developers to create innovative games within a tight 48-hour window using AI. The primary objective was to create games that incorporate at least one Open Source AI Tool. Although proprietary AI tools were allowed, we encouraged participants to integrate open-source tools into their game or workflow.</summary><published>2023-07-21</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/diffusers-turns-1</id><title>Happy 1st anniversary ü§ó Diffusers!</title><author>Steven Liu, Sayak Paul, Pedro Cuenca</author><summary>Striving for photorealism

Video pipelines

Text-to-3D models

Image editing pipelines

Faster diffusion models

Ethics and safety

Support for LoRA

Torch 2.0 optimizations

Community highlights

Building products with ü§ó Diffusers

Looking forward

ü§ó¬†Diffusers is happy to celebrate its first anniversary! It has been an exciting year, and we're proud and grateful for how far we've come thanks to our community and open-source contributors. Last year, text-to-image models like DALL-E 2, Imagen, and Stable Diffusion captured the world's attention with their ability to generate stunningly photorealistic images from text, sparking a massive surge of interest and development in generative AI. But access to these powerful models was limited. At Hugging Face, our mission is to democratize good machine learning by collaborating and helping each other build an open and ethical AI future together. Our mission motivated us to create the ü§ó Diffusers library so everyone can experiment, research, or simply play with text-to-image models. That‚Äôs why we designed the library as a modular toolbox, so you can customize a diffusion model‚Äôs components or just start using it out-of-the-box.</summary><published>2023-07-20</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/llama2</id><title>Llama 2 is here - get it on Hugging Face</title><author>Philipp Schmid, Omar Sanseviero, Pedro Cuenca, Lewis Tunstall</author><summary>Llama 2 is a family of state-of-the-art open-access large language models released by Meta today, and we‚Äôre excited to fully support the launch with comprehensive integration in Hugging Face. Llama 2 is being released with a very permissive community license and is available for commercial use. The code, pretrained models, and fine-tuned models are all being released today üî• We‚Äôve collaborated with Meta to ensure smooth integration into the Hugging Face ecosystem. You can find the 12 open-access models (3 base models &amp; 3 fine-tuned ones with the original Meta checkpoints, plus their corresponding transformers models) on the Hub. Among the features and integrations being released, we have:</summary><published>2023-07-18</published><source>Hugging Face - Blog</source></entry><entry><id>https://huggingface.co/blog/ai-webtv</id><title>Building an AI WebTV</title><author>Julian Bilcke</author><summary>Concept

Architecture

Implementing the pipeline
The text-to-video model

Calling the video chain

Using a model hosted on a Space

Post-processing

Broadcasting the stream


Observations and examples
Characters and scene composition

Simulation of dynamic scenes

Styling and effects

Failure cases


Recommendations
Using video-specific prompt keywords

Maintaining consistency between scenes

Leverage frame interpolation


Future work

The AI WebTV is an experimental demo to showcase the latest advancements in automatic video and music synthesis. üëâ Watch the stream now by going to the AI WebTV Space.</summary><published>2023-07-17</published><source>Hugging Face - Blog</source></entry></feed>