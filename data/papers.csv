year,title,authors,link,category,summary,concat
2021,highly accurate protein structure prediction with alphafold,john jumper et al,https://www.nature.com/articles/s41586-021-03819-2,dl_misc,proteins are essential to life and understanding their structure can facilitate a mechanistic understanding of their function through an enormous experimental effort12 34 the structures of around 100000 unique proteins have been determined5 but this represents a small fraction of the billions of known protein sequences67 structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure accurate computational approaches are needed to address this gap and to enable large scale structural bioinformatics predicting the three dimensional structure that a protein will adopt based solely on its amino acid sequence the structure prediction component of the protein folding problem 8 has been an important open research problem for more than 50 years9 despite recent progress1011121314 existing methods fall far short of atomic accuracy especially when no homologous structure is available here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known we validated an entirely redesigned version of our neural network based model alphafold in the challenging 14th critical assessment of protein structure prediction casp14 15 demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods underpinning the latest version of alphafold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure leveraging multi sequence alignments into the design of the deep learning algorithm,highly accurate protein structure prediction with alphafold john jumper et al proteins are essential to life and understanding their structure can facilitate a mechanistic understanding of their function through an enormous experimental effort12 34 the structures of around 100000 unique proteins have been determined5 but this represents a small fraction of the billions of known protein sequences67 structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure accurate computational approaches are needed to address this gap and to enable large scale structural bioinformatics predicting the three dimensional structure that a protein will adopt based solely on its amino acid sequence the structure prediction component of the protein folding problem 8 has been an important open research problem for more than 50 years9 despite recent progress1011121314 existing methods fall far short of atomic accuracy especially when no homologous structure is available here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known we validated an entirely redesigned version of our neural network based model alphafold in the challenging 14th critical assessment of protein structure prediction casp14 15 demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods underpinning the latest version of alphafold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure leveraging multi sequence alignments into the design of the deep learning algorithm
1986,learning representations by back propagating errors,david rumelhart et al,https://www.nature.com/articles/323533a0,dl_general,we describe a new learning procedure back propagation for networks of neurone like units the procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector as a result of the weight adjustments internal hidden units which are not part of the input or output come to represent important features of the task domain and the regularities in the task are captured by the interactions of these units the ability to create useful new features distinguishes back propagation from earlier simpler methods such as the perceptron convergence procedure1,learning representations by back propagating errors david rumelhart et al we describe a new learning procedure back propagation for networks of neurone like units the procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector as a result of the weight adjustments internal hidden units which are not part of the input or output come to represent important features of the task domain and the regularities in the task are captured by the interactions of these units the ability to create useful new features distinguishes back propagation from earlier simpler methods such as the perceptron convergence procedure1
2023,mathematical discoveries from program search with large language models funsearch,bernardino romera paredes et al,https://www.nature.com/articles/s41586-023-06924-6,dl_nlp,large language models llms have demonstrated tremendous capabilities in solving complex tasks from quantitative reasoning to understanding natural language however llms sometimes suffer from confabulations or hallucinations which can result in them making plausible but incorrect statements12 this hinders the use of current large models in scientific discovery here we introduce funsearch short for searching in the function space an evolutionary procedure based on pairing a pretrained llm with a systematic evaluator we demonstrate the effectiveness of this approach to surpass the best known results in important problems pushing the boundary of existing llm based approaches3 applying funsearch to a central problem in extremal combinatorics the cap set problem we discover new constructions of large cap sets going beyond the best known ones both in finite dimensional and asymptotic cases this shows that it is possible to make discoveries for established open problems using llms we showcase the generality of funsearch by applying it to an algorithmic problem online bin packing finding new heuristics that improve on widely used baselines in contrast to most computer search approaches funsearch searches for programs that describe how to solve a problem rather than what the solution is beyond being an effective and scalable strategy discovered programs tend to be more interpretable than raw solutions enabling feedback loops between domain experts and funsearch and the deployment of such programs in real world applications,mathematical discoveries from program search with large language models funsearch bernardino romera paredes et al large language models llms have demonstrated tremendous capabilities in solving complex tasks from quantitative reasoning to understanding natural language however llms sometimes suffer from confabulations or hallucinations which can result in them making plausible but incorrect statements12 this hinders the use of current large models in scientific discovery here we introduce funsearch short for searching in the function space an evolutionary procedure based on pairing a pretrained llm with a systematic evaluator we demonstrate the effectiveness of this approach to surpass the best known results in important problems pushing the boundary of existing llm based approaches3 applying funsearch to a central problem in extremal combinatorics the cap set problem we discover new constructions of large cap sets going beyond the best known ones both in finite dimensional and asymptotic cases this shows that it is possible to make discoveries for established open problems using llms we showcase the generality of funsearch by applying it to an algorithmic problem online bin packing finding new heuristics that improve on widely used baselines in contrast to most computer search approaches funsearch searches for programs that describe how to solve a problem rather than what the solution is beyond being an effective and scalable strategy discovered programs tend to be more interpretable than raw solutions enabling feedback loops between domain experts and funsearch and the deployment of such programs in real world applications
2015,human level control through deep reinforcement learning,volodymyr mnih et al,https://www.nature.com/articles/nature14236,dl_rl,the theory of reinforcement learning provides a normative account1 deeply rooted in psychological2 and neuroscientific3 perspectives on animal behaviour of how agents may optimize their control of an environment to use reinforcement learning successfully in situations approaching real world complexity however agents are confronted with a difficult task they must derive efficient representations of the environment from high dimensional sensory inputs and use these to generalize past experience to new situations remarkably humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems45 the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms3 while reinforcement learning agents have achieved some successes in a variety of domains67 8 their applicability has previously been limited to domains in which useful features can be handcrafted or to domains with fully observed low dimensional state spaces here we use recent advances in training deep neural networks91011 to develop a novel artificial agent termed a deep q network that can learn successful policies directly from high dimensional sensory inputs using end to end reinforcement learning we tested this agent on the challenging domain of classic atari 2600 games12 we demonstrate that the deep q network agent receiving only the pixels and the game score as inputs was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games using the same algorithm network architecture and hyperparameters this work bridges the divide between high dimensional sensory inputs and actions resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks,human level control through deep reinforcement learning volodymyr mnih et al the theory of reinforcement learning provides a normative account1 deeply rooted in psychological2 and neuroscientific3 perspectives on animal behaviour of how agents may optimize their control of an environment to use reinforcement learning successfully in situations approaching real world complexity however agents are confronted with a difficult task they must derive efficient representations of the environment from high dimensional sensory inputs and use these to generalize past experience to new situations remarkably humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems45 the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms3 while reinforcement learning agents have achieved some successes in a variety of domains67 8 their applicability has previously been limited to domains in which useful features can be handcrafted or to domains with fully observed low dimensional state spaces here we use recent advances in training deep neural networks91011 to develop a novel artificial agent termed a deep q network that can learn successful policies directly from high dimensional sensory inputs using end to end reinforcement learning we tested this agent on the challenging domain of classic atari 2600 games12 we demonstrate that the deep q network agent receiving only the pixels and the game score as inputs was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games using the same algorithm network architecture and hyperparameters this work bridges the divide between high dimensional sensory inputs and actions resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks
2022,outracing champion gran turismo drivers with deep reinforcement learning sophy,pete wurman et al,https://www.nature.com/articles/s41586-021-04357-7,dl_rl,many potential applications of artificial intelligence involve making real time decisions in physical systems while interacting with humans automobile racing represents an extreme example of these conditions drivers must execute complex tactical manoeuvres to pass or block opponents while operating their vehicles at their traction limits1 racing simulations such as the playstation game gran turismo faithfully reproduce the non linear control challenges of real race cars while also encapsulating the complex multi agent interactions here we describe how we trained agents for gran turismo that can compete with the world s best e sports drivers we combine state of the art model free deep reinforcement learning algorithms with mixed scenario training to learn an integrated control policy that combines exceptional speed with impressive tactics in addition we construct a reward function that enables the agent to be competitive while adhering to racing s important but under specified sportsmanship rules we demonstrate the capabilities of our agent gran turismo sophy by winning a head to head competition against four of the world s best gran turismo drivers by describing how we trained championship level racers we demonstrate the possibilities and challenges of using these techniques to control complex dynamical systems in domains where agents must respect imprecisely defined human norms,outracing champion gran turismo drivers with deep reinforcement learning sophy pete wurman et al many potential applications of artificial intelligence involve making real time decisions in physical systems while interacting with humans automobile racing represents an extreme example of these conditions drivers must execute complex tactical manoeuvres to pass or block opponents while operating their vehicles at their traction limits1 racing simulations such as the playstation game gran turismo faithfully reproduce the non linear control challenges of real race cars while also encapsulating the complex multi agent interactions here we describe how we trained agents for gran turismo that can compete with the world s best e sports drivers we combine state of the art model free deep reinforcement learning algorithms with mixed scenario training to learn an integrated control policy that combines exceptional speed with impressive tactics in addition we construct a reward function that enables the agent to be competitive while adhering to racing s important but under specified sportsmanship rules we demonstrate the capabilities of our agent gran turismo sophy by winning a head to head competition against four of the world s best gran turismo drivers by describing how we trained championship level racers we demonstrate the possibilities and challenges of using these techniques to control complex dynamical systems in domains where agents must respect imprecisely defined human norms
2022,magnetic control of tokamak plasmas through deep reinforcement learning,jonas degrave et al,https://www.nature.com/articles/s41586-021-04301-9,dl_rl,nuclear fusion using magnetic confinement in particular in the tokamak configuration is a promising path towards sustainable energy a core challenge is to shape and maintain a high temperature plasma within the tokamak vessel this requires high dimensional high frequency closed loop control using magnetic actuator coils further complicated by the diverse requirements across a wide range of plasma configurations in this work we introduce a previously undescribed architecture for tokamak magnetic controller design that autonomously learns to command the full set of control coils this architecture meets control objectives specified at a high level at the same time satisfying physical and operational constraints this approach has unprecedented flexibility and generality in problem specification and yields a notable reduction in design effort to produce new plasma configurations we successfully produce and control a diverse set of plasma configurations on the tokamak à configuration variable12 including elongated conventional shapes as well as advanced configurations such as negative triangularity and snowflake configurations our approach achieves accurate tracking of the location current and shape for these configurations we also demonstrate sustained droplets on tcv in which two separate plasmas are maintained simultaneously within the vessel this represents a notable advance for tokamak feedback control showing the potential of reinforcement learning to accelerate research in the fusion domain and is one of the most challenging real world systems to which reinforcement learning has been applied,magnetic control of tokamak plasmas through deep reinforcement learning jonas degrave et al nuclear fusion using magnetic confinement in particular in the tokamak configuration is a promising path towards sustainable energy a core challenge is to shape and maintain a high temperature plasma within the tokamak vessel this requires high dimensional high frequency closed loop control using magnetic actuator coils further complicated by the diverse requirements across a wide range of plasma configurations in this work we introduce a previously undescribed architecture for tokamak magnetic controller design that autonomously learns to command the full set of control coils this architecture meets control objectives specified at a high level at the same time satisfying physical and operational constraints this approach has unprecedented flexibility and generality in problem specification and yields a notable reduction in design effort to produce new plasma configurations we successfully produce and control a diverse set of plasma configurations on the tokamak à configuration variable12 including elongated conventional shapes as well as advanced configurations such as negative triangularity and snowflake configurations our approach achieves accurate tracking of the location current and shape for these configurations we also demonstrate sustained droplets on tcv in which two separate plasmas are maintained simultaneously within the vessel this represents a notable advance for tokamak feedback control showing the potential of reinforcement learning to accelerate research in the fusion domain and is one of the most challenging real world systems to which reinforcement learning has been applied
2022,discovering faster matrix multiplication algorithms with reinforcement learning alphatensor,alhussein fawzi et al,https://www.nature.com/articles/s41586-022-05172-4,dl_rl,improving the efficiency of algorithms for fundamental computations can have a widespread impact as it can affect the overall speed of a large amount of computations matrix multiplication is one such primitive task occurring in many systems from neural networks to scientific computing routines the automatic discovery of algorithms using machine learning offers the prospect of reaching beyond human intuition and outperforming the current best human designed algorithms however automating the algorithm discovery procedure is intricate as the space of possible algorithms is enormous here we report a deep reinforcement learning approach based on alphazero1 for discovering efficient and provably correct algorithms for the multiplication of arbitrary matrices our agent alphatensor is trained to play a single player game where the objective is finding tensor decompositions within a finite factor space alphatensor discovered algorithms that outperform the state of the art complexity for many matrix sizes particularly relevant is the case of 4 4 matrices in a finite field where alphatensor s algorithm improves on strassen s two level algorithm for the first time to our knowledge since its discovery 50 years ago2 we further showcase the flexibility of alphatensor through different use cases algorithms with state of the art complexity for structured matrix multiplication and improved practical efficiency by optimizing matrix multiplication for runtime on specific hardware our results highlight alphatensor s ability to accelerate the process of algorithmic discovery on a range of problems and to optimize for different criteria,discovering faster matrix multiplication algorithms with reinforcement learning alphatensor alhussein fawzi et al improving the efficiency of algorithms for fundamental computations can have a widespread impact as it can affect the overall speed of a large amount of computations matrix multiplication is one such primitive task occurring in many systems from neural networks to scientific computing routines the automatic discovery of algorithms using machine learning offers the prospect of reaching beyond human intuition and outperforming the current best human designed algorithms however automating the algorithm discovery procedure is intricate as the space of possible algorithms is enormous here we report a deep reinforcement learning approach based on alphazero1 for discovering efficient and provably correct algorithms for the multiplication of arbitrary matrices our agent alphatensor is trained to play a single player game where the objective is finding tensor decompositions within a finite factor space alphatensor discovered algorithms that outperform the state of the art complexity for many matrix sizes particularly relevant is the case of 4 4 matrices in a finite field where alphatensor s algorithm improves on strassen s two level algorithm for the first time to our knowledge since its discovery 50 years ago2 we further showcase the flexibility of alphatensor through different use cases algorithms with state of the art complexity for structured matrix multiplication and improved practical efficiency by optimizing matrix multiplication for runtime on specific hardware our results highlight alphatensor s ability to accelerate the process of algorithmic discovery on a range of problems and to optimize for different criteria
2023,faster sorting algorithms discovered using deep reinforcement learning alphadev,daniel j mankowitz et al,https://www.nature.com/articles/s41586-023-06004-9,dl_rl,fundamental algorithms such as sorting or hashing are used trillions of times on any given day1 as demand for computation grows it has become critical for these algorithms to be as performant as possible whereas remarkable progress has been achieved in the past2 making further improvements on the efficiency of these routines has proved challenging for both human scientists and computational approaches here we show how artificial intelligence can go beyond the current state of the art by discovering hitherto unknown routines to realize this we formulated the task of finding a better sorting routine as a single player game we then trained a new deep reinforcement learning agent alphadev to play this game alphadev discovered small sorting algorithms from scratch that outperformed previously known human benchmarks these algorithms have been integrated into the llvm standard c sort library3 this change to this part of the sort library represents the replacement of a component with an algorithm that has been automatically discovered using reinforcement learning we also present results in extra domains showcasing the generality of the approach,faster sorting algorithms discovered using deep reinforcement learning alphadev daniel j mankowitz et al fundamental algorithms such as sorting or hashing are used trillions of times on any given day1 as demand for computation grows it has become critical for these algorithms to be as performant as possible whereas remarkable progress has been achieved in the past2 making further improvements on the efficiency of these routines has proved challenging for both human scientists and computational approaches here we show how artificial intelligence can go beyond the current state of the art by discovering hitherto unknown routines to realize this we formulated the task of finding a better sorting routine as a single player game we then trained a new deep reinforcement learning agent alphadev to play this game alphadev discovered small sorting algorithms from scratch that outperformed previously known human benchmarks these algorithms have been integrated into the llvm standard c sort library3 this change to this part of the sort library represents the replacement of a component with an algorithm that has been automatically discovered using reinforcement learning we also present results in extra domains showcasing the generality of the approach
2012,deep neural networks for acoustic modeling in speech recognition the shared views of four research groups,g hinton et al,https://ieeexplore.ieee.org/document/6296526,audio,most current speech recognition systems use hidden markov models hmms to deal with the temporal variability of speech and gaussian mixture models gmms to determine how well each state of each hmm fits a frame or a short window of frames of coefficients that represents the acoustic input an alternative way to evaluate the fit is to use a feed forward neural network that takes several frames of coefficients as input and produces posterior probabilities over hmm states as output deep neural networks dnns that have many hidden layers and are trained using new methods have been shown to outperform gmms on a variety of speech recognition benchmarks sometimes by a large margin this article provides an overview of this progress and represents the shared views of four research groups that have had recent successes in using dnns for acoustic modeling in speech recognition,deep neural networks for acoustic modeling in speech recognition the shared views of four research groups g hinton et al most current speech recognition systems use hidden markov models hmms to deal with the temporal variability of speech and gaussian mixture models gmms to determine how well each state of each hmm fits a frame or a short window of frames of coefficients that represents the acoustic input an alternative way to evaluate the fit is to use a feed forward neural network that takes several frames of coefficients as input and produces posterior probabilities over hmm states as output deep neural networks dnns that have many hidden layers and are trained using new methods have been shown to outperform gmms on a variety of speech recognition benchmarks sometimes by a large margin this article provides an overview of this progress and represents the shared views of four research groups that have had recent successes in using dnns for acoustic modeling in speech recognition
2012,context dependent pre trained deep neural networks for large vocabulary speech recognition,g dahl et al,https://ieeexplore.ieee.org/document/5740583,audio,we propose a novel context dependent cd model for large vocabulary speech recognition lvsr that leverages recent advances in using deep belief networks for phone recognition we describe a pre trained deep neural network hidden markov model dnn hmm hybrid architecture that trains the dnn to produce a distribution over senones tied triphone states as its output the deep belief network pre training algorithm is a robust and often helpful way to initialize deep neural networks generatively that can aid in optimization and reduce generalization error we illustrate the key components of our model describe the procedure for applying cd dnn hmms to lvsr and analyze the effects of various modeling choices on performance experiments on a challenging business search dataset demonstrate that cd dnn hmms can significantly outperform the conventional context dependent gaussian mixture model gmm hmms with an absolute sentence accuracy improvement of 5 8 and 9 2 or relative error reduction of 16 0 and 23 2 over the cd gmm hmms trained using the minimum phone error rate mpe and maximum likelihood ml criteria respectively,context dependent pre trained deep neural networks for large vocabulary speech recognition g dahl et al we propose a novel context dependent cd model for large vocabulary speech recognition lvsr that leverages recent advances in using deep belief networks for phone recognition we describe a pre trained deep neural network hidden markov model dnn hmm hybrid architecture that trains the dnn to produce a distribution over senones tied triphone states as its output the deep belief network pre training algorithm is a robust and often helpful way to initialize deep neural networks generatively that can aid in optimization and reduce generalization error we illustrate the key components of our model describe the procedure for applying cd dnn hmms to lvsr and analyze the effects of various modeling choices on performance experiments on a challenging business search dataset demonstrate that cd dnn hmms can significantly outperform the conventional context dependent gaussian mixture model gmm hmms with an absolute sentence accuracy improvement of 5 8 and 9 2 or relative error reduction of 16 0 and 23 2 over the cd gmm hmms trained using the minimum phone error rate mpe and maximum likelihood ml criteria respectively
2012,acoustic modeling using deep belief networks,a mohamed et al,https://ieeexplore.ieee.org/document/5704567,audio,gaussian mixture models are currently the dominant technique for modeling the emission distribution of hidden markov models for speech recognition we show that better phone recognition on the timit dataset can be achieved by replacing gaussian mixture models by deep neural networks that contain many layers of features and a very large number of parameters these networks are first pre trained as a multi layer generative model of a window of spectral feature vectors without making use of any discriminative information once the generative pre training has designed the features we perform discriminative fine tuning using backpropagation to adjust the features slightly to make them better at predicting a probability distribution over the states of monophone hidden markov models,acoustic modeling using deep belief networks a mohamed et al gaussian mixture models are currently the dominant technique for modeling the emission distribution of hidden markov models for speech recognition we show that better phone recognition on the timit dataset can be achieved by replacing gaussian mixture models by deep neural networks that contain many layers of features and a very large number of parameters these networks are first pre trained as a multi layer generative model of a window of spectral feature vectors without making use of any discriminative information once the generative pre training has designed the features we perform discriminative fine tuning using backpropagation to adjust the features slightly to make them better at predicting a probability distribution over the states of monophone hidden markov models
2022,wavlm large scale self supervised pre training for full stack speech processing,sanyuan chen et al,https://ieeexplore.ieee.org/abstract/document/9814838,audio,self supervised learning ssl achieves great success in speech recognition while limited exploration has been attempted for other speech processing tasks as speech signal contains multi faceted information including speaker identity paralinguistics spoken content etc learning universal representations for all speech tasks is challenging to tackle the problem we propose a new pre trained model wavlm to solve full stack downstream speech tasks wavlm jointly learns masked speech prediction and denoising in pre training by this means wavlm does not only keep the speech content modeling capability by the masked speech prediction but also improves the potential to non asr tasks by the speech denoising in addition wavlm employs gated relative position bias for the transformer structure to better capture the sequence ordering of input speech we also scale up the training dataset from 60 k hours to 94 k hours wavlm large achieves state of the art performance on the superb benchmark and brings significant improvements for various speech processing tasks on their representative benchmarks,wavlm large scale self supervised pre training for full stack speech processing sanyuan chen et al self supervised learning ssl achieves great success in speech recognition while limited exploration has been attempted for other speech processing tasks as speech signal contains multi faceted information including speaker identity paralinguistics spoken content etc learning universal representations for all speech tasks is challenging to tackle the problem we propose a new pre trained model wavlm to solve full stack downstream speech tasks wavlm jointly learns masked speech prediction and denoising in pre training by this means wavlm does not only keep the speech content modeling capability by the masked speech prediction but also improves the potential to non asr tasks by the speech denoising in addition wavlm employs gated relative position bias for the transformer structure to better capture the sequence ordering of input speech we also scale up the training dataset from 60 k hours to 94 k hours wavlm large achieves state of the art performance on the superb benchmark and brings significant improvements for various speech processing tasks on their representative benchmarks
1997,long short term memory,sepp hochreiter and jurgen schmidhuber,https://ieeexplore.ieee.org/abstract/document/6795963,dl_rnn,learning to store information over extended time intervals by recurrent backpropagation takes a very long time mostly because of insufficient decaying error backflow we briefly review hochreiter s 1991 analysis of this problem then address it by introducing a novel efficient gradient based method called long short term memory lstm truncating the gradient where this does not do harm lstm can learn to bridge minimal time lags in excess of 1000 discrete time steps by enforcing constant error flow through constant error carousels within special units multiplicative gate units learn to open and close access to the constant error flow lstm is local in space and time its computational complexity per time step and weight is italic xmlns mml xmlns xlink 1 our experiments with artificial data involve local distributed real valued and noisy pattern representations in comparisons with real time recurrent learning back propagation through time recurrent cascade correlation elman nets and neural sequence chunking lstm leads to many more successful runs and learns much faster lstm also solves complex artificial long time lag tasks that have never been solved by previous recurrent network algorithms,long short term memory sepp hochreiter and jurgen schmidhuber learning to store information over extended time intervals by recurrent backpropagation takes a very long time mostly because of insufficient decaying error backflow we briefly review hochreiter s 1991 analysis of this problem then address it by introducing a novel efficient gradient based method called long short term memory lstm truncating the gradient where this does not do harm lstm can learn to bridge minimal time lags in excess of 1000 discrete time steps by enforcing constant error flow through constant error carousels within special units multiplicative gate units learn to open and close access to the constant error flow lstm is local in space and time its computational complexity per time step and weight is italic xmlns mml xmlns xlink 1 our experiments with artificial data involve local distributed real valued and noisy pattern representations in comparisons with real time recurrent learning back propagation through time recurrent cascade correlation elman nets and neural sequence chunking lstm leads to many more successful runs and learns much faster lstm also solves complex artificial long time lag tasks that have never been solved by previous recurrent network algorithms
1997,bidirectional recurrent neural networks,mike schuster and kuldip k paliwal,https://ieeexplore.ieee.org/document/650093,dl_rnn,in the first part of this paper a regular recurrent neural network rnn is extended to a bidirectional recurrent neural network brnn the brnn can be trained without the limitation of using input information just up to a preset future frame this is accomplished by training it simultaneously in positive and negative time direction structure and training procedure of the proposed network are explained in regression and classification experiments on artificial data the proposed structure gives better results than other approaches for real data classification experiments for phonemes from the timit database show the same tendency in the second part of this paper it is shown how the proposed bidirectional structure can be easily modified to allow efficient estimation of the conditional posterior probability of complete symbol sequences without making any explicit assumption about the shape of the distribution for this part experiments on real data are reported,bidirectional recurrent neural networks mike schuster and kuldip k paliwal in the first part of this paper a regular recurrent neural network rnn is extended to a bidirectional recurrent neural network brnn the brnn can be trained without the limitation of using input information just up to a preset future frame this is accomplished by training it simultaneously in positive and negative time direction structure and training procedure of the proposed network are explained in regression and classification experiments on artificial data the proposed structure gives better results than other approaches for real data classification experiments for phonemes from the timit database show the same tendency in the second part of this paper it is shown how the proposed bidirectional structure can be easily modified to allow efficient estimation of the conditional posterior probability of complete symbol sequences without making any explicit assumption about the shape of the distribution for this part experiments on real data are reported
1991,face recognition using eigenfaces,matthew turk and alex pentland,https://ieeexplore.ieee.org/document/139758,vision,an approach to the detection and identification of human faces is presented and a working near real time face recognition system which tracks a subject s head and then recognizes the person by comparing characteristics of the face to those of known individuals is described this approach treats face recognition as a two dimensional recognition problem taking advantage of the fact that faces are normally upright and thus may be described by a small set of 2 d characteristic views face images are projected onto a feature space face space that best encodes the variation among known face images the face space is defined by the eigenfaces which are the eigenvectors of the set of faces they do not necessarily correspond to isolated features such as eyes ears and noses the framework provides the ability to learn to recognize new faces in an unsupervised manner etx xmlns mml xmlns xlink,face recognition using eigenfaces matthew turk and alex pentland an approach to the detection and identification of human faces is presented and a working near real time face recognition system which tracks a subject s head and then recognizes the person by comparing characteristics of the face to those of known individuals is described this approach treats face recognition as a two dimensional recognition problem taking advantage of the fact that faces are normally upright and thus may be described by a small set of 2 d characteristic views face images are projected onto a feature space face space that best encodes the variation among known face images the face space is defined by the eigenfaces which are the eigenvectors of the set of faces they do not necessarily correspond to isolated features such as eyes ears and noses the framework provides the ability to learn to recognize new faces in an unsupervised manner etx xmlns mml xmlns xlink
1998,gradientbased learning applied to document recognition,yann lecun et al,https://ieeexplore.ieee.org/document/726791,vision,multilayer neural networks trained with the back propagation algorithm constitute the best example of a successful gradient based learning technique given an appropriate network architecture gradient based learning algorithms can be used to synthesize a complex decision surface that can classify high dimensional patterns such as handwritten characters with minimal preprocessing this paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task convolutional neural networks which are specifically designed to deal with the variability of 2d shapes are shown to outperform all other techniques real life document recognition systems are composed of multiple modules including field extraction segmentation recognition and language modeling a new learning paradigm called graph transformer networks gtn allows such multimodule systems to be trained globally using gradient based methods so as to minimize an overall performance measure two systems for online handwriting recognition are described experiments demonstrate the advantage of global training and the flexibility of graph transformer networks a graph transformer network for reading a bank cheque is also described it uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques it is deployed commercially and reads several million cheques per day,gradientbased learning applied to document recognition yann lecun et al multilayer neural networks trained with the back propagation algorithm constitute the best example of a successful gradient based learning technique given an appropriate network architecture gradient based learning algorithms can be used to synthesize a complex decision surface that can classify high dimensional patterns such as handwritten characters with minimal preprocessing this paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task convolutional neural networks which are specifically designed to deal with the variability of 2d shapes are shown to outperform all other techniques real life document recognition systems are composed of multiple modules including field extraction segmentation recognition and language modeling a new learning paradigm called graph transformer networks gtn allows such multimodule systems to be trained globally using gradient based methods so as to minimize an overall performance measure two systems for online handwriting recognition are described experiments demonstrate the advantage of global training and the flexibility of graph transformer networks a graph transformer network for reading a bank cheque is also described it uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques it is deployed commercially and reads several million cheques per day
2013,learning hierarchical features for scene labeling,c farabet et al,https://ieeexplore.ieee.org/document/6338939,vision,scene labeling consists of labeling each pixel in an image with the category of the object it belongs to we propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel the method alleviates the need for engineered features and produces a powerful representation that captures texture shape and contextual information we report results using multiple postprocessing methods to produce the final labeling among those we propose a technique to automatically retrieve from a pool of segmentation components an optimal set of components that best explain the scene these components are arbitrary for example they can be taken from a segmentation tree or from any family of oversegmentations the system yields record accuracies on the sift flow dataset 33 classes and the barcelona dataset 170 classes and near record accuracy on stanford background dataset eight classes while being an order of magnitude faster than competing approaches producing a 320 240 image labeling in less than a second including feature extraction,learning hierarchical features for scene labeling c farabet et al scene labeling consists of labeling each pixel in an image with the category of the object it belongs to we propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel the method alleviates the need for engineered features and produces a powerful representation that captures texture shape and contextual information we report results using multiple postprocessing methods to produce the final labeling among those we propose a technique to automatically retrieve from a pool of segmentation components an optimal set of components that best explain the scene these components are arbitrary for example they can be taken from a segmentation tree or from any family of oversegmentations the system yields record accuracies on the sift flow dataset 33 classes and the barcelona dataset 170 classes and near record accuracy on stanford background dataset eight classes while being an order of magnitude faster than competing approaches producing a 320 240 image labeling in less than a second including feature extraction
2013,3d convolutional neural networks for human action recognition,s ji et al,https://ieeexplore.ieee.org/document/6165309,vision,we consider the automated recognition of human actions in surveillance videos most current methods build classifiers based on complex handcrafted features computed from the raw inputs convolutional neural networks cnns are a type of deep model that can act directly on the raw inputs however such models are currently limited to handling 2d inputs in this paper we develop a novel 3d cnn model for action recognition this model extracts features from both the spatial and the temporal dimensions by performing 3d convolutions thereby capturing the motion information encoded in multiple adjacent frames the developed model generates multiple channels of information from the input frames and the final feature representation combines information from all channels to further boost the performance we propose regularizing the outputs with high level features and combining the predictions of a variety of different models we apply the developed models to recognize human actions in the real world environment of airport surveillance videos and they achieve superior performance in comparison to baseline methods,3d convolutional neural networks for human action recognition s ji et al we consider the automated recognition of human actions in surveillance videos most current methods build classifiers based on complex handcrafted features computed from the raw inputs convolutional neural networks cnns are a type of deep model that can act directly on the raw inputs however such models are currently limited to handling 2d inputs in this paper we develop a novel 3d cnn model for action recognition this model extracts features from both the spatial and the temporal dimensions by performing 3d convolutions thereby capturing the motion information encoded in multiple adjacent frames the developed model generates multiple channels of information from the input frames and the final feature representation combines information from all channels to further boost the performance we propose regularizing the outputs with high level features and combining the predictions of a variety of different models we apply the developed models to recognize human actions in the real world environment of airport surveillance videos and they achieve superior performance in comparison to baseline methods
2014,learning and transferring mid level image representations using convolutional neural networks,m oquab et al,https://ieeexplore.ieee.org/document/6909618,vision,convolutional neural networks cnn have recently shown outstanding image classification performance in the large scale visual recognition challenge ilsvrc2012 the success of cnns is attributed to their ability to learn rich mid level image representations as opposed to hand designed low level features used in other image classification methods learning cnns however amounts to estimating millions of parameters and requires a very large number of annotated image samples this property currently prevents application of cnns to problems with limited training data in this work we show how image representations learned with cnns on large scale annotated datasets can be efficiently transferred to other visual recognition tasks with limited amount of training data we design a method to reuse layers trained on the imagenet dataset to compute mid level image representation for images in the pascal voc dataset we show that despite differences in image statistics and tasks in the two datasets the transferred representation leads to significantly improved results for object and action classification outperforming the current state of the art on pascal voc 2007 and 2012 datasets we also show promising results for object and action localization,learning and transferring mid level image representations using convolutional neural networks m oquab et al convolutional neural networks cnn have recently shown outstanding image classification performance in the large scale visual recognition challenge ilsvrc2012 the success of cnns is attributed to their ability to learn rich mid level image representations as opposed to hand designed low level features used in other image classification methods learning cnns however amounts to estimating millions of parameters and requires a very large number of annotated image samples this property currently prevents application of cnns to problems with limited training data in this work we show how image representations learned with cnns on large scale annotated datasets can be efficiently transferred to other visual recognition tasks with limited amount of training data we design a method to reuse layers trained on the imagenet dataset to compute mid level image representation for images in the pascal voc dataset we show that despite differences in image statistics and tasks in the two datasets the transferred representation leads to significantly improved results for object and action classification outperforming the current state of the art on pascal voc 2007 and 2012 datasets we also show promising results for object and action localization
2014,deepface closing the gap to human level performance in face verification,y taigman et al,https://ieeexplore.ieee.org/document/6909616,vision,in modern face recognition the conventional pipeline consists of four stages detect align represent classify we revisit both the alignment step and the representation step by employing explicit 3d face modeling in order to apply a piecewise affine transformation and derive a face representation from a nine layer deep neural network this deep network involves more than 120 million parameters using several locally connected layers without weight sharing rather than the standard convolutional layers thus we trained it on the largest facial dataset to date an identity labeled dataset of four million facial images belonging to more than 4 000 identities the learned representations coupling the accurate model based alignment with the large facial database generalize remarkably well to faces in unconstrained environments even with a simple classifier our method reaches an accuracy of 97 35 on the labeled faces in the wild lfw dataset reducing the error of the current state of the art by more than 27 closely approaching human level performance,deepface closing the gap to human level performance in face verification y taigman et al in modern face recognition the conventional pipeline consists of four stages detect align represent classify we revisit both the alignment step and the representation step by employing explicit 3d face modeling in order to apply a piecewise affine transformation and derive a face representation from a nine layer deep neural network this deep network involves more than 120 million parameters using several locally connected layers without weight sharing rather than the standard convolutional layers thus we trained it on the largest facial dataset to date an identity labeled dataset of four million facial images belonging to more than 4 000 identities the learned representations coupling the accurate model based alignment with the large facial database generalize remarkably well to faces in unconstrained environments even with a simple classifier our method reaches an accuracy of 97 35 on the labeled faces in the wild lfw dataset reducing the error of the current state of the art by more than 27 closely approaching human level performance
2014,large scale video classification with convolutional neural networks,a karpathy et al,https://ieeexplore.ieee.org/document/6909619,vision,convolutional neural networks cnns have been established as a powerful class of models for image recognition problems encouraged by these results we provide an extensive empirical evaluation of cnns on large scale video classification using a new dataset of 1 million youtube videos belonging to 487 classes we study multiple approaches for extending the connectivity of a cnn in time domain to take advantage of local spatio temporal information and suggest a multiresolution foveated architecture as a promising way of speeding up the training our best spatio temporal networks display significant performance improvements compared to strong feature based baselines 55 3 to 63 9 but only a surprisingly modest improvement compared to single frame models 59 3 to 60 9 we further study the generalization performance of our best model by retraining the top layers on the ucf 101 action recognition dataset and observe significant performance improvements compared to the ucf 101 baseline model 63 3 up from 43 9,large scale video classification with convolutional neural networks a karpathy et al convolutional neural networks cnns have been established as a powerful class of models for image recognition problems encouraged by these results we provide an extensive empirical evaluation of cnns on large scale video classification using a new dataset of 1 million youtube videos belonging to 487 classes we study multiple approaches for extending the connectivity of a cnn in time domain to take advantage of local spatio temporal information and suggest a multiresolution foveated architecture as a promising way of speeding up the training our best spatio temporal networks display significant performance improvements compared to strong feature based baselines 55 3 to 63 9 but only a surprisingly modest improvement compared to single frame models 59 3 to 60 9 we further study the generalization performance of our best model by retraining the top layers on the ucf 101 action recognition dataset and observe significant performance improvements compared to the ucf 101 baseline model 63 3 up from 43 9
1967,nearest neighbor pattern classification,t m cover and p e hart,https://ieeexplore.ieee.org/document/1053964,ml_classic,the nearest neighbor decision rule assigns to an unclassified sample point the classification of the nearest of a set of previously classified points this rule is independent of the underlying joint distribution on the sample points and their classifications and hence the probability of error where these bounds are the tightest possible for all suitably smooth underlying distributions thus for any number of categories the probability of error of the nearest neighbor rule is bounded above by twice the bayes probability of error in this sense it may be said that half the classification information in an infinite sample set is contained in the nearest neighbor,nearest neighbor pattern classification t m cover and p e hart the nearest neighbor decision rule assigns to an unclassified sample point the classification of the nearest of a set of previously classified points this rule is independent of the underlying joint distribution on the sample points and their classifications and hence the probability of error where these bounds are the tightest possible for all suitably smooth underlying distributions thus for any number of categories the probability of error of the nearest neighbor rule is bounded above by twice the bayes probability of error in this sense it may be said that half the classification information in an infinite sample set is contained in the nearest neighbor
1997,no free lunch theorems for optimization,david h wolpert and william g macready,https://ieeexplore.ieee.org/document/585893,learning_theory,a framework is developed to explore the connection between effective optimization algorithms and the problems they are solving a number of no free lunch nfl theorems are presented which establish that for any algorithm any elevated performance over one class of problems is offset by performance over another class these theorems result in a geometric interpretation of what it means for an algorithm to be well suited to an optimization problem applications of the nfl theorems to information theoretic aspects of optimization and benchmark measures of performance are also presented other issues addressed include time varying optimization problems and a priori head to head minimax distinctions between optimization algorithms distinctions that result despite the nfl theorems enforcing of a type of uniformity over all algorithms,no free lunch theorems for optimization david h wolpert and william g macready a framework is developed to explore the connection between effective optimization algorithms and the problems they are solving a number of no free lunch nfl theorems are presented which establish that for any algorithm any elevated performance over one class of problems is offset by performance over another class these theorems result in a geometric interpretation of what it means for an algorithm to be well suited to an optimization problem applications of the nfl theorems to information theoretic aspects of optimization and benchmark measures of performance are also presented other issues addressed include time varying optimization problems and a priori head to head minimax distinctions between optimization algorithms distinctions that result despite the nfl theorems enforcing of a type of uniformity over all algorithms
2014,dropout a simple way to prevent neural networks from overfitting,nitish srivastava et al,https://jmlr.org/papers/v15/srivastava14a.html,dl_general,deep neural nets with a large number of parameters are very powerful machine learning systems however overfitting is a serious problem in such networks large networks are also slow to use making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time dropout is a technique for addressing this problem the key idea is to randomly drop units along with their connections from the neural network during training this prevents units from co adapting too much during training dropout samples from an exponential number of different thinned networks at test time it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights this significantly reduces overfitting and gives major improvements over other regularization methods we show that dropout improves the performance of neural networks on supervised learning tasks in vision speech recognition document classification and computational biology obtaining state of the art results on many benchmark data sets,dropout a simple way to prevent neural networks from overfitting nitish srivastava et al deep neural nets with a large number of parameters are very powerful machine learning systems however overfitting is a serious problem in such networks large networks are also slow to use making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time dropout is a technique for addressing this problem the key idea is to randomly drop units along with their connections from the neural network during training this prevents units from co adapting too much during training dropout samples from an exponential number of different thinned networks at test time it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights this significantly reduces overfitting and gives major improvements over other regularization methods we show that dropout improves the performance of neural networks on supervised learning tasks in vision speech recognition document classification and computational biology obtaining state of the art results on many benchmark data sets
2012,random search for hyper parameter optimization,james bergstra and yoshua bengio,https://www.jmlr.org/papers/v13/bergstra12a.html,ml_tuning,grid search and manual search are the most widely used strategies for hyper parameter optimization this paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper parameter optimization than trials on a grid empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks compared with neural networks configured by a pure grid search we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time granting random search the same computational budget random search finds better models by effectively searching a larger less promising configuration space compared with deep belief networks configured by a thoughtful combination of manual search and grid search purely random search over the same 32 dimensional configuration space found statistically equal performance on four of seven data sets and superior performance on one of seven a gaussian process analysis of the function from hyper parameters to validation set performance reveals that for most data sets only a few of the hyper parameters really matter but that different hyper parameters are important on different data sets this phenomenon makes grid search a poor choice for configuring algorithms for new data sets our analysis casts some light on why recent high throughput methods achieve surprising success they appear to search through a large number of hyper parameters because most hyper parameters do not matter much we anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper parameter optimization this work shows that random search is a natural baseline against which to judge progress in the development of adaptive sequential hyper parameter optimization algorithms,random search for hyper parameter optimization james bergstra and yoshua bengio grid search and manual search are the most widely used strategies for hyper parameter optimization this paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper parameter optimization than trials on a grid empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks compared with neural networks configured by a pure grid search we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time granting random search the same computational budget random search finds better models by effectively searching a larger less promising configuration space compared with deep belief networks configured by a thoughtful combination of manual search and grid search purely random search over the same 32 dimensional configuration space found statistically equal performance on four of seven data sets and superior performance on one of seven a gaussian process analysis of the function from hyper parameters to validation set performance reveals that for most data sets only a few of the hyper parameters really matter but that different hyper parameters are important on different data sets this phenomenon makes grid search a poor choice for configuring algorithms for new data sets our analysis casts some light on why recent high throughput methods achieve surprising success they appear to search through a large number of hyper parameters because most hyper parameters do not matter much we anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper parameter optimization this work shows that random search is a natural baseline against which to judge progress in the development of adaptive sequential hyper parameter optimization algorithms
2014,neural word embedding as implicit matrix factorization,omer levy and yoav goldberg,https://papers.nips.cc/paper_files/paper/2014/hash/b78666971ceae55a8e87efb7cbfd9ad4-Abstract.html,dl_nlp,we analyze skip gram with negative sampling sgns a word embedding method introduced by mikolov et al and show that it is implicitly factorizing a word context matrix whose cells are the pointwise mutual information pmi of the respective word and context pairs shifted by a global constant we find that another embedding method nce is implicitly factorizing a similar matrix where each cell is the shifted log conditional probability of a word given its context we show that using a sparse shifted positive pmi word context matrix to represent words improves results on two word similarity tasks and one of two analogy tasks when dense low dimensional vectors are preferred exact factorization with svd can achieve solutions that are at least as good as sgns s solutions for word similarity tasks on analogy questions sgns remains superior to svd we conjecture that this stems from the weighted nature of sgns s factorization we analyze skip gram with negative sampling sgns a word embedding method introduced by mikolov et al and show that it is implicitly factorizing a word context matrix whose cells are the pointwise mutual information pmi of the respective word and context pairs shifted by a global constant we find that another embedding method nce is implicitly factorizing a similar matrix where each cell is the shifted log conditional probability of a word given its context we show that using a sparse shifted positive pmi word context matrix to represent words improves results on two word similarity tasks and one of two analogy tasks when dense low dimensional vectors are preferred exact factorization with svd can achieve solutions that are at least as good as sgns s solutions for word similarity tasks on analogy questions sgns remains superior to svd we conjecture that this stems from the weighted nature of sgns s factorization,neural word embedding as implicit matrix factorization omer levy and yoav goldberg we analyze skip gram with negative sampling sgns a word embedding method introduced by mikolov et al and show that it is implicitly factorizing a word context matrix whose cells are the pointwise mutual information pmi of the respective word and context pairs shifted by a global constant we find that another embedding method nce is implicitly factorizing a similar matrix where each cell is the shifted log conditional probability of a word given its context we show that using a sparse shifted positive pmi word context matrix to represent words improves results on two word similarity tasks and one of two analogy tasks when dense low dimensional vectors are preferred exact factorization with svd can achieve solutions that are at least as good as sgns s solutions for word similarity tasks on analogy questions sgns remains superior to svd we conjecture that this stems from the weighted nature of sgns s factorization we analyze skip gram with negative sampling sgns a word embedding method introduced by mikolov et al and show that it is implicitly factorizing a word context matrix whose cells are the pointwise mutual information pmi of the respective word and context pairs shifted by a global constant we find that another embedding method nce is implicitly factorizing a similar matrix where each cell is the shifted log conditional probability of a word given its context we show that using a sparse shifted positive pmi word context matrix to represent words improves results on two word similarity tasks and one of two analogy tasks when dense low dimensional vectors are preferred exact factorization with svd can achieve solutions that are at least as good as sgns s solutions for word similarity tasks on analogy questions sgns remains superior to svd we conjecture that this stems from the weighted nature of sgns s factorization
2012,imagenet classification with deep convolutional neural networks,alex krizhevsky et al,https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html,vision,we trained a large deep convolutional neural network to classify the 1 3 million high resolution images in the lsvrc 2010 imagenet training set into the 1000 different classes on the test data we achieved top 1 and top 5 error rates of 39 7 and 18 9 which is considerably better than the previous state of the art results the neural network which has 60 million parameters and 500000 neurons consists of five convolutional layers some of which are followed by max pooling layers and two globally connected layers with a final 1000 way softmax to make training faster we used non saturating neurons and a very efficient gpu implementation of convolutional nets to reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective we trained a large deep convolutional neural network to classify the 1 3 million high resolution images in the lsvrc 2010 imagenet training set into the 1000 different classes on the test data we achieved top 1 and top 5 error rates of 39 7 and 18 9 which is considerably better than the previous state of the art results the neural network which has 60 million parameters and 500000 neurons consists of five convolutional layers some of which are followed by max pooling layers and two globally connected layers with a final 1000 way softmax to make training faster we used non saturating neurons and a very efficient gpu implementation of convolutional nets to reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective,imagenet classification with deep convolutional neural networks alex krizhevsky et al we trained a large deep convolutional neural network to classify the 1 3 million high resolution images in the lsvrc 2010 imagenet training set into the 1000 different classes on the test data we achieved top 1 and top 5 error rates of 39 7 and 18 9 which is considerably better than the previous state of the art results the neural network which has 60 million parameters and 500000 neurons consists of five convolutional layers some of which are followed by max pooling layers and two globally connected layers with a final 1000 way softmax to make training faster we used non saturating neurons and a very efficient gpu implementation of convolutional nets to reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective we trained a large deep convolutional neural network to classify the 1 3 million high resolution images in the lsvrc 2010 imagenet training set into the 1000 different classes on the test data we achieved top 1 and top 5 error rates of 39 7 and 18 9 which is considerably better than the previous state of the art results the neural network which has 60 million parameters and 500000 neurons consists of five convolutional layers some of which are followed by max pooling layers and two globally connected layers with a final 1000 way softmax to make training faster we used non saturating neurons and a very efficient gpu implementation of convolutional nets to reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective
2002,on discriminative vs generative classifiers a comparison of logistic regression and naive bayes,andrew ng and michael jordan,https://papers.nips.cc/paper_files/paper/2001/hash/7b7a53e239400a13bd6be6c91c4f6c4e-Abstract.html,ml_classic,we compare discriminative and generative learning as typified by logistic regression and naive bayes we show contrary to a widely cid 173 held belief that discriminative classifiers are almost always to be preferred that there can often be two distinct regimes of per cid 173 formance as the training set size is increased one in which each algorithm does better this stems from the observation which is borne out in repeated experiments that while discriminative learning has lower asymptotic error a generative classifier may also approach its higher asymptotic error much faster we compare discriminative and generative learning as typified by logistic regression and naive bayes we show contrary to a widely cid 173 held belief that discriminative classifiers are almost always to be preferred that there can often be two distinct regimes of per cid 173 formance as the training set size is increased one in which each algorithm does better this stems from the observation which is borne out in repeated experiments that while discriminative learning has lower asymptotic error a generative classifier may also approach its higher asymptotic error much faster,on discriminative vs generative classifiers a comparison of logistic regression and naive bayes andrew ng and michael jordan we compare discriminative and generative learning as typified by logistic regression and naive bayes we show contrary to a widely cid 173 held belief that discriminative classifiers are almost always to be preferred that there can often be two distinct regimes of per cid 173 formance as the training set size is increased one in which each algorithm does better this stems from the observation which is borne out in repeated experiments that while discriminative learning has lower asymptotic error a generative classifier may also approach its higher asymptotic error much faster we compare discriminative and generative learning as typified by logistic regression and naive bayes we show contrary to a widely cid 173 held belief that discriminative classifiers are almost always to be preferred that there can often be two distinct regimes of per cid 173 formance as the training set size is increased one in which each algorithm does better this stems from the observation which is borne out in repeated experiments that while discriminative learning has lower asymptotic error a generative classifier may also approach its higher asymptotic error much faster
1998,efficient backprop,yann lecun et al,https://link.springer.com/chapter/10.1007/978-3-642-35289-8_3,dl_general,the convergence of back propagation learning is analyzed so as to explain common phenomenon observed by practitioners many undesirable behaviors of backprop can be avoided with tricks that are rarely exposed in serious technical publications this paper gives some of those tricks and offers explanations of why they work many authors have suggested that second order optimization methods are advantageous for neural net training it is shown that most classical second order methods are impractical for large neural networks a few methods are proposed that do not have these limitations,efficient backprop yann lecun et al the convergence of back propagation learning is analyzed so as to explain common phenomenon observed by practitioners many undesirable behaviors of backprop can be avoided with tricks that are rarely exposed in serious technical publications this paper gives some of those tricks and offers explanations of why they work many authors have suggested that second order optimization methods are advantageous for neural net training it is shown that most classical second order methods are impractical for large neural networks a few methods are proposed that do not have these limitations
2022,tensorf tensorial radiance fields,anpei chen et al,https://link.springer.com/chapter/10.1007/978-3-031-19824-3_20,vision,we present tensorf a novel approach to model and reconstruct radiance fields unlike nerf that purely uses mlps we model the radiance field of a scene as a 4d tensor which represents a 3d voxel grid with per voxel multi channel features our central idea is to factorize the 4d scene tensor into multiple compact low rank tensor components we demonstrate that applying traditional candecomp parafac cp decomposition that factorizes tensors into rank one components with compact vectors in our framework leads to improvements over vanilla nerf to further boost performance we introduce a novel vector matrix vm decomposition that relaxes the low rank constraints for two modes of a tensor and factorizes tensors into compact vector and matrix factors beyond superior rendering quality our models with cp and vm decompositions lead to a significantly lower memory footprint in comparison to previous and concurrent works that directly optimize per voxel features experimentally we demonstrate that tensorf with cp decomposition achieves fast reconstruction 30 min with better rendering quality and even a smaller model size 4 mb compared to nerf moreover tensorf with vm decomposition further boosts rendering quality and outperforms previous state of the art methods while reducing the reconstruction time 10 min and retaining a compact model size 75 mb,tensorf tensorial radiance fields anpei chen et al we present tensorf a novel approach to model and reconstruct radiance fields unlike nerf that purely uses mlps we model the radiance field of a scene as a 4d tensor which represents a 3d voxel grid with per voxel multi channel features our central idea is to factorize the 4d scene tensor into multiple compact low rank tensor components we demonstrate that applying traditional candecomp parafac cp decomposition that factorizes tensors into rank one components with compact vectors in our framework leads to improvements over vanilla nerf to further boost performance we introduce a novel vector matrix vm decomposition that relaxes the low rank constraints for two modes of a tensor and factorizes tensors into compact vector and matrix factors beyond superior rendering quality our models with cp and vm decompositions lead to a significantly lower memory footprint in comparison to previous and concurrent works that directly optimize per voxel features experimentally we demonstrate that tensorf with cp decomposition achieves fast reconstruction 30 min with better rendering quality and even a smaller model size 4 mb compared to nerf moreover tensorf with vm decomposition further boosts rendering quality and outperforms previous state of the art methods while reducing the reconstruction time 10 min and retaining a compact model size 75 mb
2022,swin unetr swin transformers for semantic segmentation of brain tumors in mri images,ali hatamizadeh et al,https://link.springer.com/chapter/10.1007/978-3-031-08999-2_22,vision,semantic segmentation of brain tumors is a fundamental medical image analysis task involving multiple mri imaging modalities that can assist clinicians in diagnosing the patient and successively studying the progression of the malignant entity in recent years fully convolutional neural networks fcnns approaches have become the de facto standard for 3d medical image segmentation the popular u shaped network architecture has achieved state of the art performance benchmarks on different 2d and 3d semantic segmentation tasks and across various imaging modalities however due to the limited kernel size of convolution layers in fcnns their performance of modeling long range information is sub optimal and this can lead to deficiencies in the segmentation of tumors with variable sizes on the other hand transformer models have demonstrated excellent capabilities in capturing such long range information in multiple domains including natural language processing and computer vision inspired by the success of vision transformers and their variants we propose a novel segmentation model termed swin unet transformers swin unetr specifically the task of 3d brain tumor semantic segmentation is reformulated as a sequence to sequence prediction problem wherein multi modal input data is projected into a 1d sequence of embedding and used as an input to a hierarchical swin transformer as the encoder the swin transformer encoder extracts features at five different resolutions by utilizing shifted windows for computing self attention and is connected to an fcnn based decoder at each resolution via skip connections we have participated in brats 2021 segmentation challenge and our proposed model ranks among the top performing approaches in the validation phase code,swin unetr swin transformers for semantic segmentation of brain tumors in mri images ali hatamizadeh et al semantic segmentation of brain tumors is a fundamental medical image analysis task involving multiple mri imaging modalities that can assist clinicians in diagnosing the patient and successively studying the progression of the malignant entity in recent years fully convolutional neural networks fcnns approaches have become the de facto standard for 3d medical image segmentation the popular u shaped network architecture has achieved state of the art performance benchmarks on different 2d and 3d semantic segmentation tasks and across various imaging modalities however due to the limited kernel size of convolution layers in fcnns their performance of modeling long range information is sub optimal and this can lead to deficiencies in the segmentation of tumors with variable sizes on the other hand transformer models have demonstrated excellent capabilities in capturing such long range information in multiple domains including natural language processing and computer vision inspired by the success of vision transformers and their variants we propose a novel segmentation model termed swin unet transformers swin unetr specifically the task of 3d brain tumor semantic segmentation is reformulated as a sequence to sequence prediction problem wherein multi modal input data is projected into a 1d sequence of embedding and used as an input to a hierarchical swin transformer as the encoder the swin transformer encoder extracts features at five different resolutions by utilizing shifted windows for computing self attention and is connected to an fcnn based decoder at each resolution via skip connections we have participated in brats 2021 segmentation challenge and our proposed model ranks among the top performing approaches in the validation phase code
2022,visual prompt tuning,menglin jia et al,https://link.springer.com/chapter/10.1007/978-3-031-19827-4_41,vision,the current modus operandi in adapting pre trained models involves updating all the backbone parameters i e full fine tuning this paper introduces visual prompt tuning vpt as an efficient and effective alternative to full fine tuning for large scale transformer models in vision taking inspiration from recent advances in efficiently tuning large language models vpt introduces only a small amount less than 1 of model parameters of trainable parameters in the input space while keeping the model backbone frozen via extensive experiments on a wide variety of downstream recognition tasks we show that vpt achieves significant performance gains compared to other parameter efficient tuning protocols most importantly vpt even outperforms full fine tuning in many cases across model capacities and training data scales while reducing per task storage cost code is available at github com kmnp vpt,visual prompt tuning menglin jia et al the current modus operandi in adapting pre trained models involves updating all the backbone parameters i e full fine tuning this paper introduces visual prompt tuning vpt as an efficient and effective alternative to full fine tuning for large scale transformer models in vision taking inspiration from recent advances in efficiently tuning large language models vpt introduces only a small amount less than 1 of model parameters of trainable parameters in the input space while keeping the model backbone frozen via extensive experiments on a wide variety of downstream recognition tasks we show that vpt achieves significant performance gains compared to other parameter efficient tuning protocols most importantly vpt even outperforms full fine tuning in many cases across model capacities and training data scales while reducing per task storage cost code is available at github com kmnp vpt
1979,bootstrap methods another look at the jackknife,bradley effron,https://link.springer.com/chapter/10.1007/978-1-4612-4380-9_41,ml_sampling,we discuss the following problem given a random sample x x 1 x 2 x n from an unknown probability distribution f estimate the sampling distribution of some prespecified random variable r x f on the basis of the observed data x standard jackknife theory gives an approximate mean and variance in the case r x f theta left hat f right theta left f right θ some parameter of interest a general method called the bootstrap is introduced and shown to work satisfactorily on a variety of estimation problems the jackknife is shown to be a linear approximation method for the bootstrap the exposition proceeds by a series of examples variance of the sample median error rates in a linear discriminant analysis ratio estimation estimating regression parameters etc,bootstrap methods another look at the jackknife bradley effron we discuss the following problem given a random sample x x 1 x 2 x n from an unknown probability distribution f estimate the sampling distribution of some prespecified random variable r x f on the basis of the observed data x standard jackknife theory gives an approximate mean and variance in the case r x f theta left hat f right theta left f right θ some parameter of interest a general method called the bootstrap is introduced and shown to work satisfactorily on a variety of estimation problems the jackknife is shown to be a linear approximation method for the bootstrap the exposition proceeds by a series of examples variance of the sample median error rates in a linear discriminant analysis ratio estimation estimating regression parameters etc
1986,induction of decision trees,j r quinlan,https://link.springer.com/article/10.1007/BF00116251,ml_classic,the technology for building knowledge based systems by inductive inference from examples has been demonstrated successfully in several practical applications this paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems and it describes one such system id3 in detail results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and or incomplete a reported shortcoming of the basic algorithm is discussed and two means of overcoming it are compared the paper concludes with illustrations of current research directions,induction of decision trees j r quinlan the technology for building knowledge based systems by inductive inference from examples has been demonstrated successfully in several practical applications this paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems and it describes one such system id3 in detail results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and or incomplete a reported shortcoming of the basic algorithm is discussed and two means of overcoming it are compared the paper concludes with illustrations of current research directions
1990,the strength of weak learnability,robert e schapire,https://link.springer.com/article/10.1007/BF00116037,ml_classic,this paper addresses the problem of improving the accuracy of an hypothesis output by a learning algorithm in the distribution free pac learning model a concept class islearnable orstrongly learnable if given access to a source of examples of the unknown concept the learner with high probability is able to output an hypothesis that is correct on all but an arbitrarily small fraction of the instances the concept class isweakly learnable if the learner can produce an hypothesis that performs only slightly better than random guessing in this paper it is shown that these two notions of learnability are equivalent a method is described for converting a weak learning algorithm into one that achieves arbitrarily high accuracy this construction may have practical applications as a tool for efficiently converting a mediocre learning algorithm into one that performs extremely well in addition the construction has some interesting theoretical consequences including a set of general upper bounds on the complexity of any strong learning algorithm as a function of the allowed error ο,the strength of weak learnability robert e schapire this paper addresses the problem of improving the accuracy of an hypothesis output by a learning algorithm in the distribution free pac learning model a concept class islearnable orstrongly learnable if given access to a source of examples of the unknown concept the learner with high probability is able to output an hypothesis that is correct on all but an arbitrarily small fraction of the instances the concept class isweakly learnable if the learner can produce an hypothesis that performs only slightly better than random guessing in this paper it is shown that these two notions of learnability are equivalent a method is described for converting a weak learning algorithm into one that achieves arbitrarily high accuracy this construction may have practical applications as a tool for efficiently converting a mediocre learning algorithm into one that performs extremely well in addition the construction has some interesting theoretical consequences including a set of general upper bounds on the complexity of any strong learning algorithm as a function of the allowed error ο
1991,bagging predictors,leo breiman,https://link.springer.com/article/10.1007/BF00058655,ml_classic,bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor the aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class the multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy the vital element is the instability of the prediction method if perturbing the learning set can cause significant changes in the predictor constructed then bagging can improve accuracy,bagging predictors leo breiman bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor the aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class the multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy the vital element is the instability of the prediction method if perturbing the learning set can cause significant changes in the predictor constructed then bagging can improve accuracy
1999,improved boosting algorithms using confidence rated predictions,yoav freund and robert e schapire,https://link.springer.com/article/10.1023/A:1007614523901,ml_classic,we describe several improvements to freund and schapire s adaboost boosting algorithm particularly in a setting in which hypotheses may assign confidences to each of their predictions we give a simplified analysis of adaboost in this setting and we show how this analysis can be used to find improved parameter settings as well as a refined criterion for training weak hypotheses we give a specific method for assigning confidences to the predictions of decision trees a method closely related to one used by quinlan this method also suggests a technique for growing decision trees which turns out to be identical to one proposed by kearns and mansour we focus next on how to apply the new boosting algorithms to multiclass classification problems particularly to the multi label case in which each example may belong to more than one class we give two boosting methods for this problem plus a third method based on output coding one of these leads to a new method for handling the single label case which is simpler but as effective as techniques suggested by freund and schapire finally we give some experimental results comparing a few of the algorithms discussed in this paper,improved boosting algorithms using confidence rated predictions yoav freund and robert e schapire we describe several improvements to freund and schapire s adaboost boosting algorithm particularly in a setting in which hypotheses may assign confidences to each of their predictions we give a simplified analysis of adaboost in this setting and we show how this analysis can be used to find improved parameter settings as well as a refined criterion for training weak hypotheses we give a specific method for assigning confidences to the predictions of decision trees a method closely related to one used by quinlan this method also suggests a technique for growing decision trees which turns out to be identical to one proposed by kearns and mansour we focus next on how to apply the new boosting algorithms to multiclass classification problems particularly to the multi label case in which each example may belong to more than one class we give two boosting methods for this problem plus a third method based on output coding one of these leads to a new method for handling the single label case which is simpler but as effective as techniques suggested by freund and schapire finally we give some experimental results comparing a few of the algorithms discussed in this paper
2001,random forests,leo breiman,https://link.springer.com/article/10.1023/A:1010933404324,ml_classic,random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest the generalization error for forests converges a s to a limit as the number of trees in the forest becomes large the generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them using a random selection of features to split each node yields error rates that compare favorably to adaboost y freund r schapire machine learning proceedings of the thirteenth international conference 148 156 but are more robust with respect to noise internal estimates monitor error strength and correlation and these are used to show the response to increasing the number of features used in the splitting internal estimates are also used to measure variable importance these ideas are also applicable to regression,random forests leo breiman random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest the generalization error for forests converges a s to a limit as the number of trees in the forest becomes large the generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them using a random selection of features to split each node yields error rates that compare favorably to adaboost y freund r schapire machine learning proceedings of the thirteenth international conference 148 156 but are more robust with respect to noise internal estimates monitor error strength and correlation and these are used to show the response to increasing the number of features used in the splitting internal estimates are also used to measure variable importance these ideas are also applicable to regression
1988,learning to predict by the methods of temporal differences,richard s sutton,https://link.springer.com/article/10.1007/BF00115009,dl_rl,this article introduces a class of incremental learning procedures specialized for prediction that is for using past experience with an incompletely known system to predict its future behavior whereas conventional prediction learning methods assign credit by means of the difference between predicted and actual outcomes the new methods assign credit by means of the difference between temporally successive predictions although such temporal difference methods have been used in samuel s checker player holland s bucket brigade and the author s adaptive heuristic critic they have remained poorly understood here we prove their convergence and optimality for special cases and relate them to supervised learning methods for most real world prediction problems temporal difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions we argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal difference methods can be applied to advantage,learning to predict by the methods of temporal differences richard s sutton this article introduces a class of incremental learning procedures specialized for prediction that is for using past experience with an incompletely known system to predict its future behavior whereas conventional prediction learning methods assign credit by means of the difference between predicted and actual outcomes the new methods assign credit by means of the difference between temporally successive predictions although such temporal difference methods have been used in samuel s checker player holland s bucket brigade and the author s adaptive heuristic critic they have remained poorly understood here we prove their convergence and optimality for special cases and relate them to supervised learning methods for most real world prediction problems temporal difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions we argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal difference methods can be applied to advantage
1982,self organized formation of topologically correct feature maps,teuvo kohonen,https://link.springer.com/article/10.1007/BF00337288,ml_representation,this work contains a theoretical study and computer simulations of a new self organizing process the principal discovery is that in a simple network of adaptive physical elements which receives signals from a primary event space the signal representations are automatically mapped onto a set of output responses in such a way that the responses acquire the same topological order as that of the primary events in other words a principle has been discovered which facilitates the automatic formation of topologically correct maps of features of observable events the basic self organizing system is a one or two dimensional array of processing units resembling a network of threshold logic units and characterized by short range lateral feedback between neighbouring units several types of computer simulations are used to demonstrate the ordering process as well as the conditions under which it fails,self organized formation of topologically correct feature maps teuvo kohonen this work contains a theoretical study and computer simulations of a new self organizing process the principal discovery is that in a simple network of adaptive physical elements which receives signals from a primary event space the signal representations are automatically mapped onto a set of output responses in such a way that the responses acquire the same topological order as that of the primary events in other words a principle has been discovered which facilitates the automatic formation of topologically correct maps of features of observable events the basic self organizing system is a one or two dimensional array of processing units resembling a network of threshold logic units and characterized by short range lateral feedback between neighbouring units several types of computer simulations are used to demonstrate the ordering process as well as the conditions under which it fails
1991,recurrent networks and narma modeling,j connor et al,https://dl.acm.org/doi/10.5555/2986916.2986953,dl_rnn,there exist large classes of time series such as those with nonlinear moving average components that are not well modeled by feedforward networks or linear models but can be modeled by recurrent networks we show that recurrent neural networks are a type of nonlinear autoregressive moving average narma model practical ability will be shown in the results of a competition sponsored by the puget sound power and light company where the recurrent networks gave the best performance on electric load forecasting,recurrent networks and narma modeling j connor et al there exist large classes of time series such as those with nonlinear moving average components that are not well modeled by feedforward networks or linear models but can be modeled by recurrent networks we show that recurrent neural networks are a type of nonlinear autoregressive moving average narma model practical ability will be shown in the results of a competition sponsored by the puget sound power and light company where the recurrent networks gave the best performance on electric load forecasting
2023,3d gaussian splatting for real time radiance field rendering,bernhard kerbl et al,https://dl.acm.org/doi/abs/10.1145/3592433,vision,radiance field methods have recently revolutionized novel view synthesis of scenes captured with multiple photos or videos however achieving high visual quality still requires neural networks that are costly to train and render while recent faster methods inevitably trade off speed for quality for unbounded and complete scenes rather than isolated objects and 1080p resolution rendering no current method can achieve real time display rates we introduce three key elements that allow us to achieve state of the art visual quality while maintaining competitive training times and importantly allow high quality real time 30 fps novel view synthesis at 1080p resolution first starting from sparse points produced during camera calibration we represent the scene with 3d gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space second we perform interleaved optimization density control of the 3d gaussians notably optimizing anisotropic covariance to achieve an accurate representation of the scene third we develop a fast visibility aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering we demonstrate state of the art visual quality and real time rendering on several established datasets,3d gaussian splatting for real time radiance field rendering bernhard kerbl et al radiance field methods have recently revolutionized novel view synthesis of scenes captured with multiple photos or videos however achieving high visual quality still requires neural networks that are costly to train and render while recent faster methods inevitably trade off speed for quality for unbounded and complete scenes rather than isolated objects and 1080p resolution rendering no current method can achieve real time display rates we introduce three key elements that allow us to achieve state of the art visual quality while maintaining competitive training times and importantly allow high quality real time 30 fps novel view synthesis at 1080p resolution first starting from sparse points produced during camera calibration we represent the scene with 3d gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space second we perform interleaved optimization density control of the 3d gaussians notably optimizing anisotropic covariance to achieve an accurate representation of the scene third we develop a fast visibility aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering we demonstrate state of the art visual quality and real time rendering on several established datasets
1992,a training algorithm for optimal margin classifier,bernhard e boser et al,https://dl.acm.org/doi/10.1145/130385.130401,ml_classic,a training algorithm that maximizes the margin between the training patterns and the decision boundary is presented the technique is applicable to a wide variety of the classification functions including perceptrons polynomials and radial basis functions the effective number of parameters is adjusted automatically to match the complexity of the problem the solution is expressed as a linear combination of supporting patterns these are the subset of training patterns that are closest to the decision boundary bounds on the generalization performance based on the leave one out method and the vc dimension are given experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms,a training algorithm for optimal margin classifier bernhard e boser et al a training algorithm that maximizes the margin between the training patterns and the decision boundary is presented the technique is applicable to a wide variety of the classification functions including perceptrons polynomials and radial basis functions the effective number of parameters is adjusted automatically to match the complexity of the problem the solution is expressed as a linear combination of supporting patterns these are the subset of training patterns that are closest to the decision boundary bounds on the generalization performance based on the leave one out method and the vc dimension are given experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms
2003,document clustering based on non negative matrix factorization,wei xu et al,https://dl.acm.org/doi/10.1145/860435.860485,ml_classic,in this paper we propose a novel document clustering method based on the non negative factorization of the term document matrix of the given document corpus in the latent semantic space derived by the non negative matrix factorization nmf each axis captures the base topic of a particular document cluster and each document is represented as an additive combination of the base topics the cluster membership of each document can be easily determined by finding the base topic the axis with which the document has the largest projection value our experimental evaluations show that the proposed document clustering method surpasses the latent semantic indexing and the spectral clustering methods not only in the easy and reliable derivation of document clustering results but also in document clustering accuracies,document clustering based on non negative matrix factorization wei xu et al in this paper we propose a novel document clustering method based on the non negative factorization of the term document matrix of the given document corpus in the latent semantic space derived by the non negative matrix factorization nmf each axis captures the base topic of a particular document cluster and each document is represented as an additive combination of the base topics the cluster membership of each document can be easily determined by finding the base topic the axis with which the document has the largest projection value our experimental evaluations show that the proposed document clustering method surpasses the latent semantic indexing and the spectral clustering methods not only in the easy and reliable derivation of document clustering results but also in document clustering accuracies
1989,learnability and the vapnik chervonenkis dimension,blumer et al,https://dl.acm.org/doi/10.1145/76359.76371,learning_theory,valiant s learnability model is extended to learning classes of concepts defined by regions in euclidean space en the methods in this paper lead to a unified treatment of some of valiant s results along with previous results on distribution free convergence of certain pattern recognition algorithms it is shown that the essential condition for distribution free learnability is finiteness of the vapnik chervonenkis dimension a simple combinatorial parameter of the class of concepts to be learned using this parameter the complexity and closure properties of learnable classes are analyzed and the necessary and sufficient conditions are provided for feasible learnability,learnability and the vapnik chervonenkis dimension blumer et al valiant s learnability model is extended to learning classes of concepts defined by regions in euclidean space en the methods in this paper lead to a unified treatment of some of valiant s results along with previous results on distribution free convergence of certain pattern recognition algorithms it is shown that the essential condition for distribution free learnability is finiteness of the vapnik chervonenkis dimension a simple combinatorial parameter of the class of concepts to be learned using this parameter the complexity and closure properties of learnable classes are analyzed and the necessary and sufficient conditions are provided for feasible learnability
1987,hybrid monte carlo,simon duane et al,https://www.sciencedirect.com/science/article/abs/pii/037026938791197X,ml_sampling,we present a new method for the numerical simulation of lattice field theory a hybrid molecular dynamics langevin algorithm is used to guide a monte carlo simulation there are no discretization errors even for large step sizes the method is especially efficient for systems such as quantum chromodynamics which contain fermionic degrees of freedom detailed results are presented for four dimensional compact quantum electrodynamics including the dynamical effects of electrons,hybrid monte carlo simon duane et al we present a new method for the numerical simulation of lattice field theory a hybrid molecular dynamics langevin algorithm is used to guide a monte carlo simulation there are no discretization errors even for large step sizes the method is especially efficient for systems such as quantum chromodynamics which contain fermionic degrees of freedom detailed results are presented for four dimensional compact quantum electrodynamics including the dynamical effects of electrons
1997,a decision theoretic generalization of on line learning and an application to boosting,yoav freund and robert e schapire,https://www.sciencedirect.com/science/article/pii/S002200009791504X,ml_classic,in the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst case on line framework the model we study can be interpreted as a broad abstract extension of the well studied on line prediction model to a general decision theoretic setting we show that the multiplicative weight update littlestone warmuth rule can be adapted to this model yielding bounds that are slightly weaker in some cases but applicable to a considerably more general class of learning problems we show how the resulting learning algorithm can be applied to a variety of problems including gambling multiple outcome prediction repeated games and prediction of points in rn in the second part of the paper we apply the multiplicative weight update technique to derive a new boosting algorithm this boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm we also study generalizations of the new boosting algorithm to the problem of learning functions whose range rather than being binary is an arbitrary finite set or a bounded segment of the real line,a decision theoretic generalization of on line learning and an application to boosting yoav freund and robert e schapire in the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst case on line framework the model we study can be interpreted as a broad abstract extension of the well studied on line prediction model to a general decision theoretic setting we show that the multiplicative weight update littlestone warmuth rule can be adapted to this model yielding bounds that are slightly weaker in some cases but applicable to a considerably more general class of learning problems we show how the resulting learning algorithm can be applied to a variety of problems including gambling multiple outcome prediction repeated games and prediction of points in rn in the second part of the paper we apply the multiplicative weight update technique to derive a new boosting algorithm this boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm we also study generalizations of the new boosting algorithm to the problem of learning functions whose range rather than being binary is an arbitrary finite set or a bounded segment of the real line
1989,multilayer feedforward networks are universal approximators,hornik et al,https://www.sciencedirect.com/science/article/abs/pii/0893608089900208,learning_theory,this paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any borel measurable function from one finite dimensional space to another to any desired degree of accuracy provided sufficiently many hidden units are available in this sense multilayer feedforward networks are a class of universal approximators,multilayer feedforward networks are universal approximators hornik et al this paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any borel measurable function from one finite dimensional space to another to any desired degree of accuracy provided sufficiently many hidden units are available in this sense multilayer feedforward networks are a class of universal approximators
1991,approximation capabilities of multilayer feedforward networks,kurt hornik,https://www.sciencedirect.com/science/article/abs/pii/089360809190009T,learning_theory,we show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to lp μ performance criteria for arbitrary finite input environment measures μ provided only that sufficiently many hidden units are available if the activation function is continuous bounded and nonconstant then continuous mappings can be learned uniformly over compact input sets we also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives,approximation capabilities of multilayer feedforward networks kurt hornik we show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to lp μ performance criteria for arbitrary finite input environment measures μ provided only that sufficiently many hidden units are available if the activation function is continuous bounded and nonconstant then continuous mappings can be learned uniformly over compact input sets we also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives
1993,multilayer feedforward networks with a nonpolynomial activation function can approximate any function,moshe leshno et al,https://www.sciencedirect.com/science/article/abs/pii/S0893608005801315,learning_theory,several researchers characterized the activation function under which multilayer feedforward networks can act as universal approximators we show that most of all the characterizations that were reported thus far in the literature are special cases of the following general result a standard multilayer feedforward network with a locally bounded piecewise continuous activation function can approximate any continuous function to any degree of accuracy if and only if the network s activation function is not a polynomial we also emphasize the important role of the threshold asserting that without it the last theorem does not hold,multilayer feedforward networks with a nonpolynomial activation function can approximate any function moshe leshno et al several researchers characterized the activation function under which multilayer feedforward networks can act as universal approximators we show that most of all the characterizations that were reported thus far in the literature are special cases of the following general result a standard multilayer feedforward network with a locally bounded piecewise continuous activation function can approximate any continuous function to any degree of accuracy if and only if the network s activation function is not a polynomial we also emphasize the important role of the threshold asserting that without it the last theorem does not hold
2009,feature hashing for large scale multitask learning,kilian weinberger et al,https://arxiv.org/abs/0902.2206,ml_representation,empirical evidence suggests that hashing is an effective strategy for dimensionality reduction and practical nonparametric estimation in this paper we provide exponential tail bounds for feature hashing and show that the interaction between random subspaces is negligible with high probability we demonstrate the feasibility of this approach with experimental results for a new use case multitask learning with hundreds of thousands of tasks,feature hashing for large scale multitask learning kilian weinberger et al empirical evidence suggests that hashing is an effective strategy for dimensionality reduction and practical nonparametric estimation in this paper we provide exponential tail bounds for feature hashing and show that the interaction between random subspaces is negligible with high probability we demonstrate the feasibility of this approach with experimental results for a new use case multitask learning with hundreds of thousands of tasks
2009,gaussian process optimization in the bandit setting no regret and experimental design,niranjan srinivas et al,https://arxiv.org/abs/0912.3995,ml_tuning,many applications require optimizing an unknown noisy function that is expensive to evaluate we formalize this task as a multi armed bandit problem where the payoff function is either sampled from a gaussian process gp or has low rkhs norm we resolve the important open problem of deriving regret bounds for this setting which imply novel convergence rates for gp optimization we analyze gp ucb an intuitive upper confidence based algorithm and bound its cumulative regret in terms of maximal information gain establishing a novel connection between gp optimization and experimental design moreover by bounding the latter in terms of operator spectra we obtain explicit sublinear regret bounds for many commonly used covariance functions in some important cases our bounds have surprisingly weak dependence on the dimensionality in our experiments on real sensor data gp ucb compares favorably with other heuristical gp optimization approaches,gaussian process optimization in the bandit setting no regret and experimental design niranjan srinivas et al many applications require optimizing an unknown noisy function that is expensive to evaluate we formalize this task as a multi armed bandit problem where the payoff function is either sampled from a gaussian process gp or has low rkhs norm we resolve the important open problem of deriving regret bounds for this setting which imply novel convergence rates for gp optimization we analyze gp ucb an intuitive upper confidence based algorithm and bound its cumulative regret in terms of maximal information gain establishing a novel connection between gp optimization and experimental design moreover by bounding the latter in terms of operator spectra we obtain explicit sublinear regret bounds for many commonly used covariance functions in some important cases our bounds have surprisingly weak dependence on the dimensionality in our experiments on real sensor data gp ucb compares favorably with other heuristical gp optimization approaches
2002,smote synthetic minority over sampling technique,nitesh v chawla et al,https://arxiv.org/abs/1106.1813,ml_sampling,an approach to the construction of classifiers from imbalanced datasets is described a dataset is imbalanced if the classification categories are not approximately equally represented often real world data sets are predominately composed of normal examples with only a small percentage of abnormal or interesting examples it is also the case that the cost of misclassifying an abnormal interesting example as a normal example is often much higher than the cost of the reverse error under sampling of the majority normal class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class this paper shows that a combination of our method of over sampling the minority abnormal class and under sampling the majority normal class can achieve better classifier performance in roc space than only under sampling the majority class this paper also shows that a combination of our method of over sampling the minority class and under sampling the majority class can achieve better classifier performance in roc space than varying the loss ratios in ripper or class priors in naive bayes our method of over sampling the minority class involves creating synthetic minority class examples experiments are performed using c4 5 ripper and a naive bayes classifier the method is evaluated using the area under the receiver operating characteristic curve auc and the roc convex hull strategy,smote synthetic minority over sampling technique nitesh v chawla et al an approach to the construction of classifiers from imbalanced datasets is described a dataset is imbalanced if the classification categories are not approximately equally represented often real world data sets are predominately composed of normal examples with only a small percentage of abnormal or interesting examples it is also the case that the cost of misclassifying an abnormal interesting example as a normal example is often much higher than the cost of the reverse error under sampling of the majority normal class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class this paper shows that a combination of our method of over sampling the minority abnormal class and under sampling the majority normal class can achieve better classifier performance in roc space than only under sampling the majority class this paper also shows that a combination of our method of over sampling the minority class and under sampling the majority class can achieve better classifier performance in roc space than varying the loss ratios in ripper or class priors in naive bayes our method of over sampling the minority class involves creating synthetic minority class examples experiments are performed using c4 5 ripper and a naive bayes classifier the method is evaluated using the area under the receiver operating characteristic curve auc and the roc convex hull strategy
2011,hogwild a lock free approach to parallelizing stochastic gradient descent,feng niu et al,https://arxiv.org/abs/1106.5730,dl_general,stochastic gradient descent sgd is a popular algorithm that can achieve state of the art performance on a variety of machine learning tasks several researchers have recently proposed schemes to parallelize sgd but all require performance destroying memory locking and synchronization this work aims to show using novel theoretical analysis algorithms and implementation that sgd can be implemented without any locking we present an update scheme called hogwild which allows processors access to shared memory with the possibility of overwriting each other s work we show that when the associated optimization problem is sparse meaning most gradient updates only modify small parts of the decision variable then hogwild achieves a nearly optimal rate of convergence we demonstrate experimentally that hogwild outperforms alternative schemes that use locking by an order of magnitude,hogwild a lock free approach to parallelizing stochastic gradient descent feng niu et al stochastic gradient descent sgd is a popular algorithm that can achieve state of the art performance on a variety of machine learning tasks several researchers have recently proposed schemes to parallelize sgd but all require performance destroying memory locking and synchronization this work aims to show using novel theoretical analysis algorithms and implementation that sgd can be implemented without any locking we present an update scheme called hogwild which allows processors access to shared memory with the possibility of overwriting each other s work we show that when the associated optimization problem is sparse meaning most gradient updates only modify small parts of the decision variable then hogwild achieves a nearly optimal rate of convergence we demonstrate experimentally that hogwild outperforms alternative schemes that use locking by an order of magnitude
2013,maxout networks,i goodfellow et al,http://arxiv.org/pdf/1302.4389v4,vision,we consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout we define a simple new model called maxout so named because its output is the max of a set of inputs and because it is a natural companion to dropout designed to both facilitate optimization by dropout and improve the accuracy of dropout s fast approximate model averaging technique we empirically verify that the model successfully accomplishes both of these tasks we use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets mnist cifar 10 cifar 100 and svhn,maxout networks i goodfellow et al we consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout we define a simple new model called maxout so named because its output is the max of a set of inputs and because it is a natural companion to dropout designed to both facilitate optimization by dropout and improve the accuracy of dropout s fast approximate model averaging technique we empirically verify that the model successfully accomplishes both of these tasks we use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets mnist cifar 10 cifar 100 and svhn
2013,speech recognition with deep recurrent neural networks,a graves,http://arxiv.org/pdf/1303.5778.pdf,audio,recurrent neural networks rnns are a powerful model for sequential data end to end training methods such as connectionist temporal classification make it possible to train rnns for sequence labelling problems where the input output alignment is unknown the combination of these methods with the long short term memory rnn architecture has proved particularly fruitful delivering state of the art results in cursive handwriting recognition however rnn performance in speech recognition has so far been disappointing with better results returned by deep feedforward networks this paper investigates emph deep recurrent neural networks which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers rnns when trained end to end with suitable regularisation we find that deep long short term memory rnns achieve a test set error of 17 7 on the timit phoneme recognition benchmark which to our knowledge is the best recorded score,speech recognition with deep recurrent neural networks a graves recurrent neural networks rnns are a powerful model for sequential data end to end training methods such as connectionist temporal classification make it possible to train rnns for sequence labelling problems where the input output alignment is unknown the combination of these methods with the long short term memory rnn architecture has proved particularly fruitful delivering state of the art results in cursive handwriting recognition however rnn performance in speech recognition has so far been disappointing with better results returned by deep feedforward networks this paper investigates emph deep recurrent neural networks which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers rnns when trained end to end with suitable regularisation we find that deep long short term memory rnns achieve a test set error of 17 7 on the timit phoneme recognition benchmark which to our knowledge is the best recorded score
2014,generating sequences with recurrent neural networks,alex graves,https://arxiv.org/pdf/1308.0850.pdf,dl_nlp,this paper shows how long short term memory recurrent neural networks can be used to generate complex sequences with long range structure simply by predicting one data point at a time the approach is demonstrated for text where the data are discrete and online handwriting where the data are real valued it is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence the resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles,generating sequences with recurrent neural networks alex graves this paper shows how long short term memory recurrent neural networks can be used to generate complex sequences with long range structure simply by predicting one data point at a time the approach is demonstrated for text where the data are discrete and online handwriting where the data are real valued it is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence the resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles
2014,decaf a deep convolutional activation feature for generic visual recognition,j donahue et al,http://arxiv.org/pdf/1310.1531,vision,we evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large fixed set of object recognition tasks can be re purposed to novel generic tasks our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks we investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks including scene recognition domain adaptation and fine grained recognition challenges we compare the efficacy of relying on various network levels to define a fixed feature and report novel results that significantly outperform the state of the art on several important vision challenges we are releasing decaf an open source implementation of these deep convolutional activation features along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms,decaf a deep convolutional activation feature for generic visual recognition j donahue et al we evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large fixed set of object recognition tasks can be re purposed to novel generic tasks our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks we investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks including scene recognition domain adaptation and fine grained recognition challenges we compare the efficacy of relying on various network levels to define a fixed feature and report novel results that significantly outperform the state of the art on several important vision challenges we are releasing decaf an open source implementation of these deep convolutional activation features along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms
2013,distributed representations of words and phrases and their compositionality,tomas mikolov et al,https://arxiv.org/abs/1310.4546,dl_nlp,the recently introduced continuous skip gram model is an efficient method for learning high quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships in this paper we present several extensions that improve both the quality of the vectors and the training speed by subsampling of the frequent words we obtain significant speedup and also learn more regular word representations we also describe a simple alternative to the hierarchical softmax called negative sampling an inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases for example the meanings of canada and air cannot be easily combined to obtain air canada motivated by this example we present a simple method for finding phrases in text and show that learning good vector representations for millions of phrases is possible,distributed representations of words and phrases and their compositionality tomas mikolov et al the recently introduced continuous skip gram model is an efficient method for learning high quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships in this paper we present several extensions that improve both the quality of the vectors and the training speed by subsampling of the frequent words we obtain significant speedup and also learn more regular word representations we also describe a simple alternative to the hierarchical softmax called negative sampling an inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases for example the meanings of canada and air cannot be easily combined to obtain air canada motivated by this example we present a simple method for finding phrases in text and show that learning good vector representations for millions of phrases is possible
2014,rich feature hierarchies for accurate object detection and semantic segmentation,r girshick et al,https://arxiv.org/abs/1311.2524,vision,object detection performance as measured on the canonical pascal voc dataset has plateaued in the last few years the best performing methods are complex ensemble systems that typically combine multiple low level image features with high level context in this paper we propose a simple and scalable detection algorithm that improves mean average precision map by more than 30 relative to the previous best result on voc 2012 achieving a map of 53 3 our approach combines two key insights 1 one can apply high capacity convolutional neural networks cnns to bottom up region proposals in order to localize and segment objects and 2 when labeled training data is scarce supervised pre training for an auxiliary task followed by domain specific fine tuning yields a significant performance boost since we combine region proposals with cnns we call our method r cnn regions with cnn features we also compare r cnn to overfeat a recently proposed sliding window detector based on a similar cnn architecture we find that r cnn outperforms overfeat by a large margin on the 200 class ilsvrc2013 detection dataset source code for the complete system is available at,rich feature hierarchies for accurate object detection and semantic segmentation r girshick et al object detection performance as measured on the canonical pascal voc dataset has plateaued in the last few years the best performing methods are complex ensemble systems that typically combine multiple low level image features with high level context in this paper we propose a simple and scalable detection algorithm that improves mean average precision map by more than 30 relative to the previous best result on voc 2012 achieving a map of 53 3 our approach combines two key insights 1 one can apply high capacity convolutional neural networks cnns to bottom up region proposals in order to localize and segment objects and 2 when labeled training data is scarce supervised pre training for an auxiliary task followed by domain specific fine tuning yields a significant performance boost since we combine region proposals with cnns we call our method r cnn regions with cnn features we also compare r cnn to overfeat a recently proposed sliding window detector based on a similar cnn architecture we find that r cnn outperforms overfeat by a large margin on the 200 class ilsvrc2013 detection dataset source code for the complete system is available at
2014,visualizing and understanding convolutional networks,m zeiler and r fergus,http://arxiv.org/pdf/1311.2901,vision,large convolutional network models have recently demonstrated impressive classification performance on the imagenet benchmark however there is no clear understanding of why they perform so well or how they might be improved in this paper we address both issues we introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier we also perform an ablation study to discover the performance contribution from different model layers this enables us to find model architectures that outperform krizhevsky etal on the imagenet classification benchmark we show our imagenet model generalizes well to other datasets when the softmax classifier is retrained it convincingly beats the current state of the art results on caltech 101 and caltech 256 datasets,visualizing and understanding convolutional networks m zeiler and r fergus large convolutional network models have recently demonstrated impressive classification performance on the imagenet benchmark however there is no clear understanding of why they perform so well or how they might be improved in this paper we address both issues we introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier we also perform an ablation study to discover the performance contribution from different model layers this enables us to find model architectures that outperform krizhevsky etal on the imagenet classification benchmark we show our imagenet model generalizes well to other datasets when the softmax classifier is retrained it convincingly beats the current state of the art results on caltech 101 and caltech 256 datasets
2013,network in network,m lin et al,http://arxiv.org/pdf/1312.4400,vision,we propose a novel deep network structure called network in network nin to enhance model discriminability for local patches within the receptive field the conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input instead we build micro neural networks with more complex structures to abstract the data within the receptive field we instantiate the micro neural network with a multilayer perceptron which is a potent function approximator the feature maps are obtained by sliding the micro networks over the input in a similar manner as cnn they are then fed into the next layer deep nin can be implemented by stacking mutiple of the above described structure with enhanced local modeling via the micro network we are able to utilize global average pooling over feature maps in the classification layer which is easier to interpret and less prone to overfitting than traditional fully connected layers we demonstrated the state of the art classification performances with nin on cifar 10 and cifar 100 and reasonable performances on svhn and mnist datasets,network in network m lin et al we propose a novel deep network structure called network in network nin to enhance model discriminability for local patches within the receptive field the conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input instead we build micro neural networks with more complex structures to abstract the data within the receptive field we instantiate the micro neural network with a multilayer perceptron which is a potent function approximator the feature maps are obtained by sliding the micro networks over the input in a similar manner as cnn they are then fed into the next layer deep nin can be implemented by stacking mutiple of the above described structure with enhanced local modeling via the micro network we are able to utilize global average pooling over feature maps in the classification layer which is easier to interpret and less prone to overfitting than traditional fully connected layers we demonstrated the state of the art classification performances with nin on cifar 10 and cifar 100 and reasonable performances on svhn and mnist datasets
2013,playing atari with deep reinforcement learning,volodymyr mnih et al,https://arxiv.org/abs/1312.5602,dl_rl,we present the first deep learning model to successfully learn control policies directly from high dimensional sensory input using reinforcement learning the model is a convolutional neural network trained with a variant of q learning whose input is raw pixels and whose output is a value function estimating future rewards we apply our method to seven atari 2600 games from the arcade learning environment with no adjustment of the architecture or learning algorithm we find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them,playing atari with deep reinforcement learning volodymyr mnih et al we present the first deep learning model to successfully learn control policies directly from high dimensional sensory input using reinforcement learning the model is a convolutional neural network trained with a variant of q learning whose input is raw pixels and whose output is a value function estimating future rewards we apply our method to seven atari 2600 games from the arcade learning environment with no adjustment of the architecture or learning algorithm we find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them
2013,auto encoding variational bayes,diederik p kingma and max welling,https://arxiv.org/abs/1312.6114,ml_representation,how can we perform efficient inference and learning in directed probabilistic models in the presence of continuous latent variables with intractable posterior distributions and large datasets we introduce a stochastic variational inference and learning algorithm that scales to large datasets and under some mild differentiability conditions even works in the intractable case our contributions are two fold first we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods second we show that for i i d datasets with continuous latent variables per datapoint posterior inference can be made especially efficient by fitting an approximate inference model also called a recognition model to the intractable posterior using the proposed lower bound estimator theoretical advantages are reflected in experimental results,auto encoding variational bayes diederik p kingma and max welling how can we perform efficient inference and learning in directed probabilistic models in the presence of continuous latent variables with intractable posterior distributions and large datasets we introduce a stochastic variational inference and learning algorithm that scales to large datasets and under some mild differentiability conditions even works in the intractable case our contributions are two fold first we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods second we show that for i i d datasets with continuous latent variables per datapoint posterior inference can be made especially efficient by fitting an approximate inference model also called a recognition model to the intractable posterior using the proposed lower bound estimator theoretical advantages are reflected in experimental results
2013,overfeat integrated recognition localization and detection using convolutional networks,p sermanet et al,http://arxiv.org/pdf/1312.6229,vision,we present an integrated framework for using convolutional networks for classification localization and detection we show how a multiscale and sliding window approach can be efficiently implemented within a convnet we also introduce a novel deep learning approach to localization by learning to predict object boundaries bounding boxes are then accumulated rather than suppressed in order to increase detection confidence we show that different tasks can be learned simultaneously using a single shared network this integrated framework is the winner of the localization task of the imagenet large scale visual recognition challenge 2013 ilsvrc2013 and obtained very competitive results for the detection and classifications tasks in post competition work we establish a new state of the art for the detection task finally we release a feature extractor from our best model called overfeat,overfeat integrated recognition localization and detection using convolutional networks p sermanet et al we present an integrated framework for using convolutional networks for classification localization and detection we show how a multiscale and sliding window approach can be efficiently implemented within a convnet we also introduce a novel deep learning approach to localization by learning to predict object boundaries bounding boxes are then accumulated rather than suppressed in order to increase detection confidence we show that different tasks can be learned simultaneously using a single shared network this integrated framework is the winner of the localization task of the imagenet large scale visual recognition challenge 2013 ilsvrc2013 and obtained very competitive results for the detection and classifications tasks in post competition work we establish a new state of the art for the detection task finally we release a feature extractor from our best model called overfeat
2014,cnn features off the shelf an astounding baseline for recognition,a razavian et al,https://arxiv.org/abs/1403.6382,vision,recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful this paper adds to the mounting evidence that this is indeed the case we report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the overfeat network which was trained to perform object classification on ilsvrc13 we use features extracted from the overfeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification scene recognition fine grained recognition attribute detection and image retrieval applied to a diverse set of datasets we selected these tasks and datasets as they gradually move further away from the original task and data the overfeat network was trained to solve astonishingly we report consistent superior results compared to the highly tuned state of the art systems in all the visual classification tasks on various datasets for instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset the results are achieved using a linear svm classifier or l2 distance in case of retrieval applied to a feature representation of size 4096 extracted from a layer in the net the representations are further modified using simple augmentation techniques e g jittering the results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks,cnn features off the shelf an astounding baseline for recognition a razavian et al recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful this paper adds to the mounting evidence that this is indeed the case we report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the overfeat network which was trained to perform object classification on ilsvrc13 we use features extracted from the overfeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification scene recognition fine grained recognition attribute detection and image retrieval applied to a diverse set of datasets we selected these tasks and datasets as they gradually move further away from the original task and data the overfeat network was trained to solve astonishingly we report consistent superior results compared to the highly tuned state of the art systems in all the visual classification tasks on various datasets for instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset the results are achieved using a linear svm classifier or l2 distance in case of retrieval applied to a feature representation of size 4096 extracted from a layer in the net the representations are further modified using simple augmentation techniques e g jittering the results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks
2014,return of the devil in the details delving deep into convolutional nets,k chatfield et al,http://arxiv.org/pdf/1405.3531,vision,the latest generation of convolutional neural networks cnn have achieved impressive results in challenging benchmarks on image recognition and object detection significantly raising the interest of the community in these methods nevertheless it is still unclear how different cnn methods compare with each other and with previous state of the art shallow representations such as the bag of visual words and the improved fisher vector this paper conducts a rigorous evaluation of these new techniques exploring different deep architectures and comparing them on a common ground identifying and disclosing important implementation details we identify several useful properties of cnn based representations including the fact that the dimensionality of the cnn output layer can be reduced significantly without having an adverse effect on performance we also identify aspects of deep and shallow methods that can be successfully shared in particular we show that the data augmentation techniques commonly applied to cnn based methods can also be applied to shallow methods and result in an analogous performance boost source code and models to reproduce the experiments in the paper is made publicly available,return of the devil in the details delving deep into convolutional nets k chatfield et al the latest generation of convolutional neural networks cnn have achieved impressive results in challenging benchmarks on image recognition and object detection significantly raising the interest of the community in these methods nevertheless it is still unclear how different cnn methods compare with each other and with previous state of the art shallow representations such as the bag of visual words and the improved fisher vector this paper conducts a rigorous evaluation of these new techniques exploring different deep architectures and comparing them on a common ground identifying and disclosing important implementation details we identify several useful properties of cnn based representations including the fact that the dimensionality of the cnn output layer can be reduced significantly without having an adverse effect on performance we also identify aspects of deep and shallow methods that can be successfully shared in particular we show that the data augmentation techniques commonly applied to cnn based methods can also be applied to shallow methods and result in an analogous performance boost source code and models to reproduce the experiments in the paper is made publicly available
2014,two stream convolutional networks for action recognition in videos,k simonyan et al,https://arxiv.org/abs/1406.2199,vision,we investigate architectures of discriminatively trained deep convolutional networks convnets for action recognition in video the challenge is to capture the complementary information on appearance from still frames and motion between frames we also aim to generalise the best performing hand crafted features within a data driven learning framework our contribution is three fold first we propose a two stream convnet architecture which incorporates spatial and temporal networks second we demonstrate that a convnet trained on multi frame dense optical flow is able to achieve very good performance in spite of limited training data finally we show that multi task learning applied to two different action classification datasets can be used to increase the amount of training data and improve the performance on both our architecture is trained and evaluated on the standard video actions benchmarks of ucf 101 and hmdb 51 where it is competitive with the state of the art it also exceeds by a large margin previous attempts to use deep nets for video classification,two stream convolutional networks for action recognition in videos k simonyan et al we investigate architectures of discriminatively trained deep convolutional networks convnets for action recognition in video the challenge is to capture the complementary information on appearance from still frames and motion between frames we also aim to generalise the best performing hand crafted features within a data driven learning framework our contribution is three fold first we propose a two stream convnet architecture which incorporates spatial and temporal networks second we demonstrate that a convnet trained on multi frame dense optical flow is able to achieve very good performance in spite of limited training data finally we show that multi task learning applied to two different action classification datasets can be used to increase the amount of training data and improve the performance on both our architecture is trained and evaluated on the standard video actions benchmarks of ucf 101 and hmdb 51 where it is competitive with the state of the art it also exceeds by a large margin previous attempts to use deep nets for video classification
2013,identifying and attacking the saddle point problem in high dimensional non convex optimization,yann dauphin et al,https://arxiv.org/abs/1406.2572,dl_opt,a central challenge to many fields of science and engineering involves minimizing non convex error functions over continuous high dimensional spaces gradient descent or quasi newton methods are almost ubiquitously used to perform such minimizations and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum here we argue based on results from statistical physics random matrix theory neural network theory and empirical evidence that a deeper and more profound difficulty originates from the proliferation of saddle points not local minima especially in high dimensional problems of practical interest such saddle points are surrounded by high error plateaus that can dramatically slow down learning and give the illusory impression of the existence of a local minimum motivated by these arguments we propose a new approach to second order optimization the saddle free newton method that can rapidly escape high dimensional saddle points unlike gradient descent and quasi newton methods we apply this algorithm to deep or recurrent neural network training and provide numerical evidence for its superior optimization performance,identifying and attacking the saddle point problem in high dimensional non convex optimization yann dauphin et al a central challenge to many fields of science and engineering involves minimizing non convex error functions over continuous high dimensional spaces gradient descent or quasi newton methods are almost ubiquitously used to perform such minimizations and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum here we argue based on results from statistical physics random matrix theory neural network theory and empirical evidence that a deeper and more profound difficulty originates from the proliferation of saddle points not local minima especially in high dimensional problems of practical interest such saddle points are surrounded by high error plateaus that can dramatically slow down learning and give the illusory impression of the existence of a local minimum motivated by these arguments we propose a new approach to second order optimization the saddle free newton method that can rapidly escape high dimensional saddle points unlike gradient descent and quasi newton methods we apply this algorithm to deep or recurrent neural network training and provide numerical evidence for its superior optimization performance
2014,spatial pyramid pooling in deep convolutional networks for visual recognition,k he et al,http://arxiv.org/pdf/1406.4729,vision,existing deep convolutional neural networks cnns require a fixed size e g 224x224 input image this requirement is artificial and may reduce the recognition accuracy for the images or sub images of an arbitrary size scale in this work we equip the networks with another pooling strategy spatial pyramid pooling to eliminate the above requirement the new network structure called spp net can generate a fixed length representation regardless of image size scale pyramid pooling is also robust to object deformations with these advantages spp net should in general improve all cnn based image classification methods on the imagenet 2012 dataset we demonstrate that spp net boosts the accuracy of a variety of cnn architectures despite their different designs on the pascal voc 2007 and caltech101 datasets spp net achieves state of the art classification results using a single full image representation and no fine tuning the power of spp net is also significant in object detection using spp net we compute the feature maps from the entire image only once and then pool features in arbitrary regions sub images to generate fixed length representations for training the detectors this method avoids repeatedly computing the convolutional features in processing test images our method is 24 102x faster than the r cnn method while achieving better or comparable accuracy on pascal voc 2007 in imagenet large scale visual recognition challenge ilsvrc 2014 our methods rank 2 in object detection and 3 in image classification among all 38 teams this manuscript also introduces the improvement made for this competition,spatial pyramid pooling in deep convolutional networks for visual recognition k he et al existing deep convolutional neural networks cnns require a fixed size e g 224x224 input image this requirement is artificial and may reduce the recognition accuracy for the images or sub images of an arbitrary size scale in this work we equip the networks with another pooling strategy spatial pyramid pooling to eliminate the above requirement the new network structure called spp net can generate a fixed length representation regardless of image size scale pyramid pooling is also robust to object deformations with these advantages spp net should in general improve all cnn based image classification methods on the imagenet 2012 dataset we demonstrate that spp net boosts the accuracy of a variety of cnn architectures despite their different designs on the pascal voc 2007 and caltech101 datasets spp net achieves state of the art classification results using a single full image representation and no fine tuning the power of spp net is also significant in object detection using spp net we compute the feature maps from the entire image only once and then pool features in arbitrary regions sub images to generate fixed length representations for training the detectors this method avoids repeatedly computing the convolutional features in processing test images our method is 24 102x faster than the r cnn method while achieving better or comparable accuracy on pascal voc 2007 in imagenet large scale visual recognition challenge ilsvrc 2014 our methods rank 2 in object detection and 3 in image classification among all 38 teams this manuscript also introduces the improvement made for this competition
2014,neural machine translation by jointly learning to align and translate,dzmitry bahdanau et al,https://arxiv.org/pdf/1409.0473v7.pdf,dl_general,neural machine translation is a recently proposed approach to machine translation unlike the traditional statistical machine translation the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance the models proposed recently for neural machine translation often belong to a family of encoder decoders and consists of an encoder that encodes a source sentence into a fixed length vector from which a decoder generates a translation in this paper we conjecture that the use of a fixed length vector is a bottleneck in improving the performance of this basic encoder decoder architecture and propose to extend this by allowing a model to automatically soft search for parts of a source sentence that are relevant to predicting a target word without having to form these parts as a hard segment explicitly with this new approach we achieve a translation performance comparable to the existing state of the art phrase based system on the task of english to french translation furthermore qualitative analysis reveals that the soft alignments found by the model agree well with our intuition,neural machine translation by jointly learning to align and translate dzmitry bahdanau et al neural machine translation is a recently proposed approach to machine translation unlike the traditional statistical machine translation the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance the models proposed recently for neural machine translation often belong to a family of encoder decoders and consists of an encoder that encodes a source sentence into a fixed length vector from which a decoder generates a translation in this paper we conjecture that the use of a fixed length vector is a bottleneck in improving the performance of this basic encoder decoder architecture and propose to extend this by allowing a model to automatically soft search for parts of a source sentence that are relevant to predicting a target word without having to form these parts as a hard segment explicitly with this new approach we achieve a translation performance comparable to the existing state of the art phrase based system on the task of english to french translation furthermore qualitative analysis reveals that the soft alignments found by the model agree well with our intuition
2014,on the properties of neural machine translation encoder decoder approaches,kyunghyun cho et al,https://arxiv.org/pdf/1409.1259.pdf,ml_representation,neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks the neural machine translation models often consist of an encoder and a decoder the encoder extracts a fixed length representation from a variable length input sentence and the decoder generates a correct translation from this representation in this paper we focus on analyzing the properties of the neural machine translation using two models rnn encoder decoder and a newly proposed gated recursive convolutional neural network we show that the neural machine translation performs relatively well on short sentences without unknown words but its performance degrades rapidly as the length of the sentence and the number of unknown words increase furthermore we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically,on the properties of neural machine translation encoder decoder approaches kyunghyun cho et al neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks the neural machine translation models often consist of an encoder and a decoder the encoder extracts a fixed length representation from a variable length input sentence and the decoder generates a correct translation from this representation in this paper we focus on analyzing the properties of the neural machine translation using two models rnn encoder decoder and a newly proposed gated recursive convolutional neural network we show that the neural machine translation performs relatively well on short sentences without unknown words but its performance degrades rapidly as the length of the sentence and the number of unknown words increase furthermore we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically
2014,very deep convolutional networks for large scale image recognition,karen simonyan and andrew zisserman,https://arxiv.org/abs/1409.1556,vision,in this work we investigate the effect of the convolutional network depth on its accuracy in the large scale image recognition setting our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small 3x3 convolution filters which shows that a significant improvement on the prior art configurations can be achieved by pushing the depth to 16 19 weight layers these findings were the basis of our imagenet challenge 2014 submission where our team secured the first and the second places in the localisation and classification tracks respectively we also show that our representations generalise well to other datasets where they achieve state of the art results we have made our two best performing convnet models publicly available to facilitate further research on the use of deep visual representations in computer vision,very deep convolutional networks for large scale image recognition karen simonyan and andrew zisserman in this work we investigate the effect of the convolutional network depth on its accuracy in the large scale image recognition setting our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small 3x3 convolution filters which shows that a significant improvement on the prior art configurations can be achieved by pushing the depth to 16 19 weight layers these findings were the basis of our imagenet challenge 2014 submission where our team secured the first and the second places in the localisation and classification tracks respectively we also show that our representations generalise well to other datasets where they achieve state of the art results we have made our two best performing convnet models publicly available to facilitate further research on the use of deep visual representations in computer vision
2014,sequence to sequence learning with neural networks,ilya sutskever et al,https://arxiv.org/abs/1409.3215,dl_nlp,deep neural networks dnns are powerful models that have achieved excellent performance on difficult learning tasks although dnns work well whenever large labeled training sets are available they cannot be used to map sequences to sequences in this paper we present a general end to end approach to sequence learning that makes minimal assumptions on the sequence structure our method uses a multilayered long short term memory lstm to map the input sequence to a vector of a fixed dimensionality and then another deep lstm to decode the target sequence from the vector our main result is that on an english to french translation task from the wmt 14 dataset the translations produced by the lstm achieve a bleu score of 34 8 on the entire test set where the lstm s bleu score was penalized on out of vocabulary words additionally the lstm did not have difficulty on long sentences for comparison a phrase based smt system achieves a bleu score of 33 3 on the same dataset when we used the lstm to rerank the 1000 hypotheses produced by the aforementioned smt system its bleu score increases to 36 5 which is close to the previous best result on this task the lstm also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice finally we found that reversing the order of the words in all source sentences but not target sentences improved the lstm s performance markedly because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier,sequence to sequence learning with neural networks ilya sutskever et al deep neural networks dnns are powerful models that have achieved excellent performance on difficult learning tasks although dnns work well whenever large labeled training sets are available they cannot be used to map sequences to sequences in this paper we present a general end to end approach to sequence learning that makes minimal assumptions on the sequence structure our method uses a multilayered long short term memory lstm to map the input sequence to a vector of a fixed dimensionality and then another deep lstm to decode the target sequence from the vector our main result is that on an english to french translation task from the wmt 14 dataset the translations produced by the lstm achieve a bleu score of 34 8 on the entire test set where the lstm s bleu score was penalized on out of vocabulary words additionally the lstm did not have difficulty on long sentences for comparison a phrase based smt system achieves a bleu score of 33 3 on the same dataset when we used the lstm to rerank the 1000 hypotheses produced by the aforementioned smt system its bleu score increases to 36 5 which is close to the previous best result on this task the lstm also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice finally we found that reversing the order of the words in all source sentences but not target sentences improved the lstm s performance markedly because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier
2015,going deeper with convolutions,c szegedy et al,https://arxiv.org/abs/1409.4842,vision,we propose a deep convolutional neural network architecture codenamed inception which was responsible for setting the new state of the art for classification and detection in the imagenet large scale visual recognition challenge 2014 ilsvrc 2014 the main hallmark of this architecture is the improved utilization of the computing resources inside the network this was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant to optimize quality the architectural decisions were based on the hebbian principle and the intuition of multi scale processing one particular incarnation used in our submission for ilsvrc 2014 is called googlenet a 22 layers deep network the quality of which is assessed in the context of classification and detection,going deeper with convolutions c szegedy et al we propose a deep convolutional neural network architecture codenamed inception which was responsible for setting the new state of the art for classification and detection in the imagenet large scale visual recognition challenge 2014 ilsvrc 2014 the main hallmark of this architecture is the improved utilization of the computing resources inside the network this was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant to optimize quality the architectural decisions were based on the hebbian principle and the intuition of multi scale processing one particular incarnation used in our submission for ilsvrc 2014 is called googlenet a 22 layers deep network the quality of which is assessed in the context of classification and detection
2015,fully convolutional networks for semantic segmentation,j long et al,https://arxiv.org/abs/1411.4038,vision,convolutional networks are powerful visual models that yield hierarchies of features we show that convolutional networks by themselves trained end to end pixels to pixels exceed the state of the art in semantic segmentation our key insight is to build fully convolutional networks that take input of arbitrary size and produce correspondingly sized output with efficient inference and learning we define and detail the space of fully convolutional networks explain their application to spatially dense prediction tasks and draw connections to prior models we adapt contemporary classification networks alexnet the vgg net and googlenet into fully convolutional networks and transfer their learned representations by fine tuning to the segmentation task we then define a novel architecture that combines semantic information from a deep coarse layer with appearance information from a shallow fine layer to produce accurate and detailed segmentations our fully convolutional network achieves state of the art segmentation of pascal voc 20 relative improvement to 62 2 mean iu on 2012 nyudv2 and sift flow while inference takes one third of a second for a typical image,fully convolutional networks for semantic segmentation j long et al convolutional networks are powerful visual models that yield hierarchies of features we show that convolutional networks by themselves trained end to end pixels to pixels exceed the state of the art in semantic segmentation our key insight is to build fully convolutional networks that take input of arbitrary size and produce correspondingly sized output with efficient inference and learning we define and detail the space of fully convolutional networks explain their application to spatially dense prediction tasks and draw connections to prior models we adapt contemporary classification networks alexnet the vgg net and googlenet into fully convolutional networks and transfer their learned representations by fine tuning to the segmentation task we then define a novel architecture that combines semantic information from a deep coarse layer with appearance information from a shallow fine layer to produce accurate and detailed segmentations our fully convolutional network achieves state of the art segmentation of pascal voc 20 relative improvement to 62 2 mean iu on 2012 nyudv2 and sift flow while inference takes one third of a second for a typical image
2015,long term recurrent convolutional networks for visual recognition and description,j donahue et al,https://arxiv.org/abs/1411.4389,vision,models based on deep convolutional networks have dominated recent image interpretation tasks we investigate whether models which are also recurrent or temporally deep are effective for tasks involving sequences visual and otherwise we develop a novel recurrent convolutional architecture suitable for large scale visual learning which is end to end trainable and demonstrate the value of these models on benchmark video recognition tasks image description and retrieval problems and video narration challenges in contrast to current models which assume a fixed spatio temporal receptive field or simple temporal averaging for sequential processing recurrent convolutional models are doubly deep in that they can be compositional in spatial and temporal layers such models may have advantages when target concepts are complex and or training data are limited learning long term dependencies is possible when nonlinearities are incorporated into the network state updates long term rnn models are appealing in that they directly can map variable length inputs e g video frames to variable length outputs e g natural language text and can model complex temporal dynamics yet they can be optimized with backpropagation our recurrent long term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations our results show such models have distinct advantages over state of the art models for recognition or generation which are separately defined and or optimized,long term recurrent convolutional networks for visual recognition and description j donahue et al models based on deep convolutional networks have dominated recent image interpretation tasks we investigate whether models which are also recurrent or temporally deep are effective for tasks involving sequences visual and otherwise we develop a novel recurrent convolutional architecture suitable for large scale visual learning which is end to end trainable and demonstrate the value of these models on benchmark video recognition tasks image description and retrieval problems and video narration challenges in contrast to current models which assume a fixed spatio temporal receptive field or simple temporal averaging for sequential processing recurrent convolutional models are doubly deep in that they can be compositional in spatial and temporal layers such models may have advantages when target concepts are complex and or training data are limited learning long term dependencies is possible when nonlinearities are incorporated into the network state updates long term rnn models are appealing in that they directly can map variable length inputs e g video frames to variable length outputs e g natural language text and can model complex temporal dynamics yet they can be optimized with backpropagation our recurrent long term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations our results show such models have distinct advantages over state of the art models for recognition or generation which are separately defined and or optimized
2015,show and tell a neural image caption generator,o vinyals et al,https://arxiv.org/abs/1411.4555,vision,automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing in this paper we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image the model is trained to maximize the likelihood of the target description sentence given the training image experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions our model is often quite accurate which we verify both qualitatively and quantitatively for instance while the current state of the art bleu 1 score the higher the better on the pascal dataset is 25 our approach yields 59 to be compared to human performance around 69 we also show bleu 1 score improvements on flickr30k from 56 to 66 and on sbu from 19 to 28 lastly on the newly released coco dataset we achieve a bleu 4 of 27 7 which is the current state of the art,show and tell a neural image caption generator o vinyals et al automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing in this paper we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image the model is trained to maximize the likelihood of the target description sentence given the training image experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions our model is often quite accurate which we verify both qualitatively and quantitatively for instance while the current state of the art bleu 1 score the higher the better on the pascal dataset is 25 our approach yields 59 to be compared to human performance around 69 we also show bleu 1 score improvements on flickr30k from 56 to 66 and on sbu from 19 to 28 lastly on the newly released coco dataset we achieve a bleu 4 of 27 7 which is the current state of the art
2014,understanding deep image representations by inverting them,aravindh mahendran and andrea vedaldi,https://arxiv.org/abs/1412.0035,vision,image representations from sift and bag of visual words to convolutional neural networks cnns are a crucial component of almost any image understanding system nevertheless our understanding of them remains limited in this paper we conduct a direct analysis of the visual information contained in representations by asking the following question given an encoding of an image to which extent is it possible to reconstruct the image itself to answer this question we contribute a general framework to invert representations we show that this method can invert representations such as hog and sift more accurately than recent alternatives while being applicable to cnns too we then use this technique to study the inverse of recent state of the art cnn image representations for the first time among our findings we show that several layers in cnns retain photographically accurate information about the image with different degrees of geometric and photometric invariance,understanding deep image representations by inverting them aravindh mahendran and andrea vedaldi image representations from sift and bag of visual words to convolutional neural networks cnns are a crucial component of almost any image understanding system nevertheless our understanding of them remains limited in this paper we conduct a direct analysis of the visual information contained in representations by asking the following question given an encoding of an image to which extent is it possible to reconstruct the image itself to answer this question we contribute a general framework to invert representations we show that this method can invert representations such as hog and sift more accurately than recent alternatives while being applicable to cnns too we then use this technique to study the inverse of recent state of the art cnn image representations for the first time among our findings we show that several layers in cnns retain photographically accurate information about the image with different degrees of geometric and photometric invariance
2014,deep neural networks are easily fooled high confidence predictions for unrecognizable images,anh nguyen et al,https://arxiv.org/pdf/1412.1897.pdf,learning_theory,deep neural networks dnns have recently been achieving state of the art performance on a variety of pattern recognition tasks most notably visual classification problems given that dnns are now able to classify objects in images with near human level performance questions naturally arise as to what differences remain between computer and human vision a recent study revealed that changing an image e g of a lion in a way imperceptible to humans can cause a dnn to label the image as something else entirely e g mislabeling a lion a library here we show a related result it is easy to produce images that are completely unrecognizable to humans but that state of the art dnns believe to be recognizable objects with 99 99 confidence e g labeling with certainty that white noise static is a lion specifically we take convolutional neural networks trained to perform well on either the imagenet or mnist datasets and then find images with evolutionary algorithms or gradient ascent that dnns label with high confidence as belonging to each dataset class it is possible to produce images totally unrecognizable to human eyes that dnns believe with near certainty are familiar objects which we call fooling images more generally fooling examples our results shed light on interesting differences between human vision and current dnns and raise questions about the generality of dnn computer vision,deep neural networks are easily fooled high confidence predictions for unrecognizable images anh nguyen et al deep neural networks dnns have recently been achieving state of the art performance on a variety of pattern recognition tasks most notably visual classification problems given that dnns are now able to classify objects in images with near human level performance questions naturally arise as to what differences remain between computer and human vision a recent study revealed that changing an image e g of a lion in a way imperceptible to humans can cause a dnn to label the image as something else entirely e g mislabeling a lion a library here we show a related result it is easy to produce images that are completely unrecognizable to humans but that state of the art dnns believe to be recognizable objects with 99 99 confidence e g labeling with certainty that white noise static is a lion specifically we take convolutional neural networks trained to perform well on either the imagenet or mnist datasets and then find images with evolutionary algorithms or gradient ascent that dnns label with high confidence as belonging to each dataset class it is possible to produce images totally unrecognizable to human eyes that dnns believe with near certainty are familiar objects which we call fooling images more generally fooling examples our results shed light on interesting differences between human vision and current dnns and raise questions about the generality of dnn computer vision
2015,deep visual semantic alignments for generating image descriptions,a karpathy and l fei fei,https://arxiv.org/abs/1412.2306,vision,we present a model that generates natural language descriptions of images and their regions our approach leverages datasets of images and their sentence descriptions to learn about the inter modal correspondences between language and visual data our alignment model is based on a novel combination of convolutional neural networks over image regions bidirectional recurrent neural networks over sentences and a structured objective that aligns the two modalities through a multimodal embedding we then describe a multimodal recurrent neural network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions we demonstrate that our alignment model produces state of the art results in retrieval experiments on flickr8k flickr30k and mscoco datasets we then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region level annotations,deep visual semantic alignments for generating image descriptions a karpathy and l fei fei we present a model that generates natural language descriptions of images and their regions our approach leverages datasets of images and their sentence descriptions to learn about the inter modal correspondences between language and visual data our alignment model is based on a novel combination of convolutional neural networks over image regions bidirectional recurrent neural networks over sentences and a structured objective that aligns the two modalities through a multimodal embedding we then describe a multimodal recurrent neural network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions we demonstrate that our alignment model produces state of the art results in retrieval experiments on flickr8k flickr30k and mscoco datasets we then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region level annotations
2014,adam a method for stochastic optimization,diederik p kingma and jimmy ba,https://arxiv.org/abs/1412.6980,dl_opt,we introduce adam an algorithm for first order gradient based optimization of stochastic objective functions based on adaptive estimates of lower order moments the method is straightforward to implement is computationally efficient has little memory requirements is invariant to diagonal rescaling of the gradients and is well suited for problems that are large in terms of data and or parameters the method is also appropriate for non stationary objectives and problems with very noisy and or sparse gradients the hyper parameters have intuitive interpretations and typically require little tuning some connections to related algorithms on which adam was inspired are discussed we also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework empirical results demonstrate that adam works well in practice and compares favorably to other stochastic optimization methods finally we discuss adamax a variant of adam based on the infinity norm,adam a method for stochastic optimization diederik p kingma and jimmy ba we introduce adam an algorithm for first order gradient based optimization of stochastic objective functions based on adaptive estimates of lower order moments the method is straightforward to implement is computationally efficient has little memory requirements is invariant to diagonal rescaling of the gradients and is well suited for problems that are large in terms of data and or parameters the method is also appropriate for non stationary objectives and problems with very noisy and or sparse gradients the hyper parameters have intuitive interpretations and typically require little tuning some connections to related algorithms on which adam was inspired are discussed we also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework empirical results demonstrate that adam works well in practice and compares favorably to other stochastic optimization methods finally we discuss adamax a variant of adam based on the infinity norm
2014,semantic image segmentation with deep convolutional nets and fully connected crfs,l chen et al,https://arxiv.org/pdf/1412.7062,vision,deep convolutional neural networks dcnns have recently shown state of the art performance in high level vision tasks such as image classification and object detection this work brings together methods from dcnns and probabilistic graphical models for addressing the task of pixel level classification also called semantic image segmentation we show that responses at the final layer of dcnns are not sufficiently localized for accurate object segmentation this is due to the very invariance properties that make dcnns good for high level tasks we overcome this poor localization property of deep networks by combining the responses at the final dcnn layer with a fully connected conditional random field crf qualitatively our deeplab system is able to localize segment boundaries at a level of accuracy which is beyond previous methods quantitatively our method sets the new state of art at the pascal voc 2012 semantic image segmentation task reaching 71 6 iou accuracy in the test set we show how these results can be obtained efficiently careful network re purposing and a novel application of the hole algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern gpu,semantic image segmentation with deep convolutional nets and fully connected crfs l chen et al deep convolutional neural networks dcnns have recently shown state of the art performance in high level vision tasks such as image classification and object detection this work brings together methods from dcnns and probabilistic graphical models for addressing the task of pixel level classification also called semantic image segmentation we show that responses at the final layer of dcnns are not sufficiently localized for accurate object segmentation this is due to the very invariance properties that make dcnns good for high level tasks we overcome this poor localization property of deep networks by combining the responses at the final dcnn layer with a fully connected conditional random field crf qualitatively our deeplab system is able to localize segment boundaries at a level of accuracy which is beyond previous methods quantitatively our method sets the new state of art at the pascal voc 2012 semantic image segmentation task reaching 71 6 iou accuracy in the test set we show how these results can be obtained efficiently careful network re purposing and a novel application of the hole algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern gpu
2016,image super resolution using deep convolutional networks,c dong et al,https://arxiv.org/pdf/1501.00092v3.pdf,vision,we propose a deep learning method for single image super resolution sr our method directly learns an end to end mapping between the low high resolution images the mapping is represented as a deep convolutional neural network cnn that takes the low resolution image as the input and outputs the high resolution one we further show that traditional sparse coding based sr methods can also be viewed as a deep convolutional network but unlike traditional methods that handle each component separately our method jointly optimizes all layers our deep cnn has a lightweight structure yet demonstrates state of the art restoration quality and achieves fast speed for practical on line usage we explore different network structures and parameter settings to achieve trade offs between performance and speed moreover we extend our network to cope with three color channels simultaneously and show better overall reconstruction quality,image super resolution using deep convolutional networks c dong et al we propose a deep learning method for single image super resolution sr our method directly learns an end to end mapping between the low high resolution images the mapping is represented as a deep convolutional neural network cnn that takes the low resolution image as the input and outputs the high resolution one we further show that traditional sparse coding based sr methods can also be viewed as a deep convolutional network but unlike traditional methods that handle each component separately our method jointly optimizes all layers our deep cnn has a lightweight structure yet demonstrates state of the art restoration quality and achieves fast speed for practical on line usage we explore different network structures and parameter settings to achieve trade offs between performance and speed moreover we extend our network to cope with three color channels simultaneously and show better overall reconstruction quality
2015,show attend and tell neural image caption generation with visual attention,k xu et al,http://arxiv.org/pdf/1502.03044,vision,inspired by recent work in machine translation and object detection we introduce an attention based model that automatically learns to describe the content of images we describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound we also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence we validate the use of attention with state of the art performance on three benchmark datasets flickr8k flickr30k and ms coco,show attend and tell neural image caption generation with visual attention k xu et al inspired by recent work in machine translation and object detection we introduce an attention based model that automatically learns to describe the content of images we describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound we also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence we validate the use of attention with state of the art performance on three benchmark datasets flickr8k flickr30k and ms coco
2015,batch normalization accelerating deep network training by reducing internal covariate shift,sergey ioffe and christian szegedy,https://arxiv.org/abs/1502.03167,dl_general,training deep neural networks is complicated by the fact that the distribution of each layer s inputs changes during training as the parameters of the previous layers change this slows down the training by requiring lower learning rates and careful parameter initialization and makes it notoriously hard to train models with saturating nonlinearities we refer to this phenomenon as internal covariate shift and address the problem by normalizing layer inputs our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini batch batch normalization allows us to use much higher learning rates and be less careful about initialization it also acts as a regularizer in some cases eliminating the need for dropout applied to a state of the art image classification model batch normalization achieves the same accuracy with 14 times fewer training steps and beats the original model by a significant margin using an ensemble of batch normalized networks we improve upon the best published result on imagenet classification reaching 4 9 top 5 validation error and 4 8 test error exceeding the accuracy of human raters,batch normalization accelerating deep network training by reducing internal covariate shift sergey ioffe and christian szegedy training deep neural networks is complicated by the fact that the distribution of each layer s inputs changes during training as the parameters of the previous layers change this slows down the training by requiring lower learning rates and careful parameter initialization and makes it notoriously hard to train models with saturating nonlinearities we refer to this phenomenon as internal covariate shift and address the problem by normalizing layer inputs our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini batch batch normalization allows us to use much higher learning rates and be less careful about initialization it also acts as a regularizer in some cases eliminating the need for dropout applied to a state of the art image classification model batch normalization achieves the same accuracy with 14 times fewer training steps and beats the original model by a significant margin using an ensemble of batch normalized networks we improve upon the best published result on imagenet classification reaching 4 9 top 5 validation error and 4 8 test error exceeding the accuracy of human raters
2015,deep learning and the information bottleneck principle,naftali tishby and noga zaslavsky,https://arxiv.org/abs/1503.02406,learning_theory,deep neural networks dnns are analyzed via the theoretical framework of the information bottleneck ib principle we first show that any dnn can be quantified by the mutual information between the layers and the input and output variables using this representation we can calculate the optimal information theoretic limits of the dnn and obtain finite sample generalization bounds the advantage of getting closer to the theoretical limit is quantifiable both by the generalization bound and by the network s simplicity we argue that both the optimal architecture number of layers and features connections at each layer are related to the bifurcation points of the information bottleneck tradeoff namely relevant compression of the input layer with respect to the output layer the hierarchical representations at the layered network naturally correspond to the structural phase transitions along the information curve we believe that this new insight can lead to new optimality bounds and deep learning algorithms,deep learning and the information bottleneck principle naftali tishby and noga zaslavsky deep neural networks dnns are analyzed via the theoretical framework of the information bottleneck ib principle we first show that any dnn can be quantified by the mutual information between the layers and the input and output variables using this representation we can calculate the optimal information theoretic limits of the dnn and obtain finite sample generalization bounds the advantage of getting closer to the theoretical limit is quantifiable both by the generalization bound and by the network s simplicity we argue that both the optimal architecture number of layers and features connections at each layer are related to the bifurcation points of the information bottleneck tradeoff namely relevant compression of the input layer with respect to the output layer the hierarchical representations at the layered network naturally correspond to the structural phase transitions along the information curve we believe that this new insight can lead to new optimality bounds and deep learning algorithms
2015,deep unsupervised learning using nonequilibrium thermodynamics,jascha sohl dickstein et al,https://arxiv.org/abs/1503.03585,vision,a central problem in machine learning involves modeling complex data sets using highly flexible families of probability distributions in which learning sampling inference and evaluation are still analytically or computationally tractable here we develop an approach that simultaneously achieves both flexibility and tractability the essential idea inspired by non equilibrium statistical physics is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process we then learn a reverse diffusion process that restores structure in data yielding a highly flexible and tractable generative model of the data this approach allows us to rapidly learn sample from and evaluate probabilities in deep generative models with thousands of layers or time steps as well as to compute conditional and posterior probabilities under the learned model we additionally release an open source reference implementation of the algorithm,deep unsupervised learning using nonequilibrium thermodynamics jascha sohl dickstein et al a central problem in machine learning involves modeling complex data sets using highly flexible families of probability distributions in which learning sampling inference and evaluation are still analytically or computationally tractable here we develop an approach that simultaneously achieves both flexibility and tractability the essential idea inspired by non equilibrium statistical physics is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process we then learn a reverse diffusion process that restores structure in data yielding a highly flexible and tractable generative model of the data this approach allows us to rapidly learn sample from and evaluate probabilities in deep generative models with thousands of layers or time steps as well as to compute conditional and posterior probabilities under the learned model we additionally release an open source reference implementation of the algorithm
2015,optimizing neural networks with kronecker factored approximate curvature,james martens and roger grosse,https://arxiv.org/abs/1503.05671,dl_opt,we propose an efficient method for approximating natural gradient descent in neural networks which we call kronecker factored approximate curvature k fac k fac is based on an efficiently invertible approximation of a neural network s fisher information matrix which is neither diagonal nor low rank and in some cases is completely non sparse it is derived by approximating various large blocks of the fisher corresponding to entire layers as being the kronecker product of two much smaller matrices while only several times more expensive to compute than the plain stochastic gradient the updates produced by k fac make much more progress optimizing the objective which results in an algorithm that can be much faster than stochastic gradient descent with momentum in practice and unlike some previously proposed approximate natural gradient newton methods which use high quality non diagonal curvature matrices such as hessian free optimization k fac works very well in highly stochastic optimization regimes this is because the cost of storing and inverting k fac s approximation to the curvature matrix does not depend on the amount of data used to estimate it which is a feature typically associated only with diagonal or low rank approximations to the curvature matrix,optimizing neural networks with kronecker factored approximate curvature james martens and roger grosse we propose an efficient method for approximating natural gradient descent in neural networks which we call kronecker factored approximate curvature k fac k fac is based on an efficiently invertible approximation of a neural network s fisher information matrix which is neither diagonal nor low rank and in some cases is completely non sparse it is derived by approximating various large blocks of the fisher corresponding to entire layers as being the kronecker product of two much smaller matrices while only several times more expensive to compute than the plain stochastic gradient the updates produced by k fac make much more progress optimizing the objective which results in an algorithm that can be much faster than stochastic gradient descent with momentum in practice and unlike some previously proposed approximate natural gradient newton methods which use high quality non diagonal curvature matrices such as hessian free optimization k fac works very well in highly stochastic optimization regimes this is because the cost of storing and inverting k fac s approximation to the curvature matrix does not depend on the amount of data used to estimate it which is a feature typically associated only with diagonal or low rank approximations to the curvature matrix
2015,fast r cnn,r girshick,https://arxiv.org/abs/1504.08083,vision,this paper proposes a fast region based convolutional network method fast r cnn for object detection fast r cnn builds on previous work to efficiently classify object proposals using deep convolutional networks compared to previous work fast r cnn employs several innovations to improve training and testing speed while also increasing detection accuracy fast r cnn trains the very deep vgg16 network 9x faster than r cnn is 213x faster at test time and achieves a higher map on pascal voc 2012 compared to sppnet fast r cnn trains vgg16 3x faster tests 10x faster and is more accurate fast r cnn is implemented in python and c using caffe and is available under the open source mit license at,fast r cnn r girshick this paper proposes a fast region based convolutional network method fast r cnn for object detection fast r cnn builds on previous work to efficiently classify object proposals using deep convolutional networks compared to previous work fast r cnn employs several innovations to improve training and testing speed while also increasing detection accuracy fast r cnn trains the very deep vgg16 network 9x faster than r cnn is 213x faster at test time and achieves a higher map on pascal voc 2012 compared to sppnet fast r cnn trains vgg16 3x faster tests 10x faster and is more accurate fast r cnn is implemented in python and c using caffe and is available under the open source mit license at
2015,highway networks,rupesh kumar srivastava et al,https://arxiv.org/abs/1505.00387,dl_general,there is plenty of theoretical and empirical evidence that depth of neural networks is a crucial ingredient for their success however network training becomes more difficult with increasing depth and training of very deep networks remains an open problem in this extended abstract we introduce a new architecture designed to ease gradient based training of very deep networks we refer to networks with this architecture as highway networks since they allow unimpeded information flow across several layers on information highways the architecture is characterized by the use of gating units which learn to regulate the flow of information through a network highway networks with hundreds of layers can be trained directly using stochastic gradient descent and with a variety of activation functions opening up the possibility of studying extremely deep and efficient architectures,highway networks rupesh kumar srivastava et al there is plenty of theoretical and empirical evidence that depth of neural networks is a crucial ingredient for their success however network training becomes more difficult with increasing depth and training of very deep networks remains an open problem in this extended abstract we introduce a new architecture designed to ease gradient based training of very deep networks we refer to networks with this architecture as highway networks since they allow unimpeded information flow across several layers on information highways the architecture is characterized by the use of gating units which learn to regulate the flow of information through a network highway networks with hundreds of layers can be trained directly using stochastic gradient descent and with a variety of activation functions opening up the possibility of studying extremely deep and efficient architectures
2015,vqa visual question answering,s antol et al,https://arxiv.org/abs/1505.00468,vision,we propose the task of free form and open ended visual question answering vqa given an image and a natural language question about the image the task is to provide an accurate natural language answer mirroring real world scenarios such as helping the visually impaired both the questions and answers are open ended visual questions selectively target different areas of an image including background details and underlying context as a result a system that succeeds at vqa typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions moreover vqa is amenable to automatic evaluation since many open ended answers contain only a few words or a closed set of answers that can be provided in a multiple choice format we provide a dataset containing 0 25m images 0 76m questions and 10m answers and discuss the information it provides numerous baselines and methods for vqa are provided and compared with human performance our vqa demo is available on cloudcv,vqa visual question answering s antol et al we propose the task of free form and open ended visual question answering vqa given an image and a natural language question about the image the task is to provide an accurate natural language answer mirroring real world scenarios such as helping the visually impaired both the questions and answers are open ended visual questions selectively target different areas of an image including background details and underlying context as a result a system that succeeds at vqa typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions moreover vqa is amenable to automatic evaluation since many open ended answers contain only a few words or a closed set of answers that can be provided in a multiple choice format we provide a dataset containing 0 25m images 0 76m questions and 10m answers and discuss the information it provides numerous baselines and methods for vqa are provided and compared with human performance our vqa demo is available on cloudcv
2015,faster r cnn towards real time object detection with region proposal networks,s ren et al,https://arxiv.org/abs/1506.01497,vision,state of the art object detection networks depend on region proposal algorithms to hypothesize object locations advances like sppnet and fast r cnn have reduced the running time of these detection networks exposing region proposal computation as a bottleneck in this work we introduce a region proposal network rpn that shares full image convolutional features with the detection network thus enabling nearly cost free region proposals an rpn is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position the rpn is trained end to end to generate high quality region proposals which are used by fast r cnn for detection we further merge rpn and fast r cnn into a single network by sharing their convolutional features using the recently popular terminology of neural networks with attention mechanisms the rpn component tells the unified network where to look for the very deep vgg 16 model our detection system has a frame rate of 5fps including all steps on a gpu while achieving state of the art object detection accuracy on pascal voc 2007 2012 and ms coco datasets with only 300 proposals per image in ilsvrc and coco 2015 competitions faster r cnn and rpn are the foundations of the 1st place winning entries in several tracks code has been made publicly available,faster r cnn towards real time object detection with region proposal networks s ren et al state of the art object detection networks depend on region proposal algorithms to hypothesize object locations advances like sppnet and fast r cnn have reduced the running time of these detection networks exposing region proposal computation as a bottleneck in this work we introduce a region proposal network rpn that shares full image convolutional features with the detection network thus enabling nearly cost free region proposals an rpn is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position the rpn is trained end to end to generate high quality region proposals which are used by fast r cnn for detection we further merge rpn and fast r cnn into a single network by sharing their convolutional features using the recently popular terminology of neural networks with attention mechanisms the rpn component tells the unified network where to look for the very deep vgg 16 model our detection system has a frame rate of 5fps including all steps on a gpu while achieving state of the art object detection accuracy on pascal voc 2007 2012 and ms coco datasets with only 300 proposals per image in ilsvrc and coco 2015 competitions faster r cnn and rpn are the foundations of the 1st place winning entries in several tracks code has been made publicly available
2015,spatial transformer network,m jaderberg et al,https://arxiv.org/abs/1506.02025,vision,convolutional neural networks define an exceptionally powerful class of models but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner in this work we introduce a new learnable module the spatial transformer which explicitly allows the spatial manipulation of data within the network this differentiable module can be inserted into existing convolutional architectures giving neural networks the ability to actively spatially transform feature maps conditional on the feature map itself without any extra training supervision or modification to the optimisation process we show that the use of spatial transformers results in models which learn invariance to translation scale rotation and more generic warping resulting in state of the art performance on several benchmarks and for a number of classes of transformations,spatial transformer network m jaderberg et al convolutional neural networks define an exceptionally powerful class of models but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner in this work we introduce a new learnable module the spatial transformer which explicitly allows the spatial manipulation of data within the network this differentiable module can be inserted into existing convolutional architectures giving neural networks the ability to actively spatially transform feature maps conditional on the feature map itself without any extra training supervision or modification to the optimisation process we show that the use of spatial transformers results in models which learn invariance to translation scale rotation and more generic warping resulting in state of the art performance on several benchmarks and for a number of classes of transformations
2015,dropout as a bayesian approximation representing model uncertainty in deep learning,yarin gal and zoubin ghahramani,https://arxiv.org/pdf/1506.02142.pdf,bayesian,deep learning tools have gained tremendous attention in applied machine learning however such tools for regression and classification do not capture model uncertainty in comparison bayesian models offer a mathematically grounded framework to reason about model uncertainty but usually come with a prohibitive computational cost in this paper we develop a new theoretical framework casting dropout training in deep neural networks nns as approximate bayesian inference in deep gaussian processes a direct result of this theory gives us tools to model uncertainty with dropout nns extracting information from existing models that has been thrown away so far this mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy we perform an extensive study of the properties of dropout s uncertainty various network architectures and non linearities are assessed on tasks of regression and classification using mnist as an example we show a considerable improvement in predictive log likelihood and rmse compared to existing state of the art methods and finish by using dropout s uncertainty in deep reinforcement learning,dropout as a bayesian approximation representing model uncertainty in deep learning yarin gal and zoubin ghahramani deep learning tools have gained tremendous attention in applied machine learning however such tools for regression and classification do not capture model uncertainty in comparison bayesian models offer a mathematically grounded framework to reason about model uncertainty but usually come with a prohibitive computational cost in this paper we develop a new theoretical framework casting dropout training in deep neural networks nns as approximate bayesian inference in deep gaussian processes a direct result of this theory gives us tools to model uncertainty with dropout nns extracting information from existing models that has been thrown away so far this mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy we perform an extensive study of the properties of dropout s uncertainty various network architectures and non linearities are assessed on tasks of regression and classification using mnist as an example we show a considerable improvement in predictive log likelihood and rmse compared to existing state of the art methods and finish by using dropout s uncertainty in deep reinforcement learning
2016,you only look once unified real time object detection,j redmon et al,https://arxiv.org/abs/1506.02640,vision,we present yolo a new approach to object detection prior work on object detection repurposes classifiers to perform detection instead we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities a single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation since the whole detection pipeline is a single network it can be optimized end to end directly on detection performance our unified architecture is extremely fast our base yolo model processes images in real time at 45 frames per second a smaller version of the network fast yolo processes an astounding 155 frames per second while still achieving double the map of other real time detectors compared to state of the art detection systems yolo makes more localization errors but is far less likely to predict false detections where nothing exists finally yolo learns very general representations of objects it outperforms all other detection methods including dpm and r cnn by a wide margin when generalizing from natural images to artwork on both the picasso dataset and the people art dataset,you only look once unified real time object detection j redmon et al we present yolo a new approach to object detection prior work on object detection repurposes classifiers to perform detection instead we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities a single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation since the whole detection pipeline is a single network it can be optimized end to end directly on detection performance our unified architecture is extremely fast our base yolo model processes images in real time at 45 frames per second a smaller version of the network fast yolo processes an astounding 155 frames per second while still achieving double the map of other real time detectors compared to state of the art detection systems yolo makes more localization errors but is far less likely to predict false detections where nothing exists finally yolo learns very general representations of objects it outperforms all other detection methods including dpm and r cnn by a wide margin when generalizing from natural images to artwork on both the picasso dataset and the people art dataset
2016,end to end attention based large vocabulary speech recognition,d bahdanau et al,https://arxiv.org/pdf/1508.04395,audio,many of the current state of the art large vocabulary continuous speech recognition systems lvcsr are hybrids of neural networks and hidden markov models hmms most of these systems contain separate components that deal with the acoustic modelling language modelling and sequence decoding we investigate a more direct approach in which the hmm is replaced with a recurrent neural network rnn that performs sequence prediction directly at the character level alignment between the input features and the desired character sequence is learned automatically by an attention mechanism built into the rnn for each predicted character the attention mechanism scans the input sequence and chooses relevant frames we propose two methods to speed up this operation limiting the scan to a subset of most promising frames and pooling over time the information contained in neighboring frames thereby reducing source sequence length integrating an n gram language model into the decoding process yields recognition accuracies similar to other hmm free rnn based approaches,end to end attention based large vocabulary speech recognition d bahdanau et al many of the current state of the art large vocabulary continuous speech recognition systems lvcsr are hybrids of neural networks and hidden markov models hmms most of these systems contain separate components that deal with the acoustic modelling language modelling and sequence decoding we investigate a more direct approach in which the hmm is replaced with a recurrent neural network rnn that performs sequence prediction directly at the character level alignment between the input features and the desired character sequence is learned automatically by an attention mechanism built into the rnn for each predicted character the attention mechanism scans the input sequence and chooses relevant frames we propose two methods to speed up this operation limiting the scan to a subset of most promising frames and pooling over time the information contained in neighboring frames thereby reducing source sequence length integrating an n gram language model into the decoding process yields recognition accuracies similar to other hmm free rnn based approaches
2015,a neural algorithm of artistic style,l gatys et al,https://arxiv.org/pdf/1508.06576,vision,in fine art especially painting humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities however in other key areas of visual perception such as object and face recognition near human performance was recently demonstrated by a class of biologically inspired vision models called deep neural networks here we introduce an artificial system based on a deep neural network that creates artistic images of high perceptual quality the system uses neural representations to separate and recombine content and style of arbitrary images providing a neural algorithm for the creation of artistic images moreover in light of the striking similarities between performance optimised artificial neural networks and biological vision our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery,a neural algorithm of artistic style l gatys et al in fine art especially painting humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities however in other key areas of visual perception such as object and face recognition near human performance was recently demonstrated by a class of biologically inspired vision models called deep neural networks here we introduce an artificial system based on a deep neural network that creates artistic images of high perceptual quality the system uses neural representations to separate and recombine content and style of arbitrary images providing a neural algorithm for the creation of artistic images moreover in light of the striking similarities between performance optimised artificial neural networks and biological vision our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery
2015,neural machine translation of rare words with subword units,rico sennrich et al,https://arxiv.org/abs/1508.07909,dl_nlp,neural machine translation nmt models typically operate with a fixed vocabulary but translation is an open vocabulary problem previous work addresses the translation of out of vocabulary words by backing off to a dictionary in this paper we introduce a simpler and more effective approach making the nmt model capable of open vocabulary translation by encoding rare and unknown words as sequences of subword units this is based on the intuition that various word classes are translatable via smaller units than words for instance names via character copying or transliteration compounds via compositional translation and cognates and loanwords via phonological and morphological transformations we discuss the suitability of different word segmentation techniques including simple character n gram models and a segmentation based on the byte pair encoding compression algorithm and empirically show that subword models improve over a back off dictionary baseline for the wmt 15 translation tasks english german and english russian by 1 1 and 1 3 bleu respectively,neural machine translation of rare words with subword units rico sennrich et al neural machine translation nmt models typically operate with a fixed vocabulary but translation is an open vocabulary problem previous work addresses the translation of out of vocabulary words by backing off to a dictionary in this paper we introduce a simpler and more effective approach making the nmt model capable of open vocabulary translation by encoding rare and unknown words as sequences of subword units this is based on the intuition that various word classes are translatable via smaller units than words for instance names via character copying or transliteration compounds via compositional translation and cognates and loanwords via phonological and morphological transformations we discuss the suitability of different word segmentation techniques including simple character n gram models and a segmentation based on the byte pair encoding compression algorithm and empirically show that subword models improve over a back off dictionary baseline for the wmt 15 translation tasks english german and english russian by 1 1 and 1 3 bleu respectively
2016,rethinking the inception architecture for computer vision,c szegedy et al,https://arxiv.org/abs/1512.00567,vision,convolutional networks are at the core of most state of the art computer vision solutions for a wide variety of tasks since 2014 very deep convolutional networks started to become mainstream yielding substantial gains in various benchmarks although increased model size and computational cost tend to translate to immediate quality gains for most tasks as long as enough labeled data is provided for training computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big data scenarios here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization we benchmark our methods on the ilsvrc 2012 classification challenge validation set demonstrate substantial gains over the state of the art 21 2 top 1 and 5 6 top 5 error for single frame evaluation using a network with a computational cost of 5 billion multiply adds per inference and with using less than 25 million parameters with an ensemble of 4 models and multi crop evaluation we report 3 5 top 5 error on the validation set 3 6 error on the test set and 17 3 top 1 error on the validation set,rethinking the inception architecture for computer vision c szegedy et al convolutional networks are at the core of most state of the art computer vision solutions for a wide variety of tasks since 2014 very deep convolutional networks started to become mainstream yielding substantial gains in various benchmarks although increased model size and computational cost tend to translate to immediate quality gains for most tasks as long as enough labeled data is provided for training computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big data scenarios here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization we benchmark our methods on the ilsvrc 2012 classification challenge validation set demonstrate substantial gains over the state of the art 21 2 top 1 and 5 6 top 5 error for single frame evaluation using a network with a computational cost of 5 billion multiply adds per inference and with using less than 25 million parameters with an ensemble of 4 models and multi crop evaluation we report 3 5 top 5 error on the validation set 3 6 error on the test set and 17 3 top 1 error on the validation set
2015,deep speech 2 end to end speech recognition in english and mandarin,d amodei et al,https://arxiv.org/pdf/1512.02595,audio,we show that an end to end deep learning approach can be used to recognize either english or mandarin chinese speech two vastly different languages because it replaces entire pipelines of hand engineered components with neural networks end to end learning allows us to handle a diverse variety of speech including noisy environments accents and different languages key to our approach is our application of hpc techniques resulting in a 7x speedup over our previous system because of this efficiency experiments that previously took weeks now run in days this enables us to iterate more quickly to identify superior architectures and algorithms as a result in several cases our system is competitive with the transcription of human workers when benchmarked on standard datasets finally using a technique called batch dispatch with gpus in the data center we show that our system can be inexpensively deployed in an online setting delivering low latency when serving users at scale,deep speech 2 end to end speech recognition in english and mandarin d amodei et al we show that an end to end deep learning approach can be used to recognize either english or mandarin chinese speech two vastly different languages because it replaces entire pipelines of hand engineered components with neural networks end to end learning allows us to handle a diverse variety of speech including noisy environments accents and different languages key to our approach is our application of hpc techniques resulting in a 7x speedup over our previous system because of this efficiency experiments that previously took weeks now run in days this enables us to iterate more quickly to identify superior architectures and algorithms as a result in several cases our system is competitive with the transcription of human workers when benchmarked on standard datasets finally using a technique called batch dispatch with gpus in the data center we show that our system can be inexpensively deployed in an online setting delivering low latency when serving users at scale
2015,deep residual learning for image recognition,kaiming he et al,https://arxiv.org/pdf/1512.03385.pdf,vision,deeper neural networks are more difficult to train we present a residual learning framework to ease the training of networks that are substantially deeper than those used previously we explicitly reformulate the layers as learning residual functions with reference to the layer inputs instead of learning unreferenced functions we provide comprehensive empirical evidence showing that these residual networks are easier to optimize and can gain accuracy from considerably increased depth on the imagenet dataset we evaluate residual nets with a depth of up to 152 layers 8x deeper than vgg nets but still having lower complexity an ensemble of these residual nets achieves 3 57 error on the imagenet test set this result won the 1st place on the ilsvrc 2015 classification task we also present analysis on cifar 10 with 100 and 1000 layers the depth of representations is of central importance for many visual recognition tasks solely due to our extremely deep representations we obtain a 28 relative improvement on the coco object detection dataset deep residual nets are foundations of our submissions to ilsvrc coco 2015 competitions where we also won the 1st places on the tasks of imagenet detection imagenet localization coco detection and coco segmentation,deep residual learning for image recognition kaiming he et al deeper neural networks are more difficult to train we present a residual learning framework to ease the training of networks that are substantially deeper than those used previously we explicitly reformulate the layers as learning residual functions with reference to the layer inputs instead of learning unreferenced functions we provide comprehensive empirical evidence showing that these residual networks are easier to optimize and can gain accuracy from considerably increased depth on the imagenet dataset we evaluate residual nets with a depth of up to 152 layers 8x deeper than vgg nets but still having lower complexity an ensemble of these residual nets achieves 3 57 error on the imagenet test set this result won the 1st place on the ilsvrc 2015 classification task we also present analysis on cifar 10 with 100 and 1000 layers the depth of representations is of central importance for many visual recognition tasks solely due to our extremely deep representations we obtain a 28 relative improvement on the coco object detection dataset deep residual nets are foundations of our submissions to ilsvrc coco 2015 competitions where we also won the 1st places on the tasks of imagenet detection imagenet localization coco detection and coco segmentation
2016,inception v4 inception resnet and the impact of residual connections on learning,c szegedy et al,http://arxiv.org/pdf/1602.07261,vision,very deep convolutional networks have been central to the largest advances in image recognition performance in recent years one example is the inception architecture that has been shown to achieve very good performance at relatively low computational cost recently the introduction of residual connections in conjunction with a more traditional architecture has yielded state of the art performance in the 2015 ilsvrc challenge its performance was similar to the latest generation inception v3 network this raises the question of whether there are any benefit in combining the inception architecture with residual connections here we give clear empirical evidence that training with residual connections accelerates the training of inception networks significantly there is also some evidence of residual inception networks outperforming similarly expensive inception networks without residual connections by a thin margin we also present several new streamlined architectures for both residual and non residual inception networks these variations improve the single frame recognition performance on the ilsvrc 2012 classification task significantly we further demonstrate how proper activation scaling stabilizes the training of very wide residual inception networks with an ensemble of three residual and one inception v4 we achieve 3 08 percent top 5 error on the test set of the imagenet classification cls challenge,inception v4 inception resnet and the impact of residual connections on learning c szegedy et al very deep convolutional networks have been central to the largest advances in image recognition performance in recent years one example is the inception architecture that has been shown to achieve very good performance at relatively low computational cost recently the introduction of residual connections in conjunction with a more traditional architecture has yielded state of the art performance in the 2015 ilsvrc challenge its performance was similar to the latest generation inception v3 network this raises the question of whether there are any benefit in combining the inception architecture with residual connections here we give clear empirical evidence that training with residual connections accelerates the training of inception networks significantly there is also some evidence of residual inception networks outperforming similarly expensive inception networks without residual connections by a thin margin we also present several new streamlined architectures for both residual and non residual inception networks these variations improve the single frame recognition performance on the ilsvrc 2012 classification task significantly we further demonstrate how proper activation scaling stabilizes the training of very wide residual inception networks with an ensemble of three residual and one inception v4 we achieve 3 08 percent top 5 error on the test set of the imagenet classification cls challenge
2016,xgboost a scalable tree boosting system,tianqi chen and carlos guestrin,https://arxiv.org/abs/1603.02754,ml_classic,tree boosting is a highly effective and widely used machine learning method in this paper we describe a scalable end to end tree boosting system called xgboost which is used widely by data scientists to achieve state of the art results on many machine learning challenges we propose a novel sparsity aware algorithm for sparse data and weighted quantile sketch for approximate tree learning more importantly we provide insights on cache access patterns data compression and sharding to build a scalable tree boosting system by combining these insights xgboost scales beyond billions of examples using far fewer resources than existing systems,xgboost a scalable tree boosting system tianqi chen and carlos guestrin tree boosting is a highly effective and widely used machine learning method in this paper we describe a scalable end to end tree boosting system called xgboost which is used widely by data scientists to achieve state of the art results on many machine learning challenges we propose a novel sparsity aware algorithm for sparse data and weighted quantile sketch for approximate tree learning more importantly we provide insights on cache access patterns data compression and sharding to build a scalable tree boosting system by combining these insights xgboost scales beyond billions of examples using far fewer resources than existing systems
2016,identity mappings in deep residual networks,k he et al,https://arxiv.org/pdf/1603.05027v2.pdf,vision,deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors in this paper we analyze the propagation formulations behind the residual building blocks which suggest that the forward and backward signals can be directly propagated from one block to any other block when using identity mappings as the skip connections and after addition activation a series of ablation experiments support the importance of these identity mappings this motivates us to propose a new residual unit which makes training easier and improves generalization we report improved results using a 1001 layer resnet on cifar 10 4 62 error and cifar 100 and a 200 layer resnet on imagenet code is available at,identity mappings in deep residual networks k he et al deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors in this paper we analyze the propagation formulations behind the residual building blocks which suggest that the forward and backward signals can be directly propagated from one block to any other block when using identity mappings as the skip connections and after addition activation a series of ablation experiments support the importance of these identity mappings this motivates us to propose a new residual unit which makes training easier and improves generalization we report improved results using a 1001 layer resnet on cifar 10 4 62 error and cifar 100 and a 200 layer resnet on imagenet code is available at
2016,hyperband a novel bandit based approach to hyperparameter optimization,lisha li et al,https://arxiv.org/pdf/1603.06560.pdf,ml_tuning,performance of machine learning algorithms depends critically on identifying a good set of hyperparameters while recent approaches use bayesian optimization to adaptively select configurations we focus on speeding up random search through adaptive resource allocation and early stopping we formulate hyperparameter optimization as a pure exploration non stochastic infinite armed bandit problem where a predefined resource like iterations data samples or features is allocated to randomly sampled configurations we introduce a novel algorithm hyperband for this framework and analyze its theoretical properties providing several desirable guarantees furthermore we compare hyperband with popular bayesian optimization methods on a suite of hyperparameter optimization problems we observe that hyperband can provide over an order of magnitude speedup over our competitor set on a variety of deep learning and kernel based learning problems,hyperband a novel bandit based approach to hyperparameter optimization lisha li et al performance of machine learning algorithms depends critically on identifying a good set of hyperparameters while recent approaches use bayesian optimization to adaptively select configurations we focus on speeding up random search through adaptive resource allocation and early stopping we formulate hyperparameter optimization as a pure exploration non stochastic infinite armed bandit problem where a predefined resource like iterations data samples or features is allocated to randomly sampled configurations we introduce a novel algorithm hyperband for this framework and analyze its theoretical properties providing several desirable guarantees furthermore we compare hyperband with popular bayesian optimization methods on a suite of hyperparameter optimization problems we observe that hyperband can provide over an order of magnitude speedup over our competitor set on a variety of deep learning and kernel based learning problems
2016,gaussian error linear units gelus,dan hendrycks and kevin gimpel,https://arxiv.org/pdf/1606.08415v4.pdf,dl_general,we propose the gaussian error linear unit gelu a high performing neural network activation function the gelu activation function is x phi x where phi x the standard gaussian cumulative distribution function the gelu nonlinearity weights inputs by their value rather than gates inputs by their sign as in relus x mathbf 1 _ x 0 we perform an empirical evaluation of the gelu nonlinearity against the relu and elu activations and find performance improvements across all considered computer vision natural language processing and speech tasks,gaussian error linear units gelus dan hendrycks and kevin gimpel we propose the gaussian error linear unit gelu a high performing neural network activation function the gelu activation function is x phi x where phi x the standard gaussian cumulative distribution function the gelu nonlinearity weights inputs by their value rather than gates inputs by their sign as in relus x mathbf 1 _ x 0 we perform an empirical evaluation of the gelu nonlinearity against the relu and elu activations and find performance improvements across all considered computer vision natural language processing and speech tasks
2016,google s neural machine translation system bridging the gap between human and machine translation,yonghui wu et al,https://arxiv.org/abs/1609.08144v2,dl_nlp,neural machine translation nmt is an end to end learning approach for automated translation with the potential to overcome many of the weaknesses of conventional phrase based translation systems unfortunately nmt systems are known to be computationally expensive both in training and in translation inference also most nmt systems have difficulty with rare words these issues have hindered nmt s use in practical deployments and services where both accuracy and speed are essential in this work we present gnmt google s neural machine translation system which attempts to address many of these issues our model consists of a deep lstm network with 8 encoder and 8 decoder layers using attention and residual connections to improve parallelism and therefore decrease training time our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder to accelerate the final translation speed we employ low precision arithmetic during inference computations to improve handling of rare words we divide words into a limited set of common sub word units wordpieces for both input and output this method provides a good balance between the flexibility of character delimited models and the efficiency of word delimited models naturally handles translation of rare words and ultimately improves the overall accuracy of the system our beam search technique employs a length normalization procedure and uses a coverage penalty which encourages generation of an output sentence that is most likely to cover all the words in the source sentence on the wmt 14 english to french and english to german benchmarks gnmt achieves competitive results to state of the art using a human side by side evaluation on a set of isolated simple sentences it reduces translation errors by an average of 60 compared to google s phrase based production system,google s neural machine translation system bridging the gap between human and machine translation yonghui wu et al neural machine translation nmt is an end to end learning approach for automated translation with the potential to overcome many of the weaknesses of conventional phrase based translation systems unfortunately nmt systems are known to be computationally expensive both in training and in translation inference also most nmt systems have difficulty with rare words these issues have hindered nmt s use in practical deployments and services where both accuracy and speed are essential in this work we present gnmt google s neural machine translation system which attempts to address many of these issues our model consists of a deep lstm network with 8 encoder and 8 decoder layers using attention and residual connections to improve parallelism and therefore decrease training time our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder to accelerate the final translation speed we employ low precision arithmetic during inference computations to improve handling of rare words we divide words into a limited set of common sub word units wordpieces for both input and output this method provides a good balance between the flexibility of character delimited models and the efficiency of word delimited models naturally handles translation of rare words and ultimately improves the overall accuracy of the system our beam search technique employs a length normalization procedure and uses a coverage penalty which encourages generation of an output sentence that is most likely to cover all the words in the source sentence on the wmt 14 english to french and english to german benchmarks gnmt achieves competitive results to state of the art using a human side by side evaluation on a set of isolated simple sentences it reduces translation errors by an average of 60 compared to google s phrase based production system
2016,categorical reparameterization with gumbel softmax,eric jang et al,https://arxiv.org/abs/1611.01144v5,dl_general,categorical variables are a natural choice for representing discrete structure in the world however stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples in this work we present an efficient gradient estimator that replaces the non differentiable sample from a categorical distribution with a differentiable sample from a novel gumbel softmax distribution this distribution has the essential property that it can be smoothly annealed into a categorical distribution we show that our gumbel softmax estimator outperforms state of the art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables and enables large speedups on semi supervised classification,categorical reparameterization with gumbel softmax eric jang et al categorical variables are a natural choice for representing discrete structure in the world however stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples in this work we present an efficient gradient estimator that replaces the non differentiable sample from a categorical distribution with a differentiable sample from a novel gumbel softmax distribution this distribution has the essential property that it can be smoothly annealed into a categorical distribution we show that our gumbel softmax estimator outperforms state of the art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables and enables large speedups on semi supervised classification
2016,understanding deep learning requires rethinking generalization,chiyuan zhang et al,https://arxiv.org/pdf/1611.03530.pdf,learning_theory,despite their massive size successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance conventional wisdom attributes small generalization error either to properties of the model family or to the regularization techniques used during training through extensive systematic experiments we show how these traditional approaches fail to explain why large neural networks generalize well in practice specifically our experiments establish that state of the art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data this phenomenon is qualitatively unaffected by explicit regularization and occurs even if we replace the true images by completely unstructured random noise we corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice we interpret our experimental findings by comparison with traditional models,understanding deep learning requires rethinking generalization chiyuan zhang et al despite their massive size successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance conventional wisdom attributes small generalization error either to properties of the model family or to the regularization techniques used during training through extensive systematic experiments we show how these traditional approaches fail to explain why large neural networks generalize well in practice specifically our experiments establish that state of the art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data this phenomenon is qualitatively unaffected by explicit regularization and occurs even if we replace the true images by completely unstructured random noise we corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice we interpret our experimental findings by comparison with traditional models
2016,feature pyramid networks for object detection,tsung yi lin et al,https://arxiv.org/pdf/1612.03144.pdf,vision,feature pyramids are a basic component in recognition systems for detecting objects at different scales but recent deep learning object detectors have avoided pyramid representations in part because they are compute and memory intensive in this paper we exploit the inherent multi scale pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost a top down architecture with lateral connections is developed for building high level semantic feature maps at all scales this architecture called a feature pyramid network fpn shows significant improvement as a generic feature extractor in several applications using fpn in a basic faster r cnn system our method achieves state of the art single model results on the coco detection benchmark without bells and whistles surpassing all existing single model entries including those from the coco 2016 challenge winners in addition our method can run at 5 fps on a gpu and thus is a practical and accurate solution to multi scale object detection code will be made publicly available,feature pyramid networks for object detection tsung yi lin et al feature pyramids are a basic component in recognition systems for detecting objects at different scales but recent deep learning object detectors have avoided pyramid representations in part because they are compute and memory intensive in this paper we exploit the inherent multi scale pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost a top down architecture with lateral connections is developed for building high level semantic feature maps at all scales this architecture called a feature pyramid network fpn shows significant improvement as a generic feature extractor in several applications using fpn in a basic faster r cnn system our method achieves state of the art single model results on the coco detection benchmark without bells and whistles surpassing all existing single model entries including those from the coco 2016 challenge winners in addition our method can run at 5 fps on a gpu and thus is a practical and accurate solution to multi scale object detection code will be made publicly available
2016,a structured self attentive sentence embedding,zhouhan lin et al,https://arxiv.org/abs/1703.03130,dl_nlp,this paper proposes a new model for extracting an interpretable sentence embedding by introducing self attention instead of using a vector we use a 2 d matrix to represent the embedding with each row of the matrix attending on a different part of the sentence we also propose a self attention mechanism and a special regularization term for the model as a side effect the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding we evaluate our model on 3 different tasks author profiling sentiment classification and textual entailment results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks,a structured self attentive sentence embedding zhouhan lin et al this paper proposes a new model for extracting an interpretable sentence embedding by introducing self attention instead of using a vector we use a 2 d matrix to represent the embedding with each row of the matrix attending on a different part of the sentence we also propose a self attention mechanism and a special regularization term for the model as a side effect the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding we evaluate our model on 3 different tasks author profiling sentiment classification and textual entailment results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks
2017,arbitrary style transfer in real time with adaptive instance normalization,xun huang and serge belongie,https://arxiv.org/abs/1703.06868,dl_general,gatys et al recently introduced a neural algorithm that renders a content image in the style of another image achieving so called style transfer however their framework requires a slow iterative optimization process which limits its practical application fast approximations with feed forward neural networks have been proposed to speed up neural style transfer unfortunately the speed improvement comes at a cost the network is usually tied to a fixed set of styles and cannot adapt to arbitrary new styles in this paper we present a simple yet effective approach that for the first time enables arbitrary style transfer in real time at the heart of our method is a novel adaptive instance normalization adain layer that aligns the mean and variance of the content features with those of the style features our method achieves speed comparable to the fastest existing approach without the restriction to a pre defined set of styles in addition our approach allows flexible user controls such as content style trade off style interpolation color spatial controls all using a single feed forward neural network,arbitrary style transfer in real time with adaptive instance normalization xun huang and serge belongie gatys et al recently introduced a neural algorithm that renders a content image in the style of another image achieving so called style transfer however their framework requires a slow iterative optimization process which limits its practical application fast approximations with feed forward neural networks have been proposed to speed up neural style transfer unfortunately the speed improvement comes at a cost the network is usually tied to a fixed set of styles and cannot adapt to arbitrary new styles in this paper we present a simple yet effective approach that for the first time enables arbitrary style transfer in real time at the heart of our method is a novel adaptive instance normalization adain layer that aligns the mean and variance of the content features with those of the style features our method achieves speed comparable to the fastest existing approach without the restriction to a pre defined set of styles in addition our approach allows flexible user controls such as content style trade off style interpolation color spatial controls all using a single feed forward neural network
2017,mask r cnn,kaiming he et al,https://arxiv.org/pdf/1703.06870.pdf,vision,we present a conceptually simple flexible and general framework for object instance segmentation our approach efficiently detects objects in an image while simultaneously generating a high quality segmentation mask for each instance the method called mask r cnn extends faster r cnn by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition mask r cnn is simple to train and adds only a small overhead to faster r cnn running at 5 fps moreover mask r cnn is easy to generalize to other tasks e g allowing us to estimate human poses in the same framework we show top results in all three tracks of the coco suite of challenges including instance segmentation bounding box object detection and person keypoint detection without bells and whistles mask r cnn outperforms all existing single model entries on every task including the coco 2016 challenge winners we hope our simple and effective approach will serve as a solid baseline and help ease future research in instance level recognition code has been made available at,mask r cnn kaiming he et al we present a conceptually simple flexible and general framework for object instance segmentation our approach efficiently detects objects in an image while simultaneously generating a high quality segmentation mask for each instance the method called mask r cnn extends faster r cnn by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition mask r cnn is simple to train and adds only a small overhead to faster r cnn running at 5 fps moreover mask r cnn is easy to generalize to other tasks e g allowing us to estimate human poses in the same framework we show top results in all three tracks of the coco suite of challenges including instance segmentation bounding box object detection and person keypoint detection without bells and whistles mask r cnn outperforms all existing single model entries on every task including the coco 2016 challenge winners we hope our simple and effective approach will serve as a solid baseline and help ease future research in instance level recognition code has been made available at
2016,on the properties of the softmax function with application in game theory and reinforcement learning,bolin gao and lacra pavel,https://arxiv.org/pdf/1704.00805.pdf,dl_general,in this paper we utilize results from convex analysis and monotone operator theory to derive additional properties of the softmax function that have not yet been covered in the existing literature in particular we show that the softmax function is the monotone gradient map of the log sum exp function by exploiting this connection we show that the inverse temperature parameter determines the lipschitz and co coercivity properties of the softmax function we then demonstrate the usefulness of these properties through an application in game theoretic reinforcement learning,on the properties of the softmax function with application in game theory and reinforcement learning bolin gao and lacra pavel in this paper we utilize results from convex analysis and monotone operator theory to derive additional properties of the softmax function that have not yet been covered in the existing literature in particular we show that the softmax function is the monotone gradient map of the log sum exp function by exploiting this connection we show that the inverse temperature parameter determines the lipschitz and co coercivity properties of the softmax function we then demonstrate the usefulness of these properties through an application in game theoretic reinforcement learning
2017,stochastic gradient descent as approximate bayesian inference,stephan mandt et al,https://arxiv.org/pdf/1704.04289.pdf,bayesian,stochastic gradient descent with a constant learning rate constant sgd simulates a markov chain with a stationary distribution with this perspective we derive several new results 1 we show that constant sgd can be used as an approximate bayesian posterior inference algorithm specifically we show how to adjust the tuning parameters of constant sgd to best match the stationary distribution to a posterior minimizing the kullback leibler divergence between these two distributions 2 we demonstrate that constant sgd gives rise to a new variational em algorithm that optimizes hyperparameters in complex probabilistic models 3 we also propose sgd with momentum for sampling and show how to adjust the damping coefficient accordingly 4 we analyze mcmc algorithms for langevin dynamics and stochastic gradient fisher scoring we quantify the approximation errors due to finite learning rates finally 5 we use the stochastic process perspective to give a short proof of why polyak averaging is optimal based on this idea we propose a scalable approximate mcmc algorithm the averaged stochastic gradient sampler,stochastic gradient descent as approximate bayesian inference stephan mandt et al stochastic gradient descent with a constant learning rate constant sgd simulates a markov chain with a stationary distribution with this perspective we derive several new results 1 we show that constant sgd can be used as an approximate bayesian posterior inference algorithm specifically we show how to adjust the tuning parameters of constant sgd to best match the stationary distribution to a posterior minimizing the kullback leibler divergence between these two distributions 2 we demonstrate that constant sgd gives rise to a new variational em algorithm that optimizes hyperparameters in complex probabilistic models 3 we also propose sgd with momentum for sampling and show how to adjust the damping coefficient accordingly 4 we analyze mcmc algorithms for langevin dynamics and stochastic gradient fisher scoring we quantify the approximation errors due to finite learning rates finally 5 we use the stochastic process perspective to give a short proof of why polyak averaging is optimal based on this idea we propose a scalable approximate mcmc algorithm the averaged stochastic gradient sampler
2017,mobilenets efficient convolutional neural networks for mobile vision applications,andrew g howard et al,https://arxiv.org/pdf/1704.04861.pdf,vision,we present a class of efficient models called mobilenets for mobile and embedded vision applications mobilenets are based on a streamlined architecture that uses depth wise separable convolutions to build light weight deep neural networks we introduce two simple global hyper parameters that efficiently trade off between latency and accuracy these hyper parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem we present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on imagenet classification we then demonstrate the effectiveness of mobilenets across a wide range of applications and use cases including object detection finegrain classification face attributes and large scale geo localization,mobilenets efficient convolutional neural networks for mobile vision applications andrew g howard et al we present a class of efficient models called mobilenets for mobile and embedded vision applications mobilenets are based on a streamlined architecture that uses depth wise separable convolutions to build light weight deep neural networks we introduce two simple global hyper parameters that efficiently trade off between latency and accuracy these hyper parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem we present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on imagenet classification we then demonstrate the effectiveness of mobilenets across a wide range of applications and use cases including object detection finegrain classification face attributes and large scale geo localization
2017,attention is all you need,ashish vaswani et al,https://arxiv.org/abs/1706.03762,dl_nlp,the dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder decoder configuration the best performing models also connect the encoder and decoder through an attention mechanism we propose a new simple network architecture the transformer based solely on attention mechanisms dispensing with recurrence and convolutions entirely experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train our model achieves 28 4 bleu on the wmt 2014 english to german translation task improving over the existing best results including ensembles by over 2 bleu on the wmt 2014 english to french translation task our model establishes a new single model state of the art bleu score of 41 8 after training for 3 5 days on eight gpus a small fraction of the training costs of the best models from the literature we show that the transformer generalizes well to other tasks by applying it successfully to english constituency parsing both with large and limited training data,attention is all you need ashish vaswani et al the dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder decoder configuration the best performing models also connect the encoder and decoder through an attention mechanism we propose a new simple network architecture the transformer based solely on attention mechanisms dispensing with recurrence and convolutions entirely experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train our model achieves 28 4 bleu on the wmt 2014 english to german translation task improving over the existing best results including ensembles by over 2 bleu on the wmt 2014 english to french translation task our model establishes a new single model state of the art bleu score of 41 8 after training for 3 5 days on eight gpus a small fraction of the training costs of the best models from the literature we show that the transformer generalizes well to other tasks by applying it successfully to english constituency parsing both with large and limited training data
2017,gans trained by a two time scale update rule converge to a local nash equilibrium,martin heusel et al,https://arxiv.org/abs/1706.08500v6,learning_theory,generative adversarial networks gans excel at creating realistic images with complex models for which maximum likelihood is infeasible however the convergence of gan training has still not been proved we propose a two time scale update rule ttur for training gans with stochastic gradient descent on arbitrary gan loss functions ttur has an individual learning rate for both the discriminator and the generator using the theory of stochastic approximation we prove that the ttur converges under mild assumptions to a stationary local nash equilibrium the convergence carries over to the popular adam optimization for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape for the evaluation of the performance of gans at image generation we introduce the fr echet inception distance fid which captures the similarity of generated images to real ones better than the inception score in experiments ttur improves learning for dcgans and improved wasserstein gans wgan gp outperforming conventional gan training on celeba cifar 10 svhn lsun bedrooms and the one billion word benchmark,gans trained by a two time scale update rule converge to a local nash equilibrium martin heusel et al generative adversarial networks gans excel at creating realistic images with complex models for which maximum likelihood is infeasible however the convergence of gan training has still not been proved we propose a two time scale update rule ttur for training gans with stochastic gradient descent on arbitrary gan loss functions ttur has an individual learning rate for both the discriminator and the generator using the theory of stochastic approximation we prove that the ttur converges under mild assumptions to a stationary local nash equilibrium the convergence carries over to the popular adam optimization for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape for the evaluation of the performance of gans at image generation we introduce the fr echet inception distance fid which captures the similarity of generated images to real ones better than the inception score in experiments ttur improves learning for dcgans and improved wasserstein gans wgan gp outperforming conventional gan training on celeba cifar 10 svhn lsun bedrooms and the one billion word benchmark
2017,proximal policy optimization algorithms,john schulman et al,https://arxiv.org/abs/1707.06347,dl_rl,we propose a new family of policy gradient methods for reinforcement learning which alternate between sampling data through interaction with the environment and optimizing a surrogate objective function using stochastic gradient ascent whereas standard policy gradient methods perform one gradient update per data sample we propose a novel objective function that enables multiple epochs of minibatch updates the new methods which we call proximal policy optimization ppo have some of the benefits of trust region policy optimization trpo but they are much simpler to implement more general and have better sample complexity empirically our experiments test ppo on a collection of benchmark tasks including simulated robotic locomotion and atari game playing and we show that ppo outperforms other online policy gradient methods and overall strikes a favorable balance between sample complexity simplicity and wall time,proximal policy optimization algorithms john schulman et al we propose a new family of policy gradient methods for reinforcement learning which alternate between sampling data through interaction with the environment and optimizing a surrogate objective function using stochastic gradient ascent whereas standard policy gradient methods perform one gradient update per data sample we propose a novel objective function that enables multiple epochs of minibatch updates the new methods which we call proximal policy optimization ppo have some of the benefits of trust region policy optimization trpo but they are much simpler to implement more general and have better sample complexity empirically our experiments test ppo on a collection of benchmark tasks including simulated robotic locomotion and atari game playing and we show that ppo outperforms other online policy gradient methods and overall strikes a favorable balance between sample complexity simplicity and wall time
2017,smash one shot model architecture search through hypernetworks,andrew brock et al,https://arxiv.org/abs/1708.05344,ml_tuning,designing architectures for deep neural networks requires expert knowledge and substantial computation time we propose a technique to accelerate architecture selection by learning an auxiliary hypernet that generates the weights of a main model conditioned on that model s architecture by comparing the relative validation performance of networks with hypernet generated weights we can effectively search over a wide range of architectures at the cost of a single training run to facilitate this search we develop a flexible mechanism based on memory read writes that allows us to define a wide range of network connectivity patterns with resnet densenet and fractalnet blocks as special cases we validate our method smash on cifar 10 and cifar 100 stl 10 modelnet10 and imagenet32x32 achieving competitive performance with similarly sized hand designed networks our code is available at,smash one shot model architecture search through hypernetworks andrew brock et al designing architectures for deep neural networks requires expert knowledge and substantial computation time we propose a technique to accelerate architecture selection by learning an auxiliary hypernet that generates the weights of a main model conditioned on that model s architecture by comparing the relative validation performance of networks with hypernet generated weights we can effectively search over a wide range of architectures at the cost of a single training run to facilitate this search we develop a flexible mechanism based on memory read writes that allows us to define a wide range of network connectivity patterns with resnet densenet and fractalnet blocks as special cases we validate our method smash on cifar 10 and cifar 100 stl 10 modelnet10 and imagenet32x32 achieving competitive performance with similarly sized hand designed networks our code is available at
2017,squeeze and excitation networks,jie hu et al,https://arxiv.org/abs/1709.01507,dl_general,the central building block of convolutional neural networks cnns is the convolution operator which enables networks to construct informative features by fusing both spatial and channel wise information within local receptive fields at each layer a broad range of prior research has investigated the spatial component of this relationship seeking to strengthen the representational power of a cnn by enhancing the quality of spatial encodings throughout its feature hierarchy in this work we focus instead on the channel relationship and propose a novel architectural unit which we term the squeeze and excitation se block that adaptively recalibrates channel wise feature responses by explicitly modelling interdependencies between channels we show that these blocks can be stacked together to form senet architectures that generalise extremely effectively across different datasets we further demonstrate that se blocks bring significant improvements in performance for existing state of the art cnns at slight additional computational cost squeeze and excitation networks formed the foundation of our ilsvrc 2017 classification submission which won first place and reduced the top 5 error to 2 251 surpassing the winning entry of 2016 by a relative improvement of 25 models and code are available at,squeeze and excitation networks jie hu et al the central building block of convolutional neural networks cnns is the convolution operator which enables networks to construct informative features by fusing both spatial and channel wise information within local receptive fields at each layer a broad range of prior research has investigated the spatial component of this relationship seeking to strengthen the representational power of a cnn by enhancing the quality of spatial encodings throughout its feature hierarchy in this work we focus instead on the channel relationship and propose a novel architectural unit which we term the squeeze and excitation se block that adaptively recalibrates channel wise feature responses by explicitly modelling interdependencies between channels we show that these blocks can be stacked together to form senet architectures that generalise extremely effectively across different datasets we further demonstrate that se blocks bring significant improvements in performance for existing state of the art cnns at slight additional computational cost squeeze and excitation networks formed the foundation of our ilsvrc 2017 classification submission which won first place and reduced the top 5 error to 2 251 surpassing the winning entry of 2016 by a relative improvement of 25 models and code are available at
2017,neural discrete representation learning,aaron van den oord et al,https://arxiv.org/pdf/1711.00937.pdf,vision,learning useful representations without supervision remains a key challenge in machine learning in this paper we propose a simple yet powerful generative model that learns such discrete representations our model the vector quantised variational autoencoder vq vae differs from vaes in two key ways the encoder network outputs discrete rather than continuous codes and the prior is learnt rather than static in order to learn a discrete latent representation we incorporate ideas from vector quantisation vq using the vq method allows the model to circumvent issues of posterior collapse where the latents are ignored when they are paired with a powerful autoregressive decoder typically observed in the vae framework pairing these representations with an autoregressive prior the model can generate high quality images videos and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes providing further evidence of the utility of the learnt representations,neural discrete representation learning aaron van den oord et al learning useful representations without supervision remains a key challenge in machine learning in this paper we propose a simple yet powerful generative model that learns such discrete representations our model the vector quantised variational autoencoder vq vae differs from vaes in two key ways the encoder network outputs discrete rather than continuous codes and the prior is learnt rather than static in order to learn a discrete latent representation we incorporate ideas from vector quantisation vq using the vq method allows the model to circumvent issues of posterior collapse where the latents are ignored when they are paired with a powerful autoregressive decoder typically observed in the vae framework pairing these representations with an autoregressive prior the model can generate high quality images videos and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes providing further evidence of the utility of the learnt representations
2018,deep image prior,dmitry ulyanov et al,https://arxiv.org/abs/1711.10925,vision,deep convolutional networks have become a popular tool for image generation and restoration generally their excellent performance is imputed to their ability to learn realistic image priors from a large number of example images in this paper we show that on the contrary the structure of a generator network is sufficient to capture a great deal of low level image statistics prior to any learning in order to do so we show that a randomly initialized neural network can be used as a handcrafted prior with excellent results in standard inverse problems such as denoising super resolution and inpainting furthermore the same prior can be used to invert deep neural representations to diagnose them and to restore images based on flash no flash input pairs apart from its diverse applications our approach highlights the inductive bias captured by standard generator network architectures it also bridges the gap between two very popular families of image restoration methods learning based methods using deep convolutional networks and learning free methods based on handcrafted image priors such as self similarity code and supplementary material are available at,deep image prior dmitry ulyanov et al deep convolutional networks have become a popular tool for image generation and restoration generally their excellent performance is imputed to their ability to learn realistic image priors from a large number of example images in this paper we show that on the contrary the structure of a generator network is sufficient to capture a great deal of low level image statistics prior to any learning in order to do so we show that a randomly initialized neural network can be used as a handcrafted prior with excellent results in standard inverse problems such as denoising super resolution and inpainting furthermore the same prior can be used to invert deep neural representations to diagnose them and to restore images based on flash no flash input pairs apart from its diverse applications our approach highlights the inductive bias captured by standard generator network architectures it also bridges the gap between two very popular families of image restoration methods learning based methods using deep convolutional networks and learning free methods based on handcrafted image priors such as self similarity code and supplementary material are available at
2017,a multi horizon quantile recurrent forecaster,ruofeng wen et al,https://arxiv.org/pdf/1711.11053.pdf,dl_rnn,we propose a framework for general probabilistic multi step time series regression specifically we exploit the expressiveness and temporal nature of sequence to sequence neural networks e g recurrent and convolutional structures the nonparametric nature of quantile regression and the efficiency of direct multi horizon forecasting a new training scheme forking sequences is designed for sequential nets to boost stability and performance we show that the approach accommodates both temporal and static covariates learning across multiple related series shifting seasonality future planned event spikes and cold starts in real life large scale forecasting the performance of the framework is demonstrated in an application to predict the future demand of items sold on amazon com and in a public probabilistic forecasting competition to predict electricity price and load,a multi horizon quantile recurrent forecaster ruofeng wen et al we propose a framework for general probabilistic multi step time series regression specifically we exploit the expressiveness and temporal nature of sequence to sequence neural networks e g recurrent and convolutional structures the nonparametric nature of quantile regression and the efficiency of direct multi horizon forecasting a new training scheme forking sequences is designed for sequential nets to boost stability and performance we show that the approach accommodates both temporal and static covariates learning across multiple related series shifting seasonality future planned event spikes and cold starts in real life large scale forecasting the performance of the framework is demonstrated in an application to predict the future demand of items sold on amazon com and in a public probabilistic forecasting competition to predict electricity price and load
2018,mobilenetv2 inverted residuals and linear bottlenecks,mark sandler et al,https://arxiv.org/abs/1801.04381,vision,in this paper we describe a new mobile architecture mobilenetv2 that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes we also describe efficient ways of applying these mobile models to object detection in a novel framework we call ssdlite additionally we demonstrate how to build mobile semantic segmentation models through a reduced form of deeplabv3 which we call mobile deeplabv3 the mobilenetv2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an mobilenetv2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer additionally we find that it is important to remove non linearities in the narrow layers in order to maintain representational power we demonstrate that this improves performance and provide an intuition that led to this design finally our approach allows decoupling of the input output domains from the expressiveness of the transformation which provides a convenient framework for further analysis we measure our performance on imagenet classification coco object detection voc image segmentation we evaluate the trade offs between accuracy and number of operations measured by multiply adds madd as well as the number of parameters,mobilenetv2 inverted residuals and linear bottlenecks mark sandler et al in this paper we describe a new mobile architecture mobilenetv2 that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes we also describe efficient ways of applying these mobile models to object detection in a novel framework we call ssdlite additionally we demonstrate how to build mobile semantic segmentation models through a reduced form of deeplabv3 which we call mobile deeplabv3 the mobilenetv2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an mobilenetv2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer additionally we find that it is important to remove non linearities in the narrow layers in order to maintain representational power we demonstrate that this improves performance and provide an intuition that led to this design finally our approach allows decoupling of the input output domains from the expressiveness of the transformation which provides a convenient framework for further analysis we measure our performance on imagenet classification coco object detection voc image segmentation we evaluate the trade offs between accuracy and number of operations measured by multiply adds madd as well as the number of parameters
2017,understanding the disharmony between dropout and batch normalization by variance shift,xiang li et al,https://arxiv.org/pdf/1801.05134.pdf,dl_general,this paper first answers the question why do the two most powerful techniques dropout and batch normalization bn often lead to a worse performance when they are combined together in both theoretical and statistical aspects theoretically we find that dropout would shift the variance of a specific neural unit when we transfer the state of that network from train to test however bn would maintain its statistical variance which is accumulated from the entire learning procedure in the test phase the inconsistency of that variance we name this scheme as variance shift causes the unstable numerical behavior in inference that leads to more erroneous predictions finally when applying dropout before bn thorough experiments on densenet resnet resnext and wide resnet confirm our findings according to the uncovered mechanism we next explore several strategies that modifies dropout and try to overcome the limitations of their combination by avoiding the variance shift risks,understanding the disharmony between dropout and batch normalization by variance shift xiang li et al this paper first answers the question why do the two most powerful techniques dropout and batch normalization bn often lead to a worse performance when they are combined together in both theoretical and statistical aspects theoretically we find that dropout would shift the variance of a specific neural unit when we transfer the state of that network from train to test however bn would maintain its statistical variance which is accumulated from the entire learning procedure in the test phase the inconsistency of that variance we name this scheme as variance shift causes the unstable numerical behavior in inference that leads to more erroneous predictions finally when applying dropout before bn thorough experiments on densenet resnet resnext and wide resnet confirm our findings according to the uncovered mechanism we next explore several strategies that modifies dropout and try to overcome the limitations of their combination by avoiding the variance shift risks
2018,universal language model fine tuning for text classification,jeremy howard and sebastian ruder,https://arxiv.org/abs/1801.06146,dl_nlp,inductive transfer learning has greatly impacted computer vision but existing approaches in nlp still require task specific modifications and training from scratch we propose universal language model fine tuning ulmfit an effective transfer learning method that can be applied to any task in nlp and introduce techniques that are key for fine tuning a language model our method significantly outperforms the state of the art on six text classification tasks reducing the error by 18 24 on the majority of datasets furthermore with only 100 labeled examples it matches the performance of training from scratch on 100x more data we open source our pretrained models and code,universal language model fine tuning for text classification jeremy howard and sebastian ruder inductive transfer learning has greatly impacted computer vision but existing approaches in nlp still require task specific modifications and training from scratch we propose universal language model fine tuning ulmfit an effective transfer learning method that can be applied to any task in nlp and introduce techniques that are key for fine tuning a language model our method significantly outperforms the state of the art on six text classification tasks reducing the error by 18 24 on the majority of datasets furthermore with only 100 labeled examples it matches the performance of training from scratch on 100x more data we open source our pretrained models and code
2018,umap uniform manifold approximation and projection for dimension reduction,leland mcinnes et al,https://arxiv.org/abs/1802.03426,ml_representation,umap uniform manifold approximation and projection is a novel manifold learning technique for dimension reduction umap is constructed from a theoretical framework based in riemannian geometry and algebraic topology the result is a practical scalable algorithm that applies to real world data the umap algorithm is competitive with t sne for visualization quality and arguably preserves more of the global structure with superior run time performance furthermore umap has no computational restrictions on embedding dimension making it viable as a general purpose dimension reduction technique for machine learning,umap uniform manifold approximation and projection for dimension reduction leland mcinnes et al umap uniform manifold approximation and projection is a novel manifold learning technique for dimension reduction umap is constructed from a theoretical framework based in riemannian geometry and algebraic topology the result is a practical scalable algorithm that applies to real world data the umap algorithm is competitive with t sne for visualization quality and arguably preserves more of the global structure with superior run time performance furthermore umap has no computational restrictions on embedding dimension making it viable as a general purpose dimension reduction technique for machine learning
2018,the lottery ticket hypothesis finding sparse trainable neural networks,jonathan frankle and michael carbin,https://arxiv.org/abs/1803.03635,learning_theory,neural network pruning techniques can reduce the parameter counts of trained networks by over 90 decreasing storage requirements and improving computational performance of inference without compromising accuracy however contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start which would similarly improve training performance we find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively based on these results we articulate the lottery ticket hypothesis dense randomly initialized feed forward networks contain subnetworks winning tickets that when trained in isolation reach test accuracy comparable to the original network in a similar number of iterations the winning tickets we find have won the initialization lottery their connections have initial weights that make training particularly effective we present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations we consistently find winning tickets that are less than 10 20 of the size of several fully connected and convolutional feed forward architectures for mnist and cifar10 above this size the winning tickets that we find learn faster than the original network and reach higher test accuracy,the lottery ticket hypothesis finding sparse trainable neural networks jonathan frankle and michael carbin neural network pruning techniques can reduce the parameter counts of trained networks by over 90 decreasing storage requirements and improving computational performance of inference without compromising accuracy however contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start which would similarly improve training performance we find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively based on these results we articulate the lottery ticket hypothesis dense randomly initialized feed forward networks contain subnetworks winning tickets that when trained in isolation reach test accuracy comparable to the original network in a similar number of iterations the winning tickets we find have won the initialization lottery their connections have initial weights that make training particularly effective we present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations we consistently find winning tickets that are less than 10 20 of the size of several fully connected and convolutional feed forward architectures for mnist and cifar10 above this size the winning tickets that we find learn faster than the original network and reach higher test accuracy
2018,relational inductive biases deep learning and graph networks,peter w battaglia et al,https://arxiv.org/pdf/1806.01261.pdf,learning_theory,artificial intelligence ai has undergone a renaissance recently making major progress in key domains such as vision language control and decision making this has been due in part to cheap data and cheap compute resources which have fit the natural strengths of deep learning however many defining characteristics of human intelligence which developed under much different pressures remain out of reach for current approaches in particular generalizing beyond one s experiences a hallmark of human intelligence from infancy remains a formidable challenge for modern ai the following is part position paper part review and part unification we argue that combinatorial generalization must be a top priority for ai to achieve human like abilities and that structured representations and computations are key to realizing this objective just as biology uses nature and nurture cooperatively we reject the false choice between hand engineering and end to end learning and instead advocate for an approach which benefits from their complementary strengths we explore how using relational inductive biases within deep learning architectures can facilitate learning about entities relations and rules for composing them we present a new building block for the ai toolkit with a strong relational inductive bias the graph network which generalizes and extends various approaches for neural networks that operate on graphs and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors we discuss how graph networks can support relational reasoning and combinatorial generalization laying the foundation for more sophisticated interpretable and flexible patterns of reasoning as a companion to this paper we have released an open source software library for building graph networks with demonstrations of how to use them in practice,relational inductive biases deep learning and graph networks peter w battaglia et al artificial intelligence ai has undergone a renaissance recently making major progress in key domains such as vision language control and decision making this has been due in part to cheap data and cheap compute resources which have fit the natural strengths of deep learning however many defining characteristics of human intelligence which developed under much different pressures remain out of reach for current approaches in particular generalizing beyond one s experiences a hallmark of human intelligence from infancy remains a formidable challenge for modern ai the following is part position paper part review and part unification we argue that combinatorial generalization must be a top priority for ai to achieve human like abilities and that structured representations and computations are key to realizing this objective just as biology uses nature and nurture cooperatively we reject the false choice between hand engineering and end to end learning and instead advocate for an approach which benefits from their complementary strengths we explore how using relational inductive biases within deep learning architectures can facilitate learning about entities relations and rules for composing them we present a new building block for the ai toolkit with a strong relational inductive bias the graph network which generalizes and extends various approaches for neural networks that operate on graphs and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors we discuss how graph networks can support relational reasoning and combinatorial generalization laying the foundation for more sophisticated interpretable and flexible patterns of reasoning as a companion to this paper we have released an open source software library for building graph networks with demonstrations of how to use them in practice
2019,on the covariance hessian relation in evolution strategies,ofer m shir and amir yehudayoff,https://arxiv.org/abs/1806.03674,learning_theory,we consider evolution strategies operating only with isotropic gaussian mutations on positive quadratic objective functions and investigate the covariance matrix when constructed out of selected individuals by truncation we prove that the covariance matrix over 1 lambda selected decision vectors becomes proportional to the inverse of the landscape hessian as the population size lambda increases this generalizes a previous result that proved an equivalent phenomenon when sampling was assumed to take place in the vicinity of the optimum it further confirms the classical hypothesis that statistical learning of the landscape is an inherent characteristic of standard evolution strategies and that this distinguishing capability stems only from the usage of isotropic gaussian mutations and rank based selection we provide broad numerical validation for the proven results and present empirical evidence for its generalization to mu lambda selection,on the covariance hessian relation in evolution strategies ofer m shir and amir yehudayoff we consider evolution strategies operating only with isotropic gaussian mutations on positive quadratic objective functions and investigate the covariance matrix when constructed out of selected individuals by truncation we prove that the covariance matrix over 1 lambda selected decision vectors becomes proportional to the inverse of the landscape hessian as the population size lambda increases this generalizes a previous result that proved an equivalent phenomenon when sampling was assumed to take place in the vicinity of the optimum it further confirms the classical hypothesis that statistical learning of the landscape is an inherent characteristic of standard evolution strategies and that this distinguishing capability stems only from the usage of isotropic gaussian mutations and rank based selection we provide broad numerical validation for the proven results and present empirical evidence for its generalization to mu lambda selection
2018,neural tangent kernel convergence and generalization in neural networks,arthur jacot et al,https://arxiv.org/abs/1806.07572,learning_theory,at initialization artificial neural networks anns are equivalent to gaussian processes in the infinite width limit thus connecting them to kernel methods we prove that the evolution of an ann during training can also be described by a kernel during gradient descent on the parameters of an ann the network function f_ theta which maps input vectors to output vectors follows the kernel gradient of the functional cost which is convex in contrast to the parameter cost w r t a new kernel the neural tangent kernel ntk this kernel is central to describe the generalization features of anns while the ntk is random at initialization and varies during training in the infinite width limit it converges to an explicit limiting kernel and it stays constant during training this makes it possible to study the training of anns in function space instead of parameter space convergence of the training can then be related to the positive definiteness of the limiting ntk we prove the positive definiteness of the limiting ntk when the data is supported on the sphere and the non linearity is non polynomial we then focus on the setting of least squares regression and show that in the infinite width limit the network function f_ theta follows a linear differential equation during training the convergence is fastest along the largest kernel principal components of the input data with respect to the ntk hence suggesting a theoretical motivation for early stopping finally we study the ntk numerically observe its behavior for wide networks and compare it to the infinite width limit,neural tangent kernel convergence and generalization in neural networks arthur jacot et al at initialization artificial neural networks anns are equivalent to gaussian processes in the infinite width limit thus connecting them to kernel methods we prove that the evolution of an ann during training can also be described by a kernel during gradient descent on the parameters of an ann the network function f_ theta which maps input vectors to output vectors follows the kernel gradient of the functional cost which is convex in contrast to the parameter cost w r t a new kernel the neural tangent kernel ntk this kernel is central to describe the generalization features of anns while the ntk is random at initialization and varies during training in the infinite width limit it converges to an explicit limiting kernel and it stays constant during training this makes it possible to study the training of anns in function space instead of parameter space convergence of the training can then be related to the positive definiteness of the limiting ntk we prove the positive definiteness of the limiting ntk when the data is supported on the sphere and the non linearity is non polynomial we then focus on the setting of least squares regression and show that in the infinite width limit the network function f_ theta follows a linear differential equation during training the convergence is fastest along the largest kernel principal components of the input data with respect to the ntk hence suggesting a theoretical motivation for early stopping finally we study the ntk numerically observe its behavior for wide networks and compare it to the infinite width limit
2018,tracking emerges by colorizing videos,carl vondrick et al,https://arxiv.org/abs/1806.09594,vision,we use large amounts of unlabeled video to learn models for visual tracking without manual human supervision we leverage the natural temporal coherency of color to create a model that learns to colorize gray scale videos by copying colors from a reference frame quantitative and qualitative experiments suggest that this task causes the model to automatically learn to track visual regions although the model is trained without any ground truth labels our method learns to track well enough to outperform the latest methods based on optical flow moreover our results suggest that failures to track are correlated with failures to colorize indicating that advancing video colorization may further improve self supervised visual tracking,tracking emerges by colorizing videos carl vondrick et al we use large amounts of unlabeled video to learn models for visual tracking without manual human supervision we leverage the natural temporal coherency of color to create a model that learns to colorize gray scale videos by copying colors from a reference frame quantitative and qualitative experiments suggest that this task causes the model to automatically learn to track visual regions although the model is trained without any ground truth labels our method learns to track well enough to outperform the latest methods based on optical flow moreover our results suggest that failures to track are correlated with failures to colorize indicating that advancing video colorization may further improve self supervised visual tracking
2018,representation learning with contrastive predictive coding,aaron van den oord et al,https://arxiv.org/abs/1807.03748,ml_representation,while supervised learning has enabled great progress in many applications unsupervised learning has not seen such widespread adoption and remains an important and challenging endeavor for artificial intelligence in this work we propose a universal unsupervised learning approach to extract useful representations from high dimensional data which we call contrastive predictive coding the key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models we use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples it also makes the model tractable by using negative sampling while most prior work has focused on evaluating representations for a particular modality we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains speech images text and reinforcement learning in 3d environments,representation learning with contrastive predictive coding aaron van den oord et al while supervised learning has enabled great progress in many applications unsupervised learning has not seen such widespread adoption and remains an important and challenging endeavor for artificial intelligence in this work we propose a universal unsupervised learning approach to extract useful representations from high dimensional data which we call contrastive predictive coding the key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models we use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples it also makes the model tractable by using negative sampling while most prior work has focused on evaluating representations for a particular modality we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains speech images text and reinforcement learning in 3d environments
2018,bert pre training of deep bidirectional transformers for language understanding,jacob devlin et al,https://arxiv.org/abs/1810.04805v2,dl_nlp,we introduce a new language representation model called bert which stands for bidirectional encoder representations from transformers unlike recent language representation models bert is designed to pre train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers as a result the pre trained bert model can be fine tuned with just one additional output layer to create state of the art models for a wide range of tasks such as question answering and language inference without substantial task specific architecture modifications bert is conceptually simple and empirically powerful it obtains new state of the art results on eleven natural language processing tasks including pushing the glue score to 80 5 7 7 point absolute improvement multinli accuracy to 86 7 4 6 absolute improvement squad v1 1 question answering test f1 to 93 2 1 5 point absolute improvement and squad v2 0 test f1 to 83 1 5 1 point absolute improvement,bert pre training of deep bidirectional transformers for language understanding jacob devlin et al we introduce a new language representation model called bert which stands for bidirectional encoder representations from transformers unlike recent language representation models bert is designed to pre train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers as a result the pre trained bert model can be fine tuned with just one additional output layer to create state of the art models for a wide range of tasks such as question answering and language inference without substantial task specific architecture modifications bert is conceptually simple and empirically powerful it obtains new state of the art results on eleven natural language processing tasks including pushing the glue score to 80 5 7 7 point absolute improvement multinli accuracy to 86 7 4 6 absolute improvement squad v1 1 question answering test f1 to 93 2 1 5 point absolute improvement and squad v2 0 test f1 to 83 1 5 1 point absolute improvement
2018,occupancy networks learning 3d reconstruction in function space,lars mescheder et al,https://arxiv.org/abs/1812.03828,ml_representation,with the advent of deep neural networks learning based approaches for 3d reconstruction have gained popularity however unlike for images in 3d there is no canonical representation which is both computationally and memory efficient yet allows for representing high resolution geometry of arbitrary topology many of the state of the art learning based 3d reconstruction approaches can hence only represent very coarse 3d geometry or are limited to a restricted domain in this paper we propose occupancy networks a new representation for learning based 3d reconstruction methods occupancy networks implicitly represent the 3d surface as the continuous decision boundary of a deep neural network classifier in contrast to existing approaches our representation encodes a description of the 3d output at infinite resolution without excessive memory footprint we validate that our representation can efficiently encode 3d structure and can be inferred from various kinds of input our experiments demonstrate competitive results both qualitatively and quantitatively for the challenging tasks of 3d reconstruction from single images noisy point clouds and coarse discrete voxel grids we believe that occupancy networks will become a useful tool in a wide variety of learning based 3d tasks,occupancy networks learning 3d reconstruction in function space lars mescheder et al with the advent of deep neural networks learning based approaches for 3d reconstruction have gained popularity however unlike for images in 3d there is no canonical representation which is both computationally and memory efficient yet allows for representing high resolution geometry of arbitrary topology many of the state of the art learning based 3d reconstruction approaches can hence only represent very coarse 3d geometry or are limited to a restricted domain in this paper we propose occupancy networks a new representation for learning based 3d reconstruction methods occupancy networks implicitly represent the 3d surface as the continuous decision boundary of a deep neural network classifier in contrast to existing approaches our representation encodes a description of the 3d output at infinite resolution without excessive memory footprint we validate that our representation can efficiently encode 3d structure and can be inferred from various kinds of input our experiments demonstrate competitive results both qualitatively and quantitatively for the challenging tasks of 3d reconstruction from single images noisy point clouds and coarse discrete voxel grids we believe that occupancy networks will become a useful tool in a wide variety of learning based 3d tasks
2019,reconciling modern machine learning practice and the classical bias variance trade off,mikhail belkin et al,https://arxiv.org/abs/1812.11118,ml_classic,breakthroughs in machine learning are rapidly changing science and society yet our fundamental understanding of this technology has lagged far behind indeed one of the central tenets of the field the bias variance trade off appears to be at odds with the observed behavior of methods used in the modern machine learning practice the bias variance trade off implies that a model should balance under fitting and over fitting rich enough to express underlying structure in data simple enough to avoid fitting spurious patterns however in the modern practice very rich models such as neural networks are trained to exactly fit i e interpolate the data classically such models would be considered over fit and yet they often obtain high accuracy on test data this apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners in this paper we reconcile the classical understanding and the modern practice within a unified performance curve this double descent curve subsumes the textbook u shaped bias variance trade off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance we provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets and we posit a mechanism for its emergence this connection between the performance and the structure of machine learning models delineates the limits of classical analyses and has implications for both the theory and practice of machine learning,reconciling modern machine learning practice and the classical bias variance trade off mikhail belkin et al breakthroughs in machine learning are rapidly changing science and society yet our fundamental understanding of this technology has lagged far behind indeed one of the central tenets of the field the bias variance trade off appears to be at odds with the observed behavior of methods used in the modern machine learning practice the bias variance trade off implies that a model should balance under fitting and over fitting rich enough to express underlying structure in data simple enough to avoid fitting spurious patterns however in the modern practice very rich models such as neural networks are trained to exactly fit i e interpolate the data classically such models would be considered over fit and yet they often obtain high accuracy on test data this apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners in this paper we reconcile the classical understanding and the modern practice within a unified performance curve this double descent curve subsumes the textbook u shaped bias variance trade off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance we provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets and we posit a mechanism for its emergence this connection between the performance and the structure of machine learning models delineates the limits of classical analyses and has implications for both the theory and practice of machine learning
2019,efficientnet rethinking model scaling for convolutional neural networks,mingxing tan and quoc v le,https://arxiv.org/abs/1905.11946,vision,convolutional neural networks convnets are commonly developed at a fixed resource budget and then scaled up for better accuracy if more resources are available in this paper we systematically study model scaling and identify that carefully balancing network depth width and resolution can lead to better performance based on this observation we propose a new scaling method that uniformly scales all dimensions of depth width resolution using a simple yet highly effective compound coefficient we demonstrate the effectiveness of this method on scaling up mobilenets and resnet to go even further we use neural architecture search to design a new baseline network and scale it up to obtain a family of models called efficientnets which achieve much better accuracy and efficiency than previous convnets in particular our efficientnet b7 achieves state of the art 84 3 top 1 accuracy on imagenet while being 8 4x smaller and 6 1x faster on inference than the best existing convnet our efficientnets also transfer well and achieve state of the art accuracy on cifar 100 91 7 flowers 98 8 and 3 other transfer learning datasets with an order of magnitude fewer parameters source code is at,efficientnet rethinking model scaling for convolutional neural networks mingxing tan and quoc v le convolutional neural networks convnets are commonly developed at a fixed resource budget and then scaled up for better accuracy if more resources are available in this paper we systematically study model scaling and identify that carefully balancing network depth width and resolution can lead to better performance based on this observation we propose a new scaling method that uniformly scales all dimensions of depth width resolution using a simple yet highly effective compound coefficient we demonstrate the effectiveness of this method on scaling up mobilenets and resnet to go even further we use neural architecture search to design a new baseline network and scale it up to obtain a family of models called efficientnets which achieve much better accuracy and efficiency than previous convnets in particular our efficientnet b7 achieves state of the art 84 3 top 1 accuracy on imagenet while being 8 4x smaller and 6 1x faster on inference than the best existing convnet our efficientnets also transfer well and achieve state of the art accuracy on cifar 100 91 7 flowers 98 8 and 3 other transfer learning datasets with an order of magnitude fewer parameters source code is at
2019,generating diverse high fidelity images with vq vae 2,ali razavi et al,https://arxiv.org/pdf/1906.00446.pdf,vision,we explore the use of vector quantized variational autoencoder vq vae models for large scale image generation to this end we scale and enhance the autoregressive priors used in vq vae to generate synthetic samples of much higher coherence and fidelity than possible before we use simple feed forward encoder and decoder networks making our model an attractive candidate for applications where the encoding and or decoding speed is critical additionally vq vae requires sampling an autoregressive model only in the compressed latent space which is an order of magnitude faster than sampling in the pixel space especially for large images we demonstrate that a multi scale hierarchical organization of vq vae augmented with powerful priors over the latent codes is able to generate samples with quality that rivals that of state of the art generative adversarial networks on multifaceted datasets such as imagenet while not suffering from gan s known shortcomings such as mode collapse and lack of diversity,generating diverse high fidelity images with vq vae 2 ali razavi et al we explore the use of vector quantized variational autoencoder vq vae models for large scale image generation to this end we scale and enhance the autoregressive priors used in vq vae to generate synthetic samples of much higher coherence and fidelity than possible before we use simple feed forward encoder and decoder networks making our model an attractive candidate for applications where the encoding and or decoding speed is critical additionally vq vae requires sampling an autoregressive model only in the compressed latent space which is an order of magnitude faster than sampling in the pixel space especially for large images we demonstrate that a multi scale hierarchical organization of vq vae augmented with powerful priors over the latent codes is able to generate samples with quality that rivals that of state of the art generative adversarial networks on multifaceted datasets such as imagenet while not suffering from gan s known shortcomings such as mode collapse and lack of diversity
2020,the generalization error of random features regression precise asymptotics and double descent curve,song mei and andrea montanari,https://arxiv.org/abs/1908.05355,ml_classic,deep learning methods operate in regimes that defy the traditional statistical mindset neural network architectures often contain more parameters than training samples and are so rich that they can interpolate the observed labels even if the latter are replaced by pure noise despite their huge complexity the same architectures achieve small generalization error on real data this phenomenon has been rationalized in terms of a so called double descent curve as the model complexity increases the test error follows the usual u shaped curve at the beginning first decreasing and then peaking around the interpolation threshold when the model achieves vanishing training error however it descends again as model complexity exceeds this threshold the global minimum of the test error is found above the interpolation threshold often in the extreme overparametrization regime in which the number of parameters is much larger than the number of samples far from being a peculiar property of deep neural networks elements of this behavior have been demonstrated in much simpler settings including linear regression with random covariates in this paper we consider the problem of learning an unknown function over the d dimensional sphere mathbb s d 1 from n i i d samples boldsymbol x_i y_i in mathbb s d 1 times mathbb r i le n we perform ridge regression on n random features of the form sigma boldsymbol w_a mathsf t boldsymbol x a le n this can be equivalently described as a two layers neural network with random first layer weights we compute the precise asymptotics of the test error in the limit n n d to infty with n d and n d fixed this provides the first analytically tractable model that captures all the features of the double descent phenomenon without assuming ad hoc misspecification structures,the generalization error of random features regression precise asymptotics and double descent curve song mei and andrea montanari deep learning methods operate in regimes that defy the traditional statistical mindset neural network architectures often contain more parameters than training samples and are so rich that they can interpolate the observed labels even if the latter are replaced by pure noise despite their huge complexity the same architectures achieve small generalization error on real data this phenomenon has been rationalized in terms of a so called double descent curve as the model complexity increases the test error follows the usual u shaped curve at the beginning first decreasing and then peaking around the interpolation threshold when the model achieves vanishing training error however it descends again as model complexity exceeds this threshold the global minimum of the test error is found above the interpolation threshold often in the extreme overparametrization regime in which the number of parameters is much larger than the number of samples far from being a peculiar property of deep neural networks elements of this behavior have been demonstrated in much simpler settings including linear regression with random covariates in this paper we consider the problem of learning an unknown function over the d dimensional sphere mathbb s d 1 from n i i d samples boldsymbol x_i y_i in mathbb s d 1 times mathbb r i le n we perform ridge regression on n random features of the form sigma boldsymbol w_a mathsf t boldsymbol x a le n this can be equivalently described as a two layers neural network with random first layer weights we compute the precise asymptotics of the test error in the limit n n d to infty with n d and n d fixed this provides the first analytically tractable model that captures all the features of the double descent phenomenon without assuming ad hoc misspecification structures
2019,zero memory optimizations toward training trillion parameter models,samyam rajbhandari et al,https://arxiv.org/abs/1910.02054,dl_opt,large deep learning models offer significant accuracy gains but training billions to trillions of parameters is challenging existing solutions such as data and model parallelisms exhibit fundamental limitations to fit these models into limited device memory while obtaining computation communication and development efficiency we develop a novel solution zero redundancy optimizer zero to optimize memory vastly improving training speed while increasing the model size that can be efficiently trained zero eliminates memory redundancies in data and model parallel training while retaining low communication volume and high computational granularity allowing us to scale the model size proportional to the number of devices with sustained high efficiency our analysis on memory requirements and communication volume demonstrates zero has the potential to scale beyond 1 trillion parameters using today s hardware we implement and evaluate zero it trains large models of over 100b parameter with super linear speedup on 400 gpus achieving throughput of 15 petaflops this represents an 8x increase in model size and 10x increase in achievable performance over state of the art in terms of usability zero can train large models of up to 13b parameters e g larger than megatron gpt 8 3b and t5 11b without requiring model parallelism which is harder for scientists to apply last but not the least researchers have used the system breakthroughs of zero to create the world s largest language model turing nlg 17b parameters with record breaking accuracy,zero memory optimizations toward training trillion parameter models samyam rajbhandari et al large deep learning models offer significant accuracy gains but training billions to trillions of parameters is challenging existing solutions such as data and model parallelisms exhibit fundamental limitations to fit these models into limited device memory while obtaining computation communication and development efficiency we develop a novel solution zero redundancy optimizer zero to optimize memory vastly improving training speed while increasing the model size that can be efficiently trained zero eliminates memory redundancies in data and model parallel training while retaining low communication volume and high computational granularity allowing us to scale the model size proportional to the number of devices with sustained high efficiency our analysis on memory requirements and communication volume demonstrates zero has the potential to scale beyond 1 trillion parameters using today s hardware we implement and evaluate zero it trains large models of over 100b parameter with super linear speedup on 400 gpus achieving throughput of 15 petaflops this represents an 8x increase in model size and 10x increase in achievable performance over state of the art in terms of usability zero can train large models of up to 13b parameters e g larger than megatron gpt 8 3b and t5 11b without requiring model parallelism which is harder for scientists to apply last but not the least researchers have used the system breakthroughs of zero to create the world s largest language model turing nlg 17b parameters with record breaking accuracy
2019,on the measure of intelligence,françois chollet,https://arxiv.org/abs/1911.01547,learning_theory,to make deliberate progress towards more intelligent and more human like artificial systems we need to be following an appropriate feedback signal we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems as well as comparisons with humans over the past hundred years there has been an abundance of attempts to define and measure intelligence across both the fields of psychology and ai we summarize and critically assess these definitions and evaluation approaches while making apparent the two historical conceptions of intelligence that have implicitly guided them we note that in practice the contemporary ai community still gravitates towards benchmarking intelligence by comparing the skill exhibited by ais and humans at specific tasks such as board games and video games we argue that solely measuring skill at any given task falls short of measuring intelligence because skill is heavily modulated by prior knowledge and experience unlimited priors or unlimited training data allow experimenters to buy arbitrary levels of skills for a system in a way that masks the system s own generalization power we then articulate a new formal definition of intelligence based on algorithmic information theory describing intelligence as skill acquisition efficiency and highlighting the concepts of scope generalization difficulty priors and experience using this definition we propose a set of guidelines for what a general ai benchmark should look like finally we present a benchmark closely following these guidelines the abstraction and reasoning corpus arc built upon an explicit set of priors designed to be as close as possible to innate human priors we argue that arc can be used to measure a human like form of general fluid intelligence and that it enables fair general intelligence comparisons between ai systems and humans,on the measure of intelligence françois chollet to make deliberate progress towards more intelligent and more human like artificial systems we need to be following an appropriate feedback signal we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems as well as comparisons with humans over the past hundred years there has been an abundance of attempts to define and measure intelligence across both the fields of psychology and ai we summarize and critically assess these definitions and evaluation approaches while making apparent the two historical conceptions of intelligence that have implicitly guided them we note that in practice the contemporary ai community still gravitates towards benchmarking intelligence by comparing the skill exhibited by ais and humans at specific tasks such as board games and video games we argue that solely measuring skill at any given task falls short of measuring intelligence because skill is heavily modulated by prior knowledge and experience unlimited priors or unlimited training data allow experimenters to buy arbitrary levels of skills for a system in a way that masks the system s own generalization power we then articulate a new formal definition of intelligence based on algorithmic information theory describing intelligence as skill acquisition efficiency and highlighting the concepts of scope generalization difficulty priors and experience using this definition we propose a set of guidelines for what a general ai benchmark should look like finally we present a benchmark closely following these guidelines the abstraction and reasoning corpus arc built upon an explicit set of priors designed to be as close as possible to innate human priors we argue that arc can be used to measure a human like form of general fluid intelligence and that it enables fair general intelligence comparisons between ai systems and humans
2019,deep double descent where bigger models and more data hurt,preetum nakkiran et al,https://arxiv.org/abs/1912.02292,learning_theory,we show that a variety of modern deep learning tasks exhibit a double descent phenomenon where as we increase model size performance first gets worse and then gets better moreover we show that double descent occurs not just as a function of model size but also as a function of the number of training epochs we unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure furthermore our notion of model complexity allows us to identify certain regimes where increasing even quadrupling the number of train samples actually hurts test performance,deep double descent where bigger models and more data hurt preetum nakkiran et al we show that a variety of modern deep learning tasks exhibit a double descent phenomenon where as we increase model size performance first gets worse and then gets better moreover we show that double descent occurs not just as a function of model size but also as a function of the number of training epochs we unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure furthermore our notion of model complexity allows us to identify certain regimes where increasing even quadrupling the number of train samples actually hurts test performance
2020,nerf representing scenes as neural radiance fields for view synthesis,ben mildenhall et al,https://arxiv.org/abs/2003.08934,ml_representation,we present a method that achieves state of the art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views our algorithm represents a scene using a fully connected non convolutional deep network whose input is a single continuous 5d coordinate spatial location x y z and viewing direction theta phi and whose output is the volume density and view dependent emitted radiance at that spatial location we synthesize views by querying 5d coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image because volume rendering is naturally differentiable the only input required to optimize our representation is a set of images with known camera poses we describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance and demonstrate results that outperform prior work on neural rendering and view synthesis view synthesis results are best viewed as videos so we urge readers to view our supplementary video for convincing comparisons,nerf representing scenes as neural radiance fields for view synthesis ben mildenhall et al we present a method that achieves state of the art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views our algorithm represents a scene using a fully connected non convolutional deep network whose input is a single continuous 5d coordinate spatial location x y z and viewing direction theta phi and whose output is the volume density and view dependent emitted radiance at that spatial location we synthesize views by querying 5d coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image because volume rendering is naturally differentiable the only input required to optimize our representation is a set of images with known camera poses we describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance and demonstrate results that outperform prior work on neural rendering and view synthesis view synthesis results are best viewed as videos so we urge readers to view our supplementary video for convincing comparisons
2020,language models are few shot learners,tom b brown et al,https://arxiv.org/abs/2005.14165,dl_nlp,recent work has demonstrated substantial gains on many nlp tasks and benchmarks by pre training on a large corpus of text followed by fine tuning on a specific task while typically task agnostic in architecture this method still requires task specific fine tuning datasets of thousands or tens of thousands of examples by contrast humans can generally perform a new language task from only a few examples or from simple instructions something which current nlp systems still largely struggle to do here we show that scaling up language models greatly improves task agnostic few shot performance sometimes even reaching competitiveness with prior state of the art fine tuning approaches specifically we train gpt 3 an autoregressive language model with 175 billion parameters 10x more than any previous non sparse language model and test its performance in the few shot setting for all tasks gpt 3 is applied without any gradient updates or fine tuning with tasks and few shot demonstrations specified purely via text interaction with the model gpt 3 achieves strong performance on many nlp datasets including translation question answering and cloze tasks as well as several tasks that require on the fly reasoning or domain adaptation such as unscrambling words using a novel word in a sentence or performing 3 digit arithmetic at the same time we also identify some datasets where gpt 3 s few shot learning still struggles as well as some datasets where gpt 3 faces methodological issues related to training on large web corpora finally we find that gpt 3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans we discuss broader societal impacts of this finding and of gpt 3 in general,language models are few shot learners tom b brown et al recent work has demonstrated substantial gains on many nlp tasks and benchmarks by pre training on a large corpus of text followed by fine tuning on a specific task while typically task agnostic in architecture this method still requires task specific fine tuning datasets of thousands or tens of thousands of examples by contrast humans can generally perform a new language task from only a few examples or from simple instructions something which current nlp systems still largely struggle to do here we show that scaling up language models greatly improves task agnostic few shot performance sometimes even reaching competitiveness with prior state of the art fine tuning approaches specifically we train gpt 3 an autoregressive language model with 175 billion parameters 10x more than any previous non sparse language model and test its performance in the few shot setting for all tasks gpt 3 is applied without any gradient updates or fine tuning with tasks and few shot demonstrations specified purely via text interaction with the model gpt 3 achieves strong performance on many nlp datasets including translation question answering and cloze tasks as well as several tasks that require on the fly reasoning or domain adaptation such as unscrambling words using a novel word in a sentence or performing 3 digit arithmetic at the same time we also identify some datasets where gpt 3 s few shot learning still struggles as well as some datasets where gpt 3 faces methodological issues related to training on large web corpora finally we find that gpt 3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans we discuss broader societal impacts of this finding and of gpt 3 in general
2020,implicit neural representations with periodic activation functions,vincent sitzmann et al,https://arxiv.org/abs/2006.09661,ml_representation,implicitly defined continuous differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm offering many possible benefits over conventional representations however current network architectures for such implicit neural representations are incapable of modeling signals with fine detail and fail to represent a signal s spatial and temporal derivatives despite the fact that these are essential to many physical signals defined implicitly as the solution to partial differential equations we propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks dubbed sinusoidal representation networks or sirens are ideally suited for representing complex natural signals and their derivatives we analyze siren activation statistics to propose a principled initialization scheme and demonstrate the representation of images wavefields video sound and their derivatives further we show how sirens can be leveraged to solve challenging boundary value problems such as particular eikonal equations yielding signed distance functions the poisson equation and the helmholtz and wave equations lastly we combine sirens with hypernetworks to learn priors over the space of siren functions,implicit neural representations with periodic activation functions vincent sitzmann et al implicitly defined continuous differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm offering many possible benefits over conventional representations however current network architectures for such implicit neural representations are incapable of modeling signals with fine detail and fail to represent a signal s spatial and temporal derivatives despite the fact that these are essential to many physical signals defined implicitly as the solution to partial differential equations we propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks dubbed sinusoidal representation networks or sirens are ideally suited for representing complex natural signals and their derivatives we analyze siren activation statistics to propose a principled initialization scheme and demonstrate the representation of images wavefields video sound and their derivatives further we show how sirens can be leveraged to solve challenging boundary value problems such as particular eikonal equations yielding signed distance functions the poisson equation and the helmholtz and wave equations lastly we combine sirens with hypernetworks to learn priors over the space of siren functions
2020,fourier features let networks learn high frequency functions in low dimensional domains,matthew tancik et al,https://arxiv.org/abs/2006.10739,ml_representation,we show that passing input points through a simple fourier feature mapping enables a multilayer perceptron mlp to learn high frequency functions in low dimensional problem domains these results shed light on recent advances in computer vision and graphics that achieve state of the art results by using mlps to represent complex 3d objects and scenes using tools from the neural tangent kernel ntk literature we show that a standard mlp fails to learn high frequencies both in theory and in practice to overcome this spectral bias we use a fourier feature mapping to transform the effective ntk into a stationary kernel with a tunable bandwidth we suggest an approach for selecting problem specific fourier features that greatly improves the performance of mlps for low dimensional regression tasks relevant to the computer vision and graphics communities,fourier features let networks learn high frequency functions in low dimensional domains matthew tancik et al we show that passing input points through a simple fourier feature mapping enables a multilayer perceptron mlp to learn high frequency functions in low dimensional problem domains these results shed light on recent advances in computer vision and graphics that achieve state of the art results by using mlps to represent complex 3d objects and scenes using tools from the neural tangent kernel ntk literature we show that a standard mlp fails to learn high frequencies both in theory and in practice to overcome this spectral bias we use a fourier feature mapping to transform the effective ntk into a stationary kernel with a tunable bandwidth we suggest an approach for selecting problem specific fourier features that greatly improves the performance of mlps for low dimensional regression tasks relevant to the computer vision and graphics communities
2020,denoising diffusion probabilistic models,jonathan ho et al,https://arxiv.org/abs/2006.11239,vision,we present high quality image synthesis results using diffusion probabilistic models a class of latent variable models inspired by considerations from nonequilibrium thermodynamics our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with langevin dynamics and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding on the unconditional cifar10 dataset we obtain an inception score of 9 46 and a state of the art fid score of 3 17 on 256x256 lsun we obtain sample quality similar to progressivegan our implementation is available at,denoising diffusion probabilistic models jonathan ho et al we present high quality image synthesis results using diffusion probabilistic models a class of latent variable models inspired by considerations from nonequilibrium thermodynamics our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with langevin dynamics and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding on the unconditional cifar10 dataset we obtain an inception score of 9 46 and a state of the art fid score of 3 17 on 256x256 lsun we obtain sample quality similar to progressivegan our implementation is available at
2020,the neural tangent kernel in high dimensions triple descent and a multi scale theory of generalization,ben adlam and jeffrey pennington,https://arxiv.org/abs/2008.06786,learning_theory,modern deep learning models employ considerably more parameters than required to fit the training data whereas conventional statistical wisdom suggests such models should drastically overfit in practice these models generalize remarkably well an emerging paradigm for describing this unexpected behavior is in terms of a emph double descent curve in which increasing a model s capacity causes its test error to first decrease then increase to a maximum near the interpolation threshold and then decrease again in the overparameterized regime recent efforts to explain this phenomenon theoretically have focused on simple settings such as linear regression or kernel regression with unstructured random features which we argue are too coarse to reveal important nuances of actual neural networks we provide a precise high dimensional asymptotic analysis of generalization under kernel regression with the neural tangent kernel which characterizes the behavior of wide neural networks optimized with gradient descent our results reveal that the test error has non monotonic behavior deep in the overparameterized regime and can even exhibit additional peaks and descents when the number of parameters scales quadratically with the dataset size,the neural tangent kernel in high dimensions triple descent and a multi scale theory of generalization ben adlam and jeffrey pennington modern deep learning models employ considerably more parameters than required to fit the training data whereas conventional statistical wisdom suggests such models should drastically overfit in practice these models generalize remarkably well an emerging paradigm for describing this unexpected behavior is in terms of a emph double descent curve in which increasing a model s capacity causes its test error to first decrease then increase to a maximum near the interpolation threshold and then decrease again in the overparameterized regime recent efforts to explain this phenomenon theoretically have focused on simple settings such as linear regression or kernel regression with unstructured random features which we argue are too coarse to reveal important nuances of actual neural networks we provide a precise high dimensional asymptotic analysis of generalization under kernel regression with the neural tangent kernel which characterizes the behavior of wide neural networks optimized with gradient descent our results reveal that the test error has non monotonic behavior deep in the overparameterized regime and can even exhibit additional peaks and descents when the number of parameters scales quadratically with the dataset size
2020,an image is worth 16x16 words transformers for image recognition at scale,alexey dosovitskiy et al,https://arxiv.org/abs/2010.11929,vision,while the transformer architecture has become the de facto standard for natural language processing tasks its applications to computer vision remain limited in vision attention is either applied in conjunction with convolutional networks or used to replace certain components of convolutional networks while keeping their overall structure in place we show that this reliance on cnns is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks when pre trained on large amounts of data and transferred to multiple mid sized or small image recognition benchmarks imagenet cifar 100 vtab etc vision transformer vit attains excellent results compared to state of the art convolutional networks while requiring substantially fewer computational resources to train,an image is worth 16x16 words transformers for image recognition at scale alexey dosovitskiy et al while the transformer architecture has become the de facto standard for natural language processing tasks its applications to computer vision remain limited in vision attention is either applied in conjunction with convolutional networks or used to replace certain components of convolutional networks while keeping their overall structure in place we show that this reliance on cnns is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks when pre trained on large amounts of data and transferred to multiple mid sized or small image recognition benchmarks imagenet cifar 100 vtab etc vision transformer vit attains excellent results compared to state of the art convolutional networks while requiring substantially fewer computational resources to train
2020,bayesian workflow,andrew gelman et al,https://arxiv.org/abs/2011.01808,bayesian,the bayesian approach to data analysis provides a powerful way to handle uncertainty in all observations model parameters and model structure using probability theory probabilistic programming languages make it easier to specify and fit bayesian models but this still leaves us with many options regarding constructing evaluating and using these models along with many remaining challenges in computation using bayesian inference to solve real world problems requires not only statistical skills subject matter knowledge and programming but also awareness of the decisions made in the process of data analysis all of these aspects can be understood as part of a tangled workflow of applied bayesian statistics beyond inference the workflow also includes iterative model building model checking validation and troubleshooting of computational problems model understanding and model comparison we review all these aspects of workflow in the context of several examples keeping in mind that in practice we will be fitting many models for any given problem even if only a subset of them will ultimately be relevant for our conclusions,bayesian workflow andrew gelman et al the bayesian approach to data analysis provides a powerful way to handle uncertainty in all observations model parameters and model structure using probability theory probabilistic programming languages make it easier to specify and fit bayesian models but this still leaves us with many options regarding constructing evaluating and using these models along with many remaining challenges in computation using bayesian inference to solve real world problems requires not only statistical skills subject matter knowledge and programming but also awareness of the decisions made in the process of data analysis all of these aspects can be understood as part of a tangled workflow of applied bayesian statistics beyond inference the workflow also includes iterative model building model checking validation and troubleshooting of computational problems model understanding and model comparison we review all these aspects of workflow in the context of several examples keeping in mind that in practice we will be fitting many models for any given problem even if only a subset of them will ultimately be relevant for our conclusions
2020,taming transformers for high resolution image synthesis,patrick esser et al,https://arxiv.org/abs/2012.09841,vision,designed to learn long range interactions on sequential data transformers continue to show state of the art results on a wide variety of tasks in contrast to cnns they contain no inductive bias that prioritizes local interactions this makes them expressive but also computationally infeasible for long sequences such as high resolution images we demonstrate how combining the effectiveness of the inductive bias of cnns with the expressivity of transformers enables them to model and thereby synthesize high resolution images we show how to i use cnns to learn a context rich vocabulary of image constituents and in turn ii utilize transformers to efficiently model their composition within high resolution images our approach is readily applied to conditional synthesis tasks where both non spatial information such as object classes and spatial information such as segmentations can control the generated image in particular we present the first results on semantically guided synthesis of megapixel images with transformers and obtain the state of the art among autoregressive models on class conditional imagenet code and pretrained models can be found at,taming transformers for high resolution image synthesis patrick esser et al designed to learn long range interactions on sequential data transformers continue to show state of the art results on a wide variety of tasks in contrast to cnns they contain no inductive bias that prioritizes local interactions this makes them expressive but also computationally infeasible for long sequences such as high resolution images we demonstrate how combining the effectiveness of the inductive bias of cnns with the expressivity of transformers enables them to model and thereby synthesize high resolution images we show how to i use cnns to learn a context rich vocabulary of image constituents and in turn ii utilize transformers to efficiently model their composition within high resolution images our approach is readily applied to conditional synthesis tasks where both non spatial information such as object classes and spatial information such as segmentations can control the generated image in particular we present the first results on semantically guided synthesis of megapixel images with transformers and obtain the state of the art among autoregressive models on class conditional imagenet code and pretrained models can be found at
2020,intrinsic dimensionality explains the effectiveness of language model fine tuning,armen aghajanyan et al,https://arxiv.org/abs/2012.13255,learning_theory,although pretrained language models can be fine tuned to produce state of the art results for a very wide range of language understanding tasks the dynamics of this process are not well understood especially in the low data regime why can we use relatively vanilla gradient descent algorithms e g without strong regularization to tune a model with hundreds of millions of parameters on datasets with only hundreds or thousands of labeled examples in this paper we argue that analyzing fine tuning through the lens of intrinsic dimension provides us with empirical and theoretical intuitions to explain this remarkable phenomenon we empirically show that common pre trained models have a very low intrinsic dimension in other words there exists a low dimension reparameterization that is as effective for fine tuning as the full parameter space for example by optimizing only 200 trainable parameters randomly projected back into the full space we can tune a roberta model to achieve 90 of the full parameter performance levels on mrpc furthermore we empirically show that pre training implicitly minimizes intrinsic dimension and perhaps surprisingly larger models tend to have lower intrinsic dimension after a fixed number of pre training updates at least in part explaining their extreme effectiveness lastly we connect intrinsic dimensionality with low dimensional task representations and compression based generalization bounds to provide intrinsic dimension based generalization bounds that are independent of the full parameter count,intrinsic dimensionality explains the effectiveness of language model fine tuning armen aghajanyan et al although pretrained language models can be fine tuned to produce state of the art results for a very wide range of language understanding tasks the dynamics of this process are not well understood especially in the low data regime why can we use relatively vanilla gradient descent algorithms e g without strong regularization to tune a model with hundreds of millions of parameters on datasets with only hundreds or thousands of labeled examples in this paper we argue that analyzing fine tuning through the lens of intrinsic dimension provides us with empirical and theoretical intuitions to explain this remarkable phenomenon we empirically show that common pre trained models have a very low intrinsic dimension in other words there exists a low dimension reparameterization that is as effective for fine tuning as the full parameter space for example by optimizing only 200 trainable parameters randomly projected back into the full space we can tune a roberta model to achieve 90 of the full parameter performance levels on mrpc furthermore we empirically show that pre training implicitly minimizes intrinsic dimension and perhaps surprisingly larger models tend to have lower intrinsic dimension after a fixed number of pre training updates at least in part explaining their extreme effectiveness lastly we connect intrinsic dimensionality with low dimensional task representations and compression based generalization bounds to provide intrinsic dimension based generalization bounds that are independent of the full parameter count
2021,learning transferable visual models from natural language supervision,alec radford et al,https://arxiv.org/abs/2103.00020,ml_representation,state of the art computer vision systems are trained to predict a fixed set of predetermined object categories this restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision we demonstrate that the simple pre training task of predicting which caption goes with which image is an efficient and scalable way to learn sota image representations from scratch on a dataset of 400 million image text pairs collected from the internet after pre training natural language is used to reference learned visual concepts or describe new ones enabling zero shot transfer of the model to downstream tasks we study the performance of this approach by benchmarking on over 30 different existing computer vision datasets spanning tasks such as ocr action recognition in videos geo localization and many types of fine grained object classification the model transfers non trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training for instance we match the accuracy of the original resnet 50 on imagenet zero shot without needing to use any of the 1 28 million training examples it was trained on we release our code and pre trained model weights at,learning transferable visual models from natural language supervision alec radford et al state of the art computer vision systems are trained to predict a fixed set of predetermined object categories this restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision we demonstrate that the simple pre training task of predicting which caption goes with which image is an efficient and scalable way to learn sota image representations from scratch on a dataset of 400 million image text pairs collected from the internet after pre training natural language is used to reference learned visual concepts or describe new ones enabling zero shot transfer of the model to downstream tasks we study the performance of this approach by benchmarking on over 30 different existing computer vision datasets spanning tasks such as ocr action recognition in videos geo localization and many types of fine grained object classification the model transfers non trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training for instance we match the accuracy of the original resnet 50 on imagenet zero shot without needing to use any of the 1 28 million training examples it was trained on we release our code and pre trained model weights at
2021,an introduction to johnson lindenstrauss transforms,casper benjamin freksen,https://arxiv.org/abs/2103.00564,ml_representation,johnson lindenstrauss transforms are powerful tools for reducing the dimensionality of data while preserving key characteristics of that data and they have found use in many fields from machine learning to differential privacy and more this note explains what they are it gives an overview of their use and their development since they were introduced in the 1980s and it provides many references should the reader wish to explore these topics more deeply,an introduction to johnson lindenstrauss transforms casper benjamin freksen johnson lindenstrauss transforms are powerful tools for reducing the dimensionality of data while preserving key characteristics of that data and they have found use in many fields from machine learning to differential privacy and more this note explains what they are it gives an overview of their use and their development since they were introduced in the 1980s and it provides many references should the reader wish to explore these topics more deeply
2021,emerging properties in self supervised vision transformers,mathilde caron et al,https://arxiv.org/abs/2104.14294,ml_representation,in this paper we question if self supervised learning provides new properties to vision transformer vit that stand out compared to convolutional networks convnets beyond the fact that adapting self supervised methods to this architecture works particularly well we make the following observations first self supervised vit features contain explicit information about the semantic segmentation of an image which does not emerge as clearly with supervised vits nor with convnets second these features are also excellent k nn classifiers reaching 78 3 top 1 on imagenet with a small vit our study also underlines the importance of momentum encoder multi crop training and the use of small patches with vits we implement our findings into a simple self supervised method called dino which we interpret as a form of self distillation with no labels we show the synergy between dino and vits by achieving 80 1 top 1 on imagenet in linear evaluation with vit base,emerging properties in self supervised vision transformers mathilde caron et al in this paper we question if self supervised learning provides new properties to vision transformer vit that stand out compared to convolutional networks convnets beyond the fact that adapting self supervised methods to this architecture works particularly well we make the following observations first self supervised vit features contain explicit information about the semantic segmentation of an image which does not emerge as clearly with supervised vits nor with convnets second these features are also excellent k nn classifiers reaching 78 3 top 1 on imagenet with a small vit our study also underlines the importance of momentum encoder multi crop training and the use of small patches with vits we implement our findings into a simple self supervised method called dino which we interpret as a form of self distillation with no labels we show the synergy between dino and vits by achieving 80 1 top 1 on imagenet in linear evaluation with vit base
2021,lora low rank adaptation of large language models,edward j hu et al,https://arxiv.org/abs/2106.09685,dl_nlp,an important paradigm of natural language processing consists of large scale pre training on general domain data and adaptation to particular tasks or domains as we pre train larger models full fine tuning which retrains all model parameters becomes less feasible using gpt 3 175b as an example deploying independent instances of fine tuned models each with 175b parameters is prohibitively expensive we propose low rank adaptation or lora which freezes the pre trained model weights and injects trainable rank decomposition matrices into each layer of the transformer architecture greatly reducing the number of trainable parameters for downstream tasks compared to gpt 3 175b fine tuned with adam lora can reduce the number of trainable parameters by 10000 times and the gpu memory requirement by 3 times lora performs on par or better than fine tuning in model quality on roberta deberta gpt 2 and gpt 3 despite having fewer trainable parameters a higher training throughput and unlike adapters no additional inference latency we also provide an empirical investigation into rank deficiency in language model adaptation which sheds light on the efficacy of lora we release a package that facilitates the integration of lora with pytorch models and provide our implementations and model checkpoints for roberta deberta and gpt 2 at,lora low rank adaptation of large language models edward j hu et al an important paradigm of natural language processing consists of large scale pre training on general domain data and adaptation to particular tasks or domains as we pre train larger models full fine tuning which retrains all model parameters becomes less feasible using gpt 3 175b as an example deploying independent instances of fine tuned models each with 175b parameters is prohibitively expensive we propose low rank adaptation or lora which freezes the pre trained model weights and injects trainable rank decomposition matrices into each layer of the transformer architecture greatly reducing the number of trainable parameters for downstream tasks compared to gpt 3 175b fine tuned with adam lora can reduce the number of trainable parameters by 10000 times and the gpu memory requirement by 3 times lora performs on par or better than fine tuning in model quality on roberta deberta gpt 2 and gpt 3 despite having fewer trainable parameters a higher training throughput and unlike adapters no additional inference latency we also provide an empirical investigation into rank deficiency in language model adaptation which sheds light on the efficacy of lora we release a package that facilitates the integration of lora with pytorch models and provide our implementations and model checkpoints for roberta deberta and gpt 2 at
2022,cmt convolutional neural networks meet vision transformers,jianyuan guo et al,https://arxiv.org/abs/2107.06263,vision,vision transformers have been successfully applied to image recognition tasks due to their ability to capture long range dependencies within an image however there are still gaps in both performance and computational cost between transformers and existing convolutional neural networks cnns in this paper we aim to address this issue and develop a network that can outperform not only the canonical transformers but also the high performance convolutional models we propose a new transformer based hybrid network by taking advantage of transformers to capture long range dependencies and of cnns to model local features furthermore we scale it to obtain a family of models called cmts obtaining much better accuracy and efficiency than previous convolution and transformer based models in particular our cmt s achieves 83 5 top 1 accuracy on imagenet while being 14x and 2x smaller on flops than the existing deit and efficientnet respectively the proposed cmt s also generalizes well on cifar10 99 2 cifar100 91 7 flowers 98 7 and other challenging vision datasets such as coco 44 3 map with considerably less computational cost,cmt convolutional neural networks meet vision transformers jianyuan guo et al vision transformers have been successfully applied to image recognition tasks due to their ability to capture long range dependencies within an image however there are still gaps in both performance and computational cost between transformers and existing convolutional neural networks cnns in this paper we aim to address this issue and develop a network that can outperform not only the canonical transformers but also the high performance convolutional models we propose a new transformer based hybrid network by taking advantage of transformers to capture long range dependencies and of cnns to model local features furthermore we scale it to obtain a family of models called cmts obtaining much better accuracy and efficiency than previous convolution and transformer based models in particular our cmt s achieves 83 5 top 1 accuracy on imagenet while being 14x and 2x smaller on flops than the existing deit and efficientnet respectively the proposed cmt s also generalizes well on cifar10 99 2 cifar100 91 7 flowers 98 7 and other challenging vision datasets such as coco 44 3 map with considerably less computational cost
2022,finetuned language models are zero shot learners,jason wei et al,https://arxiv.org/abs/2109.01652,dl_nlp,this paper explores a simple method for improving the zero shot learning abilities of language models we show that instruction tuning finetuning language models on a collection of tasks described via instructions substantially improves zero shot performance on unseen tasks we take a 137b parameter pretrained language model and instruction tune it on over 60 nlp tasks verbalized via natural language instruction templates we evaluate this instruction tuned model which we call flan on unseen task types flan substantially improves the performance of its unmodified counterpart and surpasses zero shot 175b gpt 3 on 20 of 25 tasks that we evaluate flan even outperforms few shot gpt 3 by a large margin on anli rte boolq ai2 arc openbookqa and storycloze ablation studies reveal that number of finetuning datasets model scale and natural language instructions are key to the success of instruction tuning,finetuned language models are zero shot learners jason wei et al this paper explores a simple method for improving the zero shot learning abilities of language models we show that instruction tuning finetuning language models on a collection of tasks described via instructions substantially improves zero shot performance on unseen tasks we take a 137b parameter pretrained language model and instruction tune it on over 60 nlp tasks verbalized via natural language instruction templates we evaluate this instruction tuned model which we call flan on unseen task types flan substantially improves the performance of its unmodified counterpart and surpasses zero shot 175b gpt 3 on 20 of 25 tasks that we evaluate flan even outperforms few shot gpt 3 by a large margin on anli rte boolq ai2 arc openbookqa and storycloze ablation studies reveal that number of finetuning datasets model scale and natural language instructions are key to the success of instruction tuning
2021,a farewell to the bias variance tradeoff an overview of the theory of overparameterized machine learning,yehuda dar et al,https://arxiv.org/pdf/2109.02355.pdf,ml_classic,the rapid recent progress in machine learning ml has raised a number of scientific questions that challenge the longstanding dogma of the field one of the most important riddles is the good empirical generalization of overparameterized models overparameterized models are excessively complex with respect to the size of the training dataset which results in them perfectly fitting i e interpolating the training data which is usually noisy such interpolation of noisy data is traditionally associated with detrimental overfitting and yet a wide range of interpolating models from simple linear models to deep neural networks have recently been observed to generalize extremely well on fresh test data indeed the recently discovered double descent phenomenon has revealed that highly overparameterized models often improve over the best underparameterized model in test performance understanding learning in this overparameterized regime requires new theory and foundational empirical studies even for the simplest case of the linear model the underpinnings of this understanding have been laid in very recent analyses of overparameterized linear regression and related statistical learning tasks which resulted in precise analytic characterizations of double descent this paper provides a succinct overview of this emerging theory of overparameterized ml henceforth abbreviated as topml that explains these recent findings through a statistical signal processing perspective we emphasize the unique aspects that define the topml research area as a subfield of modern ml theory and outline interesting open questions that remain,a farewell to the bias variance tradeoff an overview of the theory of overparameterized machine learning yehuda dar et al the rapid recent progress in machine learning ml has raised a number of scientific questions that challenge the longstanding dogma of the field one of the most important riddles is the good empirical generalization of overparameterized models overparameterized models are excessively complex with respect to the size of the training dataset which results in them perfectly fitting i e interpolating the training data which is usually noisy such interpolation of noisy data is traditionally associated with detrimental overfitting and yet a wide range of interpolating models from simple linear models to deep neural networks have recently been observed to generalize extremely well on fresh test data indeed the recently discovered double descent phenomenon has revealed that highly overparameterized models often improve over the best underparameterized model in test performance understanding learning in this overparameterized regime requires new theory and foundational empirical studies even for the simplest case of the linear model the underpinnings of this understanding have been laid in very recent analyses of overparameterized linear regression and related statistical learning tasks which resulted in precise analytic characterizations of double descent this paper provides a succinct overview of this emerging theory of overparameterized ml henceforth abbreviated as topml that explains these recent findings through a statistical signal processing perspective we emphasize the unique aspects that define the topml research area as a subfield of modern ml theory and outline interesting open questions that remain
2022,learning to walk in minutes using massively parallel deep reinforcement learning anymal,nikita rudin et al,https://arxiv.org/abs/2109.11978,dl_rl,in this work we present and study a training set up that achieves fast policy generation for real world robotic tasks by using massive parallelism on a single workstation gpu we analyze and discuss the impact of different training algorithm components in the massively parallel regime on the final policy performance and training times in addition we present a novel game inspired curriculum that is well suited for training with thousands of simulated robots in parallel we evaluate the approach by training the quadrupedal robot anymal to walk on challenging terrain the parallel approach allows training policies for flat terrain in under four minutes and in twenty minutes for uneven terrain this represents a speedup of multiple orders of magnitude compared to previous work finally we transfer the policies to the real robot to validate the approach we open source our training code to help accelerate further research in the field of learned legged locomotion,learning to walk in minutes using massively parallel deep reinforcement learning anymal nikita rudin et al in this work we present and study a training set up that achieves fast policy generation for real world robotic tasks by using massive parallelism on a single workstation gpu we analyze and discuss the impact of different training algorithm components in the massively parallel regime on the final policy performance and training times in addition we present a novel game inspired curriculum that is well suited for training with thousands of simulated robots in parallel we evaluate the approach by training the quadrupedal robot anymal to walk on challenging terrain the parallel approach allows training policies for flat terrain in under four minutes and in twenty minutes for uneven terrain this represents a speedup of multiple orders of magnitude compared to previous work finally we transfer the policies to the real robot to validate the approach we open source our training code to help accelerate further research in the field of learned legged locomotion
2022,bigssl exploring the frontier of large scale semi supervised learning for asr,yu zhang et al,https://arxiv.org/abs/2109.13226,audio,we summarize the results of a host of efforts using giant automatic speech recognition asr models pre trained using large diverse unlabeled datasets containing approximately a million hours of audio we find that the combination of pre training self training and scaling up model size greatly increases data efficiency even for extremely large tasks with tens of thousands of hours of labeled data in particular on an asr task with 34k hours of labeled data by fine tuning an 8 billion parameter pre trained conformer model we can match state of the art sota performance with only 3 of the training data and significantly improve sota with the full training set we also report on the universal benefits gained from using big pre trained and self trained models for a large set of downstream tasks that cover a wide range of speech domains and span multiple orders of magnitudes of dataset sizes including obtaining sota performance on many public benchmarks in addition we utilize the learned representation of pre trained networks to achieve sota results on non asr tasks,bigssl exploring the frontier of large scale semi supervised learning for asr yu zhang et al we summarize the results of a host of efforts using giant automatic speech recognition asr models pre trained using large diverse unlabeled datasets containing approximately a million hours of audio we find that the combination of pre training self training and scaling up model size greatly increases data efficiency even for extremely large tasks with tens of thousands of hours of labeled data in particular on an asr task with 34k hours of labeled data by fine tuning an 8 billion parameter pre trained conformer model we can match state of the art sota performance with only 3 of the training data and significantly improve sota with the full training set we also report on the universal benefits gained from using big pre trained and self trained models for a large set of downstream tasks that cover a wide range of speech domains and span multiple orders of magnitudes of dataset sizes including obtaining sota performance on many public benchmarks in addition we utilize the learned representation of pre trained networks to achieve sota results on non asr tasks
2022,efficient training of audio transformers with patchout passt,koutini et al,https://arxiv.org/abs/2110.05069,audio,the great success of transformer based models in natural language processing nlp has led to various attempts at adapting these architectures to other domains such as vision and audio recent work has shown that transformers can outperform convolutional neural networks cnns on vision and audio tasks however one of the main shortcomings of transformer models compared to the well established cnns is the computational complexity in transformers the compute and memory complexity is known to grow quadratically with the input length therefore there has been extensive work on optimizing transformers but often at the cost of degrading predictive performance in this work we propose a novel method to optimize and regularize transformers on audio spectrograms our proposed models achieve a new state of the art performance on audioset and can be trained on a single consumer grade gpu furthermore we propose a transformer model that outperforms cnns in terms of both performance and training speed source code,efficient training of audio transformers with patchout passt koutini et al the great success of transformer based models in natural language processing nlp has led to various attempts at adapting these architectures to other domains such as vision and audio recent work has shown that transformers can outperform convolutional neural networks cnns on vision and audio tasks however one of the main shortcomings of transformer models compared to the well established cnns is the computational complexity in transformers the compute and memory complexity is known to grow quadratically with the input length therefore there has been extensive work on optimizing transformers but often at the cost of degrading predictive performance in this work we propose a novel method to optimize and regularize transformers on audio spectrograms our proposed models achieve a new state of the art performance on audioset and can be trained on a single consumer grade gpu furthermore we propose a transformer model that outperforms cnns in terms of both performance and training speed source code
2022,speecht5 unified modal encoder decoder pre training for spoken language processing,junyi ao et al,https://arxiv.org/abs/2110.07205,audio,motivated by the success of t5 text to text transfer transformer in pre trained natural language processing models we propose a unified modal speecht5 framework that explores the encoder decoder pre training for self supervised speech text representation learning the speecht5 framework consists of a shared encoder decoder network and six modal specific speech text pre post nets after preprocessing the input speech text through the pre nets the shared encoder decoder network models the sequence to sequence transformation and then the post nets generate the output in the speech text modality based on the output of the decoder leveraging large scale unlabeled speech and text data we pre train speecht5 to learn a unified modal representation hoping to improve the modeling capability for both speech and text to align the textual and speech information into this unified semantic space we propose a cross modal vector quantization approach that randomly mixes up speech text states with latent units as the interface between encoder and decoder extensive evaluations show the superiority of the proposed speecht5 framework on a wide variety of spoken language processing tasks including automatic speech recognition speech synthesis speech translation voice conversion speech enhancement and speaker identification we release our code and model at,speecht5 unified modal encoder decoder pre training for spoken language processing junyi ao et al motivated by the success of t5 text to text transfer transformer in pre trained natural language processing models we propose a unified modal speecht5 framework that explores the encoder decoder pre training for self supervised speech text representation learning the speecht5 framework consists of a shared encoder decoder network and six modal specific speech text pre post nets after preprocessing the input speech text through the pre nets the shared encoder decoder network models the sequence to sequence transformation and then the post nets generate the output in the speech text modality based on the output of the decoder leveraging large scale unlabeled speech and text data we pre train speecht5 to learn a unified modal representation hoping to improve the modeling capability for both speech and text to align the textual and speech information into this unified semantic space we propose a cross modal vector quantization approach that randomly mixes up speech text states with latent units as the interface between encoder and decoder extensive evaluations show the superiority of the proposed speecht5 framework on a wide variety of spoken language processing tasks including automatic speech recognition speech synthesis speech translation voice conversion speech enhancement and speaker identification we release our code and model at
2022,multitask prompted training enables zero shot task generalization,victor sanh et al,https://arxiv.org/abs/2110.08207,dl_nlp,large language models have recently been shown to attain reasonable zero shot generalization on a diverse set of tasks brown et al 2020 it has been hypothesized that this is a consequence of implicit multitask learning in language models pretraining radford et al 2019 can zero shot generalization instead be directly induced by explicit multitask learning to test this question at scale we develop a system for easily mapping any natural language tasks into a human readable prompted form we convert a large set of supervised datasets each with multiple prompts with diverse wording these prompted datasets allow for benchmarking the ability of a model to perform completely held out tasks we fine tune a pretrained encoder decoder model raffel et al 2020 lester et al 2021 on this multitask mixture covering a wide variety of tasks the model attains strong zero shot performance on several standard datasets often outperforming models up to 16x its size further our approach attains strong performance on a subset of tasks from the big bench benchmark outperforming models up to 6x its size all trained models are available at and all prompts are available at,multitask prompted training enables zero shot task generalization victor sanh et al large language models have recently been shown to attain reasonable zero shot generalization on a diverse set of tasks brown et al 2020 it has been hypothesized that this is a consequence of implicit multitask learning in language models pretraining radford et al 2019 can zero shot generalization instead be directly induced by explicit multitask learning to test this question at scale we develop a system for easily mapping any natural language tasks into a human readable prompted form we convert a large set of supervised datasets each with multiple prompts with diverse wording these prompted datasets allow for benchmarking the ability of a model to perform completely held out tasks we fine tune a pretrained encoder decoder model raffel et al 2020 lester et al 2021 on this multitask mixture covering a wide variety of tasks the model attains strong zero shot performance on several standard datasets often outperforming models up to 16x its size further our approach attains strong performance on a subset of tasks from the big bench benchmark outperforming models up to 6x its size all trained models are available at and all prompts are available at
2021,learning in high dimension always amounts to extrapolation,randall balestriero et al,https://arxiv.org/pdf/2110.09485.pdf,learning_theory,the notion of interpolation and extrapolation is fundamental in various fields from deep learning to function approximation interpolation occurs for a sample x whenever this sample falls inside or on the boundary of the given dataset s convex hull extrapolation occurs when x falls outside of that convex hull one fundamental mis conception is that state of the art algorithms work so well because of their ability to correctly interpolate training data a second mis conception is that interpolation happens throughout tasks and datasets in fact many intuitions and theories rely on that assumption we empirically and theoretically argue against those two points and demonstrate that on any high dimensional 100 dataset interpolation almost surely never happens those results challenge the validity of our current interpolation extrapolation definition as an indicator of generalization performances,learning in high dimension always amounts to extrapolation randall balestriero et al the notion of interpolation and extrapolation is fundamental in various fields from deep learning to function approximation interpolation occurs for a sample x whenever this sample falls inside or on the boundary of the given dataset s convex hull extrapolation occurs when x falls outside of that convex hull one fundamental mis conception is that state of the art algorithms work so well because of their ability to correctly interpolate training data a second mis conception is that interpolation happens throughout tasks and datasets in fact many intuitions and theories rely on that assumption we empirically and theoretically argue against those two points and demonstrate that on any high dimensional 100 dataset interpolation almost surely never happens those results challenge the validity of our current interpolation extrapolation definition as an indicator of generalization performances
2022,high resolution image synthesis with latent diffusion models,robin rombach et al,https://arxiv.org/abs/2112.10752,vision,by decomposing the image formation process into a sequential application of denoising autoencoders diffusion models dms achieve state of the art synthesis results on image data and beyond additionally their formulation allows for a guiding mechanism to control the image generation process without retraining however since these models typically operate directly in pixel space optimization of powerful dms often consumes hundreds of gpu days and inference is expensive due to sequential evaluations to enable dm training on limited computational resources while retaining their quality and flexibility we apply them in the latent space of powerful pretrained autoencoders in contrast to previous work training diffusion models on such a representation allows for the first time to reach a near optimal point between complexity reduction and detail preservation greatly boosting visual fidelity by introducing cross attention layers into the model architecture we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high resolution synthesis becomes possible in a convolutional manner our latent diffusion models ldms achieve a new state of the art for image inpainting and highly competitive performance on various tasks including unconditional image generation semantic scene synthesis and super resolution while significantly reducing computational requirements compared to pixel based dms code is available at,high resolution image synthesis with latent diffusion models robin rombach et al by decomposing the image formation process into a sequential application of denoising autoencoders diffusion models dms achieve state of the art synthesis results on image data and beyond additionally their formulation allows for a guiding mechanism to control the image generation process without retraining however since these models typically operate directly in pixel space optimization of powerful dms often consumes hundreds of gpu days and inference is expensive due to sequential evaluations to enable dm training on limited computational resources while retaining their quality and flexibility we apply them in the latent space of powerful pretrained autoencoders in contrast to previous work training diffusion models on such a representation allows for the first time to reach a near optimal point between complexity reduction and detail preservation greatly boosting visual fidelity by introducing cross attention layers into the model architecture we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high resolution synthesis becomes possible in a convolutional manner our latent diffusion models ldms achieve a new state of the art for image inpainting and highly competitive performance on various tasks including unconditional image generation semantic scene synthesis and super resolution while significantly reducing computational requirements compared to pixel based dms code is available at
2021,grokking generalization beyond overfitting on small algorithmic datasets,alethea power et al,https://arxiv.org/abs/2201.02177,learning_theory,in this paper we propose to study generalization of neural networks on small algorithmically generated datasets in this setting questions about data efficiency memorization generalization and speed of learning can be studied in great detail in some situations we show that neural networks learn through a process of grokking a pattern in the data improving generalization performance from random chance level to perfect generalization and that this improvement in generalization can happen well past the point of overfitting we also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization we argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning generalization of overparametrized neural networks beyond memorization of the finite training dataset,grokking generalization beyond overfitting on small algorithmic datasets alethea power et al in this paper we propose to study generalization of neural networks on small algorithmically generated datasets in this setting questions about data efficiency memorization generalization and speed of learning can be studied in great detail in some situations we show that neural networks learn through a process of grokking a pattern in the data improving generalization performance from random chance level to perfect generalization and that this improvement in generalization can happen well past the point of overfitting we also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization we argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning generalization of overparametrized neural networks beyond memorization of the finite training dataset
2022,a convnet for the 2020s,zhuang liu et al,https://arxiv.org/abs/2201.03545,vision,the roaring 20s of visual recognition began with the introduction of vision transformers vits which quickly superseded convnets as the state of the art image classification model a vanilla vit on the other hand faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation it is the hierarchical transformers e g swin transformers that reintroduced several convnet priors making transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks however the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of transformers rather than the inherent inductive biases of convolutions in this work we reexamine the design spaces and test the limits of what a pure convnet can achieve we gradually modernize a standard resnet toward the design of a vision transformer and discover several key components that contribute to the performance difference along the way the outcome of this exploration is a family of pure convnet models dubbed convnext constructed entirely from standard convnet modules convnexts compete favorably with transformers in terms of accuracy and scalability achieving 87 8 imagenet top 1 accuracy and outperforming swin transformers on coco detection and ade20k segmentation while maintaining the simplicity and efficiency of standard convnets,a convnet for the 2020s zhuang liu et al the roaring 20s of visual recognition began with the introduction of vision transformers vits which quickly superseded convnets as the state of the art image classification model a vanilla vit on the other hand faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation it is the hierarchical transformers e g swin transformers that reintroduced several convnet priors making transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks however the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of transformers rather than the inherent inductive biases of convolutions in this work we reexamine the design spaces and test the limits of what a pure convnet can achieve we gradually modernize a standard resnet toward the design of a vision transformer and discover several key components that contribute to the performance difference along the way the outcome of this exploration is a family of pure convnet models dubbed convnext constructed entirely from standard convnet modules convnexts compete favorably with transformers in terms of accuracy and scalability achieving 87 8 imagenet top 1 accuracy and outperforming swin transformers on coco detection and ade20k segmentation while maintaining the simplicity and efficiency of standard convnets
2022,learning robust perceptive locomotion for quadrupedal robots in the wild,joonho lee et al,https://arxiv.org/abs/2201.08117,dl_rl,legged robots that can operate autonomously in remote and hazardous environments will greatly increase opportunities for exploration into under explored areas exteroceptive perception is crucial for fast and energy efficient locomotion perceiving the terrain before making contact with it enables planning and adaptation of the gait ahead of time to maintain speed and stability however utilizing exteroceptive perception robustly for locomotion has remained a grand challenge in robotics snow vegetation and water visually appear as obstacles on which the robot cannot step or are missing altogether due to high reflectance additionally depth perception can degrade due to difficult lighting dust fog reflective or transparent surfaces sensor occlusion and more for this reason the most robust and general solutions to legged locomotion to date rely solely on proprioception this severely limits locomotion speed because the robot has to physically feel out the terrain before adapting its gait accordingly here we present a robust and general solution to integrating exteroceptive and proprioceptive perception for legged locomotion we leverage an attention based recurrent encoder that integrates proprioceptive and exteroceptive input the encoder is trained end to end and learns to seamlessly combine the different perception modalities without resorting to heuristics the result is a legged locomotion controller with high robustness and speed the controller was tested in a variety of challenging natural and urban environments over multiple seasons and completed an hour long hike in the alps in the time recommended for human hikers,learning robust perceptive locomotion for quadrupedal robots in the wild joonho lee et al legged robots that can operate autonomously in remote and hazardous environments will greatly increase opportunities for exploration into under explored areas exteroceptive perception is crucial for fast and energy efficient locomotion perceiving the terrain before making contact with it enables planning and adaptation of the gait ahead of time to maintain speed and stability however utilizing exteroceptive perception robustly for locomotion has remained a grand challenge in robotics snow vegetation and water visually appear as obstacles on which the robot cannot step or are missing altogether due to high reflectance additionally depth perception can degrade due to difficult lighting dust fog reflective or transparent surfaces sensor occlusion and more for this reason the most robust and general solutions to legged locomotion to date rely solely on proprioception this severely limits locomotion speed because the robot has to physically feel out the terrain before adapting its gait accordingly here we present a robust and general solution to integrating exteroceptive and proprioceptive perception for legged locomotion we leverage an attention based recurrent encoder that integrates proprioceptive and exteroceptive input the encoder is trained end to end and learns to seamlessly combine the different perception modalities without resorting to heuristics the result is a legged locomotion controller with high robustness and speed the controller was tested in a variety of challenging natural and urban environments over multiple seasons and completed an hour long hike in the alps in the time recommended for human hikers
2022,lambda language models for dialog applications,romal thoppilan et al,https://arxiv.org/abs/2201.08239,dl_nlp,we present lamda language models for dialog applications lamda is a family of transformer based neural language models specialized for dialog which have up to 137b parameters and are pre trained on 1 56t words of public dialog data and web text while model scaling alone can improve quality it shows less improvements on safety and factual grounding we demonstrate that fine tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding the first challenge safety involves ensuring that the model s responses are consistent with a set of human values such as preventing harmful suggestions and unfair bias we quantify safety using a metric based on an illustrative set of human values and we find that filtering candidate responses using a lamda classifier fine tuned with a small amount of crowdworker annotated data offers a promising approach to improving model safety the second challenge factual grounding involves enabling the model to consult external knowledge sources such as an information retrieval system a language translator and a calculator we quantify factuality using a groundedness metric and we find that our approach enables the model to generate responses grounded in known sources rather than responses that merely sound plausible finally we explore the use of lamda in the domains of education and content recommendations and analyze their helpfulness and role consistency,lambda language models for dialog applications romal thoppilan et al we present lamda language models for dialog applications lamda is a family of transformer based neural language models specialized for dialog which have up to 137b parameters and are pre trained on 1 56t words of public dialog data and web text while model scaling alone can improve quality it shows less improvements on safety and factual grounding we demonstrate that fine tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding the first challenge safety involves ensuring that the model s responses are consistent with a set of human values such as preventing harmful suggestions and unfair bias we quantify safety using a metric based on an illustrative set of human values and we find that filtering candidate responses using a lamda classifier fine tuned with a small amount of crowdworker annotated data offers a promising approach to improving model safety the second challenge factual grounding involves enabling the model to consult external knowledge sources such as an information retrieval system a language translator and a calculator we quantify factuality using a groundedness metric and we find that our approach enables the model to generate responses grounded in known sources rather than responses that merely sound plausible finally we explore the use of lamda in the domains of education and content recommendations and analyze their helpfulness and role consistency
2022,patches are all you need convmixer,asher trockman et al,https://arxiv.org/abs/2201.09792,vision,although convolutional networks have been the dominant architecture for vision tasks for many years recent experiments have shown that transformer based models most notably the vision transformer vit may exceed their performance in some settings however due to the quadratic runtime of the self attention layers in transformers vits require the use of patch embeddings which group together small regions of the image into single input features in order to be applied to larger image sizes this raises a question is the performance of vits due to the inherently more powerful transformer architecture or is it at least partly due to using patches as the input representation in this paper we present some evidence for the latter specifically we propose the convmixer an extremely simple model that is similar in spirit to the vit and the even more basic mlp mixer in that it operates directly on patches as input separates the mixing of spatial and channel dimensions and maintains equal size and resolution throughout the network in contrast however the convmixer uses only standard convolutions to achieve the mixing steps despite its simplicity we show that the convmixer outperforms the vit mlp mixer and some of their variants for similar parameter counts and data set sizes in addition to outperforming classical vision models such as the resnet our code is available at,patches are all you need convmixer asher trockman et al although convolutional networks have been the dominant architecture for vision tasks for many years recent experiments have shown that transformer based models most notably the vision transformer vit may exceed their performance in some settings however due to the quadratic runtime of the self attention layers in transformers vits require the use of patch embeddings which group together small regions of the image into single input features in order to be applied to larger image sizes this raises a question is the performance of vits due to the inherently more powerful transformer architecture or is it at least partly due to using patches as the input representation in this paper we present some evidence for the latter specifically we propose the convmixer an extremely simple model that is similar in spirit to the vit and the even more basic mlp mixer in that it operates directly on patches as input separates the mixing of spatial and channel dimensions and maintains equal size and resolution throughout the network in contrast however the convmixer uses only standard convolutions to achieve the mixing steps despite its simplicity we show that the convmixer outperforms the vit mlp mixer and some of their variants for similar parameter counts and data set sizes in addition to outperforming classical vision models such as the resnet our code is available at
2022,chain of thought prompting elicits reasoning in large language models,jason wei et al,https://arxiv.org/abs/2201.11903,dl_nlp,we explore how generating a chain of thought a series of intermediate reasoning steps significantly improves the ability of large language models to perform complex reasoning in particular we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting where a few chain of thought demonstrations are provided as exemplars in prompting experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic commonsense and symbolic reasoning tasks the empirical gains can be striking for instance prompting a 540b parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the gsm8k benchmark of math word problems surpassing even finetuned gpt 3 with a verifier,chain of thought prompting elicits reasoning in large language models jason wei et al we explore how generating a chain of thought a series of intermediate reasoning steps significantly improves the ability of large language models to perform complex reasoning in particular we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting where a few chain of thought demonstrations are provided as exemplars in prompting experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic commonsense and symbolic reasoning tasks the empirical gains can be striking for instance prompting a 540b parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the gsm8k benchmark of math word problems surpassing even finetuned gpt 3 with a verifier
2022,mslam massively multilingual joint pre training for speech and text,yu an chung et al,https://arxiv.org/abs/2202.01374,audio,we present mslam a multilingual speech and language model that learns cross lingual cross modal representations of speech and text by pre training jointly on large amounts of unlabeled speech and text in multiple languages mslam combines w2v bert pre training on speech with spanbert pre training on character level text along with connectionist temporal classification ctc losses on paired speech and transcript data to learn a single model capable of learning from and representing both speech and text signals in a shared representation space we evaluate mslam on several downstream speech understanding tasks and find that joint pre training with text improves quality on speech translation speech intent classification and speech language id while being competitive on multilingual asr when compared against speech only pre training our speech translation model demonstrates zero shot text translation without seeing any text translation data providing evidence for cross modal alignment of representations mslam also benefits from multi modal fine tuning further improving the quality of speech translation by directly leveraging text translation data during the fine tuning process our empirical analysis highlights several opportunities and challenges arising from large scale multimodal pre training suggesting directions for future research,mslam massively multilingual joint pre training for speech and text yu an chung et al we present mslam a multilingual speech and language model that learns cross lingual cross modal representations of speech and text by pre training jointly on large amounts of unlabeled speech and text in multiple languages mslam combines w2v bert pre training on speech with spanbert pre training on character level text along with connectionist temporal classification ctc losses on paired speech and transcript data to learn a single model capable of learning from and representing both speech and text signals in a shared representation space we evaluate mslam on several downstream speech understanding tasks and find that joint pre training with text improves quality on speech translation speech intent classification and speech language id while being competitive on multilingual asr when compared against speech only pre training our speech translation model demonstrates zero shot text translation without seeing any text translation data providing evidence for cross modal alignment of representations mslam also benefits from multi modal fine tuning further improving the quality of speech translation by directly leveraging text translation data during the fine tuning process our empirical analysis highlights several opportunities and challenges arising from large scale multimodal pre training suggesting directions for future research
2022,bc z zero shot task generalization with robotic imitation learning,eric jang et al,https://arxiv.org/abs/2202.02005,dl_rl,in this paper we study the problem of enabling a vision based robotic manipulation system to generalize to novel tasks a long standing challenge in robot learning we approach the challenge from an imitation learning perspective aiming to study how scaling and broadening the data collected can facilitate such generalization to that end we develop an interactive and flexible imitation learning system that can learn from both demonstrations and interventions and can be conditioned on different forms of information that convey the task including pre trained embeddings of natural language or videos of humans performing the task when scaling data collection on a real robot to more than 100 distinct tasks we find that this system can perform 24 unseen manipulation tasks with an average success rate of 44 without any robot demonstrations for those tasks,bc z zero shot task generalization with robotic imitation learning eric jang et al in this paper we study the problem of enabling a vision based robotic manipulation system to generalize to novel tasks a long standing challenge in robot learning we approach the challenge from an imitation learning perspective aiming to study how scaling and broadening the data collected can facilitate such generalization to that end we develop an interactive and flexible imitation learning system that can learn from both demonstrations and interventions and can be conditioned on different forms of information that convey the task including pre trained embeddings of natural language or videos of humans performing the task when scaling data collection on a real robot to more than 100 distinct tasks we find that this system can perform 24 unseen manipulation tasks with an average success rate of 44 without any robot demonstrations for those tasks
2022,block nerf scalable large scene neural view synthesis,matthew tancik et al,https://arxiv.org/abs/2202.05263,vision,we present block nerf a variant of neural radiance fields that can represent large scale environments specifically we demonstrate that when scaling nerf to render city scale scenes spanning multiple blocks it is vital to decompose the scene into individually trained nerfs this decomposition decouples rendering time from scene size enables rendering to scale to arbitrarily large environments and allows per block updates of the environment we adopt several architectural changes to make nerf robust to data captured over months under different environmental conditions we add appearance embeddings learned pose refinement and controllable exposure to each individual nerf and introduce a procedure for aligning appearance between adjacent nerfs so that they can be seamlessly combined we build a grid of block nerfs from 2 8 million images to create the largest neural scene representation to date capable of rendering an entire neighborhood of san francisco,block nerf scalable large scene neural view synthesis matthew tancik et al we present block nerf a variant of neural radiance fields that can represent large scale environments specifically we demonstrate that when scaling nerf to render city scale scenes spanning multiple blocks it is vital to decompose the scene into individually trained nerfs this decomposition decouples rendering time from scene size enables rendering to scale to arbitrarily large environments and allows per block updates of the environment we adopt several architectural changes to make nerf robust to data captured over months under different environmental conditions we add appearance embeddings learned pose refinement and controllable exposure to each individual nerf and introduce a procedure for aligning appearance between adjacent nerfs so that they can be seamlessly combined we build a grid of block nerfs from 2 8 million images to create the largest neural scene representation to date capable of rendering an entire neighborhood of san francisco
2022,add 2022 the first audio deep synthesis detection challenge,bhusan chettri et al,https://arxiv.org/abs/2202.08433,audio,audio deepfake detection is an emerging topic which was included in the asvspoof 2021 however the recent shared tasks have not covered many real life and challenging scenarios the first audio deep synthesis detection challenge add was motivated to fill in the gap the add 2022 includes three tracks low quality fake audio detection lf partially fake audio detection pf and audio fake game fg the lf track focuses on dealing with bona fide and fully fake utterances with various real world noises etc the pf track aims to distinguish the partially fake audio from the real the fg track is a rivalry game which includes two tasks an audio generation task and an audio fake detection task in this paper we describe the datasets evaluation metrics and protocols we also report major findings that reflect the recent advances in audio deepfake detection tasks,add 2022 the first audio deep synthesis detection challenge bhusan chettri et al audio deepfake detection is an emerging topic which was included in the asvspoof 2021 however the recent shared tasks have not covered many real life and challenging scenarios the first audio deep synthesis detection challenge add was motivated to fill in the gap the add 2022 includes three tracks low quality fake audio detection lf partially fake audio detection pf and audio fake game fg the lf track focuses on dealing with bona fide and fully fake utterances with various real world noises etc the pf track aims to distinguish the partially fake audio from the real the fg track is a rivalry game which includes two tasks an audio generation task and an audio fake detection task in this paper we describe the datasets evaluation metrics and protocols we also report major findings that reflect the recent advances in audio deepfake detection tasks
2022,mind the gap understanding the modality gap in multi modal contrastive representation learning,weixin liang et al,https://arxiv.org/abs/2203.02053,ml_representation,we present modality gap an intriguing geometric phenomenon of the representation space of multi modal models specifically we show that different data modalities e g images and text are embedded at arm s length in their shared representation in multi modal models such as clip our systematic analysis demonstrates that this gap is caused by a combination of model initialization and contrastive learning optimization in model initialization we show empirically and theoretically that the representation of a common deep neural network is restricted to a narrow cone as a consequence in a multi modal model with two encoders the representations of the two modalities are clearly apart when the model is initialized during optimization contrastive learning keeps the different modalities separate by a certain distance which is influenced by the temperature parameter in the loss function our experiments further demonstrate that varying the modality gap distance has a significant impact in improving the model s downstream zero shot classification performance and fairness our code and data are available at,mind the gap understanding the modality gap in multi modal contrastive representation learning weixin liang et al we present modality gap an intriguing geometric phenomenon of the representation space of multi modal models specifically we show that different data modalities e g images and text are embedded at arm s length in their shared representation in multi modal models such as clip our systematic analysis demonstrates that this gap is caused by a combination of model initialization and contrastive learning optimization in model initialization we show empirically and theoretically that the representation of a common deep neural network is restricted to a narrow cone as a consequence in a multi modal model with two encoders the representations of the two modalities are clearly apart when the model is initialized during optimization contrastive learning keeps the different modalities separate by a certain distance which is influenced by the temperature parameter in the loss function our experiments further demonstrate that varying the modality gap distance has a significant impact in improving the model s downstream zero shot classification performance and fairness our code and data are available at
2022,training language models to follow human instructions with human feedback,long ouyang et al,https://arxiv.org/abs/2203.02155,dl_nlp,making language models bigger does not inherently make them better at following a user s intent for example large language models can generate outputs that are untruthful toxic or simply not helpful to the user in other words these models are not aligned with their users in this paper we show an avenue for aligning language models with user intent on a wide range of tasks by fine tuning with human feedback starting with a set of labeler written prompts and prompts submitted through the openai api we collect a dataset of labeler demonstrations of the desired model behavior which we use to fine tune gpt 3 using supervised learning we then collect a dataset of rankings of model outputs which we use to further fine tune this supervised model using reinforcement learning from human feedback we call the resulting models instructgpt in human evaluations on our prompt distribution outputs from the 1 3b parameter instructgpt model are preferred to outputs from the 175b gpt 3 despite having 100x fewer parameters moreover instructgpt models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public nlp datasets even though instructgpt still makes simple mistakes our results show that fine tuning with human feedback is a promising direction for aligning language models with human intent,training language models to follow human instructions with human feedback long ouyang et al making language models bigger does not inherently make them better at following a user s intent for example large language models can generate outputs that are untruthful toxic or simply not helpful to the user in other words these models are not aligned with their users in this paper we show an avenue for aligning language models with user intent on a wide range of tasks by fine tuning with human feedback starting with a set of labeler written prompts and prompts submitted through the openai api we collect a dataset of labeler demonstrations of the desired model behavior which we use to fine tune gpt 3 using supervised learning we then collect a dataset of rankings of model outputs which we use to further fine tune this supervised model using reinforcement learning from human feedback we call the resulting models instructgpt in human evaluations on our prompt distribution outputs from the 1 3b parameter instructgpt model are preferred to outputs from the 175b gpt 3 despite having 100x fewer parameters moreover instructgpt models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public nlp datasets even though instructgpt still makes simple mistakes our results show that fine tuning with human feedback is a promising direction for aligning language models with human intent
2022,dino detr with improved denoising anchor boxes for end to end object detection,hao zhang et al,https://arxiv.org/abs/2203.03605,vision,we present dino TEXTBF d TEXTBF etr with TEXTBF i TEXTBF mproved de TEXTBF n TEXTBF oising anch TEXTBF o TEXTBF r boxes a state of the art end to end object detector in this paper dino improves over previous detr like models in performance and efficiency by using a contrastive way for denoising training a mixed query selection method for anchor initialization and a look forward twice scheme for box prediction dino achieves 49 4 ap in 12 epochs and 51 3 ap in 24 epochs on coco with a resnet 50 backbone and multi scale features yielding a significant improvement of TEXTBF 6 0 TEXTBF TEXTBF ap TEXTBF and TEXTBF 2 7 TEXTBF TEXTBF ap TEXTBF respectively compared to dn detr the previous best detr like model dino scales well in both model size and data size without bells and whistles after pre training on the objects365 dataset with a swinl backbone dino obtains the best results on both coco TEXTTT val2017 TEXTTT TEXTBF 63 2 TEXTBF TEXTBF ap TEXTBF and TEXTTT test dev TEXTTT TEXTBF textbf 63 3 TEXTBF ap compared to other models on the leaderboard dino significantly reduces its model size and pre training data size while achieving better results our code will be available at url,dino detr with improved denoising anchor boxes for end to end object detection hao zhang et al we present dino TEXTBF d TEXTBF etr with TEXTBF i TEXTBF mproved de TEXTBF n TEXTBF oising anch TEXTBF o TEXTBF r boxes a state of the art end to end object detector in this paper dino improves over previous detr like models in performance and efficiency by using a contrastive way for denoising training a mixed query selection method for anchor initialization and a look forward twice scheme for box prediction dino achieves 49 4 ap in 12 epochs and 51 3 ap in 24 epochs on coco with a resnet 50 backbone and multi scale features yielding a significant improvement of TEXTBF 6 0 TEXTBF TEXTBF ap TEXTBF and TEXTBF 2 7 TEXTBF TEXTBF ap TEXTBF respectively compared to dn detr the previous best detr like model dino scales well in both model size and data size without bells and whistles after pre training on the objects365 dataset with a swinl backbone dino obtains the best results on both coco TEXTTT val2017 TEXTTT TEXTBF 63 2 TEXTBF TEXTBF ap TEXTBF and TEXTTT test dev TEXTTT TEXTBF textbf 63 3 TEXTBF ap compared to other models on the leaderboard dino significantly reduces its model size and pre training data size while achieving better results our code will be available at url
2022,scaling up your kernels to 31 31 revisiting large kernel design in cnns,xiaohan ding et al,https://arxiv.org/abs/2203.06717,vision,we revisit large kernel design in modern convolutional neural networks cnns inspired by recent advances in vision transformers vits in this paper we demonstrate that using a few large convolutional kernels instead of a stack of small kernels could be a more powerful paradigm we suggested five guidelines e g applying re parameterized large depth wise convolutions to design efficient high performance large kernel cnns following the guidelines we propose replknet a pure cnn architecture whose kernel size is as large as 31x31 in contrast to commonly used 3x3 replknet greatly closes the performance gap between cnns and vits e g achieving comparable or superior results than swin transformer on imagenet and a few typical downstream tasks with lower latency replknet also shows nice scalability to big data and large models obtaining 87 8 top 1 accuracy on imagenet and 56 0 miou on ade20k which is very competitive among the state of the arts with similar model sizes our study further reveals that in contrast to small kernel cnns large kernel cnns have much larger effective receptive fields and higher shape bias rather than texture bias code models at,scaling up your kernels to 31 31 revisiting large kernel design in cnns xiaohan ding et al we revisit large kernel design in modern convolutional neural networks cnns inspired by recent advances in vision transformers vits in this paper we demonstrate that using a few large convolutional kernels instead of a stack of small kernels could be a more powerful paradigm we suggested five guidelines e g applying re parameterized large depth wise convolutions to design efficient high performance large kernel cnns following the guidelines we propose replknet a pure cnn architecture whose kernel size is as large as 31x31 in contrast to commonly used 3x3 replknet greatly closes the performance gap between cnns and vits e g achieving comparable or superior results than swin transformer on imagenet and a few typical downstream tasks with lower latency replknet also shows nice scalability to big data and large models obtaining 87 8 top 1 accuracy on imagenet and 56 0 miou on ade20k which is very competitive among the state of the arts with similar model sizes our study further reveals that in contrast to small kernel cnns large kernel cnns have much larger effective receptive fields and higher shape bias rather than texture bias code models at
2022,competition level code generation with alphacode,li yujia et al,https://arxiv.org/abs/2203.07814,dl_nlp,programming is a powerful and ubiquitous problem solving tool developing systems that can assist programmers or even generate programs independently could make programming more productive and accessible yet so far incorporating innovations in ai has proven challenging recent large scale language models have demonstrated an impressive ability to generate code and are now able to complete simple programming tasks however these models still perform poorly when evaluated on more complex unseen problems that require problem solving skills beyond simply translating instructions into code for example competitive programming problems which require an understanding of algorithms and complex natural language remain extremely challenging to address this gap we introduce alphacode a system for code generation that can create novel solutions to these problems that require deeper reasoning in simulated evaluations on recent programming competitions on the codeforces platform alphacode achieved on average a ranking of top 54 3 in competitions with more than 5000 participants we found that three key components were critical to achieve good and reliable performance 1 an extensive and clean competitive programming dataset for training and evaluation 2 large and efficient to sample transformer based architectures and 3 large scale model sampling to explore the search space followed by filtering based on program behavior to a small set of submissions,competition level code generation with alphacode li yujia et al programming is a powerful and ubiquitous problem solving tool developing systems that can assist programmers or even generate programs independently could make programming more productive and accessible yet so far incorporating innovations in ai has proven challenging recent large scale language models have demonstrated an impressive ability to generate code and are now able to complete simple programming tasks however these models still perform poorly when evaluated on more complex unseen problems that require problem solving skills beyond simply translating instructions into code for example competitive programming problems which require an understanding of algorithms and complex natural language remain extremely challenging to address this gap we introduce alphacode a system for code generation that can create novel solutions to these problems that require deeper reasoning in simulated evaluations on recent programming competitions on the codeforces platform alphacode achieved on average a ranking of top 54 3 in competitions with more than 5000 participants we found that three key components were critical to achieve good and reliable performance 1 an extensive and clean competitive programming dataset for training and evaluation 2 large and efficient to sample transformer based architectures and 3 large scale model sampling to explore the search space followed by filtering based on program behavior to a small set of submissions
2022,training compute optimal large language models,jordan hoffmann et al,https://arxiv.org/abs/2203.15556,dl_nlp,we investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget we find that current large language models are significantly undertrained a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant by training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens we find that for compute optimal training the model size and the number of training tokens should be scaled equally for every doubling of model size the number of training tokens should also be doubled we test this hypothesis by training a predicted compute optimal model chinchilla that uses the same compute budget as gopher but with 70b parameters and 4 times more more data chinchilla uniformly and significantly outperforms gopher 280b gpt 3 175b jurassic 1 178b and megatron turing nlg 530b on a large range of downstream evaluation tasks this also means that chinchilla uses substantially less compute for fine tuning and inference greatly facilitating downstream usage as a highlight chinchilla reaches a state of the art average accuracy of 67 5 on the mmlu benchmark greater than a 7 improvement over gopher,training compute optimal large language models jordan hoffmann et al we investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget we find that current large language models are significantly undertrained a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant by training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens we find that for compute optimal training the model size and the number of training tokens should be scaled equally for every doubling of model size the number of training tokens should also be doubled we test this hypothesis by training a predicted compute optimal model chinchilla that uses the same compute budget as gopher but with 70b parameters and 4 times more more data chinchilla uniformly and significantly outperforms gopher 280b gpt 3 175b jurassic 1 178b and megatron turing nlg 530b on a large range of downstream evaluation tasks this also means that chinchilla uses substantially less compute for fine tuning and inference greatly facilitating downstream usage as a highlight chinchilla reaches a state of the art average accuracy of 67 5 on the mmlu benchmark greater than a 7 improvement over gopher
2022,do as i can not as i say grounding language in robotic affordances,michael ahn et al,https://arxiv.org/abs/2204.01691,dl_nlp,large language models can encode a wealth of semantic knowledge about the world such knowledge could be extremely useful to robots aiming to act upon high level temporally extended instructions expressed in natural language however a significant weakness of language models is that they lack real world experience which makes it difficult to leverage them for decision making within a given embodiment for example asking a language model to describe how to clean a spill might result in a reasonable narrative but it may not be applicable to a particular agent such as a robot that needs to perform this task in a particular environment we propose to provide real world grounding by means of pretrained skills which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate the robot can act as the language model s hands and eyes while the language model supplies high level semantic knowledge about the task we show how low level skills can be combined with large language models so that the language model provides high level knowledge about the procedures for performing complex and temporally extended instructions while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment we evaluate our method on a number of real world robotic tasks where we show the need for real world grounding and that this approach is capable of completing long horizon abstract natural language instructions on a mobile manipulator the project s website and the video can be found at,do as i can not as i say grounding language in robotic affordances michael ahn et al large language models can encode a wealth of semantic knowledge about the world such knowledge could be extremely useful to robots aiming to act upon high level temporally extended instructions expressed in natural language however a significant weakness of language models is that they lack real world experience which makes it difficult to leverage them for decision making within a given embodiment for example asking a language model to describe how to clean a spill might result in a reasonable narrative but it may not be applicable to a particular agent such as a robot that needs to perform this task in a particular environment we propose to provide real world grounding by means of pretrained skills which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate the robot can act as the language model s hands and eyes while the language model supplies high level semantic knowledge about the task we show how low level skills can be combined with large language models so that the language model provides high level knowledge about the procedures for performing complex and temporally extended instructions while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment we evaluate our method on a number of real world robotic tasks where we show the need for real world grounding and that this approach is capable of completing long horizon abstract natural language instructions on a mobile manipulator the project s website and the video can be found at
2022,maxvit multi axis vision transformer,zhengzhong tu et al,https://arxiv.org/abs/2204.01697,vision,transformers have recently gained significant attention in the computer vision community however the lack of scalability of self attention mechanisms with respect to image size has limited their wide adoption in state of the art vision backbones in this paper we introduce an efficient and scalable attention model we call multi axis attention which consists of two aspects blocked local and dilated global attention these design choices allow global local spatial interactions on arbitrary input resolutions with only linear complexity we also present a new architectural element by effectively blending our proposed attention model with convolutions and accordingly propose a simple hierarchical vision backbone dubbed maxvit by simply repeating the basic building block over multiple stages notably maxvit is able to see globally throughout the entire network even in earlier high resolution stages we demonstrate the effectiveness of our model on a broad spectrum of vision tasks on image classification maxvit achieves state of the art performance under various settings without extra data maxvit attains 86 5 imagenet 1k top 1 accuracy with imagenet 21k pre training our model achieves 88 7 top 1 accuracy for downstream tasks maxvit as a backbone delivers favorable performance on object detection as well as visual aesthetic assessment we also show that our proposed model expresses strong generative modeling capability on imagenet demonstrating the superior potential of maxvit blocks as a universal vision module the source code and trained models will be available at,maxvit multi axis vision transformer zhengzhong tu et al transformers have recently gained significant attention in the computer vision community however the lack of scalability of self attention mechanisms with respect to image size has limited their wide adoption in state of the art vision backbones in this paper we introduce an efficient and scalable attention model we call multi axis attention which consists of two aspects blocked local and dilated global attention these design choices allow global local spatial interactions on arbitrary input resolutions with only linear complexity we also present a new architectural element by effectively blending our proposed attention model with convolutions and accordingly propose a simple hierarchical vision backbone dubbed maxvit by simply repeating the basic building block over multiple stages notably maxvit is able to see globally throughout the entire network even in earlier high resolution stages we demonstrate the effectiveness of our model on a broad spectrum of vision tasks on image classification maxvit achieves state of the art performance under various settings without extra data maxvit attains 86 5 imagenet 1k top 1 accuracy with imagenet 21k pre training our model achieves 88 7 top 1 accuracy for downstream tasks maxvit as a backbone delivers favorable performance on object detection as well as visual aesthetic assessment we also show that our proposed model expresses strong generative modeling capability on imagenet demonstrating the superior potential of maxvit blocks as a universal vision module the source code and trained models will be available at
2022,palm scaling language modeling with pathways,aakanksha chowdhery et al,https://arxiv.org/abs/2204.02311,dl_nlp,large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few shot learning which drastically reduces the number of task specific training examples needed to adapt the model to a particular application to further our understanding of the impact of scale on few shot learning we trained a 540 billion parameter densely activated transformer language model which we call pathways language model palm we trained palm on 6144 tpu v4 chips using pathways a new ml system which enables highly efficient training across multiple tpu pods we demonstrate continued benefits of scaling by achieving state of the art few shot learning results on hundreds of language understanding and generation benchmarks on a number of these tasks palm 540b achieves breakthrough performance outperforming the finetuned state of the art on a suite of multi step reasoning tasks and outperforming average human performance on the recently released big bench benchmark a significant number of big bench tasks showed discontinuous improvements from model scale meaning that performance steeply increased as we scaled to our largest model palm also has strong capabilities in multilingual tasks and source code generation which we demonstrate on a wide array of benchmarks we additionally provide a comprehensive analysis on bias and toxicity and study the extent of training data memorization with respect to model scale finally we discuss the ethical considerations related to large language models and discuss potential mitigation strategies,palm scaling language modeling with pathways aakanksha chowdhery et al large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few shot learning which drastically reduces the number of task specific training examples needed to adapt the model to a particular application to further our understanding of the impact of scale on few shot learning we trained a 540 billion parameter densely activated transformer language model which we call pathways language model palm we trained palm on 6144 tpu v4 chips using pathways a new ml system which enables highly efficient training across multiple tpu pods we demonstrate continued benefits of scaling by achieving state of the art few shot learning results on hundreds of language understanding and generation benchmarks on a number of these tasks palm 540b achieves breakthrough performance outperforming the finetuned state of the art on a suite of multi step reasoning tasks and outperforming average human performance on the recently released big bench benchmark a significant number of big bench tasks showed discontinuous improvements from model scale meaning that performance steeply increased as we scaled to our largest model palm also has strong capabilities in multilingual tasks and source code generation which we demonstrate on a wide array of benchmarks we additionally provide a comprehensive analysis on bias and toxicity and study the extent of training data memorization with respect to model scale finally we discuss the ethical considerations related to large language models and discuss potential mitigation strategies
2022,maestro matched speech text representations through modality matching,alexander h liu et al,https://arxiv.org/abs/2204.03409,audio,we present maestro a self supervised training method to unify representations learnt from speech and text modalities self supervised learning from speech signals aims to learn the latent structure inherent in the signal while self supervised learning from text attempts to capture lexical information learning aligned representations from unpaired speech and text sequences is a challenging task previous work either implicitly enforced the representations learnt from these two modalities to be aligned in the latent space through multitasking and parameter sharing or explicitly through conversion of modalities via speech synthesis while the former suffers from interference between the two modalities the latter introduces additional complexity in this paper we propose maestro a novel algorithm to learn unified representations from both these modalities simultaneously that can transfer to diverse downstream tasks such as automated speech recognition asr and speech translation st maestro learns unified representations through sequence alignment duration prediction and matching embeddings in the learned space through an aligned masked language model loss we establish a new state of the art sota on voxpopuli multilingual asr with a 8 relative reduction in word error rate wer multidomain speechstew asr 3 7 relative and 21 languages to english multilingual st on covost 2 with an improvement of 2 8 bleu averaged over 21 languages,maestro matched speech text representations through modality matching alexander h liu et al we present maestro a self supervised training method to unify representations learnt from speech and text modalities self supervised learning from speech signals aims to learn the latent structure inherent in the signal while self supervised learning from text attempts to capture lexical information learning aligned representations from unpaired speech and text sequences is a challenging task previous work either implicitly enforced the representations learnt from these two modalities to be aligned in the latent space through multitasking and parameter sharing or explicitly through conversion of modalities via speech synthesis while the former suffers from interference between the two modalities the latter introduces additional complexity in this paper we propose maestro a novel algorithm to learn unified representations from both these modalities simultaneously that can transfer to diverse downstream tasks such as automated speech recognition asr and speech translation st maestro learns unified representations through sequence alignment duration prediction and matching embeddings in the learned space through an aligned masked language model loss we establish a new state of the art sota on voxpopuli multilingual asr with a 8 relative reduction in word error rate wer multidomain speechstew asr 3 7 relative and 21 languages to english multilingual st on covost 2 with an improvement of 2 8 bleu averaged over 21 languages
2022,hierarchical text conditional image generation with clip latents dall e 2,aditya ramesh et al,https://arxiv.org/abs/2204.06125,vision,contrastive models like clip have been shown to learn robust representations of images that capture both semantics and style to leverage these representations for image generation we propose a two stage model a prior that generates a clip image embedding given a text caption and a decoder that generates an image conditioned on the image embedding we show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style while varying the non essential details absent from the image representation moreover the joint embedding space of clip enables language guided image manipulations in a zero shot fashion we use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior finding that the latter are computationally more efficient and produce higher quality samples,hierarchical text conditional image generation with clip latents dall e 2 aditya ramesh et al contrastive models like clip have been shown to learn robust representations of images that capture both semantics and style to leverage these representations for image generation we propose a two stage model a prior that generates a clip image embedding given a text caption and a decoder that generates an image conditioned on the image embedding we show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style while varying the non essential details absent from the image representation moreover the joint embedding space of clip enables language guided image manipulations in a zero shot fashion we use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior finding that the latter are computationally more efficient and produce higher quality samples
2022,gpt neox 20b an open source autoregressive language model,sid black et al,https://arxiv.org/abs/2204.06745,dl_nlp,we introduce gpt neox 20b a 20 billion parameter autoregressive language model trained on the pile whose weights will be made freely and openly available to the public through a permissive license it is to the best of our knowledge the largest dense autoregressive model that has publicly available weights at the time of submission in this work we describe model s architecture and training and evaluate its performance on a range of language understanding mathematics and knowledge based tasks we find that gpt neox 20b is a particularly powerful few shot reasoner and gains far more in performance when evaluated five shot than similarly sized gpt 3 and fairseq models we open source the training and evaluation code as well as the model weights at,gpt neox 20b an open source autoregressive language model sid black et al we introduce gpt neox 20b a 20 billion parameter autoregressive language model trained on the pile whose weights will be made freely and openly available to the public through a permissive license it is to the best of our knowledge the largest dense autoregressive model that has publicly available weights at the time of submission in this work we describe model s architecture and training and evaluate its performance on a range of language understanding mathematics and knowledge based tasks we find that gpt neox 20b is a particularly powerful few shot reasoner and gains far more in performance when evaluated five shot than similarly sized gpt 3 and fairseq models we open source the training and evaluation code as well as the model weights at
2022,better plain vit baselines for imagenet 1k,lucas beyer et al,https://arxiv.org/abs/2205.01580,vision,it is commonly accepted that the vision transformer model requires sophisticated regularization techniques to excel at imagenet 1k scale data surprisingly we find this is not the case and standard data augmentation is sufficient this note presents a few minor modifications to the original vision transformer vit vanilla training setting that dramatically improve the performance of plain vit models notably 90 epochs of training surpass 76 top 1 accuracy in under seven hours on a tpuv3 8 similar to the classic resnet50 baseline and 300 epochs of training reach 80 in less than one day,better plain vit baselines for imagenet 1k lucas beyer et al it is commonly accepted that the vision transformer model requires sophisticated regularization techniques to excel at imagenet 1k scale data surprisingly we find this is not the case and standard data augmentation is sufficient this note presents a few minor modifications to the original vision transformer vit vanilla training setting that dramatically improve the performance of plain vit models notably 90 epochs of training surpass 76 top 1 accuracy in under seven hours on a tpuv3 8 similar to the classic resnet50 baseline and 300 epochs of training reach 80 in less than one day
2022,towards understanding grokking an effective theory of representation learning,ziming liu et al,https://arxiv.org/abs/2205.10343,learning_theory,we aim to understand grokking a phenomenon where models generalize long after overfitting their training set we present both a microscopic analysis anchored by an effective theory and a macroscopic analysis of phase diagrams describing learning performance across hyperparameters we find that generalization originates from structured representations whose training dynamics and dependence on training set size can be predicted by our effective theory in a toy setting we observe empirically the presence of four learning phases comprehension grokking memorization and confusion we find representation learning to occur only in a goldilocks zone including comprehension and grokking between memorization and confusion we find on transformers the grokking phase stays closer to the memorization phase compared to the comprehension phase leading to delayed generalization the goldilocks phase is reminiscent of intelligence from starvation in darwinian evolution where resource limitations drive discovery of more efficient solutions this study not only provides intuitive explanations of the origin of grokking but also highlights the usefulness of physics inspired tools e g effective theories and phase diagrams for understanding deep learning,towards understanding grokking an effective theory of representation learning ziming liu et al we aim to understand grokking a phenomenon where models generalize long after overfitting their training set we present both a microscopic analysis anchored by an effective theory and a macroscopic analysis of phase diagrams describing learning performance across hyperparameters we find that generalization originates from structured representations whose training dynamics and dependence on training set size can be predicted by our effective theory in a toy setting we observe empirically the presence of four learning phases comprehension grokking memorization and confusion we find representation learning to occur only in a goldilocks zone including comprehension and grokking between memorization and confusion we find on transformers the grokking phase stays closer to the memorization phase compared to the comprehension phase leading to delayed generalization the goldilocks phase is reminiscent of intelligence from starvation in darwinian evolution where resource limitations drive discovery of more efficient solutions this study not only provides intuitive explanations of the origin of grokking but also highlights the usefulness of physics inspired tools e g effective theories and phase diagrams for understanding deep learning
2022,photorealistic text to image diffusion models with deep language understanding imagen,chitwan saharia et al,https://arxiv.org/abs/2205.11487,vision,we present imagen a text to image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high fidelity image generation our key discovery is that generic large language models e g t5 pretrained on text only corpora are surprisingly effective at encoding text for image synthesis increasing the size of the language model in imagen boosts both sample fidelity and image text alignment much more than increasing the size of the image diffusion model imagen achieves a new state of the art fid score of 7 27 on the coco dataset without ever training on coco and human raters find imagen samples to be on par with the coco data itself in image text alignment to assess text to image models in greater depth we introduce drawbench a comprehensive and challenging benchmark for text to image models with drawbench we compare imagen with recent methods including vq gan clip latent diffusion models and dall e 2 and find that human raters prefer imagen over other models in side by side comparisons both in terms of sample quality and image text alignment see for an overview of the results,photorealistic text to image diffusion models with deep language understanding imagen chitwan saharia et al we present imagen a text to image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high fidelity image generation our key discovery is that generic large language models e g t5 pretrained on text only corpora are surprisingly effective at encoding text for image synthesis increasing the size of the language model in imagen boosts both sample fidelity and image text alignment much more than increasing the size of the image diffusion model imagen achieves a new state of the art fid score of 7 27 on the coco dataset without ever training on coco and human raters find imagen samples to be on par with the coco data itself in image text alignment to assess text to image models in greater depth we introduce drawbench a comprehensive and challenging benchmark for text to image models with drawbench we compare imagen with recent methods including vq gan clip latent diffusion models and dall e 2 and find that human raters prefer imagen over other models in side by side comparisons both in terms of sample quality and image text alignment see for an overview of the results
2022,git a generative image to text transformer for vision and language,jianfeng wang et al,https://arxiv.org/abs/2205.14100,vision,in this paper we design and train a generative image to text transformer git to unify vision language tasks such as image video captioning and question answering while generative models provide a consistent network architecture between pre training and fine tuning existing work typically contains complex structures uni multi modal encoder decoder and depends on external modules such as object detectors taggers and optical character recognition ocr in git we simplify the architecture as one image encoder and one text decoder under a single language modeling task we also scale up the pre training data and the model size to boost the model performance without bells and whistles our git establishes new state of the arts on 12 challenging benchmarks with a large margin for instance our model surpasses the human performance for the first time on textcaps 138 2 vs 125 5 in cider furthermore we present a new scheme of generation based image classification and scene text recognition achieving decent performance on standard benchmarks codes are released at url,git a generative image to text transformer for vision and language jianfeng wang et al in this paper we design and train a generative image to text transformer git to unify vision language tasks such as image video captioning and question answering while generative models provide a consistent network architecture between pre training and fine tuning existing work typically contains complex structures uni multi modal encoder decoder and depends on external modules such as object detectors taggers and optical character recognition ocr in git we simplify the architecture as one image encoder and one text decoder under a single language modeling task we also scale up the pre training data and the model size to boost the model performance without bells and whistles our git establishes new state of the arts on 12 challenging benchmarks with a large margin for instance our model surpasses the human performance for the first time on textcaps 138 2 vs 125 5 in cider furthermore we present a new scheme of generation based image classification and scene text recognition achieving decent performance on standard benchmarks codes are released at url
2022,beyond the imitation game quantifying and extrapolating the capabilities of language models,srivastava et al,https://arxiv.org/abs/2206.04615,dl_nlp,language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale despite their potentially transformative impact these new capabilities are as yet poorly characterized in order to inform future research prepare for disruptive new model capabilities and ameliorate socially harmful effects it is vital that we understand the present and near future capabilities and limitations of language models to address this challenge we introduce the beyond the imitation game benchmark big bench big bench currently consists of 204 tasks contributed by 450 authors across 132 institutions task topics are diverse drawing problems from linguistics childhood development math common sense reasoning biology physics social bias software development and beyond big bench focuses on tasks that are believed to be beyond the capabilities of current language models we evaluate the behavior of openai s gpt models google internal dense transformer architectures and switch style sparse transformers on big bench across model sizes spanning millions to hundreds of billions of parameters in addition a team of human expert raters performed all tasks in order to provide a strong baseline findings include model performance and calibration both improve with scale but are poor in absolute terms and when compared with rater performance performance is remarkably similar across model classes though with benefits from sparsity tasks that improve gradually and predictably commonly involve a large knowledge or memorization component whereas tasks that exhibit breakthrough behavior at a critical scale often involve multiple steps or components or brittle metrics social bias typically increases with scale in settings with ambiguous context but this can be improved with prompting,beyond the imitation game quantifying and extrapolating the capabilities of language models srivastava et al language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale despite their potentially transformative impact these new capabilities are as yet poorly characterized in order to inform future research prepare for disruptive new model capabilities and ameliorate socially harmful effects it is vital that we understand the present and near future capabilities and limitations of language models to address this challenge we introduce the beyond the imitation game benchmark big bench big bench currently consists of 204 tasks contributed by 450 authors across 132 institutions task topics are diverse drawing problems from linguistics childhood development math common sense reasoning biology physics social bias software development and beyond big bench focuses on tasks that are believed to be beyond the capabilities of current language models we evaluate the behavior of openai s gpt models google internal dense transformer architectures and switch style sparse transformers on big bench across model sizes spanning millions to hundreds of billions of parameters in addition a team of human expert raters performed all tasks in order to provide a strong baseline findings include model performance and calibration both improve with scale but are poor in absolute terms and when compared with rater performance performance is remarkably similar across model classes though with benefits from sparsity tasks that improve gradually and predictably commonly involve a large knowledge or memorization component whereas tasks that exhibit breakthrough behavior at a critical scale often involve multiple steps or components or brittle metrics social bias typically increases with scale in settings with ambiguous context but this can be improved with prompting
2022,solving quantitative reasoning problems with language models,lewkowycz et al,https://arxiv.org/abs/2206.14858,dl_nlp,language models have achieved remarkable performance on a wide range of tasks that require natural language understanding nevertheless state of the art models have generally struggled with tasks that require quantitative reasoning such as solving mathematics science and engineering problems at the college level to help close this gap we introduce minerva a large language model pretrained on general natural language data and further trained on technical content the model achieves state of the art performance on technical benchmarks without the use of external tools we also evaluate our model on over two hundred undergraduate level problems in physics biology chemistry economics and other sciences that require quantitative reasoning and find that the model can correctly answer nearly a third of them,solving quantitative reasoning problems with language models lewkowycz et al language models have achieved remarkable performance on a wide range of tasks that require natural language understanding nevertheless state of the art models have generally struggled with tasks that require quantitative reasoning such as solving mathematics science and engineering problems at the college level to help close this gap we introduce minerva a large language model pretrained on general natural language data and further trained on technical content the model achieves state of the art performance on technical benchmarks without the use of external tools we also evaluate our model on over two hundred undergraduate level problems in physics biology chemistry economics and other sciences that require quantitative reasoning and find that the model can correctly answer nearly a third of them
2022,classifier free diffusion guidance,jonathan ho and tim salimans,https://arxiv.org/abs/2207.12598,vision,classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training in the same spirit as low temperature sampling or truncation in other types of generative models classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model it also raises the question of whether guidance can be performed without a classifier we show that guidance can be indeed performed by a pure generative model without such a classifier in what we call classifier free guidance we jointly train a conditional and an unconditional diffusion model and we combine the resulting conditional and unconditional score estimates to attain a trade off between sample quality and diversity similar to that obtained using classifier guidance,classifier free diffusion guidance jonathan ho and tim salimans classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training in the same spirit as low temperature sampling or truncation in other types of generative models classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model it also raises the question of whether guidance can be performed without a classifier we show that guidance can be indeed performed by a pure generative model without such a classifier in what we call classifier free guidance we jointly train a conditional and an unconditional diffusion model and we combine the resulting conditional and unconditional score estimates to attain a trade off between sample quality and diversity similar to that obtained using classifier guidance
2022,dreambooth fine tuning text to image diffusion models for subject driven generation,nataniel ruiz et al,https://arxiv.org/abs/2208.12242,vision,large text to image models achieved a remarkable leap in the evolution of ai enabling high quality and diverse synthesis of images from a given text prompt however these models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts in this work we present a new approach for personalization of text to image diffusion models given as input just a few images of a subject we fine tune a pretrained text to image model such that it learns to bind a unique identifier with that specific subject once the subject is embedded in the output domain of the model the unique identifier can be used to synthesize novel photorealistic images of the subject contextualized in different scenes by leveraging the semantic prior embedded in the model with a new autogenous class specific prior preservation loss our technique enables synthesizing the subject in diverse scenes poses views and lighting conditions that do not appear in the reference images we apply our technique to several previously unassailable tasks including subject recontextualization text guided view synthesis and artistic rendering all while preserving the subject s key features we also provide a new dataset and evaluation protocol for this new task of subject driven generation project page,dreambooth fine tuning text to image diffusion models for subject driven generation nataniel ruiz et al large text to image models achieved a remarkable leap in the evolution of ai enabling high quality and diverse synthesis of images from a given text prompt however these models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts in this work we present a new approach for personalization of text to image diffusion models given as input just a few images of a subject we fine tune a pretrained text to image model such that it learns to bind a unique identifier with that specific subject once the subject is embedded in the output domain of the model the unique identifier can be used to synthesize novel photorealistic images of the subject contextualized in different scenes by leveraging the semantic prior embedded in the model with a new autogenous class specific prior preservation loss our technique enables synthesizing the subject in diverse scenes poses views and lighting conditions that do not appear in the reference images we apply our technique to several previously unassailable tasks including subject recontextualization text guided view synthesis and artistic rendering all while preserving the subject s key features we also provide a new dataset and evaluation protocol for this new task of subject driven generation project page
2022,mulan a joint embedding of music audio and natural language,qingqing huang et al,https://arxiv.org/abs/2208.12415,audio,music tagging and content based retrieval systems have traditionally been constructed using pre defined ontologies covering a rigid set of music attributes or text queries this paper presents mulan a first attempt at a new generation of acoustic models that link music audio directly to unconstrained natural language music descriptions mulan takes the form of a two tower joint audio text embedding model trained using 44 million music recordings 370k hours and weakly associated free form text annotations through its compatibility with a wide range of music genres and text styles including conventional music tags the resulting audio text representation subsumes existing ontologies while graduating to true zero shot functionalities we demonstrate the versatility of the mulan embeddings with a range of experiments including transfer learning zero shot music tagging language understanding in the music domain and cross modal retrieval applications,mulan a joint embedding of music audio and natural language qingqing huang et al music tagging and content based retrieval systems have traditionally been constructed using pre defined ontologies covering a rigid set of music attributes or text queries this paper presents mulan a first attempt at a new generation of acoustic models that link music audio directly to unconstrained natural language music descriptions mulan takes the form of a two tower joint audio text embedding model trained using 44 million music recordings 370k hours and weakly associated free form text annotations through its compatibility with a wide range of music genres and text styles including conventional music tags the resulting audio text representation subsumes existing ontologies while graduating to true zero shot functionalities we demonstrate the versatility of the mulan embeddings with a range of experiments including transfer learning zero shot music tagging language understanding in the music domain and cross modal retrieval applications
2022,audiolm a language modeling approach to audio generation,zalán borsos et al,https://arxiv.org/abs/2209.03143,audio,we introduce audiolm a framework for high quality audio generation with long term consistency audiolm maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space we show how existing audio tokenizers provide different trade offs between reconstruction quality and long term structure and we propose a hybrid tokenization scheme to achieve both objectives namely we leverage the discretized activations of a masked language model pre trained on audio to capture long term structure and the discrete codes produced by a neural audio codec to achieve high quality synthesis by training on large corpora of raw audio waveforms audiolm learns to generate natural and coherent continuations given short prompts when trained on speech and without any transcript or annotation audiolm generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers furthermore we demonstrate how our approach extends beyond speech by generating coherent piano music continuations despite being trained without any symbolic representation of music,audiolm a language modeling approach to audio generation zalán borsos et al we introduce audiolm a framework for high quality audio generation with long term consistency audiolm maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space we show how existing audio tokenizers provide different trade offs between reconstruction quality and long term structure and we propose a hybrid tokenization scheme to achieve both objectives namely we leverage the discretized activations of a masked language model pre trained on audio to capture long term structure and the discrete codes produced by a neural audio codec to achieve high quality synthesis by training on large corpora of raw audio waveforms audiolm learns to generate natural and coherent continuations given short prompts when trained on speech and without any transcript or annotation audiolm generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers furthermore we demonstrate how our approach extends beyond speech by generating coherent piano music continuations despite being trained without any symbolic representation of music
2022,make a video text to video generation without text video data,uriel singer et al,https://arxiv.org/abs/2209.14792,vision,we propose make a video an approach for directly translating the tremendous recent progress in text to image t2i generation to text to video t2v our intuition is simple learn what the world looks like and how it is described from paired text image data and learn how the world moves from unsupervised video footage make a video has three advantages 1 it accelerates training of the t2v model it does not need to learn visual and multimodal representations from scratch 2 it does not require paired text video data and 3 the generated videos inherit the vastness diversity in aesthetic fantastical depictions etc of today s image generation models we design a simple yet effective way to build on t2i models with novel and effective spatial temporal modules first we decompose the full temporal u net and attention tensors and approximate them in space and time second we design a spatial temporal pipeline to generate high resolution and frame rate videos with a video decoder interpolation model and two super resolution models that can enable various applications besides t2v in all aspects spatial and temporal resolution faithfulness to text and quality make a video sets the new state of the art in text to video generation as determined by both qualitative and quantitative measures,make a video text to video generation without text video data uriel singer et al we propose make a video an approach for directly translating the tremendous recent progress in text to image t2i generation to text to video t2v our intuition is simple learn what the world looks like and how it is described from paired text image data and learn how the world moves from unsupervised video footage make a video has three advantages 1 it accelerates training of the t2v model it does not need to learn visual and multimodal representations from scratch 2 it does not require paired text video data and 3 the generated videos inherit the vastness diversity in aesthetic fantastical depictions etc of today s image generation models we design a simple yet effective way to build on t2i models with novel and effective spatial temporal modules first we decompose the full temporal u net and attention tensors and approximate them in space and time second we design a spatial temporal pipeline to generate high resolution and frame rate videos with a video decoder interpolation model and two super resolution models that can enable various applications besides t2v in all aspects spatial and temporal resolution faithfulness to text and quality make a video sets the new state of the art in text to video generation as determined by both qualitative and quantitative measures
2022,dreamfusion text to 3d using 2d diffusion,ben poole et al,https://arxiv.org/abs/2209.14988,vision,recent breakthroughs in text to image synthesis have been driven by diffusion models trained on billions of image text pairs adapting this approach to 3d synthesis would require large scale datasets of labeled 3d data and efficient architectures for denoising 3d data neither of which currently exist in this work we circumvent these limitations by using a pretrained 2d text to image diffusion model to perform text to 3d synthesis we introduce a loss based on probability density distillation that enables the use of a 2d diffusion model as a prior for optimization of a parametric image generator using this loss in a deepdream like procedure we optimize a randomly initialized 3d model a neural radiance field or nerf via gradient descent such that its 2d renderings from random angles achieve a low loss the resulting 3d model of the given text can be viewed from any angle relit by arbitrary illumination or composited into any 3d environment our approach requires no 3d training data and no modifications to the image diffusion model demonstrating the effectiveness of pretrained image diffusion models as priors,dreamfusion text to 3d using 2d diffusion ben poole et al recent breakthroughs in text to image synthesis have been driven by diffusion models trained on billions of image text pairs adapting this approach to 3d synthesis would require large scale datasets of labeled 3d data and efficient architectures for denoising 3d data neither of which currently exist in this work we circumvent these limitations by using a pretrained 2d text to image diffusion model to perform text to 3d synthesis we introduce a loss based on probability density distillation that enables the use of a 2d diffusion model as a prior for optimization of a parametric image generator using this loss in a deepdream like procedure we optimize a randomly initialized 3d model a neural radiance field or nerf via gradient descent such that its 2d renderings from random angles achieve a low loss the resulting 3d model of the given text can be viewed from any angle relit by arbitrary illumination or composited into any 3d environment our approach requires no 3d training data and no modifications to the image diffusion model demonstrating the effectiveness of pretrained image diffusion models as priors
2022,audiogen textually guided audio generation,felix kreuk et al,https://arxiv.org/abs/2209.15352,audio,we tackle the problem of generating audio samples conditioned on descriptive text captions in this work we propose aaudiogen an auto regressive generative model that generates audio samples conditioned on text inputs audiogen operates on a learnt discrete audio representation the task of text to audio generation poses multiple challenges due to the way audio travels through a medium differentiating objects can be a difficult task e g separating multiple people simultaneously speaking this is further complicated by real world recording conditions e g background noise reverberation etc scarce text annotations impose another constraint limiting the ability to scale models finally modeling high fidelity audio requires encoding audio at high sampling rate leading to extremely long sequences to alleviate the aforementioned challenges we propose an augmentation technique that mixes different audio samples driving the model to internally learn to separate multiple sources we curated 10 datasets containing different types of audio and text annotations to handle the scarcity of text audio data points for faster inference we explore the use of multi stream modeling allowing the use of shorter sequences while maintaining a similar bitrate and perceptual quality we apply classifier free guidance to improve adherence to text comparing to the evaluated baselines audiogen outperforms over both objective and subjective metrics finally we explore the ability of the proposed method to generate audio continuation conditionally and unconditionally samples,audiogen textually guided audio generation felix kreuk et al we tackle the problem of generating audio samples conditioned on descriptive text captions in this work we propose aaudiogen an auto regressive generative model that generates audio samples conditioned on text inputs audiogen operates on a learnt discrete audio representation the task of text to audio generation poses multiple challenges due to the way audio travels through a medium differentiating objects can be a difficult task e g separating multiple people simultaneously speaking this is further complicated by real world recording conditions e g background noise reverberation etc scarce text annotations impose another constraint limiting the ability to scale models finally modeling high fidelity audio requires encoding audio at high sampling rate leading to extremely long sequences to alleviate the aforementioned challenges we propose an augmentation technique that mixes different audio samples driving the model to internally learn to separate multiple sources we curated 10 datasets containing different types of audio and text annotations to handle the scarcity of text audio data points for faster inference we explore the use of multi stream modeling allowing the use of shorter sequences while maintaining a similar bitrate and perceptual quality we apply classifier free guidance to improve adherence to text comparing to the evaluated baselines audiogen outperforms over both objective and subjective metrics finally we explore the ability of the proposed method to generate audio continuation conditionally and unconditionally samples
2022,on distillation of guided diffusion models,chenlin meng et al,https://arxiv.org/abs/2210.03142,vision,classifier free guided diffusion models have recently been shown to be highly effective at high resolution image generation and they have been widely used in large scale diffusion frameworks including dalle 2 stable diffusion and imagen however a downside of classifier free guided diffusion models is that they are computationally expensive at inference time since they require evaluating two diffusion models a class conditional model and an unconditional model tens to hundreds of times to deal with this limitation we propose an approach to distilling classifier free guided diffusion models into models that are fast to sample from given a pre trained classifier free guided model we first learn a single model to match the output of the combined conditional and unconditional models and then we progressively distill that model to a diffusion model that requires much fewer sampling steps for standard diffusion models trained on the pixel space our approach is able to generate images visually comparable to that of the original model using as few as 4 sampling steps on imagenet 64x64 and cifar 10 achieving fid is scores comparable to that of the original model while being up to 256 times faster to sample from for diffusion models trained on the latent space e g stable diffusion our approach is able to generate high fidelity images using as few as 1 to 4 denoising steps accelerating inference by at least 10 fold compared to existing methods on imagenet 256x256 and laion datasets we further demonstrate the effectiveness of our approach on text guided image editing and inpainting where our distilled model is able to generate high quality results using as few as 2 4 denoising steps,on distillation of guided diffusion models chenlin meng et al classifier free guided diffusion models have recently been shown to be highly effective at high resolution image generation and they have been widely used in large scale diffusion frameworks including dalle 2 stable diffusion and imagen however a downside of classifier free guided diffusion models is that they are computationally expensive at inference time since they require evaluating two diffusion models a class conditional model and an unconditional model tens to hundreds of times to deal with this limitation we propose an approach to distilling classifier free guided diffusion models into models that are fast to sample from given a pre trained classifier free guided model we first learn a single model to match the output of the combined conditional and unconditional models and then we progressively distill that model to a diffusion model that requires much fewer sampling steps for standard diffusion models trained on the pixel space our approach is able to generate images visually comparable to that of the original model using as few as 4 sampling steps on imagenet 64x64 and cifar 10 achieving fid is scores comparable to that of the original model while being up to 256 times faster to sample from for diffusion models trained on the latent space e g stable diffusion our approach is able to generate high fidelity images using as few as 1 to 4 denoising steps accelerating inference by at least 10 fold compared to existing methods on imagenet 256x256 and laion datasets we further demonstrate the effectiveness of our approach on text guided image editing and inpainting where our distilled model is able to generate high quality results using as few as 2 4 denoising steps
2022,react synergizing reasoning and acting in language models,shunyu yao et al,https://arxiv.org/abs/2210.03629,dl_nlp,while large language models llms have demonstrated impressive capabilities across tasks in language understanding and interactive decision making their abilities for reasoning e g chain of thought prompting and acting e g action plan generation have primarily been studied as separate topics in this paper we explore the use of llms to generate both reasoning traces and task specific actions in an interleaved manner allowing for greater synergy between the two reasoning traces help the model induce track and update action plans as well as handle exceptions while actions allow it to interface with external sources such as knowledge bases or environments to gather additional information we apply our approach named react to a diverse set of language and decision making tasks and demonstrate its effectiveness over state of the art baselines as well as improved human interpretability and trustworthiness over methods without reasoning or acting components concretely on question answering hotpotqa and fact verification fever react overcomes issues of hallucination and error propagation prevalent in chain of thought reasoning by interacting with a simple wikipedia api and generates human like task solving trajectories that are more interpretable than baselines without reasoning traces on two interactive decision making benchmarks alfworld and webshop react outperforms imitation and reinforcement learning methods by an absolute success rate of 34 and 10 respectively while being prompted with only one or two in context examples project site with code,react synergizing reasoning and acting in language models shunyu yao et al while large language models llms have demonstrated impressive capabilities across tasks in language understanding and interactive decision making their abilities for reasoning e g chain of thought prompting and acting e g action plan generation have primarily been studied as separate topics in this paper we explore the use of llms to generate both reasoning traces and task specific actions in an interleaved manner allowing for greater synergy between the two reasoning traces help the model induce track and update action plans as well as handle exceptions while actions allow it to interface with external sources such as knowledge bases or environments to gather additional information we apply our approach named react to a diverse set of language and decision making tasks and demonstrate its effectiveness over state of the art baselines as well as improved human interpretability and trustworthiness over methods without reasoning or acting components concretely on question answering hotpotqa and fact verification fever react overcomes issues of hallucination and error propagation prevalent in chain of thought reasoning by interacting with a simple wikipedia api and generates human like task solving trajectories that are more interpretable than baselines without reasoning traces on two interactive decision making benchmarks alfworld and webshop react outperforms imitation and reinforcement learning methods by an absolute success rate of 34 and 10 respectively while being prompted with only one or two in context examples project site with code
2022,laion 5b an open large scale dataset for training next generation image text models,christoph schuhmann et al,https://arxiv.org/abs/2210.08402,vision,groundbreaking language vision architectures like clip and dall e proved the utility of training on large amounts of noisy image text data without relying on expensive accurate labels used in standard vision unimodal supervised learning the resulting models showed capabilities of strong text guided image generation and transfer to downstream tasks while performing remarkably at zero shot classification with noteworthy out of distribution robustness since then large scale language vision models like align basic glide flamingo and imagen made further improvements studying the training and capabilities of such models requires datasets containing billions of image text pairs until now no datasets of this size have been made openly available for the broader research community to address this problem and democratize research on large scale multi modal models we present laion 5b a dataset consisting of 5 85 billion clip filtered image text pairs of which 2 32b contain english language we show successful replication and fine tuning of foundational models like clip glide and stable diffusion using the dataset and discuss further experiments enabled with an openly available dataset of this scale additionally we provide several nearest neighbor indices an improved web interface for dataset exploration and subset generation and detection scores for watermark nsfw and toxic content detection announcement page,laion 5b an open large scale dataset for training next generation image text models christoph schuhmann et al groundbreaking language vision architectures like clip and dall e proved the utility of training on large amounts of noisy image text data without relying on expensive accurate labels used in standard vision unimodal supervised learning the resulting models showed capabilities of strong text guided image generation and transfer to downstream tasks while performing remarkably at zero shot classification with noteworthy out of distribution robustness since then large scale language vision models like align basic glide flamingo and imagen made further improvements studying the training and capabilities of such models requires datasets containing billions of image text pairs until now no datasets of this size have been made openly available for the broader research community to address this problem and democratize research on large scale multi modal models we present laion 5b a dataset consisting of 5 85 billion clip filtered image text pairs of which 2 32b contain english language we show successful replication and fine tuning of foundational models like clip glide and stable diffusion using the dataset and discuss further experiments enabled with an openly available dataset of this scale additionally we provide several nearest neighbor indices an improved web interface for dataset exploration and subset generation and detection scores for watermark nsfw and toxic content detection announcement page
2022,imagic text based real image editing with diffusion models,bahjat kawar et al,https://arxiv.org/abs/2210.09276,vision,text conditioned image editing has recently attracted considerable interest however most methods are currently either limited to specific editing types e g object overlay style transfer or apply to synthetically generated images or require multiple input images of a common object in this paper we demonstrate for the very first time the ability to apply complex e g non rigid text guided semantic edits to a single real image for example we can change the posture and composition of one or multiple objects inside an image while preserving its original characteristics our method can make a standing dog sit down or jump cause a bird to spread its wings etc each within its single high resolution natural image provided by the user contrary to previous work our proposed method requires only a single input image and a target text the desired edit it operates on real images and does not require any additional inputs such as image masks or additional views of the object our method which we call imagic leverages a pre trained text to image diffusion model for this task it produces a text embedding that aligns with both the input image and the target text while fine tuning the diffusion model to capture the image specific appearance we demonstrate the quality and versatility of our method on numerous inputs from various domains showcasing a plethora of high quality complex semantic image edits all within a single unified framework,imagic text based real image editing with diffusion models bahjat kawar et al text conditioned image editing has recently attracted considerable interest however most methods are currently either limited to specific editing types e g object overlay style transfer or apply to synthetically generated images or require multiple input images of a common object in this paper we demonstrate for the very first time the ability to apply complex e g non rigid text guided semantic edits to a single real image for example we can change the posture and composition of one or multiple objects inside an image while preserving its original characteristics our method can make a standing dog sit down or jump cause a bird to spread its wings etc each within its single high resolution natural image provided by the user contrary to previous work our proposed method requires only a single input image and a target text the desired edit it operates on real images and does not require any additional inputs such as image masks or additional views of the object our method which we call imagic leverages a pre trained text to image diffusion model for this task it produces a text embedding that aligns with both the input image and the target text while fine tuning the diffusion model to capture the image specific appearance we demonstrate the quality and versatility of our method on numerous inputs from various domains showcasing a plethora of high quality complex semantic image edits all within a single unified framework
2022,high fidelity neural audio compression encodec,alexandre défossez et al,https://arxiv.org/abs/2210.13438,audio,we introduce a state of the art real time high fidelity audio codec leveraging neural networks it consists in a streaming encoder decoder architecture with quantized latent space trained in an end to end fashion we simplify and speed up the training by using a single multiscale spectrogram adversary that efficiently reduces artifacts and produce high quality samples we introduce a novel loss balancer mechanism to stabilize training the weight of a loss now defines the fraction of the overall gradient it should represent thus decoupling the choice of this hyper parameter from the typical scale of the loss finally we study how lightweight transformer models can be used to further compress the obtained representation by up to 40 while staying faster than real time we provide a detailed description of the key design choices of the proposed model including training objective architectural changes and a study of various perceptual loss functions we present an extensive subjective evaluation mushra tests together with an ablation study for a range of bandwidths and audio domains including speech noisy reverberant speech and music our approach is superior to the baselines methods across all evaluated settings considering both 24 khz monophonic and 48 khz stereophonic audio code and models are available at github com facebookresearch encodec,high fidelity neural audio compression encodec alexandre défossez et al we introduce a state of the art real time high fidelity audio codec leveraging neural networks it consists in a streaming encoder decoder architecture with quantized latent space trained in an end to end fashion we simplify and speed up the training by using a single multiscale spectrogram adversary that efficiently reduces artifacts and produce high quality samples we introduce a novel loss balancer mechanism to stabilize training the weight of a loss now defines the fraction of the overall gradient it should represent thus decoupling the choice of this hyper parameter from the typical scale of the loss finally we study how lightweight transformer models can be used to further compress the obtained representation by up to 40 while staying faster than real time we provide a detailed description of the key design choices of the proposed model including training objective architectural changes and a study of various perceptual loss functions we present an extensive subjective evaluation mushra tests together with an ablation study for a range of bandwidths and audio domains including speech noisy reverberant speech and music our approach is superior to the baselines methods across all evaluated settings considering both 24 khz monophonic and 48 khz stereophonic audio code and models are available at github com facebookresearch encodec
2022,bloom a 176b parameter open access multilingual language model,bigscience workshop,https://arxiv.org/abs/2211.05100,dl_nlp,large language models llms have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions while these capabilities have led to widespread adoption most llms are developed by resource rich organizations and are frequently kept from the public as a step towards democratizing this powerful technology we present bloom a 176b parameter open access language model designed and built thanks to a collaboration of hundreds of researchers bloom is a decoder only transformer language model that was trained on the roots corpus a dataset comprising hundreds of sources in 46 natural and 13 programming languages 59 in total we find that bloom achieves competitive performance on a wide variety of benchmarks with stronger results after undergoing multitask prompted finetuning to facilitate future research and applications using llms we publicly release our models and code under the responsible ai license,bloom a 176b parameter open access multilingual language model bigscience workshop large language models llms have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions while these capabilities have led to widespread adoption most llms are developed by resource rich organizations and are frequently kept from the public as a step towards democratizing this powerful technology we present bloom a 176b parameter open access language model designed and built thanks to a collaboration of hundreds of researchers bloom is a decoder only transformer language model that was trained on the roots corpus a dataset comprising hundreds of sources in 46 natural and 13 programming languages 59 in total we find that bloom achieves competitive performance on a wide variety of benchmarks with stronger results after undergoing multitask prompted finetuning to facilitate future research and applications using llms we publicly release our models and code under the responsible ai license
2022,diffusiondet diffusion model for object detection,shoufa chen et al,https://arxiv.org/abs/2211.09788,vision,we propose diffusiondet a new framework that formulates object detection as a denoising diffusion process from noisy boxes to object boxes during the training stage object boxes diffuse from ground truth boxes to random distribution and the model learns to reverse this noising process in inference the model refines a set of randomly generated boxes to the output results in a progressive way our work possesses an appealing property of flexibility which enables the dynamic number of boxes and iterative evaluation the extensive experiments on the standard benchmarks show that diffusiondet achieves favorable performance compared to previous well established detectors for example diffusiondet achieves 5 3 ap and 4 8 ap gains when evaluated with more boxes and iteration steps under a zero shot transfer setting from coco to crowdhuman our code is available at,diffusiondet diffusion model for object detection shoufa chen et al we propose diffusiondet a new framework that formulates object detection as a denoising diffusion process from noisy boxes to object boxes during the training stage object boxes diffuse from ground truth boxes to random distribution and the model learns to reverse this noising process in inference the model refines a set of randomly generated boxes to the output results in a progressive way our work possesses an appealing property of flexibility which enables the dynamic number of boxes and iterative evaluation the extensive experiments on the standard benchmarks show that diffusiondet achieves favorable performance compared to previous well established detectors for example diffusiondet achieves 5 3 ap and 4 8 ap gains when evaluated with more boxes and iteration steps under a zero shot transfer setting from coco to crowdhuman our code is available at
2022,instructpix2pix learning to follow image editing instructions,tim brooks et al,https://arxiv.org/abs/2211.09800,vision,we propose a method for editing images from human instructions given an input image and a written instruction that tells the model what to do our model follows these instructions to edit the image to obtain training data for this problem we combine the knowledge of two large pretrained models a language model gpt 3 and a text to image model stable diffusion to generate a large dataset of image editing examples our conditional diffusion model instructpix2pix is trained on our generated data and generalizes to real images and user written instructions at inference time since it performs edits in the forward pass and does not require per example fine tuning or inversion our model edits images quickly in a matter of seconds we show compelling editing results for a diverse collection of input images and written instructions,instructpix2pix learning to follow image editing instructions tim brooks et al we propose a method for editing images from human instructions given an input image and a written instruction that tells the model what to do our model follows these instructions to edit the image to obtain training data for this problem we combine the knowledge of two large pretrained models a language model gpt 3 and a text to image model stable diffusion to generate a large dataset of image editing examples our conditional diffusion model instructpix2pix is trained on our generated data and generalizes to real images and user written instructions at inference time since it performs edits in the forward pass and does not require per example fine tuning or inversion our model edits images quickly in a matter of seconds we show compelling editing results for a diverse collection of input images and written instructions
2022,magic3d high resolution text to 3d content creation,chen hsuan lin et al,https://arxiv.org/abs/2211.10440,vision,dreamfusion has recently demonstrated the utility of a pre trained text to image diffusion model to optimize neural radiance fields nerf achieving remarkable text to 3d synthesis results however the method has two inherent limitations a extremely slow optimization of nerf and b low resolution image space supervision on nerf leading to low quality 3d models with a long processing time in this paper we address these limitations by utilizing a two stage optimization framework first we obtain a coarse model using a low resolution diffusion prior and accelerate with a sparse 3d hash grid structure using the coarse representation as the initialization we further optimize a textured 3d mesh model with an efficient differentiable renderer interacting with a high resolution latent diffusion model our method dubbed magic3d can create high quality 3d mesh models in 40 minutes which is 2x faster than dreamfusion reportedly taking 1 5 hours on average while also achieving higher resolution user studies show 61 7 raters to prefer our approach over dreamfusion together with the image conditioned generation capabilities we provide users with new ways to control 3d synthesis opening up new avenues to various creative applications,magic3d high resolution text to 3d content creation chen hsuan lin et al dreamfusion has recently demonstrated the utility of a pre trained text to image diffusion model to optimize neural radiance fields nerf achieving remarkable text to 3d synthesis results however the method has two inherent limitations a extremely slow optimization of nerf and b low resolution image space supervision on nerf leading to low quality 3d models with a long processing time in this paper we address these limitations by utilizing a two stage optimization framework first we obtain a coarse model using a low resolution diffusion prior and accelerate with a sparse 3d hash grid structure using the coarse representation as the initialization we further optimize a textured 3d mesh model with an efficient differentiable renderer interacting with a high resolution latent diffusion model our method dubbed magic3d can create high quality 3d mesh models in 40 minutes which is 2x faster than dreamfusion reportedly taking 1 5 hours on average while also achieving higher resolution user studies show 61 7 raters to prefer our approach over dreamfusion together with the image conditioned generation capabilities we provide users with new ways to control 3d synthesis opening up new avenues to various creative applications
2022,robust speech recognition via large scale weak supervision whisper,alec radford et al,https://arxiv.org/abs/2212.04356,audio,we study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet when scaled to 680000 hours of multilingual and multitask supervision the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero shot transfer setting without the need for any fine tuning when compared to humans the models approach their accuracy and robustness we are releasing models and inference code to serve as a foundation for further work on robust speech processing,robust speech recognition via large scale weak supervision whisper alec radford et al we study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet when scaled to 680000 hours of multilingual and multitask supervision the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero shot transfer setting without the need for any fine tuning when compared to humans the models approach their accuracy and robustness we are releasing models and inference code to serve as a foundation for further work on robust speech processing
2022,multi concept customization of text to image diffusion custom diffusion,nupur kumari et al,https://arxiv.org/abs/2212.04488,vision,while generative models produce high quality images of concepts learned from a large scale database a user often wishes to synthesize instantiations of their own concepts for example their family pets or items can we teach a model to quickly acquire a new concept given a few examples furthermore can we compose multiple new concepts together we propose custom diffusion an efficient method for augmenting existing text to image models we find that only optimizing a few parameters in the text to image conditioning mechanism is sufficiently powerful to represent new concepts while enabling fast tuning 6 minutes additionally we can jointly train for multiple concepts or combine multiple fine tuned models into one via closed form constrained optimization our fine tuned model generates variations of multiple new concepts and seamlessly composes them with existing concepts in novel settings our method outperforms or performs on par with several baselines and concurrent works in both qualitative and quantitative evaluations while being memory and computationally efficient,multi concept customization of text to image diffusion custom diffusion nupur kumari et al while generative models produce high quality images of concepts learned from a large scale database a user often wishes to synthesize instantiations of their own concepts for example their family pets or items can we teach a model to quickly acquire a new concept given a few examples furthermore can we compose multiple new concepts together we propose custom diffusion an efficient method for augmenting existing text to image models we find that only optimizing a few parameters in the text to image conditioning mechanism is sufficiently powerful to represent new concepts while enabling fast tuning 6 minutes additionally we can jointly train for multiple concepts or combine multiple fine tuned models into one via closed form constrained optimization our fine tuned model generates variations of multiple new concepts and seamlessly composes them with existing concepts in novel settings our method outperforms or performs on par with several baselines and concurrent works in both qualitative and quantitative evaluations while being memory and computationally efficient
2022,scalable diffusion models with transformers dit,william peebles and saining xie,https://arxiv.org/abs/2212.09748,vision,we explore a new class of diffusion models based on the transformer architecture we train latent diffusion models of images replacing the commonly used u net backbone with a transformer that operates on latent patches we analyze the scalability of our diffusion transformers dits through the lens of forward pass complexity as measured by gflops we find that dits with higher gflops through increased transformer depth width or increased number of input tokens consistently have lower fid in addition to possessing good scalability properties our largest dit xl 2 models outperform all prior diffusion models on the class conditional imagenet 512x512 and 256x256 benchmarks achieving a state of the art fid of 2 27 on the latter,scalable diffusion models with transformers dit william peebles and saining xie we explore a new class of diffusion models based on the transformer architecture we train latent diffusion models of images replacing the commonly used u net backbone with a transformer that operates on latent patches we analyze the scalability of our diffusion transformers dits through the lens of forward pass complexity as measured by gflops we find that dits with higher gflops through increased transformer depth width or increased number of input tokens consistently have lower fid in addition to possessing good scalability properties our largest dit xl 2 models outperform all prior diffusion models on the class conditional imagenet 512x512 and 256x256 benchmarks achieving a state of the art fid of 2 27 on the latter
2022,large language models encode clinical knowledge,singhal et al,https://arxiv.org/abs/2212.13138,dl_nlp,large language models llms have demonstrated impressive capabilities in natural language understanding and generation but the quality bar for medical and clinical applications is high today attempts to assess models clinical knowledge typically rely on automated evaluations on limited benchmarks there is no standard to evaluate model predictions and reasoning across a breadth of tasks to address this we present multimedqa a benchmark combining six existing open question answering datasets spanning professional medical exams research and consumer queries and healthsearchqa a new free response dataset of medical questions searched online we propose a framework for human evaluation of model answers along multiple axes including factuality precision possible harm and bias in addition we evaluate palm a 540 billion parameter llm and its instruction tuned variant flan palm on multimedqa using a combination of prompting strategies flan palm achieves state of the art accuracy on every multimedqa multiple choice dataset medqa medmcqa pubmedqa mmlu clinical topics including 67 6 accuracy on medqa us medical license exam questions surpassing prior state of the art by over 17 however human evaluation reveals key gaps in flan palm responses to resolve this we introduce instruction prompt tuning a parameter efficient approach for aligning llms to new domains using a few exemplars the resulting model med palm performs encouragingly but remains inferior to clinicians we show that comprehension recall of knowledge and medical reasoning improve with model scale and instruction prompt tuning suggesting the potential utility of llms in medicine our human evaluations reveal important limitations of today s models reinforcing the importance of both evaluation frameworks and method development in creating safe helpful llm models for clinical applications,large language models encode clinical knowledge singhal et al large language models llms have demonstrated impressive capabilities in natural language understanding and generation but the quality bar for medical and clinical applications is high today attempts to assess models clinical knowledge typically rely on automated evaluations on limited benchmarks there is no standard to evaluate model predictions and reasoning across a breadth of tasks to address this we present multimedqa a benchmark combining six existing open question answering datasets spanning professional medical exams research and consumer queries and healthsearchqa a new free response dataset of medical questions searched online we propose a framework for human evaluation of model answers along multiple axes including factuality precision possible harm and bias in addition we evaluate palm a 540 billion parameter llm and its instruction tuned variant flan palm on multimedqa using a combination of prompting strategies flan palm achieves state of the art accuracy on every multimedqa multiple choice dataset medqa medmcqa pubmedqa mmlu clinical topics including 67 6 accuracy on medqa us medical license exam questions surpassing prior state of the art by over 17 however human evaluation reveals key gaps in flan palm responses to resolve this we introduce instruction prompt tuning a parameter efficient approach for aligning llms to new domains using a few exemplars the resulting model med palm performs encouragingly but remains inferior to clinicians we show that comprehension recall of knowledge and medical reasoning improve with model scale and instruction prompt tuning suggesting the potential utility of llms in medicine our human evaluations reveal important limitations of today s models reinforcing the importance of both evaluation frameworks and method development in creating safe helpful llm models for clinical applications
2023,muse text to image generation via masked generative transformers,huiwen chang et al,https://arxiv.org/abs/2301.00704,vision,we present muse a text to image transformer model that achieves state of the art image generation performance while being significantly more efficient than diffusion or autoregressive models muse is trained on a masked modeling task in discrete token space given the text embedding extracted from a pre trained large language model llm muse is trained to predict randomly masked image tokens compared to pixel space diffusion models such as imagen and dall e 2 muse is significantly more efficient due to the use of discrete tokens and requiring fewer sampling iterations compared to autoregressive models such as parti muse is more efficient due to the use of parallel decoding the use of a pre trained llm enables fine grained language understanding translating to high fidelity image generation and the understanding of visual concepts such as objects their spatial relationships pose cardinality etc our 900m parameter model achieves a new sota on cc3m with an fid score of 6 06 the muse 3b parameter model achieves an fid of 7 88 on zero shot coco evaluation along with a clip score of 0 32 muse also directly enables a number of image editing applications without the need to fine tune or invert the model inpainting outpainting and mask free editing more results are available at,muse text to image generation via masked generative transformers huiwen chang et al we present muse a text to image transformer model that achieves state of the art image generation performance while being significantly more efficient than diffusion or autoregressive models muse is trained on a masked modeling task in discrete token space given the text embedding extracted from a pre trained large language model llm muse is trained to predict randomly masked image tokens compared to pixel space diffusion models such as imagen and dall e 2 muse is significantly more efficient due to the use of discrete tokens and requiring fewer sampling iterations compared to autoregressive models such as parti muse is more efficient due to the use of parallel decoding the use of a pre trained llm enables fine grained language understanding translating to high fidelity image generation and the understanding of visual concepts such as objects their spatial relationships pose cardinality etc our 900m parameter model achieves a new sota on cc3m with an fid score of 6 06 the muse 3b parameter model achieves an fid of 7 88 on zero shot coco evaluation along with a clip score of 0 32 muse also directly enables a number of image editing applications without the need to fine tune or invert the model inpainting outpainting and mask free editing more results are available at
2023,neural codec language models are zero shot text to speech synthesizers vall e,chengyi wang et al,https://arxiv.org/abs/2301.02111,audio,we introduce a language modeling approach for text to speech synthesis tts specifically we train a neural codec language model called vall e using discrete codes derived from an off the shelf neural audio codec model and regard tts as a conditional language modeling task rather than continuous signal regression as in previous work during the pre training stage we scale up the tts training data to 60k hours of english speech which is hundreds of times larger than existing systems vall e emerges in context learning capabilities and can be used to synthesize high quality personalized speech with only a 3 second enrolled recording of an unseen speaker as an acoustic prompt experiment results show that vall e significantly outperforms the state of the art zero shot tts system in terms of speech naturalness and speaker similarity in addition we find vall e could preserve the speaker s emotion and acoustic environment of the acoustic prompt in synthesis see for demos of our work,neural codec language models are zero shot text to speech synthesizers vall e chengyi wang et al we introduce a language modeling approach for text to speech synthesis tts specifically we train a neural codec language model called vall e using discrete codes derived from an off the shelf neural audio codec model and regard tts as a conditional language modeling task rather than continuous signal regression as in previous work during the pre training stage we scale up the tts training data to 60k hours of english speech which is hundreds of times larger than existing systems vall e emerges in context learning capabilities and can be used to synthesize high quality personalized speech with only a 3 second enrolled recording of an unseen speaker as an acoustic prompt experiment results show that vall e significantly outperforms the state of the art zero shot tts system in terms of speech naturalness and speaker similarity in addition we find vall e could preserve the speaker s emotion and acoustic environment of the acoustic prompt in synthesis see for demos of our work
2023,mastering diverse domains through world models dreamerv3,danijar hafner et al,https://arxiv.org/abs/2301.04104,dl_rl,developing a general algorithm that learns to solve tasks across a wide range of applications has been a fundamental challenge in artificial intelligence although current reinforcement learning algorithms can be readily applied to tasks similar to what they have been developed for configuring them for new application domains requires significant human expertise and experimentation we present dreamerv3 a general algorithm that outperforms specialized methods across over 150 diverse tasks with a single configuration dreamer learns a model of the environment and improves its behavior by imagining future scenarios robustness techniques based on normalization balancing and transformations enable stable learning across domains applied out of the box dreamer is the first algorithm to collect diamonds in minecraft from scratch without human data or curricula this achievement has been posed as a significant challenge in artificial intelligence that requires exploring farsighted strategies from pixels and sparse rewards in an open world our work allows solving challenging control problems without extensive experimentation making reinforcement learning broadly applicable,mastering diverse domains through world models dreamerv3 danijar hafner et al developing a general algorithm that learns to solve tasks across a wide range of applications has been a fundamental challenge in artificial intelligence although current reinforcement learning algorithms can be readily applied to tasks similar to what they have been developed for configuring them for new application domains requires significant human expertise and experimentation we present dreamerv3 a general algorithm that outperforms specialized methods across over 150 diverse tasks with a single configuration dreamer learns a model of the environment and improves its behavior by imagining future scenarios robustness techniques based on normalization balancing and transformations enable stable learning across domains applied out of the box dreamer is the first algorithm to collect diamonds in minecraft from scratch without human data or curricula this achievement has been posed as a significant challenge in artificial intelligence that requires exploring farsighted strategies from pixels and sparse rewards in an open world our work allows solving challenging control problems without extensive experimentation making reinforcement learning broadly applicable
2023,detectgpt zero shot machine generated text detection using probability curvature,eric mitchell et al,https://arxiv.org/abs/2301.11305,dl_nlp,the increasing fluency and widespread usage of large language models llms highlight the desirability of corresponding tools aiding detection of llm generated text in this paper we identify a property of the structure of an llm s probability function that is useful for such detection specifically we demonstrate that text sampled from an llm tends to occupy negative curvature regions of the model s log probability function leveraging this observation we then define a new curvature based criterion for judging if a passage is generated from a given llm this approach which we call detectgpt does not require training a separate classifier collecting a dataset of real or generated passages or explicitly watermarking generated text it uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre trained language model e g t5 we find detectgpt is more discriminative than existing zero shot methods for model sample detection notably improving detection of fake news articles generated by 20b parameter gpt neox from 0 81 auroc for the strongest zero shot baseline to 0 95 auroc for detectgpt see for code data and other project information,detectgpt zero shot machine generated text detection using probability curvature eric mitchell et al the increasing fluency and widespread usage of large language models llms highlight the desirability of corresponding tools aiding detection of llm generated text in this paper we identify a property of the structure of an llm s probability function that is useful for such detection specifically we demonstrate that text sampled from an llm tends to occupy negative curvature regions of the model s log probability function leveraging this observation we then define a new curvature based criterion for judging if a passage is generated from a given llm this approach which we call detectgpt does not require training a separate classifier collecting a dataset of real or generated passages or explicitly watermarking generated text it uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre trained language model e g t5 we find detectgpt is more discriminative than existing zero shot methods for model sample detection notably improving detection of fake news articles generated by 20b parameter gpt neox from 0 81 auroc for the strongest zero shot baseline to 0 95 auroc for detectgpt see for code data and other project information
2023,musiclm generating music from text,andrea agostinelli et al,https://arxiv.org/abs/2301.11325,audio,we introduce musiclm a model generating high fidelity music from text descriptions such as a calming violin melody backed by a distorted guitar riff musiclm casts the process of conditional music generation as a hierarchical sequence to sequence modeling task and it generates music at 24 khz that remains consistent over several minutes our experiments show that musiclm outperforms previous systems both in audio quality and adherence to the text description moreover we demonstrate that musiclm can be conditioned on both text and a melody in that it can transform whistled and hummed melodies according to the style described in a text caption to support future research we publicly release musiccaps a dataset composed of 5 5k music text pairs with rich text descriptions provided by human experts,musiclm generating music from text andrea agostinelli et al we introduce musiclm a model generating high fidelity music from text descriptions such as a calming violin melody backed by a distorted guitar riff musiclm casts the process of conditional music generation as a hierarchical sequence to sequence modeling task and it generates music at 24 khz that remains consistent over several minutes our experiments show that musiclm outperforms previous systems both in audio quality and adherence to the text description moreover we demonstrate that musiclm can be conditioned on both text and a melody in that it can transform whistled and hummed melodies according to the style described in a text caption to support future research we publicly release musiccaps a dataset composed of 5 5k music text pairs with rich text descriptions provided by human experts
2023,audioldm text to audio generation with latent diffusion models,haohe liu et al,https://arxiv.org/abs/2301.12503,audio,text to audio tta system has recently gained attention for its ability to synthesize general audio based on text descriptions however previous studies in tta have limited generation quality with high computational costs in this study we propose audioldm a tta system that is built on a latent space to learn the continuous audio representations from contrastive language audio pretraining clap latents the pretrained clap models enable us to train ldms with audio embedding while providing text embedding as a condition during sampling by learning the latent representations of audio signals and their compositions without modeling the cross modal relationship audioldm is advantageous in both generation quality and computational efficiency trained on audiocaps with a single gpu audioldm achieves state of the art tta performance measured by both objective and subjective metrics e g frechet distance moreover audioldm is the first tta system that enables various text guided audio manipulations e g style transfer in a zero shot fashion our implementation and demos are available at,audioldm text to audio generation with latent diffusion models haohe liu et al text to audio tta system has recently gained attention for its ability to synthesize general audio based on text descriptions however previous studies in tta have limited generation quality with high computational costs in this study we propose audioldm a tta system that is built on a latent space to learn the continuous audio representations from contrastive language audio pretraining clap latents the pretrained clap models enable us to train ldms with audio embedding while providing text embedding as a condition during sampling by learning the latent representations of audio signals and their compositions without modeling the cross modal relationship audioldm is advantageous in both generation quality and computational efficiency trained on audiocaps with a single gpu audioldm achieves state of the art tta performance measured by both objective and subjective metrics e g frechet distance moreover audioldm is the first tta system that enables various text guided audio manipulations e g style transfer in a zero shot fashion our implementation and demos are available at
2023,grounding large language models in interactive environments with online rl glam,thomas carta et al,https://arxiv.org/abs/2302.02662,dl_rl,recent works successfully leveraged large language models llm abilities to capture abstract knowledge about world s physics to solve decision making problems yet the alignment between llms knowledge and the environment can be wrong and limit functional competence due to lack of grounding in this paper we study an approach named glam to achieve this alignment through functional grounding we consider an agent using an llm as a policy that is progressively updated as the agent interacts with the environment leveraging online reinforcement learning to improve its performance to solve goals using an interactive textual environment designed to study higher level forms of functional grounding and a set of spatial and navigation tasks we study several scientific questions 1 can llms boost sample efficiency for online learning of various rl tasks 2 how can it boost different forms of generalization 3 what is the impact of online learning we study these questions by functionally grounding several variants size architecture of flan t5,grounding large language models in interactive environments with online rl glam thomas carta et al recent works successfully leveraged large language models llm abilities to capture abstract knowledge about world s physics to solve decision making problems yet the alignment between llms knowledge and the environment can be wrong and limit functional competence due to lack of grounding in this paper we study an approach named glam to achieve this alignment through functional grounding we consider an agent using an llm as a policy that is progressively updated as the agent interacts with the environment leveraging online reinforcement learning to improve its performance to solve goals using an interactive textual environment designed to study higher level forms of functional grounding and a set of spatial and navigation tasks we study several scientific questions 1 can llms boost sample efficiency for online learning of various rl tasks 2 how can it boost different forms of generalization 3 what is the impact of online learning we study these questions by functionally grounding several variants size architecture of flan t5
2023,efficient online reinforcement learning with offline data rlpd,tatsuya matsushima et al,https://arxiv.org/abs/2302.02948,dl_rl,sample efficiency and exploration remain major challenges in online reinforcement learning rl a powerful approach that can be applied to address these issues is the inclusion of offline data such as prior trajectories from a human expert or a sub optimal exploration policy previous methods have relied on extensive modifications and additional complexity to ensure the effective use of this data instead we ask can we simply apply existing off policy methods to leverage offline data when learning online in this work we demonstrate that the answer is yes however a set of minimal but important changes to existing off policy rl algorithms are required to achieve reliable performance we extensively ablate these design choices demonstrating the key factors that most affect performance and arrive at a set of recommendations that practitioners can readily apply whether their data comprise a small number of expert demonstrations or large volumes of sub optimal trajectories we see that correct application of these simple recommendations can provide a mathbf 2 5 times improvement over existing approaches across a diverse set of competitive benchmarks with no additional computational overhead we have released our code at,efficient online reinforcement learning with offline data rlpd tatsuya matsushima et al sample efficiency and exploration remain major challenges in online reinforcement learning rl a powerful approach that can be applied to address these issues is the inclusion of offline data such as prior trajectories from a human expert or a sub optimal exploration policy previous methods have relied on extensive modifications and additional complexity to ensure the effective use of this data instead we ask can we simply apply existing off policy methods to leverage offline data when learning online in this work we demonstrate that the answer is yes however a set of minimal but important changes to existing off policy rl algorithms are required to achieve reliable performance we extensively ablate these design choices demonstrating the key factors that most affect performance and arrive at a set of recommendations that practitioners can readily apply whether their data comprise a small number of expert demonstrations or large volumes of sub optimal trajectories we see that correct application of these simple recommendations can provide a mathbf 2 5 times improvement over existing approaches across a diverse set of competitive benchmarks with no additional computational overhead we have released our code at
2023,structure and content guided video synthesis with diffusion models gen 1,omer bar tal et al,https://arxiv.org/abs/2302.03011,vision,text guided generative diffusion models unlock powerful image creation and editing tools while these have been extended to video generation current approaches that edit the content of existing footage while retaining structure require expensive re training for every input or rely on error prone propagation of image edits across frames in this work we present a structure and content guided video diffusion model that edits videos based on visual or textual descriptions of the desired output conflicts between user provided content edits and structure representations occur due to insufficient disentanglement between the two aspects as a solution we show that training on monocular depth estimates with varying levels of detail provides control over structure and content fidelity our model is trained jointly on images and videos which also exposes explicit control of temporal consistency through a novel guidance method our experiments demonstrate a wide variety of successes fine grained control over output characteristics customization based on a few reference images and a strong user preference towards results by our model,structure and content guided video synthesis with diffusion models gen 1 omer bar tal et al text guided generative diffusion models unlock powerful image creation and editing tools while these have been extended to video generation current approaches that edit the content of existing footage while retaining structure require expensive re training for every input or rely on error prone propagation of image edits across frames in this work we present a structure and content guided video diffusion model that edits videos based on visual or textual descriptions of the desired output conflicts between user provided content edits and structure representations occur due to insufficient disentanglement between the two aspects as a solution we show that training on monocular depth estimates with varying levels of detail provides control over structure and content fidelity our model is trained jointly on images and videos which also exposes explicit control of temporal consistency through a novel guidance method our experiments demonstrate a wide variety of successes fine grained control over output characteristics customization based on a few reference images and a strong user preference towards results by our model
2023,toolformer language models can teach themselves to use tools,timo schick et al,https://arxiv.org/abs/2302.04761,dl_nlp,language models lms exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions especially at scale they also paradoxically struggle with basic functionality such as arithmetic or factual lookup where much simpler and smaller models excel in this paper we show that lms can teach themselves to use external tools via simple apis and achieve the best of both worlds we introduce toolformer a model trained to decide which apis to call when to call them what arguments to pass and how to best incorporate the results into future token prediction this is done in a self supervised way requiring nothing more than a handful of demonstrations for each api we incorporate a range of tools including a calculator a q a system two different search engines a translation system and a calendar toolformer achieves substantially improved zero shot performance across a variety of downstream tasks often competitive with much larger models without sacrificing its core language modeling abilities,toolformer language models can teach themselves to use tools timo schick et al language models lms exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions especially at scale they also paradoxically struggle with basic functionality such as arithmetic or factual lookup where much simpler and smaller models excel in this paper we show that lms can teach themselves to use external tools via simple apis and achieve the best of both worlds we introduce toolformer a model trained to decide which apis to call when to call them what arguments to pass and how to best incorporate the results into future token prediction this is done in a self supervised way requiring nothing more than a handful of demonstrations for each api we incorporate a range of tools including a calculator a q a system two different search engines a translation system and a calendar toolformer achieves substantially improved zero shot performance across a variety of downstream tasks often competitive with much larger models without sacrificing its core language modeling abilities
2023,scaling vision transformers to 22 billion parameters,mostafa dehghani et al,https://arxiv.org/abs/2302.05442,vision,the scaling of transformers has driven breakthrough capabilities for language models at present the largest large language models llms contain upwards of 100b parameters vision transformers vit have introduced the same architecture to image and video modelling but these have not yet been successfully scaled to nearly the same degree the largest dense vit contains 4b parameters chen et al 2022 we present a recipe for highly efficient and stable training of a 22b parameter vit vit 22b and perform a wide variety of experiments on the resulting model when evaluated on downstream tasks often with a lightweight linear model on frozen features vit 22b demonstrates increasing performance with scale we further observe other interesting benefits of scale including an improved tradeoff between fairness and performance state of the art alignment to human visual perception in terms of shape texture bias and improved robustness vit 22b demonstrates the potential for llm like scaling in vision and provides key steps towards getting there,scaling vision transformers to 22 billion parameters mostafa dehghani et al the scaling of transformers has driven breakthrough capabilities for language models at present the largest large language models llms contain upwards of 100b parameters vision transformers vit have introduced the same architecture to image and video modelling but these have not yet been successfully scaled to nearly the same degree the largest dense vit contains 4b parameters chen et al 2022 we present a recipe for highly efficient and stable training of a 22b parameter vit vit 22b and perform a wide variety of experiments on the resulting model when evaluated on downstream tasks often with a lightweight linear model on frozen features vit 22b demonstrates increasing performance with scale we further observe other interesting benefits of scale including an improved tradeoff between fairness and performance state of the art alignment to human visual perception in terms of shape texture bias and improved robustness vit 22b demonstrates the potential for llm like scaling in vision and provides key steps towards getting there
2023,adding conditional control to text to image diffusion models controlnet,lvmin zhang et al,https://arxiv.org/abs/2302.05543,vision,we present controlnet a neural network architecture to add spatial conditioning controls to large pretrained text to image diffusion models controlnet locks the production ready large diffusion models and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls the neural architecture is connected with zero convolutions zero initialized convolution layers that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning we test various conditioning controls eg edges depth segmentation human pose etc with stable diffusion using single or multiple conditions with or without prompts we show that the training of controlnets is robust with small 1m datasets extensive results show that controlnet may facilitate wider applications to control image diffusion models,adding conditional control to text to image diffusion models controlnet lvmin zhang et al we present controlnet a neural network architecture to add spatial conditioning controls to large pretrained text to image diffusion models controlnet locks the production ready large diffusion models and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls the neural architecture is connected with zero convolutions zero initialized convolution layers that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning we test various conditioning controls eg edges depth segmentation human pose etc with stable diffusion using single or multiple conditions with or without prompts we show that the training of controlnets is robust with small 1m datasets extensive results show that controlnet may facilitate wider applications to control image diffusion models
2023,llama open and efficient foundation language models,hugo touvron et al,https://arxiv.org/abs/2302.13971,dl_nlp,we introduce llama a collection of foundation language models ranging from 7b to 65b parameters we train our models on trillions of tokens and show that it is possible to train state of the art models using publicly available datasets exclusively without resorting to proprietary and inaccessible datasets in particular llama 13b outperforms gpt 3 175b on most benchmarks and llama 65b is competitive with the best models chinchilla 70b and palm 540b we release all our models to the research community,llama open and efficient foundation language models hugo touvron et al we introduce llama a collection of foundation language models ranging from 7b to 65b parameters we train our models on trillions of tokens and show that it is possible to train state of the art models using publicly available datasets exclusively without resorting to proprietary and inaccessible datasets in particular llama 13b outperforms gpt 3 175b on most benchmarks and llama 65b is competitive with the best models chinchilla 70b and palm 540b we release all our models to the research community
2023,reward design with language models,natasha jaques et al,https://arxiv.org/abs/2303.00001,dl_rl,reward design in reinforcement learning rl is challenging since specifying human notions of desired behavior may be difficult via reward functions or require many expert demonstrations can we instead cheaply design rewards using a natural language interface this paper explores how to simplify reward design by prompting a large language model llm such as gpt 3 as a proxy reward function where the user provides a textual prompt containing a few examples few shot or a description zero shot of the desired behavior our approach leverages this proxy reward function in an rl framework specifically users specify a prompt once at the beginning of training during training the llm evaluates an rl agent s behavior against the desired behavior described by the prompt and outputs a corresponding reward signal the rl agent then uses this reward to update its behavior we evaluate whether our approach can train agents aligned with user objectives in the ultimatum game matrix games and the dealornodeal negotiation task in all three tasks we show that rl agents trained with our framework are well aligned with the user s objectives and outperform rl agents trained with reward functions learned via supervised learning,reward design with language models natasha jaques et al reward design in reinforcement learning rl is challenging since specifying human notions of desired behavior may be difficult via reward functions or require many expert demonstrations can we instead cheaply design rewards using a natural language interface this paper explores how to simplify reward design by prompting a large language model llm such as gpt 3 as a proxy reward function where the user provides a textual prompt containing a few examples few shot or a description zero shot of the desired behavior our approach leverages this proxy reward function in an rl framework specifically users specify a prompt once at the beginning of training during training the llm evaluates an rl agent s behavior against the desired behavior described by the prompt and outputs a corresponding reward signal the rl agent then uses this reward to update its behavior we evaluate whether our approach can train agents aligned with user objectives in the ultimatum game matrix games and the dealornodeal negotiation task in all three tasks we show that rl agents trained with our framework are well aligned with the user s objectives and outperform rl agents trained with reward functions learned via supervised learning
2023,google usm scaling automatic speech recognition beyond 100 languages,yu zhang et al,https://arxiv.org/abs/2303.01037,audio,we introduce the universal speech model usm a single large model that performs automatic speech recognition asr across 100 languages this is achieved by pre training the encoder of the model on a large unlabeled multilingual dataset of 12 million m hours spanning over 300 languages and fine tuning on a smaller labeled dataset we use multilingual pre training with random projection quantization and speech text modality matching to achieve state of the art performance on downstream multilingual asr and speech to text translation tasks we also demonstrate that despite using a labeled training set 1 7 th the size of that used for the whisper model our model exhibits comparable or better performance on both in domain and out of domain speech recognition tasks across many languages,google usm scaling automatic speech recognition beyond 100 languages yu zhang et al we introduce the universal speech model usm a single large model that performs automatic speech recognition asr across 100 languages this is achieved by pre training the encoder of the model on a large unlabeled multilingual dataset of 12 million m hours spanning over 300 languages and fine tuning on a smaller labeled dataset we use multilingual pre training with random projection quantization and speech text modality matching to achieve state of the art performance on downstream multilingual asr and speech to text translation tasks we also demonstrate that despite using a labeled training set 1 7 th the size of that used for the whisper model our model exhibits comparable or better performance on both in domain and out of domain speech recognition tasks across many languages
2023,visual chatgpt talking drawing and editing with visual foundation models,chenfei wu et al,https://arxiv.org/abs/2303.04671,vision,chatgpt is attracting a cross field interest as it provides a language interface with remarkable conversational competency and reasoning capabilities across many domains however since chatgpt is trained with languages it is currently not capable of processing or generating images from the visual world at the same time visual foundation models such as visual transformers or stable diffusion although showing great visual understanding and generation capabilities they are only experts on specific tasks with one round fixed inputs and outputs to this end we build a system called TEXTBF visual chatgpt TEXTBF incorporating different visual foundation models to enable the user to interact with chatgpt by 1 sending and receiving not only languages but also images 2 providing complex visual questions or visual editing instructions that require the collaboration of multiple ai models with multi steps 3 providing feedback and asking for corrected results we design a series of prompts to inject the visual model information into chatgpt considering models of multiple inputs outputs and models that require visual feedback experiments show that visual chatgpt opens the door to investigating the visual roles of chatgpt with the help of visual foundation models our system is publicly available at url,visual chatgpt talking drawing and editing with visual foundation models chenfei wu et al chatgpt is attracting a cross field interest as it provides a language interface with remarkable conversational competency and reasoning capabilities across many domains however since chatgpt is trained with languages it is currently not capable of processing or generating images from the visual world at the same time visual foundation models such as visual transformers or stable diffusion although showing great visual understanding and generation capabilities they are only experts on specific tasks with one round fixed inputs and outputs to this end we build a system called TEXTBF visual chatgpt TEXTBF incorporating different visual foundation models to enable the user to interact with chatgpt by 1 sending and receiving not only languages but also images 2 providing complex visual questions or visual editing instructions that require the collaboration of multiple ai models with multi steps 3 providing feedback and asking for corrected results we design a series of prompts to inject the visual model information into chatgpt considering models of multiple inputs outputs and models that require visual feedback experiments show that visual chatgpt opens the door to investigating the visual roles of chatgpt with the help of visual foundation models our system is publicly available at url
2023,scaling up gans for text to image synthesis gigagan,minguk kang et al,https://arxiv.org/abs/2303.05511,vision,the recent success of text to image synthesis has taken the world by storm and captured the general public s imagination from a technical standpoint it also marked a drastic change in the favored architecture to design generative image models gans used to be the de facto choice with techniques like stylegan with dall e 2 auto regressive and diffusion models became the new standard for large scale generative models overnight this rapid shift raises a fundamental question can we scale up gans to benefit from large datasets like laion we find that na ively increasing the capacity of the stylegan architecture quickly becomes unstable we introduce gigagan a new gan architecture that far exceeds this limit demonstrating gans as a viable option for text to image synthesis gigagan offers three major advantages first it is orders of magnitude faster at inference time taking only 0 13 seconds to synthesize a 512px image second it can synthesize high resolution images for example 16 megapixel pixels in 3 66 seconds finally gigagan supports various latent space editing applications such as latent interpolation style mixing and vector arithmetic operations,scaling up gans for text to image synthesis gigagan minguk kang et al the recent success of text to image synthesis has taken the world by storm and captured the general public s imagination from a technical standpoint it also marked a drastic change in the favored architecture to design generative image models gans used to be the de facto choice with techniques like stylegan with dall e 2 auto regressive and diffusion models became the new standard for large scale generative models overnight this rapid shift raises a fundamental question can we scale up gans to benefit from large datasets like laion we find that na ively increasing the capacity of the stylegan architecture quickly becomes unstable we introduce gigagan a new gan architecture that far exceeds this limit demonstrating gans as a viable option for text to image synthesis gigagan offers three major advantages first it is orders of magnitude faster at inference time taking only 0 13 seconds to synthesize a 512px image second it can synthesize high resolution images for example 16 megapixel pixels in 3 66 seconds finally gigagan supports various latent space editing applications such as latent interpolation style mixing and vector arithmetic operations
2023,stabilizing transformer training by preventing attention entropy collapse,shuangfei zhai et al,https://arxiv.org/abs/2303.06296,dl_nlp,training stability is of great importance to transformers in this work we investigate the training dynamics of transformers by examining the evolution of the attention layers in particular we track the attention entropy for each attention head during the course of training which is a proxy for model sharpness we identify a common pattern across different architectures and tasks where low attention entropy is accompanied by high training instability which can take the form of oscillating loss or divergence we denote the pathologically low attention entropy corresponding to highly concentrated attention scores as TEXTIT entropy collapse TEXTIT as a remedy we propose sigma reparam a simple and efficient solution where we reparametrize all linear layers with spectral normalization and an additional learned scalar we demonstrate that sigma reparam successfully prevents entropy collapse in the attention layers promoting more stable training additionally we prove a tight lower bound of the attention entropy which decreases exponentially fast with the spectral norm of the attention logits providing additional motivation for our approach we conduct experiments with sigma reparam on image classification image self supervised learning machine translation speech recognition and language modeling tasks we show that sigma reparam provides stability and robustness with respect to the choice of hyperparameters going so far as enabling training a a vision transformer to competitive performance without warmup weight decay layer normalization or adaptive optimizers b deep architectures in machine translation and c speech recognition to competitive performance without warmup and adaptive optimizers code is available at url,stabilizing transformer training by preventing attention entropy collapse shuangfei zhai et al training stability is of great importance to transformers in this work we investigate the training dynamics of transformers by examining the evolution of the attention layers in particular we track the attention entropy for each attention head during the course of training which is a proxy for model sharpness we identify a common pattern across different architectures and tasks where low attention entropy is accompanied by high training instability which can take the form of oscillating loss or divergence we denote the pathologically low attention entropy corresponding to highly concentrated attention scores as TEXTIT entropy collapse TEXTIT as a remedy we propose sigma reparam a simple and efficient solution where we reparametrize all linear layers with spectral normalization and an additional learned scalar we demonstrate that sigma reparam successfully prevents entropy collapse in the attention layers promoting more stable training additionally we prove a tight lower bound of the attention entropy which decreases exponentially fast with the spectral norm of the attention logits providing additional motivation for our approach we conduct experiments with sigma reparam on image classification image self supervised learning machine translation speech recognition and language modeling tasks we show that sigma reparam provides stability and robustness with respect to the choice of hyperparameters going so far as enabling training a a vision transformer to competitive performance without warmup weight decay layer normalization or adaptive optimizers b deep architectures in machine translation and c speech recognition to competitive performance without warmup and adaptive optimizers code is available at url
2023,sparks of artificial general intelligence early experiments with gpt 4,sébastien bubeck et al,https://arxiv.org/abs/2303.12712,dl_nlp,artificial intelligence ai researchers have been developing and refining large language models llms that exhibit remarkable capabilities across a variety of domains and tasks challenging our understanding of learning and cognition the latest model developed by openai gpt 4 was trained using an unprecedented scale of compute and data in this paper we report on our investigation of an early version of gpt 4 when it was still in active development by openai we contend that this early version of gpt 4 is part of a new cohort of llms along with chatgpt and google s palm for example that exhibit more general intelligence than previous ai models we discuss the rising capabilities and implications of these models we demonstrate that beyond its mastery of language gpt 4 can solve novel and difficult tasks that span mathematics coding vision medicine law psychology and more without needing any special prompting moreover in all of these tasks gpt 4 s performance is strikingly close to human level performance and often vastly surpasses prior models such as chatgpt given the breadth and depth of gpt 4 s capabilities we believe that it could reasonably be viewed as an early yet still incomplete version of an artificial general intelligence agi system in our exploration of gpt 4 we put special emphasis on discovering its limitations and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of agi including the possible need for pursuing a new paradigm that moves beyond next word prediction we conclude with reflections on societal influences of the recent technological leap and future research directions,sparks of artificial general intelligence early experiments with gpt 4 sébastien bubeck et al artificial intelligence ai researchers have been developing and refining large language models llms that exhibit remarkable capabilities across a variety of domains and tasks challenging our understanding of learning and cognition the latest model developed by openai gpt 4 was trained using an unprecedented scale of compute and data in this paper we report on our investigation of an early version of gpt 4 when it was still in active development by openai we contend that this early version of gpt 4 is part of a new cohort of llms along with chatgpt and google s palm for example that exhibit more general intelligence than previous ai models we discuss the rising capabilities and implications of these models we demonstrate that beyond its mastery of language gpt 4 can solve novel and difficult tasks that span mathematics coding vision medicine law psychology and more without needing any special prompting moreover in all of these tasks gpt 4 s performance is strikingly close to human level performance and often vastly surpasses prior models such as chatgpt given the breadth and depth of gpt 4 s capabilities we believe that it could reasonably be viewed as an early yet still incomplete version of an artificial general intelligence agi system in our exploration of gpt 4 we put special emphasis on discovering its limitations and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of agi including the possible need for pursuing a new paradigm that moves beyond next word prediction we conclude with reflections on societal influences of the recent technological leap and future research directions
2023,bloomberggpt a large language model for finance,shijie wu et al,https://arxiv.org/abs/2303.17564,dl_nlp,the use of nlp in the realm of financial technology is broad and complex with applications ranging from sentiment analysis and named entity recognition to question answering large language models llms have been shown to be effective on a variety of tasks however no llm specialized for the financial domain has been reported in literature in this work we present bloomberggpt a 50 billion parameter language model that is trained on a wide range of financial data we construct a 363 billion token dataset based on bloomberg s extensive data sources perhaps the largest domain specific dataset yet augmented with 345 billion tokens from general purpose datasets we validate bloomberggpt on standard llm benchmarks open financial benchmarks and a suite of internal benchmarks that most accurately reflect our intended usage our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general llm benchmarks additionally we explain our modeling choices training process and evaluation methodology we release training chronicles appendix c detailing our experience in training bloomberggpt,bloomberggpt a large language model for finance shijie wu et al the use of nlp in the realm of financial technology is broad and complex with applications ranging from sentiment analysis and named entity recognition to question answering large language models llms have been shown to be effective on a variety of tasks however no llm specialized for the financial domain has been reported in literature in this work we present bloomberggpt a 50 billion parameter language model that is trained on a wide range of financial data we construct a 363 billion token dataset based on bloomberg s extensive data sources perhaps the largest domain specific dataset yet augmented with 345 billion tokens from general purpose datasets we validate bloomberggpt on standard llm benchmarks open financial benchmarks and a suite of internal benchmarks that most accurately reflect our intended usage our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general llm benchmarks additionally we explain our modeling choices training process and evaluation methodology we release training chronicles appendix c detailing our experience in training bloomberggpt
2023,hugginggpt solving ai tasks with chatgpt and its friends in huggingface,yongliang shen et al,https://arxiv.org/abs/2303.17580,dl_nlp,solving complicated ai tasks with different domains and modalities is a key step toward artificial general intelligence while there are numerous ai models available for various domains and modalities they cannot handle complicated ai tasks autonomously considering large language models llms have exhibited exceptional abilities in language understanding generation interaction and reasoning we advocate that llms could act as a controller to manage existing ai models to solve complicated ai tasks with language serving as a generic interface to empower this based on this philosophy we present hugginggpt an llm powered agent that leverages llms e g chatgpt to connect various ai models in machine learning communities e g hugging face to solve ai tasks specifically we use chatgpt to conduct task planning when receiving a user request select models according to their function descriptions available in hugging face execute each subtask with the selected ai model and summarize the response according to the execution results by leveraging the strong language capability of chatgpt and abundant ai models in hugging face hugginggpt can tackle a wide range of sophisticated ai tasks spanning different modalities and domains and achieve impressive results in language vision speech and other challenging tasks which paves a new way towards the realization of artificial general intelligence,hugginggpt solving ai tasks with chatgpt and its friends in huggingface yongliang shen et al solving complicated ai tasks with different domains and modalities is a key step toward artificial general intelligence while there are numerous ai models available for various domains and modalities they cannot handle complicated ai tasks autonomously considering large language models llms have exhibited exceptional abilities in language understanding generation interaction and reasoning we advocate that llms could act as a controller to manage existing ai models to solve complicated ai tasks with language serving as a generic interface to empower this based on this philosophy we present hugginggpt an llm powered agent that leverages llms e g chatgpt to connect various ai models in machine learning communities e g hugging face to solve ai tasks specifically we use chatgpt to conduct task planning when receiving a user request select models according to their function descriptions available in hugging face execute each subtask with the selected ai model and summarize the response according to the execution results by leveraging the strong language capability of chatgpt and abundant ai models in hugging face hugginggpt can tackle a wide range of sophisticated ai tasks spanning different modalities and domains and achieve impressive results in language vision speech and other challenging tasks which paves a new way towards the realization of artificial general intelligence
2023,instruction tuning with gpt 4,baolin peng et al,https://arxiv.org/abs/2304.03277,dl_nlp,prior work has shown that finetuning large language models llms using machine generated instruction following data enables such models to achieve remarkable zero shot capabilities on new tasks and no human written instructions are needed in this paper we present the first attempt to use gpt 4 to generate instruction following data for llm finetuning our early experiments on instruction tuned llama models show that the 52k english and chinese instruction following data generated by gpt 4 leads to superior zero shot performance on new tasks to the instruction following data generated by previous state of the art models we also collect feedback and comparison data from gpt 4 to enable a comprehensive evaluation and reward model training we make our data generated using gpt 4 as well as our codebase publicly available,instruction tuning with gpt 4 baolin peng et al prior work has shown that finetuning large language models llms using machine generated instruction following data enables such models to achieve remarkable zero shot capabilities on new tasks and no human written instructions are needed in this paper we present the first attempt to use gpt 4 to generate instruction following data for llm finetuning our early experiments on instruction tuned llama models show that the 52k english and chinese instruction following data generated by gpt 4 leads to superior zero shot performance on new tasks to the instruction following data generated by previous state of the art models we also collect feedback and comparison data from gpt 4 to enable a comprehensive evaluation and reward model training we make our data generated using gpt 4 as well as our codebase publicly available
2023,generative agents interactive simulacra of human behavior,joon sung park et al,https://arxiv.org/abs/2304.03442,dl_nlp,believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools in this paper we introduce generative agents computational software agents that simulate believable human behavior generative agents wake up cook breakfast and head to work artists paint while authors write they form opinions notice each other and initiate conversations they remember and reflect on days past as they plan the next day to enable generative agents we describe an architecture that extends a large language model to store a complete record of the agent s experiences using natural language synthesize those memories over time into higher level reflections and retrieve them dynamically to plan behavior we instantiate generative agents to populate an interactive sandbox environment inspired by the sims where end users can interact with a small town of twenty five agents using natural language in an evaluation these generative agents produce believable individual and emergent social behaviors for example starting with only a single user specified notion that one agent wants to throw a valentine s day party the agents autonomously spread invitations to the party over the next two days make new acquaintances ask each other out on dates to the party and coordinate to show up for the party together at the right time we demonstrate through ablation that the components of our agent architecture observation planning and reflection each contribute critically to the believability of agent behavior by fusing large language models with computational interactive agents this work introduces architectural and interaction patterns for enabling believable simulations of human behavior,generative agents interactive simulacra of human behavior joon sung park et al believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools in this paper we introduce generative agents computational software agents that simulate believable human behavior generative agents wake up cook breakfast and head to work artists paint while authors write they form opinions notice each other and initiate conversations they remember and reflect on days past as they plan the next day to enable generative agents we describe an architecture that extends a large language model to store a complete record of the agent s experiences using natural language synthesize those memories over time into higher level reflections and retrieve them dynamically to plan behavior we instantiate generative agents to populate an interactive sandbox environment inspired by the sims where end users can interact with a small town of twenty five agents using natural language in an evaluation these generative agents produce believable individual and emergent social behaviors for example starting with only a single user specified notion that one agent wants to throw a valentine s day party the agents autonomously spread invitations to the party over the next two days make new acquaintances ask each other out on dates to the party and coordinate to show up for the party together at the right time we demonstrate through ablation that the components of our agent architecture observation planning and reflection each contribute critically to the believability of agent behavior by fusing large language models with computational interactive agents this work introduces architectural and interaction patterns for enabling believable simulations of human behavior
2023,dinov2 learning robust visual features without supervision,maxime oquab et al,https://arxiv.org/abs/2304.07193,vision,the recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision these models could greatly simplify the use of images in any system by producing all purpose visual features i e features that work across image distributions and tasks without finetuning this work shows that existing pretraining methods especially self supervised methods can produce such features if trained on enough curated data from diverse sources we revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size most of the technical contributions aim at accelerating and stabilizing the training at scale in terms of data we propose an automatic pipeline to build a dedicated diverse and curated image dataset instead of uncurated data as typically done in the self supervised literature in terms of models we train a vit model dosovitskiy et al 2020 with 1b parameters and distill it into a series of smaller models that surpass the best available all purpose features openclip ilharco et al 2021 on most of the benchmarks at image and pixel levels,dinov2 learning robust visual features without supervision maxime oquab et al the recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision these models could greatly simplify the use of images in any system by producing all purpose visual features i e features that work across image distributions and tasks without finetuning this work shows that existing pretraining methods especially self supervised methods can produce such features if trained on enough curated data from diverse sources we revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size most of the technical contributions aim at accelerating and stabilizing the training at scale in terms of data we propose an automatic pipeline to build a dedicated diverse and curated image dataset instead of uncurated data as typically done in the self supervised literature in terms of models we train a vit model dosovitskiy et al 2020 with 1b parameters and distill it into a series of smaller models that surpass the best available all purpose features openclip ilharco et al 2021 on most of the benchmarks at image and pixel levels
2023,synthetic data from diffusion models improves imagenet classification,shekoofeh azizi et al,https://arxiv.org/abs/2304.08466,vision,deep generative models are becoming increasingly powerful now generating diverse high fidelity photo realistic samples given text prompts have they reached the point where models of natural images can be used for generative data augmentation helping to improve challenging discriminative tasks we show that large scale text to image diffusion models can be fine tuned to produce class conditional models with sota fid 1 76 at 256x256 resolution and inception score 239 at 256x256 the model also yields a new sota in classification accuracy scores 64 96 for 256x256 generative samples improving to 69 24 for 1024x1024 samples augmenting the imagenet training set with samples from the resulting models yields significant improvements in imagenet classification accuracy over strong resnet and vision transformer baselines,synthetic data from diffusion models improves imagenet classification shekoofeh azizi et al deep generative models are becoming increasingly powerful now generating diverse high fidelity photo realistic samples given text prompts have they reached the point where models of natural images can be used for generative data augmentation helping to improve challenging discriminative tasks we show that large scale text to image diffusion models can be fine tuned to produce class conditional models with sota fid 1 76 at 256x256 resolution and inception score 239 at 256x256 the model also yields a new sota in classification accuracy scores 64 96 for 256x256 generative samples improving to 69 24 for 1024x1024 samples augmenting the imagenet training set with samples from the resulting models yields significant improvements in imagenet classification accuracy over strong resnet and vision transformer baselines
2023,visual instruction tuning,haotian liu et al,https://arxiv.org/abs/2304.08485,vision,instruction tuning large language models llms using machine generated instruction following data has improved zero shot capabilities on new tasks but the idea is less explored in the multimodal field in this paper we present the first attempt to use language only gpt 4 to generate multimodal language image instruction following data by instruction tuning on such generated data we introduce llava large language and vision assistant an end to end trained large multimodal model that connects a vision encoder and llm for general purpose visual and language understanding our early experiments show that llava demonstrates impressive multimodel chat abilities sometimes exhibiting the behaviors of multimodal gpt 4 on unseen images instructions and yields a 85 1 relative score compared with gpt 4 on a synthetic multimodal instruction following dataset when fine tuned on science qa the synergy of llava and gpt 4 achieves a new state of the art accuracy of 92 53 we make gpt 4 generated visual instruction tuning data our model and code base publicly available,visual instruction tuning haotian liu et al instruction tuning large language models llms using machine generated instruction following data has improved zero shot capabilities on new tasks but the idea is less explored in the multimodal field in this paper we present the first attempt to use language only gpt 4 to generate multimodal language image instruction following data by instruction tuning on such generated data we introduce llava large language and vision assistant an end to end trained large multimodal model that connects a vision encoder and llm for general purpose visual and language understanding our early experiments show that llava demonstrates impressive multimodel chat abilities sometimes exhibiting the behaviors of multimodal gpt 4 on unseen images instructions and yields a 85 1 relative score compared with gpt 4 on a synthetic multimodal instruction following dataset when fine tuned on science qa the synergy of llava and gpt 4 achieves a new state of the art accuracy of 92 53 we make gpt 4 generated visual instruction tuning data our model and code base publicly available
2023,align your latents high resolution video synthesis with latent diffusion models,andreas blattmann et al,https://arxiv.org/abs/2304.08818,vision,latent diffusion models ldms enable high quality image synthesis while avoiding excessive compute demands by training a diffusion model in a compressed lower dimensional latent space here we apply the ldm paradigm to high resolution video generation a particularly resource intensive task we first pre train an ldm on images only then we turn the image generator into a video generator by introducing a temporal dimension to the latent space diffusion model and fine tuning on encoded image sequences i e videos similarly we temporally align diffusion model upsamplers turning them into temporally consistent video super resolution models we focus on two relevant real world applications simulation of in the wild driving data and creative content creation with text to video modeling in particular we validate our video ldm on real driving videos of resolution 512 x 1024 achieving state of the art performance furthermore our approach can easily leverage off the shelf pre trained image ldms as we only need to train a temporal alignment model in that case doing so we turn the publicly available state of the art text to image ldm stable diffusion into an efficient and expressive text to video model with resolution up to 1280 x 2048 we show that the temporal layers trained in this way generalize to different fine tuned text to image ldms utilizing this property we show the first results for personalized text to video generation opening exciting directions for future content creation project page,align your latents high resolution video synthesis with latent diffusion models andreas blattmann et al latent diffusion models ldms enable high quality image synthesis while avoiding excessive compute demands by training a diffusion model in a compressed lower dimensional latent space here we apply the ldm paradigm to high resolution video generation a particularly resource intensive task we first pre train an ldm on images only then we turn the image generator into a video generator by introducing a temporal dimension to the latent space diffusion model and fine tuning on encoded image sequences i e videos similarly we temporally align diffusion model upsamplers turning them into temporally consistent video super resolution models we focus on two relevant real world applications simulation of in the wild driving data and creative content creation with text to video modeling in particular we validate our video ldm on real driving videos of resolution 512 x 1024 achieving state of the art performance furthermore our approach can easily leverage off the shelf pre trained image ldms as we only need to train a temporal alignment model in that case doing so we turn the publicly available state of the art text to image ldm stable diffusion into an efficient and expressive text to video model with resolution up to 1280 x 2048 we show that the temporal layers trained in this way generalize to different fine tuned text to image ldms utilizing this property we show the first results for personalized text to video generation opening exciting directions for future content creation project page
2023,segment anything in medical images,junlong cheng et al,https://arxiv.org/abs/2304.12306,vision,medical image segmentation is a critical component in clinical practice facilitating accurate diagnosis treatment planning and disease monitoring however existing methods often tailored to specific modalities or disease types lack generalizability across the diverse spectrum of medical image segmentation tasks here we present medsam a foundation model designed for bridging this gap by enabling universal medical image segmentation the model is developed on a large scale medical image dataset with 1570263 image mask pairs covering 10 imaging modalities and over 30 cancer types we conduct a comprehensive evaluation on 86 internal validation tasks and 60 external validation tasks demonstrating better accuracy and robustness than modality wise specialist models by delivering accurate and efficient segmentation across a wide spectrum of tasks medsam holds significant potential to expedite the evolution of diagnostic tools and the personalization of treatment plans,segment anything in medical images junlong cheng et al medical image segmentation is a critical component in clinical practice facilitating accurate diagnosis treatment planning and disease monitoring however existing methods often tailored to specific modalities or disease types lack generalizability across the diverse spectrum of medical image segmentation tasks here we present medsam a foundation model designed for bridging this gap by enabling universal medical image segmentation the model is developed on a large scale medical image dataset with 1570263 image mask pairs covering 10 imaging modalities and over 30 cancer types we conduct a comprehensive evaluation on 86 internal validation tasks and 60 external validation tasks demonstrating better accuracy and robustness than modality wise specialist models by delivering accurate and efficient segmentation across a wide spectrum of tasks medsam holds significant potential to expedite the evolution of diagnostic tools and the personalization of treatment plans
2023,palm 2 technical report,rohan anil et al,https://arxiv.org/abs/2305.10403,dl_nlp,we introduce palm 2 a new state of the art language model that has better multilingual and reasoning capabilities and is more compute efficient than its predecessor palm palm 2 is a transformer based model trained using a mixture of objectives through extensive evaluations on english and multilingual language and reasoning tasks we demonstrate that palm 2 has significantly improved quality on downstream tasks across different model sizes while simultaneously exhibiting faster and more efficient inference compared to palm this improved efficiency enables broader deployment while also allowing the model to respond faster for a more natural pace of interaction palm 2 demonstrates robust reasoning capabilities exemplified by large improvements over palm on big bench and other reasoning tasks palm 2 exhibits stable performance on a suite of responsible ai evaluations and enables inference time control over toxicity without additional overhead or impact on other capabilities overall palm 2 achieves state of the art performance across a diverse set of tasks and capabilities when discussing the palm 2 family it is important to distinguish between pre trained models of various sizes fine tuned variants of these models and the user facing products that use these models in particular user facing products typically include additional pre and post processing steps additionally the underlying models may evolve over time therefore one should not expect the performance of user facing products to exactly match the results reported in this report,palm 2 technical report rohan anil et al we introduce palm 2 a new state of the art language model that has better multilingual and reasoning capabilities and is more compute efficient than its predecessor palm palm 2 is a transformer based model trained using a mixture of objectives through extensive evaluations on english and multilingual language and reasoning tasks we demonstrate that palm 2 has significantly improved quality on downstream tasks across different model sizes while simultaneously exhibiting faster and more efficient inference compared to palm this improved efficiency enables broader deployment while also allowing the model to respond faster for a more natural pace of interaction palm 2 demonstrates robust reasoning capabilities exemplified by large improvements over palm on big bench and other reasoning tasks palm 2 exhibits stable performance on a suite of responsible ai evaluations and enables inference time control over toxicity without additional overhead or impact on other capabilities overall palm 2 achieves state of the art performance across a diverse set of tasks and capabilities when discussing the palm 2 family it is important to distinguish between pre trained models of various sizes fine tuned variants of these models and the user facing products that use these models in particular user facing products typically include additional pre and post processing steps additionally the underlying models may evolve over time therefore one should not expect the performance of user facing products to exactly match the results reported in this report
2023,tree of thoughts deliberate problem solving with large language models,yao fu et al,https://arxiv.org/abs/2305.10601,dl_nlp,language models are increasingly being deployed for general problem solving across a wide range of tasks but are still confined to token level left to right decision making processes during inference this means they can fall short in tasks that require exploration strategic lookahead or where initial decisions play a pivotal role to surmount these challenges we introduce a new framework for language model inference tree of thoughts tot which generalizes over the popular chain of thought approach to prompting language models and enables exploration over coherent units of text thoughts that serve as intermediate steps toward problem solving tot allows lms to perform deliberate decision making by considering multiple different reasoning paths and self evaluating choices to decide the next course of action as well as looking ahead or backtracking when necessary to make global choices our experiments show that tot significantly enhances language models problem solving abilities on three novel tasks requiring non trivial planning or search game of 24 creative writing and mini crosswords for instance in game of 24 while gpt 4 with chain of thought prompting only solved 4 of tasks our method achieved a success rate of 74 code repo with all prompts,tree of thoughts deliberate problem solving with large language models yao fu et al language models are increasingly being deployed for general problem solving across a wide range of tasks but are still confined to token level left to right decision making processes during inference this means they can fall short in tasks that require exploration strategic lookahead or where initial decisions play a pivotal role to surmount these challenges we introduce a new framework for language model inference tree of thoughts tot which generalizes over the popular chain of thought approach to prompting language models and enables exploration over coherent units of text thoughts that serve as intermediate steps toward problem solving tot allows lms to perform deliberate decision making by considering multiple different reasoning paths and self evaluating choices to decide the next course of action as well as looking ahead or backtracking when necessary to make global choices our experiments show that tot significantly enhances language models problem solving abilities on three novel tasks requiring non trivial planning or search game of 24 creative writing and mini crosswords for instance in game of 24 while gpt 4 with chain of thought prompting only solved 4 of tasks our method achieved a success rate of 74 code repo with all prompts
2023,drag your gan interactive point based manipulation on the generative image manifold,xingang pan et al,https://arxiv.org/abs/2305.10973,vision,synthesizing visual content that meets users needs often requires flexible and precise controllability of the pose shape expression and layout of the generated objects existing approaches gain controllability of generative adversarial networks gans via manually annotated training data or a prior 3d model which often lack flexibility precision and generality in this work we study a powerful yet much less explored way of controlling gans that is to drag any points of the image to precisely reach target points in a user interactive manner as shown in fig 1 to achieve this we propose draggan which consists of two main components 1 a feature based motion supervision that drives the handle point to move towards the target position and 2 a new point tracking approach that leverages the discriminative generator features to keep localizing the position of the handle points through draggan anyone can deform an image with precise control over where pixels go thus manipulating the pose shape expression and layout of diverse categories such as animals cars humans landscapes etc as these manipulations are performed on the learned generative image manifold of a gan they tend to produce realistic outputs even for challenging scenarios such as hallucinating occluded content and deforming shapes that consistently follow the object s rigidity both qualitative and quantitative comparisons demonstrate the advantage of draggan over prior approaches in the tasks of image manipulation and point tracking we also showcase the manipulation of real images through gan inversion,drag your gan interactive point based manipulation on the generative image manifold xingang pan et al synthesizing visual content that meets users needs often requires flexible and precise controllability of the pose shape expression and layout of the generated objects existing approaches gain controllability of generative adversarial networks gans via manually annotated training data or a prior 3d model which often lack flexibility precision and generality in this work we study a powerful yet much less explored way of controlling gans that is to drag any points of the image to precisely reach target points in a user interactive manner as shown in fig 1 to achieve this we propose draggan which consists of two main components 1 a feature based motion supervision that drives the handle point to move towards the target position and 2 a new point tracking approach that leverages the discriminative generator features to keep localizing the position of the handle points through draggan anyone can deform an image with precise control over where pixels go thus manipulating the pose shape expression and layout of diverse categories such as animals cars humans landscapes etc as these manipulations are performed on the learned generative image manifold of a gan they tend to produce realistic outputs even for challenging scenarios such as hallucinating occluded content and deforming shapes that consistently follow the object s rigidity both qualitative and quantitative comparisons demonstrate the advantage of draggan over prior approaches in the tasks of image manipulation and point tracking we also showcase the manipulation of real images through gan inversion
2023,lima less is more for alignment,chunting zhou et al,https://arxiv.org/abs/2305.11206,dl_nlp,large language models are trained in two stages 1 unsupervised pretraining from raw text to learn general purpose representations and 2 large scale instruction tuning and reinforcement learning to better align to end tasks and user preferences we measure the relative importance of these two stages by training lima a 65b parameter llama language model fine tuned with the standard supervised loss on only 1000 carefully curated prompts and responses without any reinforcement learning or human preference modeling lima demonstrates remarkably strong performance learning to follow specific response formats from only a handful of examples in the training data including complex queries that range from planning trip itineraries to speculating about alternate history moreover the model tends to generalize well to unseen tasks that did not appear in the training data in a controlled human study responses from lima are either equivalent or strictly preferred to gpt 4 in 43 of cases this statistic is as high as 58 when compared to bard and 65 versus davinci003 which was trained with human feedback taken together these results strongly suggest that almost all knowledge in large language models is learned during pretraining and only limited instruction tuning data is necessary to teach models to produce high quality output,lima less is more for alignment chunting zhou et al large language models are trained in two stages 1 unsupervised pretraining from raw text to learn general purpose representations and 2 large scale instruction tuning and reinforcement learning to better align to end tasks and user preferences we measure the relative importance of these two stages by training lima a 65b parameter llama language model fine tuned with the standard supervised loss on only 1000 carefully curated prompts and responses without any reinforcement learning or human preference modeling lima demonstrates remarkably strong performance learning to follow specific response formats from only a handful of examples in the training data including complex queries that range from planning trip itineraries to speculating about alternate history moreover the model tends to generalize well to unseen tasks that did not appear in the training data in a controlled human study responses from lima are either equivalent or strictly preferred to gpt 4 in 43 of cases this statistic is as high as 58 when compared to bard and 65 versus davinci003 which was trained with human feedback taken together these results strongly suggest that almost all knowledge in large language models is learned during pretraining and only limited instruction tuning data is necessary to teach models to produce high quality output
2023,scaling speech technology to 1000 languages mms,vineel pratap et al,https://arxiv.org/abs/2305.13516,audio,expanding the language coverage of speech technology has the potential to improve access to information for many more people however current speech technology is restricted to about one hundred languages which is a small fraction of the over 7000 languages spoken around the world the massively multilingual speech mms project increases the number of supported languages by 10 40x depending on the task the main ingredients are a new dataset based on readings of publicly available religious texts and effectively leveraging self supervised learning we built pre trained wav2vec 2 0 models covering 1406 languages a single multilingual automatic speech recognition model for 1107 languages speech synthesis models for the same number of languages as well as a language identification model for 4017 languages experiments show that our multilingual speech recognition model more than halves the word error rate of whisper on 54 languages of the fleurs benchmark while being trained on a small fraction of the labeled data,scaling speech technology to 1000 languages mms vineel pratap et al expanding the language coverage of speech technology has the potential to improve access to information for many more people however current speech technology is restricted to about one hundred languages which is a small fraction of the over 7000 languages spoken around the world the massively multilingual speech mms project increases the number of supported languages by 10 40x depending on the task the main ingredients are a new dataset based on readings of publicly available religious texts and effectively leveraging self supervised learning we built pre trained wav2vec 2 0 models covering 1406 languages a single multilingual automatic speech recognition model for 1107 languages speech synthesis models for the same number of languages as well as a language identification model for 4017 languages experiments show that our multilingual speech recognition model more than halves the word error rate of whisper on 54 languages of the fleurs benchmark while being trained on a small fraction of the labeled data
2023,qlora efficient finetuning of quantized llms,tim dettmers et al,https://arxiv.org/abs/2305.14314,dl_nlp,we present qlora an efficient finetuning approach that reduces memory usage enough to finetune a 65b parameter model on a single 48gb gpu while preserving full 16 bit finetuning task performance qlora backpropagates gradients through a frozen 4 bit quantized pretrained language model into low rank adapters lora our best model family which we name guanaco outperforms all previous openly released models on the vicuna benchmark reaching 99 3 of the performance level of chatgpt while only requiring 24 hours of finetuning on a single gpu qlora introduces a number of innovations to save memory without sacrificing performance a 4 bit normalfloat nf4 a new data type that is information theoretically optimal for normally distributed weights b double quantization to reduce the average memory footprint by quantizing the quantization constants and c paged optimziers to manage memory spikes we use qlora to finetune more than 1000 models providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets multiple model types llama t5 and model scales that would be infeasible to run with regular finetuning e g 33b and 65b parameter models our results show that qlora finetuning on a small high quality dataset leads to state of the art results even when using smaller models than the previous sota we provide a detailed analysis of chatbot performance based on both human and gpt 4 evaluations showing that gpt 4 evaluations are a cheap and reasonable alternative to human evaluation furthermore we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots a lemon picked analysis demonstrates where guanaco fails compared to chatgpt we release all of our models and code including cuda kernels for 4 bit training,qlora efficient finetuning of quantized llms tim dettmers et al we present qlora an efficient finetuning approach that reduces memory usage enough to finetune a 65b parameter model on a single 48gb gpu while preserving full 16 bit finetuning task performance qlora backpropagates gradients through a frozen 4 bit quantized pretrained language model into low rank adapters lora our best model family which we name guanaco outperforms all previous openly released models on the vicuna benchmark reaching 99 3 of the performance level of chatgpt while only requiring 24 hours of finetuning on a single gpu qlora introduces a number of innovations to save memory without sacrificing performance a 4 bit normalfloat nf4 a new data type that is information theoretically optimal for normally distributed weights b double quantization to reduce the average memory footprint by quantizing the quantization constants and c paged optimziers to manage memory spikes we use qlora to finetune more than 1000 models providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets multiple model types llama t5 and model scales that would be infeasible to run with regular finetuning e g 33b and 65b parameter models our results show that qlora finetuning on a small high quality dataset leads to state of the art results even when using smaller models than the previous sota we provide a detailed analysis of chatbot performance based on both human and gpt 4 evaluations showing that gpt 4 evaluations are a cheap and reasonable alternative to human evaluation furthermore we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots a lemon picked analysis demonstrates where guanaco fails compared to chatgpt we release all of our models and code including cuda kernels for 4 bit training
2023,voyager an open ended embodied agent with large language models,guanzhi wang et al,https://arxiv.org/abs/2305.16291,dl_nlp,we introduce voyager the first llm powered embodied lifelong learning agent in minecraft that continuously explores the world acquires diverse skills and makes novel discoveries without human intervention voyager consists of three key components 1 an automatic curriculum that maximizes exploration 2 an ever growing skill library of executable code for storing and retrieving complex behaviors and 3 a new iterative prompting mechanism that incorporates environment feedback execution errors and self verification for program improvement voyager interacts with gpt 4 via blackbox queries which bypasses the need for model parameter fine tuning the skills developed by voyager are temporally extended interpretable and compositional which compounds the agent s abilities rapidly and alleviates catastrophic forgetting empirically voyager shows strong in context lifelong learning capability and exhibits exceptional proficiency in playing minecraft it obtains 3 3x more unique items travels 2 3x longer distances and unlocks key tech tree milestones up to 15 3x faster than prior sota voyager is able to utilize the learned skill library in a new minecraft world to solve novel tasks from scratch while other techniques struggle to generalize we open source our full codebase and prompts at,voyager an open ended embodied agent with large language models guanzhi wang et al we introduce voyager the first llm powered embodied lifelong learning agent in minecraft that continuously explores the world acquires diverse skills and makes novel discoveries without human intervention voyager consists of three key components 1 an automatic curriculum that maximizes exploration 2 an ever growing skill library of executable code for storing and retrieving complex behaviors and 3 a new iterative prompting mechanism that incorporates environment feedback execution errors and self verification for program improvement voyager interacts with gpt 4 via blackbox queries which bypasses the need for model parameter fine tuning the skills developed by voyager are temporally extended interpretable and compositional which compounds the agent s abilities rapidly and alleviates catastrophic forgetting empirically voyager shows strong in context lifelong learning capability and exhibits exceptional proficiency in playing minecraft it obtains 3 3x more unique items travels 2 3x longer distances and unlocks key tech tree milestones up to 15 3x faster than prior sota voyager is able to utilize the learned skill library in a new minecraft world to solve novel tasks from scratch while other techniques struggle to generalize we open source our full codebase and prompts at
2023,direct preference optimization your language model is secretly a reward model,rafael rafailov et al,https://arxiv.org/abs/2305.18290,dl_rl,while large scale unsupervised language models lms learn broad world knowledge and some reasoning skills achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine tune the unsupervised lm to align with these preferences often with reinforcement learning from human feedback rlhf however rlhf is a complex and often unstable procedure first fitting a reward model that reflects the human preferences and then fine tuning the large unsupervised lm using reinforcement learning to maximize this estimated reward without drifting too far from the original model in this paper we introduce a new parameterization of the reward model in rlhf that enables extraction of the corresponding optimal policy in closed form allowing us to solve the standard rlhf problem with only a simple classification loss the resulting algorithm which we call direct preference optimization dpo is stable performant and computationally lightweight eliminating the need for sampling from the lm during fine tuning or performing significant hyperparameter tuning our experiments show that dpo can fine tune lms to align with human preferences as well as or better than existing methods notably fine tuning with dpo exceeds ppo based rlhf in ability to control sentiment of generations and matches or improves response quality in summarization and single turn dialogue while being substantially simpler to implement and train,direct preference optimization your language model is secretly a reward model rafael rafailov et al while large scale unsupervised language models lms learn broad world knowledge and some reasoning skills achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine tune the unsupervised lm to align with these preferences often with reinforcement learning from human feedback rlhf however rlhf is a complex and often unstable procedure first fitting a reward model that reflects the human preferences and then fine tuning the large unsupervised lm using reinforcement learning to maximize this estimated reward without drifting too far from the original model in this paper we introduce a new parameterization of the reward model in rlhf that enables extraction of the corresponding optimal policy in closed form allowing us to solve the standard rlhf problem with only a simple classification loss the resulting algorithm which we call direct preference optimization dpo is stable performant and computationally lightweight eliminating the need for sampling from the lm during fine tuning or performing significant hyperparameter tuning our experiments show that dpo can fine tune lms to align with human preferences as well as or better than existing methods notably fine tuning with dpo exceeds ppo based rlhf in ability to control sentiment of generations and matches or improves response quality in summarization and single turn dialogue while being substantially simpler to implement and train
2023,the impact of positional encoding on length generalization in transformers,amirhossein kazemnejad et al,https://arxiv.org/abs/2305.19466,dl_nlp,length generalization the ability to generalize from small training context sizes to larger ones is a critical challenge in the development of transformer based language models positional encoding pe has been identified as a major factor influencing length generalization but the exact impact of different pe schemes on extrapolation in downstream tasks remains unclear in this paper we conduct a systematic empirical study comparing the length generalization performance of decoder only transformers with five different position encoding approaches including absolute position embedding ape t5 s relative pe alibi and rotary in addition to transformers without positional encoding nope our evaluation encompasses a battery of reasoning and mathematical tasks our findings reveal that the most commonly used positional encoding methods such as alibi rotary and ape are not well suited for length generalization in downstream tasks more importantly nope outperforms other explicit positional encoding methods while requiring no additional computation we theoretically demonstrate that nope can represent both absolute and relative pes but when trained with sgd it mostly resembles t5 s relative pe attention patterns finally we find that scratchpad is not always helpful to solve length generalization and its format highly impacts the model s performance overall our work suggests that explicit position embeddings are not essential for decoder only transformers to generalize well to longer sequences,the impact of positional encoding on length generalization in transformers amirhossein kazemnejad et al length generalization the ability to generalize from small training context sizes to larger ones is a critical challenge in the development of transformer based language models positional encoding pe has been identified as a major factor influencing length generalization but the exact impact of different pe schemes on extrapolation in downstream tasks remains unclear in this paper we conduct a systematic empirical study comparing the length generalization performance of decoder only transformers with five different position encoding approaches including absolute position embedding ape t5 s relative pe alibi and rotary in addition to transformers without positional encoding nope our evaluation encompasses a battery of reasoning and mathematical tasks our findings reveal that the most commonly used positional encoding methods such as alibi rotary and ape are not well suited for length generalization in downstream tasks more importantly nope outperforms other explicit positional encoding methods while requiring no additional computation we theoretically demonstrate that nope can represent both absolute and relative pes but when trained with sgd it mostly resembles t5 s relative pe attention patterns finally we find that scratchpad is not always helpful to solve length generalization and its format highly impacts the model s performance overall our work suggests that explicit position embeddings are not essential for decoder only transformers to generalize well to longer sequences
2023,neuralangelo high fidelity neural surface reconstruction,zhaoshuo li et al,https://arxiv.org/abs/2306.03092,vision,neural surface reconstruction has been shown to be powerful for recovering dense 3d surfaces via image based neural rendering however current methods struggle to recover detailed structures of real world scenes to address the issue we present neuralangelo which combines the representation power of multi resolution 3d hash grids with neural surface rendering two key ingredients enable our approach 1 numerical gradients for computing higher order derivatives as a smoothing operation and 2 coarse to fine optimization on the hash grids controlling different levels of details even without auxiliary inputs such as depth neuralangelo can effectively recover dense 3d surface structures from multi view images with fidelity significantly surpassing previous methods enabling detailed large scale scene reconstruction from rgb video captures,neuralangelo high fidelity neural surface reconstruction zhaoshuo li et al neural surface reconstruction has been shown to be powerful for recovering dense 3d surfaces via image based neural rendering however current methods struggle to recover detailed structures of real world scenes to address the issue we present neuralangelo which combines the representation power of multi resolution 3d hash grids with neural surface rendering two key ingredients enable our approach 1 numerical gradients for computing higher order derivatives as a smoothing operation and 2 coarse to fine optimization on the hash grids controlling different levels of details even without auxiliary inputs such as depth neuralangelo can effectively recover dense 3d surface structures from multi view images with fidelity significantly surpassing previous methods enabling detailed large scale scene reconstruction from rgb video captures
2023,simple and controllable music generation musicgen,jade copet et al,https://arxiv.org/abs/2306.05284,audio,we tackle the task of conditional music generation we introduce musicgen a single language model lm that operates over several streams of compressed discrete music representation i e tokens unlike prior work musicgen is comprised of a single stage transformer lm together with efficient token interleaving patterns which eliminates the need for cascading several models e g hierarchically or upsampling following this approach we demonstrate how musicgen can generate high quality samples both mono and stereo while being conditioned on textual description or melodic features allowing better controls over the generated output we conduct extensive empirical evaluation considering both automatic and human studies showing the proposed approach is superior to the evaluated baselines on a standard text to music benchmark through ablation studies we shed light over the importance of each of the components comprising musicgen music samples code and models are available at,simple and controllable music generation musicgen jade copet et al we tackle the task of conditional music generation we introduce musicgen a single language model lm that operates over several streams of compressed discrete music representation i e tokens unlike prior work musicgen is comprised of a single stage transformer lm together with efficient token interleaving patterns which eliminates the need for cascading several models e g hierarchically or upsampling following this approach we demonstrate how musicgen can generate high quality samples both mono and stereo while being conditioned on textual description or melodic features allowing better controls over the generated output we conduct extensive empirical evaluation considering both automatic and human studies showing the proposed approach is superior to the evaluated baselines on a standard text to music benchmark through ablation studies we shed light over the importance of each of the components comprising musicgen music samples code and models are available at
2023,audiopalm a large language model that can speak and listen,paul k rubenstein et al,https://arxiv.org/abs/2306.12925,audio,we introduce audiopalm a large language model for speech understanding and generation audiopalm fuses text based and speech based language models palm 2 anil et al 2023 and audiolm borsos et al 2022 into a unified multimodal architecture that can process and generate text and speech with applications including speech recognition and speech to speech translation audiopalm inherits the capability to preserve paralinguistic information such as speaker identity and intonation from audiolm and the linguistic knowledge present only in text large language models such as palm 2 we demonstrate that initializing audiopalm with the weights of a text only large language model improves speech processing successfully leveraging the larger quantity of text training data used in pretraining to assist with the speech tasks the resulting model significantly outperforms existing systems for speech translation tasks and has the ability to perform zero shot speech to text translation for many languages for which input target language combinations were not seen in training audiopalm also demonstrates features of audio language models such as transferring a voice across languages based on a short spoken prompt we release examples of our method at,audiopalm a large language model that can speak and listen paul k rubenstein et al we introduce audiopalm a large language model for speech understanding and generation audiopalm fuses text based and speech based language models palm 2 anil et al 2023 and audiolm borsos et al 2022 into a unified multimodal architecture that can process and generate text and speech with applications including speech recognition and speech to speech translation audiopalm inherits the capability to preserve paralinguistic information such as speaker identity and intonation from audiolm and the linguistic knowledge present only in text large language models such as palm 2 we demonstrate that initializing audiopalm with the weights of a text only large language model improves speech processing successfully leveraging the larger quantity of text training data used in pretraining to assist with the speech tasks the resulting model significantly outperforms existing systems for speech translation tasks and has the ability to perform zero shot speech to text translation for many languages for which input target language combinations were not seen in training audiopalm also demonstrates features of audio language models such as transferring a voice across languages based on a short spoken prompt we release examples of our method at
2023,voicebox text guided multilingual universal speech generation at scale,matthew le et al,https://arxiv.org/abs/2306.15687,audio,large scale generative models such as gpt and dall e have revolutionized the research community these models not only generate high fidelity outputs but are also generalists which can solve tasks not explicitly taught in contrast speech generative models are still primitive in terms of scale and task generalization in this paper we present voicebox the most versatile text guided generative model for speech at scale voicebox is a non autoregressive flow matching model trained to infill speech given audio context and text trained on over 50k hours of speech that are not filtered or enhanced similar to gpt voicebox can perform many different tasks through in context learning but is more flexible as it can also condition on future context voicebox can be used for mono or cross lingual zero shot text to speech synthesis noise removal content editing style conversion and diverse sample generation in particular voicebox outperforms the state of the art zero shot tts model vall e on both intelligibility 5 9 vs 1 9 word error rates and audio similarity 0 580 vs 0 681 while being up to 20 times faster audio samples can be found in url,voicebox text guided multilingual universal speech generation at scale matthew le et al large scale generative models such as gpt and dall e have revolutionized the research community these models not only generate high fidelity outputs but are also generalists which can solve tasks not explicitly taught in contrast speech generative models are still primitive in terms of scale and task generalization in this paper we present voicebox the most versatile text guided generative model for speech at scale voicebox is a non autoregressive flow matching model trained to infill speech given audio context and text trained on over 50k hours of speech that are not filtered or enhanced similar to gpt voicebox can perform many different tasks through in context learning but is more flexible as it can also condition on future context voicebox can be used for mono or cross lingual zero shot text to speech synthesis noise removal content editing style conversion and diverse sample generation in particular voicebox outperforms the state of the art zero shot tts model vall e on both intelligibility 5 9 vs 1 9 word error rates and audio similarity 0 580 vs 0 681 while being up to 20 times faster audio samples can be found in url
2023,sdxl improving latent diffusion models for high resolution image synthesis,dustin podell et al,https://arxiv.org/abs/2307.01952,vision,we present sdxl a latent diffusion model for text to image synthesis compared to previous versions of stable diffusion sdxl leverages a three times larger unet backbone the increase of model parameters is mainly due to more attention blocks and a larger cross attention context as sdxl uses a second text encoder we design multiple novel conditioning schemes and train sdxl on multiple aspect ratios we also introduce a refinement model which is used to improve the visual fidelity of samples generated by sdxl using a post hoc image to image technique we demonstrate that sdxl shows drastically improved performance compared the previous versions of stable diffusion and achieves results competitive with those of black box state of the art image generators in the spirit of promoting open research and fostering transparency in large model training and evaluation we provide access to code and model weights at,sdxl improving latent diffusion models for high resolution image synthesis dustin podell et al we present sdxl a latent diffusion model for text to image synthesis compared to previous versions of stable diffusion sdxl leverages a three times larger unet backbone the increase of model parameters is mainly due to more attention blocks and a larger cross attention context as sdxl uses a second text encoder we design multiple novel conditioning schemes and train sdxl on multiple aspect ratios we also introduce a refinement model which is used to improve the visual fidelity of samples generated by sdxl using a post hoc image to image technique we demonstrate that sdxl shows drastically improved performance compared the previous versions of stable diffusion and achieves results competitive with those of black box state of the art image generators in the spirit of promoting open research and fostering transparency in large model training and evaluation we provide access to code and model weights at
2023,toolllm facilitating large language models to master 16000 real world apis,yujia qin et al,https://arxiv.org/abs/2307.16789,dl_nlp,despite the advancements of open source large language models llms e g llama they remain significantly limited in tool use capabilities i e using external tools apis to fulfill human instructions the reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool use domain this is in contrast to the excellent tool use capabilities of state of the art sota closed source llms e g chatgpt to bridge this gap we introduce toolllm a general tool use framework encompassing data construction model training and evaluation we first present toolbench an instruction tuning dataset for tool use which is constructed automatically using chatgpt specifically the construction can be divided into three stages i api collection we collect 16464 real world restful apis spanning 49 categories from rapidapi hub ii instruction generation we prompt chatgpt to generate diverse instructions involving these apis covering both single tool and multi tool scenarios iii solution path annotation we use chatgpt to search for a valid solution path chain of api calls for each instruction to enhance the reasoning capabilities of llms we develop a novel depth first search based decision tree algorithm it enables llms to evaluate multiple reasoning traces and expand the search space moreover to evaluate the tool use capabilities of llms we develop an automatic evaluator tooleval based on toolbench we fine tune llama to obtain an llm toolllama and equip it with a neural api retriever to recommend appropriate apis for each instruction experiments show that toolllama demonstrates a remarkable ability to execute complex instructions and generalize to unseen apis and exhibits comparable performance to chatgpt our toolllama also demonstrates strong zero shot generalization ability in an out of distribution tool use dataset apibench,toolllm facilitating large language models to master 16000 real world apis yujia qin et al despite the advancements of open source large language models llms e g llama they remain significantly limited in tool use capabilities i e using external tools apis to fulfill human instructions the reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool use domain this is in contrast to the excellent tool use capabilities of state of the art sota closed source llms e g chatgpt to bridge this gap we introduce toolllm a general tool use framework encompassing data construction model training and evaluation we first present toolbench an instruction tuning dataset for tool use which is constructed automatically using chatgpt specifically the construction can be divided into three stages i api collection we collect 16464 real world restful apis spanning 49 categories from rapidapi hub ii instruction generation we prompt chatgpt to generate diverse instructions involving these apis covering both single tool and multi tool scenarios iii solution path annotation we use chatgpt to search for a valid solution path chain of api calls for each instruction to enhance the reasoning capabilities of llms we develop a novel depth first search based decision tree algorithm it enables llms to evaluate multiple reasoning traces and expand the search space moreover to evaluate the tool use capabilities of llms we develop an automatic evaluator tooleval based on toolbench we fine tune llama to obtain an llm toolllama and equip it with a neural api retriever to recommend appropriate apis for each instruction experiments show that toolllama demonstrates a remarkable ability to execute complex instructions and generalize to unseen apis and exhibits comparable performance to chatgpt our toolllama also demonstrates strong zero shot generalization ability in an out of distribution tool use dataset apibench
2023,metagpt meta programming for multi agent collaborative framework,sirui hong et al,https://arxiv.org/abs/2308.00352,dl_nlp,remarkable progress has been made on automated problem solving through societies of agents based on large language models llms existing llm based multi agent systems can already solve simple dialogue tasks solutions to more complex tasks however are complicated through logic inconsistencies due to cascading hallucinations caused by naively chaining llms here we introduce metagpt an innovative meta programming framework incorporating efficient human workflows into llm based multi agent collaborations metagpt encodes standardized operating procedures sops into prompt sequences for more streamlined workflows thus allowing agents with human like domain expertise to verify intermediate results and reduce errors metagpt utilizes an assembly line paradigm to assign diverse roles to various agents efficiently breaking down complex tasks into subtasks involving many agents working together on collaborative software engineering benchmarks metagpt generates more coherent solutions than previous chat based multi agent systems our project can be found at,metagpt meta programming for multi agent collaborative framework sirui hong et al remarkable progress has been made on automated problem solving through societies of agents based on large language models llms existing llm based multi agent systems can already solve simple dialogue tasks solutions to more complex tasks however are complicated through logic inconsistencies due to cascading hallucinations caused by naively chaining llms here we introduce metagpt an innovative meta programming framework incorporating efficient human workflows into llm based multi agent collaborations metagpt encodes standardized operating procedures sops into prompt sequences for more streamlined workflows thus allowing agents with human like domain expertise to verify intermediate results and reduce errors metagpt utilizes an assembly line paradigm to assign diverse roles to various agents efficiently breaking down complex tasks into subtasks involving many agents working together on collaborative software engineering benchmarks metagpt generates more coherent solutions than previous chat based multi agent systems our project can be found at
2023,retroformer retrospective large language agents with policy gradient optimization,wenlong huang et al,https://arxiv.org/abs/2308.02151,dl_rl,recent months have seen the emergence of a powerful new trend in which large language models llms are augmented to become autonomous language agents capable of performing objective oriented multi step tasks on their own rather than merely responding to queries from human users most existing language agents however are not optimized using environment specific rewards although some agents enable iterative refinement through verbal feedback they do not reason and plan in ways that are compatible with gradient based learning from rewards this paper introduces a principled framework for reinforcing large language agents by learning a retrospective model which automatically tunes the language agent prompts from environment feedback through policy gradient specifically our proposed agent architecture learns from rewards across multiple environments and tasks for fine tuning a pre trained language model which refines the language agent prompt by summarizing the root cause of prior failed attempts and proposing action plans experimental results on various tasks demonstrate that the language agents improve over time and that our approach considerably outperforms baselines that do not properly leverage gradients from the environment this demonstrates that using policy gradient optimization to improve language agents for which we believe our work is one of the first seems promising and can be applied to optimize other models in the agent architecture to enhance agent performances over time,retroformer retrospective large language agents with policy gradient optimization wenlong huang et al recent months have seen the emergence of a powerful new trend in which large language models llms are augmented to become autonomous language agents capable of performing objective oriented multi step tasks on their own rather than merely responding to queries from human users most existing language agents however are not optimized using environment specific rewards although some agents enable iterative refinement through verbal feedback they do not reason and plan in ways that are compatible with gradient based learning from rewards this paper introduces a principled framework for reinforcing large language agents by learning a retrospective model which automatically tunes the language agent prompts from environment feedback through policy gradient specifically our proposed agent architecture learns from rewards across multiple environments and tasks for fine tuning a pre trained language model which refines the language agent prompt by summarizing the root cause of prior failed attempts and proposing action plans experimental results on various tasks demonstrate that the language agents improve over time and that our approach considerably outperforms baselines that do not properly leverage gradients from the environment this demonstrates that using policy gradient optimization to improve language agents for which we believe our work is one of the first seems promising and can be applied to optimize other models in the agent architecture to enhance agent performances over time
2023,code llama open foundation models for code,baptiste rozière et al,https://arxiv.org/abs/2308.12950,dl_nlp,we release code llama a family of large language models for code based on llama 2 providing state of the art performance among open models infilling capabilities support for large input contexts and zero shot instruction following ability for programming tasks we provide multiple flavors to cover a wide range of applications foundation models code llama python specializations code llama python and instruction following models code llama instruct with 7b 13b 34b and 70b parameters each all models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens 7b 13b and 70b code llama and code llama instruct variants support infilling based on surrounding content code llama reaches state of the art performance among open models on several code benchmarks with scores of up to 67 and 65 on humaneval and mbpp respectively notably code llama python 7b outperforms llama 2 70b on humaneval and mbpp and all our models outperform every other publicly available model on multipl e we release code llama under a permissive license that allows for both research and commercial use,code llama open foundation models for code baptiste rozière et al we release code llama a family of large language models for code based on llama 2 providing state of the art performance among open models infilling capabilities support for large input contexts and zero shot instruction following ability for programming tasks we provide multiple flavors to cover a wide range of applications foundation models code llama python specializations code llama python and instruction following models code llama instruct with 7b 13b 34b and 70b parameters each all models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens 7b 13b and 70b code llama and code llama instruct variants support infilling based on surrounding content code llama reaches state of the art performance among open models on several code benchmarks with scores of up to 67 and 65 on humaneval and mbpp respectively notably code llama python 7b outperforms llama 2 70b on humaneval and mbpp and all our models outperform every other publicly available model on multipl e we release code llama under a permissive license that allows for both research and commercial use
2023,qwen vl a versatile vision language model for understanding localization and generation,jinze bai et al,https://arxiv.org/abs/2308.12966,vision,in this work we introduce the qwen vl series a set of large scale vision language models lvlms designed to perceive and understand both texts and images starting from the qwen lm as a foundation we endow it with visual capacity by the meticulously designed i visual receptor ii input output interface iii 3 stage training pipeline and iv multilingual multimodal cleaned corpus beyond the conventional image description and question answering we implement the grounding and text reading ability of qwen vls by aligning image caption box tuples the resulting models including qwen vl and qwen vl chat set new records for generalist models under similar model scales on a broad range of visual centric benchmarks e g image captioning question answering visual grounding and different settings e g zero shot few shot moreover on real world dialog benchmarks our instruction tuned qwen vl chat also demonstrates superiority compared to existing vision language chatbots code demo and models are available at,qwen vl a versatile vision language model for understanding localization and generation jinze bai et al in this work we introduce the qwen vl series a set of large scale vision language models lvlms designed to perceive and understand both texts and images starting from the qwen lm as a foundation we endow it with visual capacity by the meticulously designed i visual receptor ii input output interface iii 3 stage training pipeline and iv multilingual multimodal cleaned corpus beyond the conventional image description and question answering we implement the grounding and text reading ability of qwen vls by aligning image caption box tuples the resulting models including qwen vl and qwen vl chat set new records for generalist models under similar model scales on a broad range of visual centric benchmarks e g image captioning question answering visual grounding and different settings e g zero shot few shot moreover on real world dialog benchmarks our instruction tuned qwen vl chat also demonstrates superiority compared to existing vision language chatbots code demo and models are available at
2023,mvdream multi view diffusion for 3d generation,yichun shi et al,https://arxiv.org/abs/2308.16512,vision,we introduce mvdream a diffusion model that is able to generate consistent multi view images from a given text prompt learning from both 2d and 3d data a multi view diffusion model can achieve the generalizability of 2d diffusion models and the consistency of 3d renderings we demonstrate that such a multi view diffusion model is implicitly a generalizable 3d prior agnostic to 3d representations it can be applied to 3d generation via score distillation sampling significantly enhancing the consistency and stability of existing 2d lifting methods it can also learn new concepts from a few 2d examples akin to dreambooth but for 3d generation,mvdream multi view diffusion for 3d generation yichun shi et al we introduce mvdream a diffusion model that is able to generate consistent multi view images from a given text prompt learning from both 2d and 3d data a multi view diffusion model can achieve the generalizability of 2d diffusion models and the consistency of 3d renderings we demonstrate that such a multi view diffusion model is implicitly a generalizable 3d prior agnostic to 3d representations it can be applied to 3d generation via score distillation sampling significantly enhancing the consistency and stability of existing 2d lifting methods it can also learn new concepts from a few 2d examples akin to dreambooth but for 3d generation
2023,rlaif scaling reinforcement learning from human feedback with ai feedback,harrison lee et al,https://arxiv.org/abs/2309.00267,dl_nlp,reinforcement learning from human feedback rlhf has proven effective in aligning large language models llms with human preferences but gathering high quality preference labels is expensive rl from ai feedback rlaif introduced in bai et al offers a promising alternative that trains the reward model rm on preferences generated by an off the shelf llm across the tasks of summarization helpful dialogue generation and harmless dialogue generation we show that rlaif achieves comparable performance to rlhf furthermore we take a step towards self improvement by demonstrating that rlaif can outperform a supervised fine tuned baseline even when the ai labeler is the same size as the policy or even the exact same checkpoint as the initial policy finally we introduce direct rlaif d rlaif a technique that circumvents rm training by obtaining rewards directly from an off the shelf llm during rl which achieves superior performance to canonical rlaif our results suggest that rlaif can achieve performance on par with using human feedback offering a potential solution to the scalability limitations of rlhf,rlaif scaling reinforcement learning from human feedback with ai feedback harrison lee et al reinforcement learning from human feedback rlhf has proven effective in aligning large language models llms with human preferences but gathering high quality preference labels is expensive rl from ai feedback rlaif introduced in bai et al offers a promising alternative that trains the reward model rm on preferences generated by an off the shelf llm across the tasks of summarization helpful dialogue generation and harmless dialogue generation we show that rlaif achieves comparable performance to rlhf furthermore we take a step towards self improvement by demonstrating that rlaif can outperform a supervised fine tuned baseline even when the ai labeler is the same size as the policy or even the exact same checkpoint as the initial policy finally we introduce direct rlaif d rlaif a technique that circumvents rm training by obtaining rewards directly from an off the shelf llm during rl which achieves superior performance to canonical rlaif our results suggest that rlaif can achieve performance on par with using human feedback offering a potential solution to the scalability limitations of rlhf
2023,large language models as optimizers,chengrun yang et al,https://arxiv.org/abs/2309.03409,dl_nlp,optimization is ubiquitous while derivative based algorithms have been powerful tools for various problems the absence of gradient imposes challenges on many real world applications in this work we propose optimization by prompting opro a simple and effective approach to leverage large language models llms as optimizers where the optimization task is described in natural language in each optimization step the llm generates new solutions from the prompt that contains previously generated solutions with their values then the new solutions are evaluated and added to the prompt for the next optimization step we first showcase opro on linear regression and traveling salesman problems then move on to our main application in prompt optimization where the goal is to find instructions that maximize the task accuracy with a variety of llms we demonstrate that the best prompts optimized by opro outperform human designed prompts by up to 8 on gsm8k and by up to 50 on big bench hard tasks code at,large language models as optimizers chengrun yang et al optimization is ubiquitous while derivative based algorithms have been powerful tools for various problems the absence of gradient imposes challenges on many real world applications in this work we propose optimization by prompting opro a simple and effective approach to leverage large language models llms as optimizers where the optimization task is described in natural language in each optimization step the llm generates new solutions from the prompt that contains previously generated solutions with their values then the new solutions are evaluated and added to the prompt for the next optimization step we first showcase opro on linear regression and traveling salesman problems then move on to our main application in prompt optimization where the goal is to find instructions that maximize the task accuracy with a variety of llms we demonstrate that the best prompts optimized by opro outperform human designed prompts by up to 8 on gsm8k and by up to 50 on big bench hard tasks code at
2023,vision transformers need registers,t darcet el al, https://arxiv.org/abs/2309.16588,vision,transformers have recently emerged as a powerful tool for learning visual representations in this paper we identify and characterize artifacts in feature maps of both supervised and self supervised vit networks the artifacts correspond to high norm tokens appearing during inference primarily in low informative background areas of images that are repurposed for internal computations we propose a simple yet effective solution based on providing additional tokens to the input sequence of the vision transformer to fill that role we show that this solution fixes that problem entirely for both supervised and self supervised models sets a new state of the art for self supervised visual models on dense visual prediction tasks enables object discovery methods with larger models and most importantly leads to smoother feature maps and attention maps for downstream visual processing,vision transformers need registers t darcet el al transformers have recently emerged as a powerful tool for learning visual representations in this paper we identify and characterize artifacts in feature maps of both supervised and self supervised vit networks the artifacts correspond to high norm tokens appearing during inference primarily in low informative background areas of images that are repurposed for internal computations we propose a simple yet effective solution based on providing additional tokens to the input sequence of the vision transformer to fill that role we show that this solution fixes that problem entirely for both supervised and self supervised models sets a new state of the art for self supervised visual models on dense visual prediction tasks enables object discovery methods with larger models and most importantly leads to smoother feature maps and attention maps for downstream visual processing
2023,efficient streaming language models with attention sinks,guangxuan xiao et al,https://arxiv.org/abs/2309.17453,dl_nlp,deploying large language models llms in streaming applications such as multi round dialogue where long interactions are expected is urgently needed but poses two major challenges firstly during the decoding stage caching previous tokens key and value states kv consumes extensive memory secondly popular llms cannot generalize to longer texts than the training sequence length window attention where only the most recent kvs are cached is a natural approach but we show that it fails when the text length surpasses the cache size we observe an interesting phenomenon namely attention sink that keeping the kv of initial tokens will largely recover the performance of window attention in this paper we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a sink even if they are not semantically important based on the above analysis we introduce streamingllm an efficient framework that enables llms trained with a finite length attention window to generalize to infinite sequence lengths without any fine tuning we show that streamingllm can enable llama 2 mpt falcon and pythia to perform stable and efficient language modeling with up to 4 million tokens and more in addition we discover that adding a placeholder token as a dedicated attention sink during pre training can further improve streaming deployment in streaming settings streamingllm outperforms the sliding window recomputation baseline by up to 22 2x speedup code and datasets are provided at,efficient streaming language models with attention sinks guangxuan xiao et al deploying large language models llms in streaming applications such as multi round dialogue where long interactions are expected is urgently needed but poses two major challenges firstly during the decoding stage caching previous tokens key and value states kv consumes extensive memory secondly popular llms cannot generalize to longer texts than the training sequence length window attention where only the most recent kvs are cached is a natural approach but we show that it fails when the text length surpasses the cache size we observe an interesting phenomenon namely attention sink that keeping the kv of initial tokens will largely recover the performance of window attention in this paper we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a sink even if they are not semantically important based on the above analysis we introduce streamingllm an efficient framework that enables llms trained with a finite length attention window to generalize to infinite sequence lengths without any fine tuning we show that streamingllm can enable llama 2 mpt falcon and pythia to perform stable and efficient language modeling with up to 4 million tokens and more in addition we discover that adding a placeholder token as a dedicated attention sink during pre training can further improve streaming deployment in streaming settings streamingllm outperforms the sliding window recomputation baseline by up to 22 2x speedup code and datasets are provided at
2023,eureka human level reward design via coding large language models,yecheng jason ma et al,https://arxiv.org/abs/2310.12931,dl_nlp,large language models llms have excelled as high level semantic planners for sequential decision making tasks however harnessing them to learn complex low level manipulation tasks such as dexterous pen spinning remains an open problem we bridge this fundamental gap and present eureka a human level reward design algorithm powered by llms eureka exploits the remarkable zero shot generation code writing and in context improvement capabilities of state of the art llms such as gpt 4 to perform evolutionary optimization over reward code the resulting rewards can then be used to acquire complex skills via reinforcement learning without any task specific prompting or pre defined reward templates eureka generates reward functions that outperform expert human engineered rewards in a diverse suite of 29 open source rl environments that include 10 distinct robot morphologies eureka outperforms human experts on 83 of the tasks leading to an average normalized improvement of 52 the generality of eureka also enables a new gradient free in context learning approach to reinforcement learning from human feedback rlhf readily incorporating human inputs to improve the quality and the safety of the generated rewards without model updating finally using eureka rewards in a curriculum learning setting we demonstrate for the first time a simulated shadow hand capable of performing pen spinning tricks adeptly manipulating a pen in circles at rapid speed,eureka human level reward design via coding large language models yecheng jason ma et al large language models llms have excelled as high level semantic planners for sequential decision making tasks however harnessing them to learn complex low level manipulation tasks such as dexterous pen spinning remains an open problem we bridge this fundamental gap and present eureka a human level reward design algorithm powered by llms eureka exploits the remarkable zero shot generation code writing and in context improvement capabilities of state of the art llms such as gpt 4 to perform evolutionary optimization over reward code the resulting rewards can then be used to acquire complex skills via reinforcement learning without any task specific prompting or pre defined reward templates eureka generates reward functions that outperform expert human engineered rewards in a diverse suite of 29 open source rl environments that include 10 distinct robot morphologies eureka outperforms human experts on 83 of the tasks leading to an average normalized improvement of 52 the generality of eureka also enables a new gradient free in context learning approach to reinforcement learning from human feedback rlhf readily incorporating human inputs to improve the quality and the safety of the generated rewards without model updating finally using eureka rewards in a curriculum learning setting we demonstrate for the first time a simulated shadow hand capable of performing pen spinning tricks adeptly manipulating a pen in circles at rapid speed
2023,florence 2 advancing a unified representation for a variety of vision tasks,bin xiao et al,https://arxiv.org/abs/2311.06242,vision,we introduce florence 2 a novel vision foundation model with a unified prompt based representation for a variety of computer vision and vision language tasks while existing large vision models excel in transfer learning they struggle to perform a diversity of tasks with simple instructions a capability that implies handling the complexity of various spatial hierarchy and semantic granularity florence 2 was designed to take text prompt as task instructions and generate desirable results in text forms whether it be captioning object detection grounding or segmentation this multi task learning setup demands large scale high quality annotated data to this end we co developed fld 5b that consists of 5 4 billion comprehensive visual annotations on 126 million images using an iterative strategy of automated image annotation and model refinement we adopted a sequence to sequence structure to train florence 2 to perform versatile and comprehensive vision tasks extensive evaluations on numerous tasks demonstrated florence 2 to be a strong vision foundation model contender with unprecedented zero shot and fine tuning capabilities,florence 2 advancing a unified representation for a variety of vision tasks bin xiao et al we introduce florence 2 a novel vision foundation model with a unified prompt based representation for a variety of computer vision and vision language tasks while existing large vision models excel in transfer learning they struggle to perform a diversity of tasks with simple instructions a capability that implies handling the complexity of various spatial hierarchy and semantic granularity florence 2 was designed to take text prompt as task instructions and generate desirable results in text forms whether it be captioning object detection grounding or segmentation this multi task learning setup demands large scale high quality annotated data to this end we co developed fld 5b that consists of 5 4 billion comprehensive visual annotations on 126 million images using an iterative strategy of automated image annotation and model refinement we adopted a sequence to sequence structure to train florence 2 to perform versatile and comprehensive vision tasks extensive evaluations on numerous tasks demonstrated florence 2 to be a strong vision foundation model contender with unprecedented zero shot and fine tuning capabilities
2023,videopoet a large language model for zero shot video generation,paul vicol et al,https://arxiv.org/abs/2312.14125,vision,we present videopoet a language model capable of synthesizing high quality video with matching audio from a large variety of conditioning signals videopoet employs a decoder only transformer architecture that processes multimodal inputs including images videos text and audio the training protocol follows that of large language models llms consisting of two stages pretraining and task specific adaptation during pretraining videopoet incorporates a mixture of multimodal generative objectives within an autoregressive transformer framework the pretrained llm serves as a foundation that can be adapted for a range of video generation tasks we present empirical results demonstrating the model s state of the art capabilities in zero shot video generation specifically highlighting videopoet s ability to generate high fidelity motions project page,videopoet a large language model for zero shot video generation paul vicol et al we present videopoet a language model capable of synthesizing high quality video with matching audio from a large variety of conditioning signals videopoet employs a decoder only transformer architecture that processes multimodal inputs including images videos text and audio the training protocol follows that of large language models llms consisting of two stages pretraining and task specific adaptation during pretraining videopoet incorporates a mixture of multimodal generative objectives within an autoregressive transformer framework the pretrained llm serves as a foundation that can be adapted for a range of video generation tasks we present empirical results demonstrating the model s state of the art capabilities in zero shot video generation specifically highlighting videopoet s ability to generate high fidelity motions project page
1951,a stochastic approximation method,h robbins and s monro,Let denote the expected value at level of the response to a certain experiment. is assumed to be a monotone function of but is unknown to the experimenter  and it is desired to find the solution of the equation   where is a given constant. We give a method for making successive experiments at levels in such a way that will tend to in probability.,dl_general,let denote the expected value at level of the response to a certain experiment is assumed to be a monotone function of but is unknown to the experimenter and it is desired to find the solution of the equation where is a given constant we give a method for making successive experiments at levels in such a way that will tend to in probability,a stochastic approximation method h robbins and s monro let denote the expected value at level of the response to a certain experiment is assumed to be a monotone function of but is unknown to the experimenter and it is desired to find the solution of the equation where is a given constant we give a method for making successive experiments at levels in such a way that will tend to in probability
1958,the perceptron a probabilistic model for information storage and organization in the brain,rosenblatt f, To answer the questions of how information about the physical world is sensed  in what form is information remembered  and how does information retained in memory influence recognition and behavior  a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems,dl_general,to answer the questions of how information about the physical world is sensed in what form is information remembered and how does information retained in memory influence recognition and behavior a theory is developed for a hypothetical nervous system called a perceptron the theory serves as a bridge between biophysics and psychology it is possible to predict learning curves from neurological variables and vice versa the quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems,the perceptron a probabilistic model for information storage and organization in the brain rosenblatt f to answer the questions of how information about the physical world is sensed in what form is information remembered and how does information retained in memory influence recognition and behavior a theory is developed for a hypothetical nervous system called a perceptron the theory serves as a bridge between biophysics and psychology it is possible to predict learning curves from neurological variables and vice versa the quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems
2010,understanding the difficulty of training deep feedforward neural networks,xavier glorot and yoshua bengio,Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained  since then several algorithms have been shown to successfully train them  with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks  to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value  which can drive especially the top hidden layer into saturation. Surprisingly  we find that saturated units can move out of saturation by themselves  albeit slowly  and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally  we study how activations and gradients vary across layers and during training  with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations  we propose a new initialization scheme that brings substantially faster convergence.,dl_general,whereas before 2006 it appears that deep multilayer neural networks were not successfully trained since then several algorithms have been shown to successfully train them with experimental results showing the superiority of deeper vs less deep architectures all these experimental results were obtained with new initialization or training mechanisms our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks to better understand these recent relative successes and help design better algorithms in the future we first observe the influence of the non linear activations functions we find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value which can drive especially the top hidden layer into saturation surprisingly we find that saturated units can move out of saturation by themselves albeit slowly and explaining the plateaus sometimes seen when training neural networks we find that a new non linearity that saturates less can often be beneficial finally we study how activations and gradients vary across layers and during training with the idea that training may be more difficult when the singular values of the jacobian associated with each layer are far from 1 based on these considerations we propose a new initialization scheme that brings substantially faster convergence,understanding the difficulty of training deep feedforward neural networks xavier glorot and yoshua bengio whereas before 2006 it appears that deep multilayer neural networks were not successfully trained since then several algorithms have been shown to successfully train them with experimental results showing the superiority of deeper vs less deep architectures all these experimental results were obtained with new initialization or training mechanisms our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks to better understand these recent relative successes and help design better algorithms in the future we first observe the influence of the non linear activations functions we find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value which can drive especially the top hidden layer into saturation surprisingly we find that saturated units can move out of saturation by themselves albeit slowly and explaining the plateaus sometimes seen when training neural networks we find that a new non linearity that saturates less can often be beneficial finally we study how activations and gradients vary across layers and during training with the idea that training may be more difficult when the singular values of the jacobian associated with each layer are far from 1 based on these considerations we propose a new initialization scheme that brings substantially faster convergence
2011,deep sparse rectifier neural networks,xavier glorot et al,While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons  the latter work better for training multi-layer neural networks. This pa-per shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiable at zero  creating sparse representations with true zeros  which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data  deep rectifier net-works can reach their best performance with-out requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence  these results can be seen as a new milestone in the attempts at under-standing the difficulty in training deep but purely supervised neural networks  and closing the performance gap between neural net-works learnt with and without unsupervised pre-training.,dl_general,while logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons the latter work better for training multi layer neural networks this pa per shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non linearity and non differentiable at zero creating sparse representations with true zeros which seem remarkably suitable for naturally sparse data even though they can take advantage of semi supervised setups with extra unlabeled data deep rectifier net works can reach their best performance with out requiring any unsupervised pre training on purely supervised tasks with large labeled datasets hence these results can be seen as a new milestone in the attempts at under standing the difficulty in training deep but purely supervised neural networks and closing the performance gap between neural net works learnt with and without unsupervised pre training,deep sparse rectifier neural networks xavier glorot et al while logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons the latter work better for training multi layer neural networks this pa per shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non linearity and non differentiable at zero creating sparse representations with true zeros which seem remarkably suitable for naturally sparse data even though they can take advantage of semi supervised setups with extra unlabeled data deep rectifier net works can reach their best performance with out requiring any unsupervised pre training on purely supervised tasks with large labeled datasets hence these results can be seen as a new milestone in the attempts at under standing the difficulty in training deep but purely supervised neural networks and closing the performance gap between neural net works learnt with and without unsupervised pre training
1960,a new approach to linear filtering and prediction problems,r e kalman,The classical filtering and prediction problem is re-examined using the Bode-Shannon representation of random processes and the state-transition method of analysis of dynamic systems. New results are: (1) The formulation and methods of solution of the problem apply without modification to stationary and nonstationary statistics and to growing-memory and infinite-memory filters. (2) A nonlinear difference (or differential) equation is derived for the covariance matrix of the optimal estimation error. From the solution of this equation the co-efficients of the difference (or differential) equation of the optimal linear filter are obtained without further calculations. (3) The filtering problem is shown to be the dual of the noise-free regulator problem. The new method developed here is applied to two well-known problems  confirming and extending earlier results. The discussion is largely self-contained and proceeds from first principles; basic concepts of the theory of random processes are reviewed in the Appendix.,signal,the classical filtering and prediction problem is re examined using the bode shannon representation of random processes and the state transition method of analysis of dynamic systems new results are 1 the formulation and methods of solution of the problem apply without modification to stationary and nonstationary statistics and to growing memory and infinite memory filters 2 a nonlinear difference or differential equation is derived for the covariance matrix of the optimal estimation error from the solution of this equation the co efficients of the difference or differential equation of the optimal linear filter are obtained without further calculations 3 the filtering problem is shown to be the dual of the noise free regulator problem the new method developed here is applied to two well known problems confirming and extending earlier results the discussion is largely self contained and proceeds from first principles basic concepts of the theory of random processes are reviewed in the appendix,a new approach to linear filtering and prediction problems r e kalman the classical filtering and prediction problem is re examined using the bode shannon representation of random processes and the state transition method of analysis of dynamic systems new results are 1 the formulation and methods of solution of the problem apply without modification to stationary and nonstationary statistics and to growing memory and infinite memory filters 2 a nonlinear difference or differential equation is derived for the covariance matrix of the optimal estimation error from the solution of this equation the co efficients of the difference or differential equation of the optimal linear filter are obtained without further calculations 3 the filtering problem is shown to be the dual of the noise free regulator problem the new method developed here is applied to two well known problems confirming and extending earlier results the discussion is largely self contained and proceeds from first principles basic concepts of the theory of random processes are reviewed in the appendix
2010,recurrent neural network based language model,tomas mikolov et al,A new recurrent neural network based language model (RNN LM) with applications to speech recognition is presented. Results indicate that it is possible to obtain around 50% reduction of perplexity by using mixture of several RNN LMs  compared to a state of the art backoff language model. Speech recognition experiments show around 18% reduction of word error rate on the Wall Street Journal task when comparing models trained on the same amount of data  and around 5% on the much harder NIST RT05 task  even when the backoff model is trained on much more data than the RNN LM. We provide ample empirical evidence to suggest that connectionist language models are superior to standard n-gram techniques  except their high computational (training) complexity.,dl_nlp,a new recurrent neural network based language model rnn lm with applications to speech recognition is presented results indicate that it is possible to obtain around 50 reduction of perplexity by using mixture of several rnn lms compared to a state of the art backoff language model speech recognition experiments show around 18 reduction of word error rate on the wall street journal task when comparing models trained on the same amount of data and around 5 on the much harder nist rt05 task even when the backoff model is trained on much more data than the rnn lm we provide ample empirical evidence to suggest that connectionist language models are superior to standard n gram techniques except their high computational training complexity,recurrent neural network based language model tomas mikolov et al a new recurrent neural network based language model rnn lm with applications to speech recognition is presented results indicate that it is possible to obtain around 50 reduction of perplexity by using mixture of several rnn lms compared to a state of the art backoff language model speech recognition experiments show around 18 reduction of word error rate on the wall street journal task when comparing models trained on the same amount of data and around 5 on the much harder nist rt05 task even when the backoff model is trained on much more data than the rnn lm we provide ample empirical evidence to suggest that connectionist language models are superior to standard n gram techniques except their high computational training complexity
2017,improving language understanding by generative pre training,a radford et al,Natural language understanding comprises a wide range of diverse tasks such as textual entailment question answering semantic similarity assessment and document classification. Although large unlabeled text corpora are abundant labeled data for learning these specific tasks is scarce making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text followed by discriminative fine-tuning on each specific task. In contrast to previous approaches we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test) 5.7% on question answering (RACE) and 1.5% on textual entailment (MultiNLI),dl_nlp,natural language understanding comprises a wide range of diverse tasks such as textual entailment question answering semantic similarity assessment and document classification although large unlabeled text corpora are abundant labeled data for learning these specific tasks is scarce making it challenging for discriminatively trained models to perform adequately we demonstrate that large gains on these tasks can be realized by generative pre training of a language model on a diverse corpus of unlabeled text followed by discriminative fine tuning on each specific task in contrast to previous approaches we make use of task aware input transformations during fine tuning to achieve effective transfer while requiring minimal changes to the model architecture we demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding our general task agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task significantly improving upon the state of the art in 9 out of the 12 tasks studied for instance we achieve absolute improvements of 8 9 on commonsense reasoning stories cloze test 5 7 on question answering race and 1 5 on textual entailment multinli,improving language understanding by generative pre training a radford et al natural language understanding comprises a wide range of diverse tasks such as textual entailment question answering semantic similarity assessment and document classification although large unlabeled text corpora are abundant labeled data for learning these specific tasks is scarce making it challenging for discriminatively trained models to perform adequately we demonstrate that large gains on these tasks can be realized by generative pre training of a language model on a diverse corpus of unlabeled text followed by discriminative fine tuning on each specific task in contrast to previous approaches we make use of task aware input transformations during fine tuning to achieve effective transfer while requiring minimal changes to the model architecture we demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding our general task agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task significantly improving upon the state of the art in 9 out of the 12 tasks studied for instance we achieve absolute improvements of 8 9 on commonsense reasoning stories cloze test 5 7 on question answering race and 1 5 on textual entailment multinli
2015,inceptionism going deeper into neural networks,alexander mordvintsev et al,Artificialneural networks have spurred remarkable recent progress in image classification and speech recognition but even though these are very useful tools based on wellknown mathematical methods we actually understand surprisingly little of why certain models work and others dont so lets take a look at some simple techniques for peeking inside these networkswe train an artificial neural network by showing it millions of training examples and gradually adjusting the network parameters until it gives the classifications we want the network typically consists of 1030 stacked layers of artificial neurons each image is fed into the input layer which then talks to the next layer until eventually the output layer is reached the networks answer comes from this final output layer,vision,artificialneural networks have spurred remarkable recent progress in image classification and speech recognition but even though these are very useful tools based on wellknown mathematical methods we actually understand surprisingly little of why certain models work and others dont so lets take a look at some simple techniques for peeking inside these networkswe train an artificial neural network by showing it millions of training examples and gradually adjusting the network parameters until it gives the classifications we want the network typically consists of 1030 stacked layers of artificial neurons each image is fed into the input layer which then talks to the next layer until eventually the output layer is reached the networks answer comes from this final output layer,inceptionism going deeper into neural networks alexander mordvintsev et al artificialneural networks have spurred remarkable recent progress in image classification and speech recognition but even though these are very useful tools based on wellknown mathematical methods we actually understand surprisingly little of why certain models work and others dont so lets take a look at some simple techniques for peeking inside these networkswe train an artificial neural network by showing it millions of training examples and gradually adjusting the network parameters until it gives the classifications we want the network typically consists of 1030 stacked layers of artificial neurons each image is fed into the input layer which then talks to the next layer until eventually the output layer is reached the networks answer comes from this final output layer
1988,indexing by latent semantic analysis,scott deerwester et al,"A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher?order structure in the association of terms with documents (�semantic structure�) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular?value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo?document vectors formed from weighted combinations of terms, and documents with supra?threshold cosine values are returned. Initial tests find this completely automatic method for retrieval to be promising. � 1990 John Wiley & Sons, Inc.",ml_classic,a new method for automatic indexing and retrieval is described the approach is to take advantage of implicit higher order structure in the association of terms with documents semantic structure in order to improve the detection of relevant documents on the basis of terms found in queries the particular technique used is singular value decomposition in which a large term by document matrix is decomposed into a set of ca 100 orthogonal factors from which the original matrix can be approximated by linear combination documents are represented by ca 100 item vectors of factor weights queries are represented as pseudo document vectors formed from weighted combinations of terms and documents with supra threshold cosine values are returned initial tests find this completely automatic method for retrieval to be promising 1990 john wiley sons inc,indexing by latent semantic analysis scott deerwester et al a new method for automatic indexing and retrieval is described the approach is to take advantage of implicit higher order structure in the association of terms with documents semantic structure in order to improve the detection of relevant documents on the basis of terms found in queries the particular technique used is singular value decomposition in which a large term by document matrix is decomposed into a set of ca 100 orthogonal factors from which the original matrix can be approximated by linear combination documents are represented by ca 100 item vectors of factor weights queries are represented as pseudo document vectors formed from weighted combinations of terms and documents with supra threshold cosine values are returned initial tests find this completely automatic method for retrieval to be promising 1990 john wiley sons inc
1991,multivariate adaptive regression splines,jerome h friedman,A new method is presented for flexible regression modeling of high dimensional data. The model takes the form of an expansion in product spline basis functions  where the number of basis functions as well as the parameters associated with each one (product degree and knot locations) are automatically determined by the data. This procedure is motivated by the recursive partitioning approach to regression and shares its attractive properties. Unlike recursive partitioning  however  this method produces continuous models with continuous derivatives. It has more power and flexibility to model relationships that are nearly additive or involve interactions in at most a few variables. In addition  the model can be represented in a form that separately identifies the additive contributions and those associated with the different multivariable interactions.,ml_classic,a new method is presented for flexible regression modeling of high dimensional data the model takes the form of an expansion in product spline basis functions where the number of basis functions as well as the parameters associated with each one product degree and knot locations are automatically determined by the data this procedure is motivated by the recursive partitioning approach to regression and shares its attractive properties unlike recursive partitioning however this method produces continuous models with continuous derivatives it has more power and flexibility to model relationships that are nearly additive or involve interactions in at most a few variables in addition the model can be represented in a form that separately identifies the additive contributions and those associated with the different multivariable interactions,multivariate adaptive regression splines jerome h friedman a new method is presented for flexible regression modeling of high dimensional data the model takes the form of an expansion in product spline basis functions where the number of basis functions as well as the parameters associated with each one product degree and knot locations are automatically determined by the data this procedure is motivated by the recursive partitioning approach to regression and shares its attractive properties unlike recursive partitioning however this method produces continuous models with continuous derivatives it has more power and flexibility to model relationships that are nearly additive or involve interactions in at most a few variables in addition the model can be represented in a form that separately identifies the additive contributions and those associated with the different multivariable interactions
1996,regression shrinkage and selection via the lasso,robert tibshirani,We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.,ml_classic,we propose a new method for estimation in linear models the lasso minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression it produces interpretable models like subset selection and exhibits the stability of ridge regression there is also an interesting relationship with recent work in adaptive function estimation by donoho and johnstone the lasso idea is quite general and can be applied in a variety of statistical models extensions to generalized regression models and tree based models are briefly described,regression shrinkage and selection via the lasso robert tibshirani we propose a new method for estimation in linear models the lasso minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression it produces interpretable models like subset selection and exhibits the stability of ridge regression there is also an interesting relationship with recent work in adaptive function estimation by donoho and johnstone the lasso idea is quite general and can be applied in a variety of statistical models extensions to generalized regression models and tree based models are briefly described
1997,bias plus variance decomposition for zero one loss functions,ron kohavi and david h wolpert,We present a bias-variance decomposition of expected misclassification rate  the most commonly used loss function in supervised classification learning. The bias-variance decomposition for quadratic loss functions is well known and serves as an important tool for analyzing learning algorithms  yet no decomposition was offered for the more commonly used zero-one (misclassification) loss functions until the recent work of Kong & Dietterich (1995) and Breiman (1996). Their decomposition suffers from some major shortcomings though (eg  potentially negative variance)  which our decomposition avoids. We show that  in practice  the naive frequency-based estimation of the decomposition terms is by itself biased and show how to correct for this bias. We illustrate the decomposition on various algorithms and datasets from the UCI repository.,ml_classic,we present a bias variance decomposition of expected misclassification rate the most commonly used loss function in supervised classification learning the bias variance decomposition for quadratic loss functions is well known and serves as an important tool for analyzing learning algorithms yet no decomposition was offered for the more commonly used zero one misclassification loss functions until the recent work of kong dietterich 1995 and breiman 1996 their decomposition suffers from some major shortcomings though eg potentially negative variance which our decomposition avoids we show that in practice the naive frequency based estimation of the decomposition terms is by itself biased and show how to correct for this bias we illustrate the decomposition on various algorithms and datasets from the uci repository,bias plus variance decomposition for zero one loss functions ron kohavi and david h wolpert we present a bias variance decomposition of expected misclassification rate the most commonly used loss function in supervised classification learning the bias variance decomposition for quadratic loss functions is well known and serves as an important tool for analyzing learning algorithms yet no decomposition was offered for the more commonly used zero one misclassification loss functions until the recent work of kong dietterich 1995 and breiman 1996 their decomposition suffers from some major shortcomings though eg potentially negative variance which our decomposition avoids we show that in practice the naive frequency based estimation of the decomposition terms is by itself biased and show how to correct for this bias we illustrate the decomposition on various algorithms and datasets from the uci repository
2003,latent dirichlet allocation,david m blei et al,We describe latent Dirichlet allocation (LDA)  a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model  in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is  in turn  modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling  the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling  text classification  and collaborative filtering  comparing to a mixture of unigrams model and the probabilistic LSI model.,ml_classic,we describe latent dirichlet allocation lda a generative probabilistic model for collections of discrete data such as text corpora lda is a three level hierarchical bayesian model in which each item of a collection is modeled as a finite mixture over an underlying set of topics each topic is in turn modeled as an infinite mixture over an underlying set of topic probabilities in the context of text modeling the topic probabilities provide an explicit representation of a document we present efficient approximate inference techniques based on variational methods and an em algorithm for empirical bayes parameter estimation we report results in document modeling text classification and collaborative filtering comparing to a mixture of unigrams model and the probabilistic lsi model,latent dirichlet allocation david m blei et al we describe latent dirichlet allocation lda a generative probabilistic model for collections of discrete data such as text corpora lda is a three level hierarchical bayesian model in which each item of a collection is modeled as a finite mixture over an underlying set of topics each topic is in turn modeled as an infinite mixture over an underlying set of topic probabilities in the context of text modeling the topic probabilities provide an explicit representation of a document we present efficient approximate inference techniques based on variational methods and an em algorithm for empirical bayes parameter estimation we report results in document modeling text classification and collaborative filtering comparing to a mixture of unigrams model and the probabilistic lsi model
1997,computational power of neural networks a characterization in terms of kolmogorov complexity,josé l balcázar et al,The computational power of recurrent neural networks is shown to depend ultimately on the complexity of the real constants (weights) of the network. The complexity  or information contents  of the weights is measured by a variant of resource-bounded Kolmogorov (1965) complexity  taking into account the time required for constructing the numbers. In particular  we reveal a full and proper hierarchy of nonuniform complexity classes associated with networks having weights of increasing Kolmogorov complexity,learning_theory,the computational power of recurrent neural networks is shown to depend ultimately on the complexity of the real constants weights of the network the complexity or information contents of the weights is measured by a variant of resource bounded kolmogorov 1965 complexity taking into account the time required for constructing the numbers in particular we reveal a full and proper hierarchy of nonuniform complexity classes associated with networks having weights of increasing kolmogorov complexity,computational power of neural networks a characterization in terms of kolmogorov complexity josé l balcázar et al the computational power of recurrent neural networks is shown to depend ultimately on the complexity of the real constants weights of the network the complexity or information contents of the weights is measured by a variant of resource bounded kolmogorov 1965 complexity taking into account the time required for constructing the numbers in particular we reveal a full and proper hierarchy of nonuniform complexity classes associated with networks having weights of increasing kolmogorov complexity
1999,generalization in a linear perceptron in the presence of noise,anders krogh and john a hertz,The authors study the evolution of the generalization ability of a simple linear perceptron with N inputs which learns to imitate a 'teacher perceptron'. The system is trained on p= alpha N example inputs drawn from some distribution and the generalization ability is measured by the average agreement with the teacher on test examples drawn from the same distribution. The dynamics may be solved analytically and exhibits a phase transition from imperfect to perfect generalization at alpha =1  when there are no errors (static noise) in the training examples. If the examples are produced by an erroneous teacher  overfitting is observed  i.e. the generalization error starts to increase after a finite time of training. It is shown that a weight decay of the same size as the variance of the noise (errors) on the teacher improves on the generalization and suppresses the overfitting. The generalization error as a function of time is calculated numerically for various values of the parameters. Finally dynamic noise in the training is considered. White noise on the input corresponds on average to a weight decay  and can thus improve generalization  whereas white noise on the weights or the output degrades generalization. Generalization is particularly sensitive to noise on the weights (for alpha (1) where it makes the error constantly increase with time  but this effect is also shown to be damped by a weight decay. Weight noise and output noise acts similarly above the transition at alpha =1.,learning_theory,the authors study the evolution of the generalization ability of a simple linear perceptron with n inputs which learns to imitate a teacher perceptron the system is trained on p alpha n example inputs drawn from some distribution and the generalization ability is measured by the average agreement with the teacher on test examples drawn from the same distribution the dynamics may be solved analytically and exhibits a phase transition from imperfect to perfect generalization at alpha 1 when there are no errors static noise in the training examples if the examples are produced by an erroneous teacher overfitting is observed i e the generalization error starts to increase after a finite time of training it is shown that a weight decay of the same size as the variance of the noise errors on the teacher improves on the generalization and suppresses the overfitting the generalization error as a function of time is calculated numerically for various values of the parameters finally dynamic noise in the training is considered white noise on the input corresponds on average to a weight decay and can thus improve generalization whereas white noise on the weights or the output degrades generalization generalization is particularly sensitive to noise on the weights for alpha 1 where it makes the error constantly increase with time but this effect is also shown to be damped by a weight decay weight noise and output noise acts similarly above the transition at alpha 1,generalization in a linear perceptron in the presence of noise anders krogh and john a hertz the authors study the evolution of the generalization ability of a simple linear perceptron with n inputs which learns to imitate a teacher perceptron the system is trained on p alpha n example inputs drawn from some distribution and the generalization ability is measured by the average agreement with the teacher on test examples drawn from the same distribution the dynamics may be solved analytically and exhibits a phase transition from imperfect to perfect generalization at alpha 1 when there are no errors static noise in the training examples if the examples are produced by an erroneous teacher overfitting is observed i e the generalization error starts to increase after a finite time of training it is shown that a weight decay of the same size as the variance of the noise errors on the teacher improves on the generalization and suppresses the overfitting the generalization error as a function of time is calculated numerically for various values of the parameters finally dynamic noise in the training is considered white noise on the input corresponds on average to a weight decay and can thus improve generalization whereas white noise on the weights or the output degrades generalization generalization is particularly sensitive to noise on the weights for alpha 1 where it makes the error constantly increase with time but this effect is also shown to be damped by a weight decay weight noise and output noise acts similarly above the transition at alpha 1
1989,learning from delayed rewards,christopher watkins,In this paper  a collection of value-based quantum reinforcement learning algorithms are introduced which use Grover’s algorithm to update the policy  which is stored as a superposition of qubits associated with each possible action  and their parameters are explored. These algorithms may be grouped in two classes  one class which uses value functions (V(s)) and new class which uses action value functions (Q(s a)). The new (Q(s a))-based quantum algorithms are found to converge faster than V(s)-based algorithms  and in general the quantum algorithms are found to converge in fewer iterations than their classical counterparts  netting larger returns during training. This is due to fact that the (Q(s a)) algorithms are more precise than those based on V(s)  meaning that updates are incorporated into the value function more efficiently. This effect is also enhanced by the observation that the Q(s a)-based algorithms may be trained with higher learning rates. These algorithms are then extended by adding multiple value functions  which are observed to allow larger learning rates and have improved convergence properties in environments with stochastic rewards  the latter of which is further improved by the probabilistic nature of the quantum algorithms. Finally  the quantum algorithms were found to use less CPU time than their classical counterparts overall  meaning that their benefits may be realized even without a full quantum computer.,dl_rl,in this paper a collection of value based quantum reinforcement learning algorithms are introduced which use grover s algorithm to update the policy which is stored as a superposition of qubits associated with each possible action and their parameters are explored these algorithms may be grouped in two classes one class which uses value functions v s and new class which uses action value functions q s a the new q s a based quantum algorithms are found to converge faster than v s based algorithms and in general the quantum algorithms are found to converge in fewer iterations than their classical counterparts netting larger returns during training this is due to fact that the q s a algorithms are more precise than those based on v s meaning that updates are incorporated into the value function more efficiently this effect is also enhanced by the observation that the q s a based algorithms may be trained with higher learning rates these algorithms are then extended by adding multiple value functions which are observed to allow larger learning rates and have improved convergence properties in environments with stochastic rewards the latter of which is further improved by the probabilistic nature of the quantum algorithms finally the quantum algorithms were found to use less cpu time than their classical counterparts overall meaning that their benefits may be realized even without a full quantum computer,learning from delayed rewards christopher watkins in this paper a collection of value based quantum reinforcement learning algorithms are introduced which use grover s algorithm to update the policy which is stored as a superposition of qubits associated with each possible action and their parameters are explored these algorithms may be grouped in two classes one class which uses value functions v s and new class which uses action value functions q s a the new q s a based quantum algorithms are found to converge faster than v s based algorithms and in general the quantum algorithms are found to converge in fewer iterations than their classical counterparts netting larger returns during training this is due to fact that the q s a algorithms are more precise than those based on v s meaning that updates are incorporated into the value function more efficiently this effect is also enhanced by the observation that the q s a based algorithms may be trained with higher learning rates these algorithms are then extended by adding multiple value functions which are observed to allow larger learning rates and have improved convergence properties in environments with stochastic rewards the latter of which is further improved by the probabilistic nature of the quantum algorithms finally the quantum algorithms were found to use less cpu time than their classical counterparts overall meaning that their benefits may be realized even without a full quantum computer
1991,nonlinear principal component analysis using autoassociative neural networks,mark a kramer,Nonlinear principal component analysis is a novel technique for multivariate data analysis  similar to the well-known method of principal component analysis. NLPCA  like PCA  is used to identify and remove correlations among problem variables as an aid to dimensionality reduction  visualization  and exploratory data analysis. While PCA identifies only linear correlations between variables  NLPCA uncovers both linear and nonlinear correlations  without restriction on the character of the nonlinearities present in the data. NLPCA operates by training a feedforward neural network to perform the identity mapping  where the network inputs are reproduced at the output layer. The network contains an internal “bottleneck” layer (containing fewer nodes than input or output layers)  which forces the network to develop a compact representation of the input data  and two additional hidden layers. The NLPCA method is demonstrated using time-dependent  simulated batch reaction data. Results show that NLPCA successfully reduces dimensionality and produces a feature space map resembling the actual distribution of the underlying system parameters.,ml_representation,nonlinear principal component analysis is a novel technique for multivariate data analysis similar to the well known method of principal component analysis nlpca like pca is used to identify and remove correlations among problem variables as an aid to dimensionality reduction visualization and exploratory data analysis while pca identifies only linear correlations between variables nlpca uncovers both linear and nonlinear correlations without restriction on the character of the nonlinearities present in the data nlpca operates by training a feedforward neural network to perform the identity mapping where the network inputs are reproduced at the output layer the network contains an internal bottleneck layer containing fewer nodes than input or output layers which forces the network to develop a compact representation of the input data and two additional hidden layers the nlpca method is demonstrated using time dependent simulated batch reaction data results show that nlpca successfully reduces dimensionality and produces a feature space map resembling the actual distribution of the underlying system parameters,nonlinear principal component analysis using autoassociative neural networks mark a kramer nonlinear principal component analysis is a novel technique for multivariate data analysis similar to the well known method of principal component analysis nlpca like pca is used to identify and remove correlations among problem variables as an aid to dimensionality reduction visualization and exploratory data analysis while pca identifies only linear correlations between variables nlpca uncovers both linear and nonlinear correlations without restriction on the character of the nonlinearities present in the data nlpca operates by training a feedforward neural network to perform the identity mapping where the network inputs are reproduced at the output layer the network contains an internal bottleneck layer containing fewer nodes than input or output layers which forces the network to develop a compact representation of the input data and two additional hidden layers the nlpca method is demonstrated using time dependent simulated batch reaction data results show that nlpca successfully reduces dimensionality and produces a feature space map resembling the actual distribution of the underlying system parameters
2002,stochastic neighbor embedding,geoffrey hinton and sam roweis,We describe a probabilistic approach to the task of placing objects  de- scribed by high-dimensional vectors or by pairwise dissimilarities  in a low-dimensional space in a way that preserves neighbor identities. A Gaussian is centered on each object in the high-dimensional space and the densities under this Gaussian (or the given dissimilarities) are used to de?ne a probability distribution over all the potential neighbors of the object. The aim of the embedding is to approximate this distribu- tion as well as possible when the same operation is performed on the low-dimensional of the objects. A natural cost function is a sum of Kullback-Leibler divergences  one per object  which leads to a simple gradient for adjusting the positions of the low-dimensional im- ages. Unlike other dimensionality reduction methods  this probabilistic framework makes it easy to represent each object by a mixture of widely separated low-dimensional images. This allows ambiguous objects  like the document count vector for the word bank  to have versions close to the images of both river and nance without forcing the images of outdoor concepts to be located close to those of corporate concepts.,ml_representation,we describe a probabilistic approach to the task of placing objects de scribed by high dimensional vectors or by pairwise dissimilarities in a low dimensional space in a way that preserves neighbor identities a gaussian is centered on each object in the high dimensional space and the densities under this gaussian or the given dissimilarities are used to de ne a probability distribution over all the potential neighbors of the object the aim of the embedding is to approximate this distribu tion as well as possible when the same operation is performed on the low dimensional of the objects a natural cost function is a sum of kullback leibler divergences one per object which leads to a simple gradient for adjusting the positions of the low dimensional im ages unlike other dimensionality reduction methods this probabilistic framework makes it easy to represent each object by a mixture of widely separated low dimensional images this allows ambiguous objects like the document count vector for the word bank to have versions close to the images of both river and nance without forcing the images of outdoor concepts to be located close to those of corporate concepts,stochastic neighbor embedding geoffrey hinton and sam roweis we describe a probabilistic approach to the task of placing objects de scribed by high dimensional vectors or by pairwise dissimilarities in a low dimensional space in a way that preserves neighbor identities a gaussian is centered on each object in the high dimensional space and the densities under this gaussian or the given dissimilarities are used to de ne a probability distribution over all the potential neighbors of the object the aim of the embedding is to approximate this distribu tion as well as possible when the same operation is performed on the low dimensional of the objects a natural cost function is a sum of kullback leibler divergences one per object which leads to a simple gradient for adjusting the positions of the low dimensional im ages unlike other dimensionality reduction methods this probabilistic framework makes it easy to represent each object by a mixture of widely separated low dimensional images this allows ambiguous objects like the document count vector for the word bank to have versions close to the images of both river and nance without forcing the images of outdoor concepts to be located close to those of corporate concepts
2006,reducing the dimensionality of data with neural networks,geoff hinton and r r salakhutdinov,High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.,ml_representation,high dimensional data can be converted to low dimensional codes by training a multilayer neural network with a small central layer to reconstruct high dimensional input vectors gradient descent can be used for fine tuning the weights in such autoencoder networks but this works well only if the initial weights are close to a good solution we describe an effective way of initializing the weights that allows deep autoencoder networks to learn low dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data,reducing the dimensionality of data with neural networks geoff hinton and r r salakhutdinov high dimensional data can be converted to low dimensional codes by training a multilayer neural network with a small central layer to reconstruct high dimensional input vectors gradient descent can be used for fine tuning the weights in such autoencoder networks but this works well only if the initial weights are close to a good solution we describe an effective way of initializing the weights that allows deep autoencoder networks to learn low dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data
2008,visualizing data using t sne,laurens van der maaten and geoffrey hinton,"We present a new technique called ""t-SNE"" that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis  2002) that is much easier to optimize  and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different  but related  low-dimensional manifolds  such as images ofobjects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets  we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques  including Sammon mapping  Isomap  and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets",ml_representation,we present a new technique called t sne that visualizes high dimensional data by giving each datapoint a location in a two or three dimensional map the technique is a variation of stochastic neighbor embedding hinton and roweis 2002 that is much easier to optimize and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map t sne is better than existing techniques at creating a single map that reveals structure at many different scales this is particularly important for high dimensional data that lie on several different but related low dimensional manifolds such as images ofobjects from multiple classes seen from multiple viewpoints for visualizing the structure of very large data sets we show how t sne can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed we illustrate the performance of t sne on a wide variety of data sets and compare it with many other non parametric visualization techniques including sammon mapping isomap and locally linear embedding the visualizations produced by t sne are significantly better than those produced by the other techniques on almost all of the data sets,visualizing data using t sne laurens van der maaten and geoffrey hinton we present a new technique called t sne that visualizes high dimensional data by giving each datapoint a location in a two or three dimensional map the technique is a variation of stochastic neighbor embedding hinton and roweis 2002 that is much easier to optimize and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map t sne is better than existing techniques at creating a single map that reveals structure at many different scales this is particularly important for high dimensional data that lie on several different but related low dimensional manifolds such as images ofobjects from multiple classes seen from multiple viewpoints for visualizing the structure of very large data sets we show how t sne can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed we illustrate the performance of t sne on a wide variety of data sets and compare it with many other non parametric visualization techniques including sammon mapping isomap and locally linear embedding the visualizations produced by t sne are significantly better than those produced by the other techniques on almost all of the data sets
2014,locality preserving hashing,yi hsuan tsai and ming hsuan yang,The spectral hashing algorithm relaxes and solves an objective function for generating hash codes such that data similarity is preserved in the Hamming space. However  the assumption of uniform global data distribution limits its applicability. In the paper  we introduce locality preserving projection to determine the data distribution adaptively  and a spectral method is adopted to estimate the eigen functions of the under-lying graph Laplacian. Furthermore  pairwise label similarity can be further incorporated in the weight matrix to bridge the semantic gap between data and hash codes. Experiments on three benchmark datasets show the proposed algorithm per-forms favorably against state-of-the-art hashing methods,ml_representation,the spectral hashing algorithm relaxes and solves an objective function for generating hash codes such that data similarity is preserved in the hamming space however the assumption of uniform global data distribution limits its applicability in the paper we introduce locality preserving projection to determine the data distribution adaptively and a spectral method is adopted to estimate the eigen functions of the under lying graph laplacian furthermore pairwise label similarity can be further incorporated in the weight matrix to bridge the semantic gap between data and hash codes experiments on three benchmark datasets show the proposed algorithm per forms favorably against state of the art hashing methods,locality preserving hashing yi hsuan tsai and ming hsuan yang the spectral hashing algorithm relaxes and solves an objective function for generating hash codes such that data similarity is preserved in the hamming space however the assumption of uniform global data distribution limits its applicability in the paper we introduce locality preserving projection to determine the data distribution adaptively and a spectral method is adopted to estimate the eigen functions of the under lying graph laplacian furthermore pairwise label similarity can be further incorporated in the weight matrix to bridge the semantic gap between data and hash codes experiments on three benchmark datasets show the proposed algorithm per forms favorably against state of the art hashing methods
2015,siamese neural networks for one shot image recognition,gregory koch,The process of learning good features for machine learning applications can be very computationally expensive and may prove difficult in cases where little data is available. A prototypical example of this is the one-shot learning setting  in which we must correctly make predictions given only a single example of each new class. In this paper  we explore a method for learning siamese neural networks which employ a unique structure to naturally rank similarity between inputs. Once a network has been tuned  we can then capitalize on powerful discriminative features to generalize the predictive power of the network not just to new data  but to entirely new classes from unknown distributions. Using a convolutional architecture  we are able to achieve strong results which exceed those of other deep learning models with near state-of-the-art performance on one-shot classification tasks.,ml_representation,the process of learning good features for machine learning applications can be very computationally expensive and may prove difficult in cases where little data is available a prototypical example of this is the one shot learning setting in which we must correctly make predictions given only a single example of each new class in this paper we explore a method for learning siamese neural networks which employ a unique structure to naturally rank similarity between inputs once a network has been tuned we can then capitalize on powerful discriminative features to generalize the predictive power of the network not just to new data but to entirely new classes from unknown distributions using a convolutional architecture we are able to achieve strong results which exceed those of other deep learning models with near state of the art performance on one shot classification tasks,siamese neural networks for one shot image recognition gregory koch the process of learning good features for machine learning applications can be very computationally expensive and may prove difficult in cases where little data is available a prototypical example of this is the one shot learning setting in which we must correctly make predictions given only a single example of each new class in this paper we explore a method for learning siamese neural networks which employ a unique structure to naturally rank similarity between inputs once a network has been tuned we can then capitalize on powerful discriminative features to generalize the predictive power of the network not just to new data but to entirely new classes from unknown distributions using a convolutional architecture we are able to achieve strong results which exceed those of other deep learning models with near state of the art performance on one shot classification tasks
