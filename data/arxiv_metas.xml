<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3D%26id_list%3D0803.0476%2C0912.3995%2C1103.0398%2C1112.6209%2C1206.5538%2C1207.0580%2C1301.3781%2C1301.3781%2C1302.4389v4%2C1303.5778%2C1308.0850%2C1308.0850%2C1310.1531%2C1311.2901%2C1312.4400%2C1312.5602%2C1312.5602%2C1312.6114%2C1312.6114%2C1312.6199%2C1312.6229%2C1404.2188v1%2C1404.7828%2C1405.3531%2C1405.4053%2C1406.1078%2C1406.2572%2C1406.4729%2C1406.6247%2C1408.5093%2C1408.5882%2C1409.0473%2C1409.0473%2C1409.0473v7%2C1409.0575%2C1409.1259%2C1409.1556%2C1409.1556%2C1409.2329%2C1409.3215%2C1410.3831%2C1410.3916%2C1410.5401%2C1410.5401%2C1410.8206%2C1412.0035%2C1412.1842%2C1412.1897%2C1412.1897%2C1412.3555%2C1412.4564%2C1412.6621%2C1412.6980%2C1412.6980%2C1412.7062%2C1501.00092v3%2C1502.03044%2C1502.03167%2C1502.03167%2C1502.04623%2C1502.05082%2C1502.05698%2C1503.00075%2C1503.00949%2C1503.02406%2C1503.02531%2C1503.02531%2C1503.03585%2C1503.04069%2C1503.05671%2C1505.00387%2C1505.03540%2C1505.04366v1%2C1505.04597%2C1506.02078%2C1506.02142%2C1506.05869%2C1506.06579%2C1506.07285%2C1508.04025%2C1508.04395%2C1508.06576%2C1508.06615%2C1508.07909%2C1509.02971%2C1509.06461%2C1510.00149%2C1511.06434v2%2C1511.07289%2C1512.02325%2C1512.02595%2C1512.03385%2C1512.03385%2C1512.03385%2C1601.01705%2C1601.06759v2%2C1602.01528%2C1602.02410%2C1602.02830%2C1602.07261%2C1602.07360%2C1603.02199%2C1603.02754%2C1603.04467%2C1603.05027v2%2C1603.06147%2C1603.06560%2C1603.08155%2C1603.08511%2C1603.08983%2C1603.09382%2C1604.00788%2C1604.08610%2C1605.09081v1%2C1606.00704v1%2C1606.01540%2C1606.01781%2C1606.02858%2C1606.04474v1%2C1606.05250%2C1606.05908%2C1606.08415v4%2C1607.01759%2C1607.06450v1%2C1608.00060%2C1608.06993v1%2C1609.03499%2C1609.03499v2%2C1609.03552%2C1609.08144%2C1609.08144v2%2C1610.09038%2C1611.01144v5%2C1611.03530%2C1611.04076v2%2C1612.03144%2C1701.07274v2%2C1701.07875v1%2C1702.01932%2C1702.03275%2C1702.06506v1%2C1702.07800%2C1702.07825v2%2C1703.01619v1%2C1703.03130%2C1703.03864v1%2C1703.05192v1%2C1703.06211v2%2C1703.06868%2C1703.06870%2C1703.07511v1%2C1703.10135%2C1704.00805%2C1704.04289%2C1704.04861%2C1705.03122%2C1706.03762%2C1706.06859%2C1706.08500v6%2C1707.06347%2C1708.05344%2C1709.01507%2C1711.00937%2C1711.11053%2C1801.04381%2C1801.05134%2C1801.06146%2C1802.03426%2C1803.03635%2C1806.01261%2C1806.07572%2C1806.09594%2C1807.03748%2C1810.04805%2C1810.04805v2%2C1812.03828%2C1906.00446%2C1908.05355%2C1910.02054%2C1911.01547%2C1912.02292%2C2003.08934%2C2005.14165%2C2006.09661%2C2006.10739%2C2006.11239%2C2010.11929%2C2011.01808v1%2C2012.09841%2C2012.13255%2C2103.00020%2C2103.00564%2C2104.13478%2C2104.14294%2C2106.09685%2C2108.01073%2C2109.01652%2C2109.02355%2C2109.11978%2C2109.13226%2C2110.05069%2C2110.07205%2C2110.08207%2C2110.09485%2C2111.02358%2C2112.06825%2C2112.10752%2C2201.02177%2C2201.03545%2C2201.08239%2C2201.09792%2C2201.11903%2C2202.01374%2C2202.08433%2C2202.11214%2C2203.02053%2C2203.02155%2C2203.15556%2C2204.01691%2C2204.01697%2C2204.02311%2C2204.03162%2C2204.03409%2C2204.06125%2C2204.06745%2C2204.14198%2C2205.01580%2C2205.01917%2C2205.06175%2C2205.10343%2C2205.11487%2C2205.14100%2C2206.00364%2C2206.04615%2C2206.13170%2C2206.14858%2C2207.12598%2C2208.09392%2C2208.10442v2%2C2208.12242%2C2208.12415%2C2209.03143%2C2209.06794%2C2209.14792%2C2209.14988%2C2209.15352%2C2210.02186%2C2210.03142%2C2210.03629%2C2210.08402%2C2210.09276%2C2210.13438%2C2211.05100%2C2211.09788%2C2211.09800%2C2211.10440%2C2212.04356%2C2212.04488%2C2212.06817%2C2212.09748%2C2212.13138%2C2301.00704%2C2301.02111%2C2301.04104%2C2301.11305%2C2301.11325%2C2301.12503%2C2302.01318%2C2302.02662%2C2302.02948%2C2302.03011%2C2302.04761%2C2302.05442%2C2302.05543%2C2302.06675%2C2302.13971%2C2302.14045v2%2C2303.00001%2C2303.01037%2C2303.03378%2C2303.04671%2C2303.05511%2C2303.06296%2C2303.12712%2C2303.17564%2C2303.17580%2C2304.02643%2C2304.03277%2C2304.03442%2C2304.07193%2C2304.08466%2C2304.08485%2C2304.08818%2C2304.12306%2C2304.12995%2C2305.10403%2C2305.10601%2C2305.10973%2C2305.11206%2C2305.13516%2C2305.14314%2C2305.16291%2C2305.16367%2C2305.18290%2C2305.19466%2C2306.03092%2C2306.05284%2C2306.12925%2C2306.15687%2C2307.01952%2C2307.10802%2C2307.15818%2C2307.16789%2C2308.00352%2C2308.02151%2C2308.11596%2C2308.12950%2C2308.12966%2C2308.16512%2C2309.00267%2C2309.03409%2C2309.16588%2C2309.17453%2C2310.12931%2C2311.06242%2C2312.14125%26start%3D0%26max_results%3D326" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=&amp;id_list=0803.0476,0912.3995,1103.0398,1112.6209,1206.5538,1207.0580,1301.3781,1301.3781,1302.4389v4,1303.5778,1308.0850,1308.0850,1310.1531,1311.2901,1312.4400,1312.5602,1312.5602,1312.6114,1312.6114,1312.6199,1312.6229,1404.2188v1,1404.7828,1405.3531,1405.4053,1406.1078,1406.2572,1406.4729,1406.6247,1408.5093,1408.5882,1409.0473,1409.0473,1409.0473v7,1409.0575,1409.1259,1409.1556,1409.1556,1409.2329,1409.3215,1410.3831,1410.3916,1410.5401,1410.5401,1410.8206,1412.0035,1412.1842,1412.1897,1412.1897,1412.3555,1412.4564,1412.6621,1412.6980,1412.6980,1412.7062,1501.00092v3,1502.03044,1502.03167,1502.03167,1502.04623,1502.05082,1502.05698,1503.00075,1503.00949,1503.02406,1503.02531,1503.02531,1503.03585,1503.04069,1503.05671,1505.00387,1505.03540,1505.04366v1,1505.04597,1506.02078,1506.02142,1506.05869,1506.06579,1506.07285,1508.04025,1508.04395,1508.06576,1508.06615,1508.07909,1509.02971,1509.06461,1510.00149,1511.06434v2,1511.07289,1512.02325,1512.02595,1512.03385,1512.03385,1512.03385,1601.01705,1601.06759v2,1602.01528,1602.02410,1602.02830,1602.07261,1602.07360,1603.02199,1603.02754,1603.04467,1603.05027v2,1603.06147,1603.06560,1603.08155,1603.08511,1603.08983,1603.09382,1604.00788,1604.08610,1605.09081v1,1606.00704v1,1606.01540,1606.01781,1606.02858,1606.04474v1,1606.05250,1606.05908,1606.08415v4,1607.01759,1607.06450v1,1608.00060,1608.06993v1,1609.03499,1609.03499v2,1609.03552,1609.08144,1609.08144v2,1610.09038,1611.01144v5,1611.03530,1611.04076v2,1612.03144,1701.07274v2,1701.07875v1,1702.01932,1702.03275,1702.06506v1,1702.07800,1702.07825v2,1703.01619v1,1703.03130,1703.03864v1,1703.05192v1,1703.06211v2,1703.06868,1703.06870,1703.07511v1,1703.10135,1704.00805,1704.04289,1704.04861,1705.03122,1706.03762,1706.06859,1706.08500v6,1707.06347,1708.05344,1709.01507,1711.00937,1711.11053,1801.04381,1801.05134,1801.06146,1802.03426,1803.03635,1806.01261,1806.07572,1806.09594,1807.03748,1810.04805,1810.04805v2,1812.03828,1906.00446,1908.05355,1910.02054,1911.01547,1912.02292,2003.08934,2005.14165,2006.09661,2006.10739,2006.11239,2010.11929,2011.01808v1,2012.09841,2012.13255,2103.00020,2103.00564,2104.13478,2104.14294,2106.09685,2108.01073,2109.01652,2109.02355,2109.11978,2109.13226,2110.05069,2110.07205,2110.08207,2110.09485,2111.02358,2112.06825,2112.10752,2201.02177,2201.03545,2201.08239,2201.09792,2201.11903,2202.01374,2202.08433,2202.11214,2203.02053,2203.02155,2203.15556,2204.01691,2204.01697,2204.02311,2204.03162,2204.03409,2204.06125,2204.06745,2204.14198,2205.01580,2205.01917,2205.06175,2205.10343,2205.11487,2205.14100,2206.00364,2206.04615,2206.13170,2206.14858,2207.12598,2208.09392,2208.10442v2,2208.12242,2208.12415,2209.03143,2209.06794,2209.14792,2209.14988,2209.15352,2210.02186,2210.03142,2210.03629,2210.08402,2210.09276,2210.13438,2211.05100,2211.09788,2211.09800,2211.10440,2212.04356,2212.04488,2212.06817,2212.09748,2212.13138,2301.00704,2301.02111,2301.04104,2301.11305,2301.11325,2301.12503,2302.01318,2302.02662,2302.02948,2302.03011,2302.04761,2302.05442,2302.05543,2302.06675,2302.13971,2302.14045v2,2303.00001,2303.01037,2303.03378,2303.04671,2303.05511,2303.06296,2303.12712,2303.17564,2303.17580,2304.02643,2304.03277,2304.03442,2304.07193,2304.08466,2304.08485,2304.08818,2304.12306,2304.12995,2305.10403,2305.10601,2305.10973,2305.11206,2305.13516,2305.14314,2305.16291,2305.16367,2305.18290,2305.19466,2306.03092,2306.05284,2306.12925,2306.15687,2307.01952,2307.10802,2307.15818,2307.16789,2308.00352,2308.02151,2308.11596,2308.12950,2308.12966,2308.16512,2309.00267,2309.03409,2309.16588,2309.17453,2310.12931,2311.06242,2312.14125&amp;start=0&amp;max_results=326</title>
  <id>http://arxiv.org/api/pCGb7EUbb1oOBjUSn16EpxoZ9Ms</id>
  <updated>2025-03-06T00:00:00-05:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">326</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">326</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/0803.0476v2</id>
    <updated>2008-07-25T09:52:42Z</updated>
    <published>2008-03-04T15:29:44Z</published>
    <title>Fast unfolding of communities in large networks</title>
    <summary>  We propose a simple method to extract the community structure of large
networks. Our method is a heuristic method that is based on modularity
optimization. It is shown to outperform all other known community detection
method in terms of computation time. Moreover, the quality of the communities
detected is very good, as measured by the so-called modularity. This is shown
first by identifying language communities in a Belgian mobile phone network of
2.6 million customers and by analyzing a web graph of 118 million nodes and
more than one billion links. The accuracy of our algorithm is also verified on
ad-hoc modular networks. .
</summary>
    <author>
      <name>Vincent D. Blondel</name>
    </author>
    <author>
      <name>Jean-Loup Guillaume</name>
    </author>
    <author>
      <name>Renaud Lambiotte</name>
    </author>
    <author>
      <name>Etienne Lefebvre</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1742-5468/2008/10/P10008</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1742-5468/2008/10/P10008" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 5 figures, 1 table; new version with new figures in order to
  clarify our method, where we look more carefully at the role played by the
  ordering of the nodes and where we compare our method with that of Wakita and
  Tsurumi</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">J. Stat. Mech. (2008) P10008</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0803.0476v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.0476v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.3995v4</id>
    <updated>2010-06-09T23:24:13Z</updated>
    <published>2009-12-21T00:08:19Z</published>
    <title>Gaussian Process Optimization in the Bandit Setting: No Regret and
  Experimental Design</title>
    <summary>  Many applications require optimizing an unknown, noisy function that is
expensive to evaluate. We formalize this task as a multi-armed bandit problem,
where the payoff function is either sampled from a Gaussian process (GP) or has
low RKHS norm. We resolve the important open problem of deriving regret bounds
for this setting, which imply novel convergence rates for GP optimization. We
analyze GP-UCB, an intuitive upper-confidence based algorithm, and bound its
cumulative regret in terms of maximal information gain, establishing a novel
connection between GP optimization and experimental design. Moreover, by
bounding the latter in terms of operator spectra, we obtain explicit sublinear
regret bounds for many commonly used covariance functions. In some important
cases, our bounds have surprisingly weak dependence on the dimensionality. In
our experiments on real sensor data, GP-UCB compares favorably with other
heuristical GP optimization approaches.
</summary>
    <author>
      <name>Niranjan Srinivas</name>
    </author>
    <author>
      <name>Andreas Krause</name>
    </author>
    <author>
      <name>Sham M. Kakade</name>
    </author>
    <author>
      <name>Matthias Seeger</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TIT.2011.2182033</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TIT.2011.2182033" rel="related"/>
    <link href="http://arxiv.org/abs/0912.3995v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.3995v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.0398v1</id>
    <updated>2011-03-02T11:34:50Z</updated>
    <published>2011-03-02T11:34:50Z</published>
    <title>Natural Language Processing (almost) from Scratch</title>
    <summary>  We propose a unified neural network architecture and learning algorithm that
can be applied to various natural language processing tasks including:
part-of-speech tagging, chunking, named entity recognition, and semantic role
labeling. This versatility is achieved by trying to avoid task-specific
engineering and therefore disregarding a lot of prior knowledge. Instead of
exploiting man-made input features carefully optimized for each task, our
system learns internal representations on the basis of vast amounts of mostly
unlabeled training data. This work is then used as a basis for building a
freely available tagging system with good performance and minimal computational
requirements.
</summary>
    <author>
      <name>Ronan Collobert</name>
    </author>
    <author>
      <name>Jason Weston</name>
    </author>
    <author>
      <name>Leon Bottou</name>
    </author>
    <author>
      <name>Michael Karlen</name>
    </author>
    <author>
      <name>Koray Kavukcuoglu</name>
    </author>
    <author>
      <name>Pavel Kuksa</name>
    </author>
    <link href="http://arxiv.org/abs/1103.0398v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.0398v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1112.6209v5</id>
    <updated>2012-07-12T04:32:50Z</updated>
    <published>2011-12-29T00:26:54Z</published>
    <title>Building high-level features using large scale unsupervised learning</title>
    <summary>  We consider the problem of building high-level, class-specific feature
detectors from only unlabeled data. For example, is it possible to learn a face
detector using only unlabeled images? To answer this, we train a 9-layered
locally connected sparse autoencoder with pooling and local contrast
normalization on a large dataset of images (the model has 1 billion
connections, the dataset has 10 million 200x200 pixel images downloaded from
the Internet). We train this network using model parallelism and asynchronous
SGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to
what appears to be a widely-held intuition, our experimental results reveal
that it is possible to train a face detector without having to label images as
containing a face or not. Control experiments show that this feature detector
is robust not only to translation but also to scaling and out-of-plane
rotation. We also find that the same network is sensitive to other high-level
concepts such as cat faces and human bodies. Starting with these learned
features, we trained our network to obtain 15.8% accuracy in recognizing 20,000
object categories from ImageNet, a leap of 70% relative improvement over the
previous state-of-the-art.
</summary>
    <author>
      <name>Quoc V. Le</name>
    </author>
    <author>
      <name>Marc'Aurelio Ranzato</name>
    </author>
    <author>
      <name>Rajat Monga</name>
    </author>
    <author>
      <name>Matthieu Devin</name>
    </author>
    <author>
      <name>Kai Chen</name>
    </author>
    <author>
      <name>Greg S. Corrado</name>
    </author>
    <author>
      <name>Jeff Dean</name>
    </author>
    <author>
      <name>Andrew Y. Ng</name>
    </author>
    <link href="http://arxiv.org/abs/1112.6209v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.6209v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.5538v3</id>
    <updated>2014-04-23T11:48:51Z</updated>
    <published>2012-06-24T20:51:38Z</published>
    <title>Representation Learning: A Review and New Perspectives</title>
    <summary>  The success of machine learning algorithms generally depends on data
representation, and we hypothesize that this is because different
representations can entangle and hide more or less the different explanatory
factors of variation behind the data. Although specific domain knowledge can be
used to help design representations, learning with generic priors can also be
used, and the quest for AI is motivating the design of more powerful
representation-learning algorithms implementing such priors. This paper reviews
recent work in the area of unsupervised feature learning and deep learning,
covering advances in probabilistic models, auto-encoders, manifold learning,
and deep networks. This motivates longer-term unanswered questions about the
appropriate objectives for learning good representations, for computing
representations (i.e., inference), and the geometrical connections between
representation learning, density estimation and manifold learning.
</summary>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <author>
      <name>Aaron Courville</name>
    </author>
    <author>
      <name>Pascal Vincent</name>
    </author>
    <link href="http://arxiv.org/abs/1206.5538v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.5538v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.0580v1</id>
    <updated>2012-07-03T06:35:15Z</updated>
    <published>2012-07-03T06:35:15Z</published>
    <title>Improving neural networks by preventing co-adaptation of feature
  detectors</title>
    <summary>  When a large feedforward neural network is trained on a small training set,
it typically performs poorly on held-out test data. This "overfitting" is
greatly reduced by randomly omitting half of the feature detectors on each
training case. This prevents complex co-adaptations in which a feature detector
is only helpful in the context of several other specific feature detectors.
Instead, each neuron learns to detect a feature that is generally helpful for
producing the correct answer given the combinatorially large variety of
internal contexts in which it must operate. Random "dropout" gives big
improvements on many benchmark tasks and sets new records for speech and object
recognition.
</summary>
    <author>
      <name>Geoffrey E. Hinton</name>
    </author>
    <author>
      <name>Nitish Srivastava</name>
    </author>
    <author>
      <name>Alex Krizhevsky</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Ruslan R. Salakhutdinov</name>
    </author>
    <link href="http://arxiv.org/abs/1207.0580v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.0580v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.3781v3</id>
    <updated>2013-09-07T00:30:40Z</updated>
    <published>2013-01-16T18:24:43Z</published>
    <title>Efficient Estimation of Word Representations in Vector Space</title>
    <summary>  We propose two novel model architectures for computing continuous vector
representations of words from very large data sets. The quality of these
representations is measured in a word similarity task, and the results are
compared to the previously best performing techniques based on different types
of neural networks. We observe large improvements in accuracy at much lower
computational cost, i.e. it takes less than a day to learn high quality word
vectors from a 1.6 billion words data set. Furthermore, we show that these
vectors provide state-of-the-art performance on our test set for measuring
syntactic and semantic word similarities.
</summary>
    <author>
      <name>Tomas Mikolov</name>
    </author>
    <author>
      <name>Kai Chen</name>
    </author>
    <author>
      <name>Greg Corrado</name>
    </author>
    <author>
      <name>Jeffrey Dean</name>
    </author>
    <link href="http://arxiv.org/abs/1301.3781v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.3781v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.3781v3</id>
    <updated>2013-09-07T00:30:40Z</updated>
    <published>2013-01-16T18:24:43Z</published>
    <title>Efficient Estimation of Word Representations in Vector Space</title>
    <summary>  We propose two novel model architectures for computing continuous vector
representations of words from very large data sets. The quality of these
representations is measured in a word similarity task, and the results are
compared to the previously best performing techniques based on different types
of neural networks. We observe large improvements in accuracy at much lower
computational cost, i.e. it takes less than a day to learn high quality word
vectors from a 1.6 billion words data set. Furthermore, we show that these
vectors provide state-of-the-art performance on our test set for measuring
syntactic and semantic word similarities.
</summary>
    <author>
      <name>Tomas Mikolov</name>
    </author>
    <author>
      <name>Kai Chen</name>
    </author>
    <author>
      <name>Greg Corrado</name>
    </author>
    <author>
      <name>Jeffrey Dean</name>
    </author>
    <link href="http://arxiv.org/abs/1301.3781v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.3781v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.4389v4</id>
    <updated>2013-09-20T08:54:35Z</updated>
    <published>2013-02-18T18:59:07Z</published>
    <title>Maxout Networks</title>
    <summary>  We consider the problem of designing models to leverage a recently introduced
approximate model averaging technique called dropout. We define a simple new
model called maxout (so named because its output is the max of a set of inputs,
and because it is a natural companion to dropout) designed to both facilitate
optimization by dropout and improve the accuracy of dropout's fast approximate
model averaging technique. We empirically verify that the model successfully
accomplishes both of these tasks. We use maxout and dropout to demonstrate
state of the art classification performance on four benchmark datasets: MNIST,
CIFAR-10, CIFAR-100, and SVHN.
</summary>
    <author>
      <name>Ian J. Goodfellow</name>
    </author>
    <author>
      <name>David Warde-Farley</name>
    </author>
    <author>
      <name>Mehdi Mirza</name>
    </author>
    <author>
      <name>Aaron Courville</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is the version of the paper that appears in ICML 2013</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">JMLR WCP 28 (3): 1319-1327, 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1302.4389v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.4389v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.5778v1</id>
    <updated>2013-03-22T20:55:48Z</updated>
    <published>2013-03-22T20:55:48Z</published>
    <title>Speech Recognition with Deep Recurrent Neural Networks</title>
    <summary>  Recurrent neural networks (RNNs) are a powerful model for sequential data.
End-to-end training methods such as Connectionist Temporal Classification make
it possible to train RNNs for sequence labelling problems where the
input-output alignment is unknown. The combination of these methods with the
Long Short-term Memory RNN architecture has proved particularly fruitful,
delivering state-of-the-art results in cursive handwriting recognition. However
RNN performance in speech recognition has so far been disappointing, with
better results returned by deep feedforward networks. This paper investigates
\emph{deep recurrent neural networks}, which combine the multiple levels of
representation that have proved so effective in deep networks with the flexible
use of long range context that empowers RNNs. When trained end-to-end with
suitable regularisation, we find that deep Long Short-term Memory RNNs achieve
a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to
our knowledge is the best recorded score.
</summary>
    <author>
      <name>Alex Graves</name>
    </author>
    <author>
      <name>Abdel-rahman Mohamed</name>
    </author>
    <author>
      <name>Geoffrey Hinton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ICASSP 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.5778v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.5778v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.0850v5</id>
    <updated>2014-06-05T16:04:02Z</updated>
    <published>2013-08-04T21:04:36Z</published>
    <title>Generating Sequences With Recurrent Neural Networks</title>
    <summary>  This paper shows how Long Short-term Memory recurrent neural networks can be
used to generate complex sequences with long-range structure, simply by
predicting one data point at a time. The approach is demonstrated for text
(where the data are discrete) and online handwriting (where the data are
real-valued). It is then extended to handwriting synthesis by allowing the
network to condition its predictions on a text sequence. The resulting system
is able to generate highly realistic cursive handwriting in a wide variety of
styles.
</summary>
    <author>
      <name>Alex Graves</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Thanks to Peng Liu and Sergey Zyrianov for various corrections</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.0850v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.0850v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.0850v5</id>
    <updated>2014-06-05T16:04:02Z</updated>
    <published>2013-08-04T21:04:36Z</published>
    <title>Generating Sequences With Recurrent Neural Networks</title>
    <summary>  This paper shows how Long Short-term Memory recurrent neural networks can be
used to generate complex sequences with long-range structure, simply by
predicting one data point at a time. The approach is demonstrated for text
(where the data are discrete) and online handwriting (where the data are
real-valued). It is then extended to handwriting synthesis by allowing the
network to condition its predictions on a text sequence. The resulting system
is able to generate highly realistic cursive handwriting in a wide variety of
styles.
</summary>
    <author>
      <name>Alex Graves</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Thanks to Peng Liu and Sergey Zyrianov for various corrections</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.0850v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.0850v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.1531v1</id>
    <updated>2013-10-06T02:48:17Z</updated>
    <published>2013-10-06T02:48:17Z</published>
    <title>DeCAF: A Deep Convolutional Activation Feature for Generic Visual
  Recognition</title>
    <summary>  We evaluate whether features extracted from the activation of a deep
convolutional network trained in a fully supervised fashion on a large, fixed
set of object recognition tasks can be re-purposed to novel generic tasks. Our
generic tasks may differ significantly from the originally trained tasks and
there may be insufficient labeled or unlabeled data to conventionally train or
adapt a deep architecture to the new tasks. We investigate and visualize the
semantic clustering of deep convolutional features with respect to a variety of
such tasks, including scene recognition, domain adaptation, and fine-grained
recognition challenges. We compare the efficacy of relying on various network
levels to define a fixed feature, and report novel results that significantly
outperform the state-of-the-art on several important vision challenges. We are
releasing DeCAF, an open-source implementation of these deep convolutional
activation features, along with all associated network parameters to enable
vision researchers to be able to conduct experimentation with deep
representations across a range of visual concept learning paradigms.
</summary>
    <author>
      <name>Jeff Donahue</name>
    </author>
    <author>
      <name>Yangqing Jia</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Judy Hoffman</name>
    </author>
    <author>
      <name>Ning Zhang</name>
    </author>
    <author>
      <name>Eric Tzeng</name>
    </author>
    <author>
      <name>Trevor Darrell</name>
    </author>
    <link href="http://arxiv.org/abs/1310.1531v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.1531v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.2901v3</id>
    <updated>2013-11-28T23:04:01Z</updated>
    <published>2013-11-12T20:02:22Z</published>
    <title>Visualizing and Understanding Convolutional Networks</title>
    <summary>  Large Convolutional Network models have recently demonstrated impressive
classification performance on the ImageNet benchmark. However there is no clear
understanding of why they perform so well, or how they might be improved. In
this paper we address both issues. We introduce a novel visualization technique
that gives insight into the function of intermediate feature layers and the
operation of the classifier. We also perform an ablation study to discover the
performance contribution from different model layers. This enables us to find
model architectures that outperform Krizhevsky \etal on the ImageNet
classification benchmark. We show our ImageNet model generalizes well to other
datasets: when the softmax classifier is retrained, it convincingly beats the
current state-of-the-art results on Caltech-101 and Caltech-256 datasets.
</summary>
    <author>
      <name>Matthew D Zeiler</name>
    </author>
    <author>
      <name>Rob Fergus</name>
    </author>
    <link href="http://arxiv.org/abs/1311.2901v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.2901v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.4400v3</id>
    <updated>2014-03-04T05:15:42Z</updated>
    <published>2013-12-16T15:34:13Z</published>
    <title>Network In Network</title>
    <summary>  We propose a novel deep network structure called "Network In Network" (NIN)
to enhance model discriminability for local patches within the receptive field.
The conventional convolutional layer uses linear filters followed by a
nonlinear activation function to scan the input. Instead, we build micro neural
networks with more complex structures to abstract the data within the receptive
field. We instantiate the micro neural network with a multilayer perceptron,
which is a potent function approximator. The feature maps are obtained by
sliding the micro networks over the input in a similar manner as CNN; they are
then fed into the next layer. Deep NIN can be implemented by stacking mutiple
of the above described structure. With enhanced local modeling via the micro
network, we are able to utilize global average pooling over feature maps in the
classification layer, which is easier to interpret and less prone to
overfitting than traditional fully connected layers. We demonstrated the
state-of-the-art classification performances with NIN on CIFAR-10 and
CIFAR-100, and reasonable performances on SVHN and MNIST datasets.
</summary>
    <author>
      <name>Min Lin</name>
    </author>
    <author>
      <name>Qiang Chen</name>
    </author>
    <author>
      <name>Shuicheng Yan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures, for iclr2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.4400v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.4400v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.5602v1</id>
    <updated>2013-12-19T16:00:08Z</updated>
    <published>2013-12-19T16:00:08Z</published>
    <title>Playing Atari with Deep Reinforcement Learning</title>
    <summary>  We present the first deep learning model to successfully learn control
policies directly from high-dimensional sensory input using reinforcement
learning. The model is a convolutional neural network, trained with a variant
of Q-learning, whose input is raw pixels and whose output is a value function
estimating future rewards. We apply our method to seven Atari 2600 games from
the Arcade Learning Environment, with no adjustment of the architecture or
learning algorithm. We find that it outperforms all previous approaches on six
of the games and surpasses a human expert on three of them.
</summary>
    <author>
      <name>Volodymyr Mnih</name>
    </author>
    <author>
      <name>Koray Kavukcuoglu</name>
    </author>
    <author>
      <name>David Silver</name>
    </author>
    <author>
      <name>Alex Graves</name>
    </author>
    <author>
      <name>Ioannis Antonoglou</name>
    </author>
    <author>
      <name>Daan Wierstra</name>
    </author>
    <author>
      <name>Martin Riedmiller</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NIPS Deep Learning Workshop 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.5602v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.5602v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.5602v1</id>
    <updated>2013-12-19T16:00:08Z</updated>
    <published>2013-12-19T16:00:08Z</published>
    <title>Playing Atari with Deep Reinforcement Learning</title>
    <summary>  We present the first deep learning model to successfully learn control
policies directly from high-dimensional sensory input using reinforcement
learning. The model is a convolutional neural network, trained with a variant
of Q-learning, whose input is raw pixels and whose output is a value function
estimating future rewards. We apply our method to seven Atari 2600 games from
the Arcade Learning Environment, with no adjustment of the architecture or
learning algorithm. We find that it outperforms all previous approaches on six
of the games and surpasses a human expert on three of them.
</summary>
    <author>
      <name>Volodymyr Mnih</name>
    </author>
    <author>
      <name>Koray Kavukcuoglu</name>
    </author>
    <author>
      <name>David Silver</name>
    </author>
    <author>
      <name>Alex Graves</name>
    </author>
    <author>
      <name>Ioannis Antonoglou</name>
    </author>
    <author>
      <name>Daan Wierstra</name>
    </author>
    <author>
      <name>Martin Riedmiller</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NIPS Deep Learning Workshop 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.5602v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.5602v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.6114v11</id>
    <updated>2022-12-10T21:04:00Z</updated>
    <published>2013-12-20T20:58:10Z</published>
    <title>Auto-Encoding Variational Bayes</title>
    <summary>  How can we perform efficient inference and learning in directed probabilistic
models, in the presence of continuous latent variables with intractable
posterior distributions, and large datasets? We introduce a stochastic
variational inference and learning algorithm that scales to large datasets and,
under some mild differentiability conditions, even works in the intractable
case. Our contributions are two-fold. First, we show that a reparameterization
of the variational lower bound yields a lower bound estimator that can be
straightforwardly optimized using standard stochastic gradient methods. Second,
we show that for i.i.d. datasets with continuous latent variables per
datapoint, posterior inference can be made especially efficient by fitting an
approximate inference model (also called a recognition model) to the
intractable posterior using the proposed lower bound estimator. Theoretical
advantages are reflected in experimental results.
</summary>
    <author>
      <name>Diederik P Kingma</name>
    </author>
    <author>
      <name>Max Welling</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Fixes a typo in the abstract, no other changes</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.6114v11" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.6114v11" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.6114v11</id>
    <updated>2022-12-10T21:04:00Z</updated>
    <published>2013-12-20T20:58:10Z</published>
    <title>Auto-Encoding Variational Bayes</title>
    <summary>  How can we perform efficient inference and learning in directed probabilistic
models, in the presence of continuous latent variables with intractable
posterior distributions, and large datasets? We introduce a stochastic
variational inference and learning algorithm that scales to large datasets and,
under some mild differentiability conditions, even works in the intractable
case. Our contributions are two-fold. First, we show that a reparameterization
of the variational lower bound yields a lower bound estimator that can be
straightforwardly optimized using standard stochastic gradient methods. Second,
we show that for i.i.d. datasets with continuous latent variables per
datapoint, posterior inference can be made especially efficient by fitting an
approximate inference model (also called a recognition model) to the
intractable posterior using the proposed lower bound estimator. Theoretical
advantages are reflected in experimental results.
</summary>
    <author>
      <name>Diederik P Kingma</name>
    </author>
    <author>
      <name>Max Welling</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Fixes a typo in the abstract, no other changes</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.6114v11" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.6114v11" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.6199v4</id>
    <updated>2014-02-19T16:33:14Z</updated>
    <published>2013-12-21T03:36:08Z</published>
    <title>Intriguing properties of neural networks</title>
    <summary>  Deep neural networks are highly expressive models that have recently achieved
state of the art performance on speech and visual recognition tasks. While
their expressiveness is the reason they succeed, it also causes them to learn
uninterpretable solutions that could have counter-intuitive properties. In this
paper we report two such properties.
  First, we find that there is no distinction between individual high level
units and random linear combinations of high level units, according to various
methods of unit analysis. It suggests that it is the space, rather than the
individual units, that contains of the semantic information in the high layers
of neural networks.
  Second, we find that deep neural networks learn input-output mappings that
are fairly discontinuous to a significant extend. We can cause the network to
misclassify an image by applying a certain imperceptible perturbation, which is
found by maximizing the network's prediction error. In addition, the specific
nature of these perturbations is not a random artifact of learning: the same
perturbation can cause a different network, that was trained on a different
subset of the dataset, to misclassify the same input.
</summary>
    <author>
      <name>Christian Szegedy</name>
    </author>
    <author>
      <name>Wojciech Zaremba</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Joan Bruna</name>
    </author>
    <author>
      <name>Dumitru Erhan</name>
    </author>
    <author>
      <name>Ian Goodfellow</name>
    </author>
    <author>
      <name>Rob Fergus</name>
    </author>
    <link href="http://arxiv.org/abs/1312.6199v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.6199v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.6229v4</id>
    <updated>2014-02-24T03:38:17Z</updated>
    <published>2013-12-21T09:52:33Z</published>
    <title>OverFeat: Integrated Recognition, Localization and Detection using
  Convolutional Networks</title>
    <summary>  We present an integrated framework for using Convolutional Networks for
classification, localization and detection. We show how a multiscale and
sliding window approach can be efficiently implemented within a ConvNet. We
also introduce a novel deep learning approach to localization by learning to
predict object boundaries. Bounding boxes are then accumulated rather than
suppressed in order to increase detection confidence. We show that different
tasks can be learned simultaneously using a single shared network. This
integrated framework is the winner of the localization task of the ImageNet
Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very
competitive results for the detection and classifications tasks. In
post-competition work, we establish a new state of the art for the detection
task. Finally, we release a feature extractor from our best model called
OverFeat.
</summary>
    <author>
      <name>Pierre Sermanet</name>
    </author>
    <author>
      <name>David Eigen</name>
    </author>
    <author>
      <name>Xiang Zhang</name>
    </author>
    <author>
      <name>Michael Mathieu</name>
    </author>
    <author>
      <name>Rob Fergus</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
    <link href="http://arxiv.org/abs/1312.6229v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.6229v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.2188v1</id>
    <updated>2014-04-08T15:46:44Z</updated>
    <published>2014-04-08T15:46:44Z</published>
    <title>A Convolutional Neural Network for Modelling Sentences</title>
    <summary>  The ability to accurately represent sentences is central to language
understanding. We describe a convolutional architecture dubbed the Dynamic
Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of
sentences. The network uses Dynamic k-Max Pooling, a global pooling operation
over linear sequences. The network handles input sentences of varying length
and induces a feature graph over the sentence that is capable of explicitly
capturing short and long-range relations. The network does not rely on a parse
tree and is easily applicable to any language. We test the DCNN in four
experiments: small scale binary and multi-class sentiment prediction, six-way
question classification and Twitter sentiment prediction by distant
supervision. The network achieves excellent performance in the first three
tasks and a greater than 25% error reduction in the last task with respect to
the strongest baseline.
</summary>
    <author>
      <name>Nal Kalchbrenner</name>
    </author>
    <author>
      <name>Edward Grefenstette</name>
    </author>
    <author>
      <name>Phil Blunsom</name>
    </author>
    <link href="http://arxiv.org/abs/1404.2188v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.2188v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.7828v4</id>
    <updated>2014-10-08T10:00:38Z</updated>
    <published>2014-04-30T18:39:00Z</published>
    <title>Deep Learning in Neural Networks: An Overview</title>
    <summary>  In recent years, deep artificial neural networks (including recurrent ones)
have won numerous contests in pattern recognition and machine learning. This
historical survey compactly summarises relevant work, much of it from the
previous millennium. Shallow and deep learners are distinguished by the depth
of their credit assignment paths, which are chains of possibly learnable,
causal links between actions and effects. I review deep supervised learning
(also recapitulating the history of backpropagation), unsupervised learning,
reinforcement learning &amp; evolutionary computation, and indirect search for
short programs encoding deep and large networks.
</summary>
    <author>
      <name>Juergen Schmidhuber</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.neunet.2014.09.003</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.neunet.2014.09.003" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">88 pages, 888 references</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Networks, Vol 61, pp 85-117, Jan 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.7828v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.7828v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.3531v4</id>
    <updated>2014-11-05T08:34:48Z</updated>
    <published>2014-05-14T15:19:22Z</published>
    <title>Return of the Devil in the Details: Delving Deep into Convolutional Nets</title>
    <summary>  The latest generation of Convolutional Neural Networks (CNN) have achieved
impressive results in challenging benchmarks on image recognition and object
detection, significantly raising the interest of the community in these
methods. Nevertheless, it is still unclear how different CNN methods compare
with each other and with previous state-of-the-art shallow representations such
as the Bag-of-Visual-Words and the Improved Fisher Vector. This paper conducts
a rigorous evaluation of these new techniques, exploring different deep
architectures and comparing them on a common ground, identifying and disclosing
important implementation details. We identify several useful properties of
CNN-based representations, including the fact that the dimensionality of the
CNN output layer can be reduced significantly without having an adverse effect
on performance. We also identify aspects of deep and shallow methods that can
be successfully shared. In particular, we show that the data augmentation
techniques commonly applied to CNN-based methods can also be applied to shallow
methods, and result in an analogous performance boost. Source code and models
to reproduce the experiments in the paper is made publicly available.
</summary>
    <author>
      <name>Ken Chatfield</name>
    </author>
    <author>
      <name>Karen Simonyan</name>
    </author>
    <author>
      <name>Andrea Vedaldi</name>
    </author>
    <author>
      <name>Andrew Zisserman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in proceedings of BMVC 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.3531v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.3531v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.4053v2</id>
    <updated>2014-05-22T23:23:19Z</updated>
    <published>2014-05-16T07:12:16Z</published>
    <title>Distributed Representations of Sentences and Documents</title>
    <summary>  Many machine learning algorithms require the input to be represented as a
fixed-length feature vector. When it comes to texts, one of the most common
fixed-length features is bag-of-words. Despite their popularity, bag-of-words
features have two major weaknesses: they lose the ordering of the words and
they also ignore semantics of the words. For example, "powerful," "strong" and
"Paris" are equally distant. In this paper, we propose Paragraph Vector, an
unsupervised algorithm that learns fixed-length feature representations from
variable-length pieces of texts, such as sentences, paragraphs, and documents.
Our algorithm represents each document by a dense vector which is trained to
predict words in the document. Its construction gives our algorithm the
potential to overcome the weaknesses of bag-of-words models. Empirical results
show that Paragraph Vectors outperform bag-of-words models as well as other
techniques for text representations. Finally, we achieve new state-of-the-art
results on several text classification and sentiment analysis tasks.
</summary>
    <author>
      <name>Quoc V. Le</name>
    </author>
    <author>
      <name>Tomas Mikolov</name>
    </author>
    <link href="http://arxiv.org/abs/1405.4053v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.4053v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.1078v3</id>
    <updated>2014-09-03T00:25:02Z</updated>
    <published>2014-06-03T17:47:08Z</published>
    <title>Learning Phrase Representations using RNN Encoder-Decoder for
  Statistical Machine Translation</title>
    <summary>  In this paper, we propose a novel neural network model called RNN
Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN
encodes a sequence of symbols into a fixed-length vector representation, and
the other decodes the representation into another sequence of symbols. The
encoder and decoder of the proposed model are jointly trained to maximize the
conditional probability of a target sequence given a source sequence. The
performance of a statistical machine translation system is empirically found to
improve by using the conditional probabilities of phrase pairs computed by the
RNN Encoder-Decoder as an additional feature in the existing log-linear model.
Qualitatively, we show that the proposed model learns a semantically and
syntactically meaningful representation of linguistic phrases.
</summary>
    <author>
      <name>Kyunghyun Cho</name>
    </author>
    <author>
      <name>Bart van Merrienboer</name>
    </author>
    <author>
      <name>Caglar Gulcehre</name>
    </author>
    <author>
      <name>Dzmitry Bahdanau</name>
    </author>
    <author>
      <name>Fethi Bougares</name>
    </author>
    <author>
      <name>Holger Schwenk</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.1078v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.1078v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.2572v1</id>
    <updated>2014-06-10T14:52:14Z</updated>
    <published>2014-06-10T14:52:14Z</published>
    <title>Identifying and attacking the saddle point problem in high-dimensional
  non-convex optimization</title>
    <summary>  A central challenge to many fields of science and engineering involves
minimizing non-convex error functions over continuous, high dimensional spaces.
Gradient descent or quasi-Newton methods are almost ubiquitously used to
perform such minimizations, and it is often thought that a main source of
difficulty for these local methods to find the global minimum is the
proliferation of local minima with much higher error than the global minimum.
Here we argue, based on results from statistical physics, random matrix theory,
neural network theory, and empirical evidence, that a deeper and more profound
difficulty originates from the proliferation of saddle points, not local
minima, especially in high dimensional problems of practical interest. Such
saddle points are surrounded by high error plateaus that can dramatically slow
down learning, and give the illusory impression of the existence of a local
minimum. Motivated by these arguments, we propose a new approach to
second-order optimization, the saddle-free Newton method, that can rapidly
escape high dimensional saddle points, unlike gradient descent and quasi-Newton
methods. We apply this algorithm to deep or recurrent neural network training,
and provide numerical evidence for its superior optimization performance.
</summary>
    <author>
      <name>Yann Dauphin</name>
    </author>
    <author>
      <name>Razvan Pascanu</name>
    </author>
    <author>
      <name>Caglar Gulcehre</name>
    </author>
    <author>
      <name>Kyunghyun Cho</name>
    </author>
    <author>
      <name>Surya Ganguli</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The theoretical review and analysis in this article draw heavily from
  arXiv:1405.4604 [cs.LG]</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.2572v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.2572v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.4729v4</id>
    <updated>2015-04-23T07:33:24Z</updated>
    <published>2014-06-18T14:24:17Z</published>
    <title>Spatial Pyramid Pooling in Deep Convolutional Networks for Visual
  Recognition</title>
    <summary>  Existing deep convolutional neural networks (CNNs) require a fixed-size
(e.g., 224x224) input image. This requirement is "artificial" and may reduce
the recognition accuracy for the images or sub-images of an arbitrary
size/scale. In this work, we equip the networks with another pooling strategy,
"spatial pyramid pooling", to eliminate the above requirement. The new network
structure, called SPP-net, can generate a fixed-length representation
regardless of image size/scale. Pyramid pooling is also robust to object
deformations. With these advantages, SPP-net should in general improve all
CNN-based image classification methods. On the ImageNet 2012 dataset, we
demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures
despite their different designs. On the Pascal VOC 2007 and Caltech101
datasets, SPP-net achieves state-of-the-art classification results using a
single full-image representation and no fine-tuning.
  The power of SPP-net is also significant in object detection. Using SPP-net,
we compute the feature maps from the entire image only once, and then pool
features in arbitrary regions (sub-images) to generate fixed-length
representations for training the detectors. This method avoids repeatedly
computing the convolutional features. In processing test images, our method is
24-102x faster than the R-CNN method, while achieving better or comparable
accuracy on Pascal VOC 2007.
  In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our
methods rank #2 in object detection and #3 in image classification among all 38
teams. This manuscript also introduces the improvement made for this
competition.
</summary>
    <author>
      <name>Kaiming He</name>
    </author>
    <author>
      <name>Xiangyu Zhang</name>
    </author>
    <author>
      <name>Shaoqing Ren</name>
    </author>
    <author>
      <name>Jian Sun</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-10578-9_23</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-10578-9_23" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This manuscript is the accepted version for IEEE Transactions on
  Pattern Analysis and Machine Intelligence (TPAMI) 2015. See Changelog</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.4729v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.4729v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.6247v1</id>
    <updated>2014-06-24T14:16:56Z</updated>
    <published>2014-06-24T14:16:56Z</published>
    <title>Recurrent Models of Visual Attention</title>
    <summary>  Applying convolutional neural networks to large images is computationally
expensive because the amount of computation scales linearly with the number of
image pixels. We present a novel recurrent neural network model that is capable
of extracting information from an image or video by adaptively selecting a
sequence of regions or locations and only processing the selected regions at
high resolution. Like convolutional neural networks, the proposed model has a
degree of translation invariance built-in, but the amount of computation it
performs can be controlled independently of the input image size. While the
model is non-differentiable, it can be trained using reinforcement learning
methods to learn task-specific policies. We evaluate our model on several image
classification tasks, where it significantly outperforms a convolutional neural
network baseline on cluttered images, and on a dynamic visual control problem,
where it learns to track a simple object without an explicit training signal
for doing so.
</summary>
    <author>
      <name>Volodymyr Mnih</name>
    </author>
    <author>
      <name>Nicolas Heess</name>
    </author>
    <author>
      <name>Alex Graves</name>
    </author>
    <author>
      <name>Koray Kavukcuoglu</name>
    </author>
    <link href="http://arxiv.org/abs/1406.6247v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.6247v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.5093v1</id>
    <updated>2014-06-20T23:00:32Z</updated>
    <published>2014-06-20T23:00:32Z</published>
    <title>Caffe: Convolutional Architecture for Fast Feature Embedding</title>
    <summary>  Caffe provides multimedia scientists and practitioners with a clean and
modifiable framework for state-of-the-art deep learning algorithms and a
collection of reference models. The framework is a BSD-licensed C++ library
with Python and MATLAB bindings for training and deploying general-purpose
convolutional neural networks and other deep models efficiently on commodity
architectures. Caffe fits industry and internet-scale media needs by CUDA GPU
computation, processing over 40 million images a day on a single K40 or Titan
GPU ($\approx$ 2.5 ms per image). By separating model representation from
actual implementation, Caffe allows experimentation and seamless switching
among platforms for ease of development and deployment from prototyping
machines to cloud environments. Caffe is maintained and developed by the
Berkeley Vision and Learning Center (BVLC) with the help of an active community
of contributors on GitHub. It powers ongoing research projects, large-scale
industrial applications, and startup prototypes in vision, speech, and
multimedia.
</summary>
    <author>
      <name>Yangqing Jia</name>
    </author>
    <author>
      <name>Evan Shelhamer</name>
    </author>
    <author>
      <name>Jeff Donahue</name>
    </author>
    <author>
      <name>Sergey Karayev</name>
    </author>
    <author>
      <name>Jonathan Long</name>
    </author>
    <author>
      <name>Ross Girshick</name>
    </author>
    <author>
      <name>Sergio Guadarrama</name>
    </author>
    <author>
      <name>Trevor Darrell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Tech report for the Caffe software at http://github.com/BVLC/Caffe/</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.5093v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.5093v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.5882v2</id>
    <updated>2014-09-03T03:09:02Z</updated>
    <published>2014-08-25T19:48:04Z</published>
    <title>Convolutional Neural Networks for Sentence Classification</title>
    <summary>  We report on a series of experiments with convolutional neural networks (CNN)
trained on top of pre-trained word vectors for sentence-level classification
tasks. We show that a simple CNN with little hyperparameter tuning and static
vectors achieves excellent results on multiple benchmarks. Learning
task-specific vectors through fine-tuning offers further gains in performance.
We additionally propose a simple modification to the architecture to allow for
the use of both task-specific and static vectors. The CNN models discussed
herein improve upon the state of the art on 4 out of 7 tasks, which include
sentiment analysis and question classification.
</summary>
    <author>
      <name>Yoon Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in EMNLP 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.5882v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.5882v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.0473v7</id>
    <updated>2016-05-19T21:53:22Z</updated>
    <published>2014-09-01T16:33:02Z</published>
    <title>Neural Machine Translation by Jointly Learning to Align and Translate</title>
    <summary>  Neural machine translation is a recently proposed approach to machine
translation. Unlike the traditional statistical machine translation, the neural
machine translation aims at building a single neural network that can be
jointly tuned to maximize the translation performance. The models proposed
recently for neural machine translation often belong to a family of
encoder-decoders and consists of an encoder that encodes a source sentence into
a fixed-length vector from which a decoder generates a translation. In this
paper, we conjecture that the use of a fixed-length vector is a bottleneck in
improving the performance of this basic encoder-decoder architecture, and
propose to extend this by allowing a model to automatically (soft-)search for
parts of a source sentence that are relevant to predicting a target word,
without having to form these parts as a hard segment explicitly. With this new
approach, we achieve a translation performance comparable to the existing
state-of-the-art phrase-based system on the task of English-to-French
translation. Furthermore, qualitative analysis reveals that the
(soft-)alignments found by the model agree well with our intuition.
</summary>
    <author>
      <name>Dzmitry Bahdanau</name>
    </author>
    <author>
      <name>Kyunghyun Cho</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ICLR 2015 as oral presentation</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.0473v7" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.0473v7" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.0473v7</id>
    <updated>2016-05-19T21:53:22Z</updated>
    <published>2014-09-01T16:33:02Z</published>
    <title>Neural Machine Translation by Jointly Learning to Align and Translate</title>
    <summary>  Neural machine translation is a recently proposed approach to machine
translation. Unlike the traditional statistical machine translation, the neural
machine translation aims at building a single neural network that can be
jointly tuned to maximize the translation performance. The models proposed
recently for neural machine translation often belong to a family of
encoder-decoders and consists of an encoder that encodes a source sentence into
a fixed-length vector from which a decoder generates a translation. In this
paper, we conjecture that the use of a fixed-length vector is a bottleneck in
improving the performance of this basic encoder-decoder architecture, and
propose to extend this by allowing a model to automatically (soft-)search for
parts of a source sentence that are relevant to predicting a target word,
without having to form these parts as a hard segment explicitly. With this new
approach, we achieve a translation performance comparable to the existing
state-of-the-art phrase-based system on the task of English-to-French
translation. Furthermore, qualitative analysis reveals that the
(soft-)alignments found by the model agree well with our intuition.
</summary>
    <author>
      <name>Dzmitry Bahdanau</name>
    </author>
    <author>
      <name>Kyunghyun Cho</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ICLR 2015 as oral presentation</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.0473v7" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.0473v7" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.0473v7</id>
    <updated>2016-05-19T21:53:22Z</updated>
    <published>2014-09-01T16:33:02Z</published>
    <title>Neural Machine Translation by Jointly Learning to Align and Translate</title>
    <summary>  Neural machine translation is a recently proposed approach to machine
translation. Unlike the traditional statistical machine translation, the neural
machine translation aims at building a single neural network that can be
jointly tuned to maximize the translation performance. The models proposed
recently for neural machine translation often belong to a family of
encoder-decoders and consists of an encoder that encodes a source sentence into
a fixed-length vector from which a decoder generates a translation. In this
paper, we conjecture that the use of a fixed-length vector is a bottleneck in
improving the performance of this basic encoder-decoder architecture, and
propose to extend this by allowing a model to automatically (soft-)search for
parts of a source sentence that are relevant to predicting a target word,
without having to form these parts as a hard segment explicitly. With this new
approach, we achieve a translation performance comparable to the existing
state-of-the-art phrase-based system on the task of English-to-French
translation. Furthermore, qualitative analysis reveals that the
(soft-)alignments found by the model agree well with our intuition.
</summary>
    <author>
      <name>Dzmitry Bahdanau</name>
    </author>
    <author>
      <name>Kyunghyun Cho</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ICLR 2015 as oral presentation</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.0473v7" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.0473v7" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.0575v3</id>
    <updated>2015-01-30T01:23:59Z</updated>
    <published>2014-09-01T22:29:38Z</published>
    <title>ImageNet Large Scale Visual Recognition Challenge</title>
    <summary>  The ImageNet Large Scale Visual Recognition Challenge is a benchmark in
object category classification and detection on hundreds of object categories
and millions of images. The challenge has been run annually from 2010 to
present, attracting participation from more than fifty institutions.
  This paper describes the creation of this benchmark dataset and the advances
in object recognition that have been possible as a result. We discuss the
challenges of collecting large-scale ground truth annotation, highlight key
breakthroughs in categorical object recognition, provide a detailed analysis of
the current state of the field of large-scale image classification and object
detection, and compare the state-of-the-art computer vision accuracy with human
accuracy. We conclude with lessons learned in the five years of the challenge,
and propose future directions and improvements.
</summary>
    <author>
      <name>Olga Russakovsky</name>
    </author>
    <author>
      <name>Jia Deng</name>
    </author>
    <author>
      <name>Hao Su</name>
    </author>
    <author>
      <name>Jonathan Krause</name>
    </author>
    <author>
      <name>Sanjeev Satheesh</name>
    </author>
    <author>
      <name>Sean Ma</name>
    </author>
    <author>
      <name>Zhiheng Huang</name>
    </author>
    <author>
      <name>Andrej Karpathy</name>
    </author>
    <author>
      <name>Aditya Khosla</name>
    </author>
    <author>
      <name>Michael Bernstein</name>
    </author>
    <author>
      <name>Alexander C. Berg</name>
    </author>
    <author>
      <name>Li Fei-Fei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">43 pages, 16 figures. v3 includes additional comparisons with PASCAL
  VOC (per-category comparisons in Table 3, distribution of localization
  difficulty in Fig 16), a list of queries used for obtaining object detection
  images (Appendix C), and some additional references</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.0575v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.0575v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.8; I.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.1259v2</id>
    <updated>2014-10-07T18:08:30Z</updated>
    <published>2014-09-03T21:03:41Z</published>
    <title>On the Properties of Neural Machine Translation: Encoder-Decoder
  Approaches</title>
    <summary>  Neural machine translation is a relatively new approach to statistical
machine translation based purely on neural networks. The neural machine
translation models often consist of an encoder and a decoder. The encoder
extracts a fixed-length representation from a variable-length input sentence,
and the decoder generates a correct translation from this representation. In
this paper, we focus on analyzing the properties of the neural machine
translation using two models; RNN Encoder--Decoder and a newly proposed gated
recursive convolutional neural network. We show that the neural machine
translation performs relatively well on short sentences without unknown words,
but its performance degrades rapidly as the length of the sentence and the
number of unknown words increase. Furthermore, we find that the proposed gated
recursive convolutional network learns a grammatical structure of a sentence
automatically.
</summary>
    <author>
      <name>Kyunghyun Cho</name>
    </author>
    <author>
      <name>Bart van Merrienboer</name>
    </author>
    <author>
      <name>Dzmitry Bahdanau</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Eighth Workshop on Syntax, Semantics and Structure in Statistical
  Translation (SSST-8)</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.1259v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.1259v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.1556v6</id>
    <updated>2015-04-10T16:25:04Z</updated>
    <published>2014-09-04T19:48:04Z</published>
    <title>Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
    <summary>  In this work we investigate the effect of the convolutional network depth on
its accuracy in the large-scale image recognition setting. Our main
contribution is a thorough evaluation of networks of increasing depth using an
architecture with very small (3x3) convolution filters, which shows that a
significant improvement on the prior-art configurations can be achieved by
pushing the depth to 16-19 weight layers. These findings were the basis of our
ImageNet Challenge 2014 submission, where our team secured the first and the
second places in the localisation and classification tracks respectively. We
also show that our representations generalise well to other datasets, where
they achieve state-of-the-art results. We have made our two best-performing
ConvNet models publicly available to facilitate further research on the use of
deep visual representations in computer vision.
</summary>
    <author>
      <name>Karen Simonyan</name>
    </author>
    <author>
      <name>Andrew Zisserman</name>
    </author>
    <link href="http://arxiv.org/abs/1409.1556v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.1556v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.1556v6</id>
    <updated>2015-04-10T16:25:04Z</updated>
    <published>2014-09-04T19:48:04Z</published>
    <title>Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
    <summary>  In this work we investigate the effect of the convolutional network depth on
its accuracy in the large-scale image recognition setting. Our main
contribution is a thorough evaluation of networks of increasing depth using an
architecture with very small (3x3) convolution filters, which shows that a
significant improvement on the prior-art configurations can be achieved by
pushing the depth to 16-19 weight layers. These findings were the basis of our
ImageNet Challenge 2014 submission, where our team secured the first and the
second places in the localisation and classification tracks respectively. We
also show that our representations generalise well to other datasets, where
they achieve state-of-the-art results. We have made our two best-performing
ConvNet models publicly available to facilitate further research on the use of
deep visual representations in computer vision.
</summary>
    <author>
      <name>Karen Simonyan</name>
    </author>
    <author>
      <name>Andrew Zisserman</name>
    </author>
    <link href="http://arxiv.org/abs/1409.1556v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.1556v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.2329v5</id>
    <updated>2015-02-19T14:46:00Z</updated>
    <published>2014-09-08T13:08:00Z</published>
    <title>Recurrent Neural Network Regularization</title>
    <summary>  We present a simple regularization technique for Recurrent Neural Networks
(RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful
technique for regularizing neural networks, does not work well with RNNs and
LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show
that it substantially reduces overfitting on a variety of tasks. These tasks
include language modeling, speech recognition, image caption generation, and
machine translation.
</summary>
    <author>
      <name>Wojciech Zaremba</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <link href="http://arxiv.org/abs/1409.2329v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.2329v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.3215v3</id>
    <updated>2014-12-14T20:59:51Z</updated>
    <published>2014-09-10T19:55:35Z</published>
    <title>Sequence to Sequence Learning with Neural Networks</title>
    <summary>  Deep Neural Networks (DNNs) are powerful models that have achieved excellent
performance on difficult learning tasks. Although DNNs work well whenever large
labeled training sets are available, they cannot be used to map sequences to
sequences. In this paper, we present a general end-to-end approach to sequence
learning that makes minimal assumptions on the sequence structure. Our method
uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to
a vector of a fixed dimensionality, and then another deep LSTM to decode the
target sequence from the vector. Our main result is that on an English to
French translation task from the WMT'14 dataset, the translations produced by
the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's
BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did
not have difficulty on long sentences. For comparison, a phrase-based SMT
system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM
to rerank the 1000 hypotheses produced by the aforementioned SMT system, its
BLEU score increases to 36.5, which is close to the previous best result on
this task. The LSTM also learned sensible phrase and sentence representations
that are sensitive to word order and are relatively invariant to the active and
the passive voice. Finally, we found that reversing the order of the words in
all source sentences (but not target sentences) improved the LSTM's performance
markedly, because doing so introduced many short term dependencies between the
source and the target sentence which made the optimization problem easier.
</summary>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Quoc V. Le</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.3215v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.3215v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.3831v1</id>
    <updated>2014-10-14T20:00:09Z</updated>
    <published>2014-10-14T20:00:09Z</published>
    <title>An exact mapping between the Variational Renormalization Group and Deep
  Learning</title>
    <summary>  Deep learning is a broad set of techniques that uses multiple layers of
representation to automatically learn relevant features directly from
structured data. Recently, such techniques have yielded record-breaking results
on a diverse set of difficult machine learning tasks in computer vision, speech
recognition, and natural language processing. Despite the enormous success of
deep learning, relatively little is understood theoretically about why these
techniques are so successful at feature learning and compression. Here, we show
that deep learning is intimately related to one of the most important and
successful techniques in theoretical physics, the renormalization group (RG).
RG is an iterative coarse-graining scheme that allows for the extraction of
relevant features (i.e. operators) as a physical system is examined at
different length scales. We construct an exact mapping from the variational
renormalization group, first introduced by Kadanoff, and deep learning
architectures based on Restricted Boltzmann Machines (RBMs). We illustrate
these ideas using the nearest-neighbor Ising Model in one and two-dimensions.
Our results suggests that deep learning algorithms may be employing a
generalized RG-like scheme to learn relevant features from data.
</summary>
    <author>
      <name>Pankaj Mehta</name>
    </author>
    <author>
      <name>David J. Schwab</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.3831v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.3831v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.3916v11</id>
    <updated>2015-11-29T07:00:41Z</updated>
    <published>2014-10-15T03:13:18Z</published>
    <title>Memory Networks</title>
    <summary>  We describe a new class of learning models called memory networks. Memory
networks reason with inference components combined with a long-term memory
component; they learn how to use these jointly. The long-term memory can be
read and written to, with the goal of using it for prediction. We investigate
these models in the context of question answering (QA) where the long-term
memory effectively acts as a (dynamic) knowledge base, and the output is a
textual response. We evaluate them on a large-scale QA task, and a smaller, but
more complex, toy task generated from a simulated world. In the latter, we show
the reasoning power of such models by chaining multiple supporting sentences to
answer questions that require understanding the intension of verbs.
</summary>
    <author>
      <name>Jason Weston</name>
    </author>
    <author>
      <name>Sumit Chopra</name>
    </author>
    <author>
      <name>Antoine Bordes</name>
    </author>
    <link href="http://arxiv.org/abs/1410.3916v11" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.3916v11" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.5401v2</id>
    <updated>2014-12-10T16:01:39Z</updated>
    <published>2014-10-20T19:28:26Z</published>
    <title>Neural Turing Machines</title>
    <summary>  We extend the capabilities of neural networks by coupling them to external
memory resources, which they can interact with by attentional processes. The
combined system is analogous to a Turing Machine or Von Neumann architecture
but is differentiable end-to-end, allowing it to be efficiently trained with
gradient descent. Preliminary results demonstrate that Neural Turing Machines
can infer simple algorithms such as copying, sorting, and associative recall
from input and output examples.
</summary>
    <author>
      <name>Alex Graves</name>
    </author>
    <author>
      <name>Greg Wayne</name>
    </author>
    <author>
      <name>Ivo Danihelka</name>
    </author>
    <link href="http://arxiv.org/abs/1410.5401v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.5401v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.5401v2</id>
    <updated>2014-12-10T16:01:39Z</updated>
    <published>2014-10-20T19:28:26Z</published>
    <title>Neural Turing Machines</title>
    <summary>  We extend the capabilities of neural networks by coupling them to external
memory resources, which they can interact with by attentional processes. The
combined system is analogous to a Turing Machine or Von Neumann architecture
but is differentiable end-to-end, allowing it to be efficiently trained with
gradient descent. Preliminary results demonstrate that Neural Turing Machines
can infer simple algorithms such as copying, sorting, and associative recall
from input and output examples.
</summary>
    <author>
      <name>Alex Graves</name>
    </author>
    <author>
      <name>Greg Wayne</name>
    </author>
    <author>
      <name>Ivo Danihelka</name>
    </author>
    <link href="http://arxiv.org/abs/1410.5401v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.5401v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.8206v4</id>
    <updated>2015-05-30T19:57:28Z</updated>
    <published>2014-10-30T00:20:31Z</published>
    <title>Addressing the Rare Word Problem in Neural Machine Translation</title>
    <summary>  Neural Machine Translation (NMT) is a new approach to machine translation
that has shown promising results that are comparable to traditional approaches.
A significant weakness in conventional NMT systems is their inability to
correctly translate very rare words: end-to-end NMTs tend to have relatively
small vocabularies with a single unk symbol that represents every possible
out-of-vocabulary (OOV) word. In this paper, we propose and implement an
effective technique to address this problem. We train an NMT system on data
that is augmented by the output of a word alignment algorithm, allowing the NMT
system to emit, for each OOV word in the target sentence, the position of its
corresponding word in the source sentence. This information is later utilized
in a post-processing step that translates every OOV word using a dictionary.
Our experiments on the WMT14 English to French translation task show that this
method provides a substantial improvement of up to 2.8 BLEU points over an
equivalent NMT system that does not use this technique. With 37.5 BLEU points,
our NMT system is the first to surpass the best result achieved on a WMT14
contest task.
</summary>
    <author>
      <name>Minh-Thang Luong</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Quoc V. Le</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Wojciech Zaremba</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACL 2015 camera-ready version</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.8206v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.8206v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.0035v1</id>
    <updated>2014-11-26T18:51:52Z</updated>
    <published>2014-11-26T18:51:52Z</published>
    <title>Understanding Deep Image Representations by Inverting Them</title>
    <summary>  Image representations, from SIFT and Bag of Visual Words to Convolutional
Neural Networks (CNNs), are a crucial component of almost any image
understanding system. Nevertheless, our understanding of them remains limited.
In this paper we conduct a direct analysis of the visual information contained
in representations by asking the following question: given an encoding of an
image, to which extent is it possible to reconstruct the image itself? To
answer this question we contribute a general framework to invert
representations. We show that this method can invert representations such as
HOG and SIFT more accurately than recent alternatives while being applicable to
CNNs too. We then use this technique to study the inverse of recent
state-of-the-art CNN image representations for the first time. Among our
findings, we show that several layers in CNNs retain photographically accurate
information about the image, with different degrees of geometric and
photometric invariance.
</summary>
    <author>
      <name>Aravindh Mahendran</name>
    </author>
    <author>
      <name>Andrea Vedaldi</name>
    </author>
    <link href="http://arxiv.org/abs/1412.0035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.0035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.1842v1</id>
    <updated>2014-12-04T21:14:59Z</updated>
    <published>2014-12-04T21:14:59Z</published>
    <title>Reading Text in the Wild with Convolutional Neural Networks</title>
    <summary>  In this work we present an end-to-end system for text spotting -- localising
and recognising text in natural scene images -- and text based image retrieval.
This system is based on a region proposal mechanism for detection and deep
convolutional neural networks for recognition. Our pipeline uses a novel
combination of complementary proposal generation techniques to ensure high
recall, and a fast subsequent filtering stage for improving precision. For the
recognition and ranking of proposals, we train very large convolutional neural
networks to perform word recognition on the whole proposal region at the same
time, departing from the character classifier based systems of the past. These
networks are trained solely on data produced by a synthetic text generation
engine, requiring no human labelled data.
  Analysing the stages of our pipeline, we show state-of-the-art performance
throughout. We perform rigorous experiments across a number of standard
end-to-end text spotting benchmarks and text-based image retrieval datasets,
showing a large improvement over all previous methods. Finally, we demonstrate
a real-world application of our text spotting system to allow thousands of
hours of news footage to be instantly searchable via a text query.
</summary>
    <author>
      <name>Max Jaderberg</name>
    </author>
    <author>
      <name>Karen Simonyan</name>
    </author>
    <author>
      <name>Andrea Vedaldi</name>
    </author>
    <author>
      <name>Andrew Zisserman</name>
    </author>
    <link href="http://arxiv.org/abs/1412.1842v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.1842v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.1897v4</id>
    <updated>2015-04-02T23:12:56Z</updated>
    <published>2014-12-05T05:29:43Z</published>
    <title>Deep Neural Networks are Easily Fooled: High Confidence Predictions for
  Unrecognizable Images</title>
    <summary>  Deep neural networks (DNNs) have recently been achieving state-of-the-art
performance on a variety of pattern-recognition tasks, most notably visual
classification problems. Given that DNNs are now able to classify objects in
images with near-human-level performance, questions naturally arise as to what
differences remain between computer and human vision. A recent study revealed
that changing an image (e.g. of a lion) in a way imperceptible to humans can
cause a DNN to label the image as something else entirely (e.g. mislabeling a
lion a library). Here we show a related result: it is easy to produce images
that are completely unrecognizable to humans, but that state-of-the-art DNNs
believe to be recognizable objects with 99.99% confidence (e.g. labeling with
certainty that white noise static is a lion). Specifically, we take
convolutional neural networks trained to perform well on either the ImageNet or
MNIST datasets and then find images with evolutionary algorithms or gradient
ascent that DNNs label with high confidence as belonging to each dataset class.
It is possible to produce images totally unrecognizable to human eyes that DNNs
believe with near certainty are familiar objects, which we call "fooling
images" (more generally, fooling examples). Our results shed light on
interesting differences between human vision and current DNNs, and raise
questions about the generality of DNN computer vision.
</summary>
    <author>
      <name>Anh Nguyen</name>
    </author>
    <author>
      <name>Jason Yosinski</name>
    </author>
    <author>
      <name>Jeff Clune</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at CVPR 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.1897v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.1897v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.1897v4</id>
    <updated>2015-04-02T23:12:56Z</updated>
    <published>2014-12-05T05:29:43Z</published>
    <title>Deep Neural Networks are Easily Fooled: High Confidence Predictions for
  Unrecognizable Images</title>
    <summary>  Deep neural networks (DNNs) have recently been achieving state-of-the-art
performance on a variety of pattern-recognition tasks, most notably visual
classification problems. Given that DNNs are now able to classify objects in
images with near-human-level performance, questions naturally arise as to what
differences remain between computer and human vision. A recent study revealed
that changing an image (e.g. of a lion) in a way imperceptible to humans can
cause a DNN to label the image as something else entirely (e.g. mislabeling a
lion a library). Here we show a related result: it is easy to produce images
that are completely unrecognizable to humans, but that state-of-the-art DNNs
believe to be recognizable objects with 99.99% confidence (e.g. labeling with
certainty that white noise static is a lion). Specifically, we take
convolutional neural networks trained to perform well on either the ImageNet or
MNIST datasets and then find images with evolutionary algorithms or gradient
ascent that DNNs label with high confidence as belonging to each dataset class.
It is possible to produce images totally unrecognizable to human eyes that DNNs
believe with near certainty are familiar objects, which we call "fooling
images" (more generally, fooling examples). Our results shed light on
interesting differences between human vision and current DNNs, and raise
questions about the generality of DNN computer vision.
</summary>
    <author>
      <name>Anh Nguyen</name>
    </author>
    <author>
      <name>Jason Yosinski</name>
    </author>
    <author>
      <name>Jeff Clune</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at CVPR 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.1897v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.1897v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.3555v1</id>
    <updated>2014-12-11T06:46:53Z</updated>
    <published>2014-12-11T06:46:53Z</published>
    <title>Empirical Evaluation of Gated Recurrent Neural Networks on Sequence
  Modeling</title>
    <summary>  In this paper we compare different types of recurrent units in recurrent
neural networks (RNNs). Especially, we focus on more sophisticated units that
implement a gating mechanism, such as a long short-term memory (LSTM) unit and
a recently proposed gated recurrent unit (GRU). We evaluate these recurrent
units on the tasks of polyphonic music modeling and speech signal modeling. Our
experiments revealed that these advanced recurrent units are indeed better than
more traditional recurrent units such as tanh units. Also, we found GRU to be
comparable to LSTM.
</summary>
    <author>
      <name>Junyoung Chung</name>
    </author>
    <author>
      <name>Caglar Gulcehre</name>
    </author>
    <author>
      <name>KyungHyun Cho</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented in NIPS 2014 Deep Learning and Representation Learning
  Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.3555v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.3555v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.4564v3</id>
    <updated>2016-05-05T14:31:06Z</updated>
    <published>2014-12-15T12:23:35Z</published>
    <title>MatConvNet - Convolutional Neural Networks for MATLAB</title>
    <summary>  MatConvNet is an implementation of Convolutional Neural Networks (CNNs) for
MATLAB. The toolbox is designed with an emphasis on simplicity and flexibility.
It exposes the building blocks of CNNs as easy-to-use MATLAB functions,
providing routines for computing linear convolutions with filter banks, feature
pooling, and many more. In this manner, MatConvNet allows fast prototyping of
new CNN architectures; at the same time, it supports efficient computation on
CPU and GPU allowing to train complex models on large datasets such as ImageNet
ILSVRC. This document provides an overview of CNNs and how they are implemented
in MatConvNet and gives the technical details of each computational block in
the toolbox.
</summary>
    <author>
      <name>Andrea Vedaldi</name>
    </author>
    <author>
      <name>Karel Lenc</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Updated for release v1.0-beta20</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.4564v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.4564v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.6621v3</id>
    <updated>2015-02-28T07:19:35Z</updated>
    <published>2014-12-20T07:28:46Z</published>
    <title>Why does Deep Learning work? - A perspective from Group Theory</title>
    <summary>  Why does Deep Learning work? What representations does it capture? How do
higher-order representations emerge? We study these questions from the
perspective of group theory, thereby opening a new approach towards a theory of
Deep learning.
  One factor behind the recent resurgence of the subject is a key algorithmic
step called pre-training: first search for a good generative model for the
input samples, and repeat the process one layer at a time. We show deeper
implications of this simple principle, by establishing a connection with the
interplay of orbits and stabilizers of group actions. Although the neural
networks themselves may not form groups, we show the existence of {\em shadow}
groups whose elements serve as close approximations.
  Over the shadow groups, the pre-training step, originally introduced as a
mechanism to better initialize a network, becomes equivalent to a search for
features with minimal orbits. Intuitively, these features are in a way the {\em
simplest}. Which explains why a deep learning network learns simple features
first. Next, we show how the same principle, when repeated in the deeper
layers, can capture higher order representations, and why representation
complexity increases as the layers get deeper.
</summary>
    <author>
      <name>Arnab Paul</name>
    </author>
    <author>
      <name>Suresh Venkatasubramanian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.6621v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.6621v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.6980v9</id>
    <updated>2017-01-30T01:27:54Z</updated>
    <published>2014-12-22T13:54:29Z</published>
    <title>Adam: A Method for Stochastic Optimization</title>
    <summary>  We introduce Adam, an algorithm for first-order gradient-based optimization
of stochastic objective functions, based on adaptive estimates of lower-order
moments. The method is straightforward to implement, is computationally
efficient, has little memory requirements, is invariant to diagonal rescaling
of the gradients, and is well suited for problems that are large in terms of
data and/or parameters. The method is also appropriate for non-stationary
objectives and problems with very noisy and/or sparse gradients. The
hyper-parameters have intuitive interpretations and typically require little
tuning. Some connections to related algorithms, on which Adam was inspired, are
discussed. We also analyze the theoretical convergence properties of the
algorithm and provide a regret bound on the convergence rate that is comparable
to the best known results under the online convex optimization framework.
Empirical results demonstrate that Adam works well in practice and compares
favorably to other stochastic optimization methods. Finally, we discuss AdaMax,
a variant of Adam based on the infinity norm.
</summary>
    <author>
      <name>Diederik P. Kingma</name>
    </author>
    <author>
      <name>Jimmy Ba</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a conference paper at the 3rd International Conference
  for Learning Representations, San Diego, 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.6980v9" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.6980v9" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.6980v9</id>
    <updated>2017-01-30T01:27:54Z</updated>
    <published>2014-12-22T13:54:29Z</published>
    <title>Adam: A Method for Stochastic Optimization</title>
    <summary>  We introduce Adam, an algorithm for first-order gradient-based optimization
of stochastic objective functions, based on adaptive estimates of lower-order
moments. The method is straightforward to implement, is computationally
efficient, has little memory requirements, is invariant to diagonal rescaling
of the gradients, and is well suited for problems that are large in terms of
data and/or parameters. The method is also appropriate for non-stationary
objectives and problems with very noisy and/or sparse gradients. The
hyper-parameters have intuitive interpretations and typically require little
tuning. Some connections to related algorithms, on which Adam was inspired, are
discussed. We also analyze the theoretical convergence properties of the
algorithm and provide a regret bound on the convergence rate that is comparable
to the best known results under the online convex optimization framework.
Empirical results demonstrate that Adam works well in practice and compares
favorably to other stochastic optimization methods. Finally, we discuss AdaMax,
a variant of Adam based on the infinity norm.
</summary>
    <author>
      <name>Diederik P. Kingma</name>
    </author>
    <author>
      <name>Jimmy Ba</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a conference paper at the 3rd International Conference
  for Learning Representations, San Diego, 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.6980v9" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.6980v9" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.7062v4</id>
    <updated>2016-06-07T04:00:08Z</updated>
    <published>2014-12-22T17:18:33Z</published>
    <title>Semantic Image Segmentation with Deep Convolutional Nets and Fully
  Connected CRFs</title>
    <summary>  Deep Convolutional Neural Networks (DCNNs) have recently shown state of the
art performance in high level vision tasks, such as image classification and
object detection. This work brings together methods from DCNNs and
probabilistic graphical models for addressing the task of pixel-level
classification (also called "semantic image segmentation"). We show that
responses at the final layer of DCNNs are not sufficiently localized for
accurate object segmentation. This is due to the very invariance properties
that make DCNNs good for high level tasks. We overcome this poor localization
property of deep networks by combining the responses at the final DCNN layer
with a fully connected Conditional Random Field (CRF). Qualitatively, our
"DeepLab" system is able to localize segment boundaries at a level of accuracy
which is beyond previous methods. Quantitatively, our method sets the new
state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching
71.6% IOU accuracy in the test set. We show how these results can be obtained
efficiently: Careful network re-purposing and a novel application of the 'hole'
algorithm from the wavelet community allow dense computation of neural net
responses at 8 frames per second on a modern GPU.
</summary>
    <author>
      <name>Liang-Chieh Chen</name>
    </author>
    <author>
      <name>George Papandreou</name>
    </author>
    <author>
      <name>Iasonas Kokkinos</name>
    </author>
    <author>
      <name>Kevin Murphy</name>
    </author>
    <author>
      <name>Alan L. Yuille</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages. Updated related work</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.7062v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.7062v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.00092v3</id>
    <updated>2015-07-31T09:13:32Z</updated>
    <published>2014-12-31T08:35:09Z</published>
    <title>Image Super-Resolution Using Deep Convolutional Networks</title>
    <summary>  We propose a deep learning method for single image super-resolution (SR). Our
method directly learns an end-to-end mapping between the low/high-resolution
images. The mapping is represented as a deep convolutional neural network (CNN)
that takes the low-resolution image as the input and outputs the
high-resolution one. We further show that traditional sparse-coding-based SR
methods can also be viewed as a deep convolutional network. But unlike
traditional methods that handle each component separately, our method jointly
optimizes all layers. Our deep CNN has a lightweight structure, yet
demonstrates state-of-the-art restoration quality, and achieves fast speed for
practical on-line usage. We explore different network structures and parameter
settings to achieve trade-offs between performance and speed. Moreover, we
extend our network to cope with three color channels simultaneously, and show
better overall reconstruction quality.
</summary>
    <author>
      <name>Chao Dong</name>
    </author>
    <author>
      <name>Chen Change Loy</name>
    </author>
    <author>
      <name>Kaiming He</name>
    </author>
    <author>
      <name>Xiaoou Tang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 14 figures, journal</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.00092v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.00092v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.5; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.03044v3</id>
    <updated>2016-04-19T16:43:09Z</updated>
    <published>2015-02-10T19:18:29Z</published>
    <title>Show, Attend and Tell: Neural Image Caption Generation with Visual
  Attention</title>
    <summary>  Inspired by recent work in machine translation and object detection, we
introduce an attention based model that automatically learns to describe the
content of images. We describe how we can train this model in a deterministic
manner using standard backpropagation techniques and stochastically by
maximizing a variational lower bound. We also show through visualization how
the model is able to automatically learn to fix its gaze on salient objects
while generating the corresponding words in the output sequence. We validate
the use of attention with state-of-the-art performance on three benchmark
datasets: Flickr8k, Flickr30k and MS COCO.
</summary>
    <author>
      <name>Kelvin Xu</name>
    </author>
    <author>
      <name>Jimmy Ba</name>
    </author>
    <author>
      <name>Ryan Kiros</name>
    </author>
    <author>
      <name>Kyunghyun Cho</name>
    </author>
    <author>
      <name>Aaron Courville</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <author>
      <name>Richard Zemel</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <link href="http://arxiv.org/abs/1502.03044v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.03044v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.03167v3</id>
    <updated>2015-03-02T20:44:12Z</updated>
    <published>2015-02-11T01:44:18Z</published>
    <title>Batch Normalization: Accelerating Deep Network Training by Reducing
  Internal Covariate Shift</title>
    <summary>  Training Deep Neural Networks is complicated by the fact that the
distribution of each layer's inputs changes during training, as the parameters
of the previous layers change. This slows down the training by requiring lower
learning rates and careful parameter initialization, and makes it notoriously
hard to train models with saturating nonlinearities. We refer to this
phenomenon as internal covariate shift, and address the problem by normalizing
layer inputs. Our method draws its strength from making normalization a part of
the model architecture and performing the normalization for each training
mini-batch. Batch Normalization allows us to use much higher learning rates and
be less careful about initialization. It also acts as a regularizer, in some
cases eliminating the need for Dropout. Applied to a state-of-the-art image
classification model, Batch Normalization achieves the same accuracy with 14
times fewer training steps, and beats the original model by a significant
margin. Using an ensemble of batch-normalized networks, we improve upon the
best published result on ImageNet classification: reaching 4.9% top-5
validation error (and 4.8% test error), exceeding the accuracy of human raters.
</summary>
    <author>
      <name>Sergey Ioffe</name>
    </author>
    <author>
      <name>Christian Szegedy</name>
    </author>
    <link href="http://arxiv.org/abs/1502.03167v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.03167v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.03167v3</id>
    <updated>2015-03-02T20:44:12Z</updated>
    <published>2015-02-11T01:44:18Z</published>
    <title>Batch Normalization: Accelerating Deep Network Training by Reducing
  Internal Covariate Shift</title>
    <summary>  Training Deep Neural Networks is complicated by the fact that the
distribution of each layer's inputs changes during training, as the parameters
of the previous layers change. This slows down the training by requiring lower
learning rates and careful parameter initialization, and makes it notoriously
hard to train models with saturating nonlinearities. We refer to this
phenomenon as internal covariate shift, and address the problem by normalizing
layer inputs. Our method draws its strength from making normalization a part of
the model architecture and performing the normalization for each training
mini-batch. Batch Normalization allows us to use much higher learning rates and
be less careful about initialization. It also acts as a regularizer, in some
cases eliminating the need for Dropout. Applied to a state-of-the-art image
classification model, Batch Normalization achieves the same accuracy with 14
times fewer training steps, and beats the original model by a significant
margin. Using an ensemble of batch-normalized networks, we improve upon the
best published result on ImageNet classification: reaching 4.9% top-5
validation error (and 4.8% test error), exceeding the accuracy of human raters.
</summary>
    <author>
      <name>Sergey Ioffe</name>
    </author>
    <author>
      <name>Christian Szegedy</name>
    </author>
    <link href="http://arxiv.org/abs/1502.03167v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.03167v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.04623v2</id>
    <updated>2015-05-20T15:29:42Z</updated>
    <published>2015-02-16T16:48:56Z</published>
    <title>DRAW: A Recurrent Neural Network For Image Generation</title>
    <summary>  This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural
network architecture for image generation. DRAW networks combine a novel
spatial attention mechanism that mimics the foveation of the human eye, with a
sequential variational auto-encoding framework that allows for the iterative
construction of complex images. The system substantially improves on the state
of the art for generative models on MNIST, and, when trained on the Street View
House Numbers dataset, it generates images that cannot be distinguished from
real data with the naked eye.
</summary>
    <author>
      <name>Karol Gregor</name>
    </author>
    <author>
      <name>Ivo Danihelka</name>
    </author>
    <author>
      <name>Alex Graves</name>
    </author>
    <author>
      <name>Danilo Jimenez Rezende</name>
    </author>
    <author>
      <name>Daan Wierstra</name>
    </author>
    <link href="http://arxiv.org/abs/1502.04623v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.04623v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.05082v3</id>
    <updated>2015-08-01T16:33:25Z</updated>
    <published>2015-02-17T22:45:14Z</published>
    <title>What makes for effective detection proposals?</title>
    <summary>  Current top performing object detectors employ detection proposals to guide
the search for objects, thereby avoiding exhaustive sliding window search
across images. Despite the popularity and widespread use of detection
proposals, it is unclear which trade-offs are made when using them during
object detection. We provide an in-depth analysis of twelve proposal methods
along with four baselines regarding proposal repeatability, ground truth
annotation recall on PASCAL, ImageNet, and MS COCO, and their impact on DPM,
R-CNN, and Fast R-CNN detection performance. Our analysis shows that for object
detection improving proposal localisation accuracy is as important as improving
recall. We introduce a novel metric, the average recall (AR), which rewards
both high recall and good localisation and correlates surprisingly well with
detection performance. Our findings show common strengths and weaknesses of
existing methods, and provide insights and metrics for selecting and tuning
proposal methods.
</summary>
    <author>
      <name>Jan Hosang</name>
    </author>
    <author>
      <name>Rodrigo Benenson</name>
    </author>
    <author>
      <name>Piotr Dollár</name>
    </author>
    <author>
      <name>Bernt Schiele</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TPAMI.2015.2465908</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TPAMI.2015.2465908" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">TPAMI final version, duplicate proposals removed in experiments</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.05082v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.05082v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.05698v10</id>
    <updated>2015-12-31T13:08:14Z</updated>
    <published>2015-02-19T20:46:10Z</published>
    <title>Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks</title>
    <summary>  One long-term goal of machine learning research is to produce methods that
are applicable to reasoning and natural language, in particular building an
intelligent dialogue agent. To measure progress towards that goal, we argue for
the usefulness of a set of proxy tasks that evaluate reading comprehension via
question answering. Our tasks measure understanding in several ways: whether a
system is able to answer questions via chaining facts, simple induction,
deduction and many more. The tasks are designed to be prerequisites for any
system that aims to be capable of conversing with a human. We believe many
existing learning systems can currently not solve them, and hence our aim is to
classify these tasks into skill sets, so that researchers can identify (and
then rectify) the failings of their systems. We also extend and improve the
recently introduced Memory Networks model, and show it is able to solve some,
but not all, of the tasks.
</summary>
    <author>
      <name>Jason Weston</name>
    </author>
    <author>
      <name>Antoine Bordes</name>
    </author>
    <author>
      <name>Sumit Chopra</name>
    </author>
    <author>
      <name>Alexander M. Rush</name>
    </author>
    <author>
      <name>Bart van Merriënboer</name>
    </author>
    <author>
      <name>Armand Joulin</name>
    </author>
    <author>
      <name>Tomas Mikolov</name>
    </author>
    <link href="http://arxiv.org/abs/1502.05698v10" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.05698v10" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.00075v3</id>
    <updated>2015-05-30T06:51:20Z</updated>
    <published>2015-02-28T06:31:50Z</published>
    <title>Improved Semantic Representations From Tree-Structured Long Short-Term
  Memory Networks</title>
    <summary>  Because of their superior ability to preserve sequence information over time,
Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with
a more complex computational unit, have obtained strong results on a variety of
sequence modeling tasks. The only underlying LSTM structure that has been
explored so far is a linear chain. However, natural language exhibits syntactic
properties that would naturally combine words to phrases. We introduce the
Tree-LSTM, a generalization of LSTMs to tree-structured network topologies.
Tree-LSTMs outperform all existing systems and strong LSTM baselines on two
tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task
1) and sentiment classification (Stanford Sentiment Treebank).
</summary>
    <author>
      <name>Kai Sheng Tai</name>
    </author>
    <author>
      <name>Richard Socher</name>
    </author>
    <author>
      <name>Christopher D. Manning</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication at ACL 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.00075v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.00075v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.00949v3</id>
    <updated>2016-02-22T20:26:43Z</updated>
    <published>2015-03-03T14:06:02Z</published>
    <title>Weakly Supervised Object Localization with Multi-fold Multiple Instance
  Learning</title>
    <summary>  Object category localization is a challenging problem in computer vision.
Standard supervised training requires bounding box annotations of object
instances. This time-consuming annotation process is sidestepped in weakly
supervised learning. In this case, the supervised information is restricted to
binary labels that indicate the absence/presence of object instances in the
image, without their locations. We follow a multiple-instance learning approach
that iteratively trains the detector and infers the object locations in the
positive training images. Our main contribution is a multi-fold multiple
instance learning procedure, which prevents training from prematurely locking
onto erroneous object locations. This procedure is particularly important when
using high-dimensional representations, such as Fisher vectors and
convolutional neural network features. We also propose a window refinement
method, which improves the localization accuracy by incorporating an objectness
prior. We present a detailed experimental evaluation using the PASCAL VOC 2007
dataset, which verifies the effectiveness of our approach.
</summary>
    <author>
      <name>Ramazan Gokberk Cinbis</name>
    </author>
    <author>
      <name>Jakob Verbeek</name>
    </author>
    <author>
      <name>Cordelia Schmid</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TPAMI.2016.2535231</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TPAMI.2016.2535231" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI)</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.00949v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.00949v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.02406v1</id>
    <updated>2015-03-09T09:39:41Z</updated>
    <published>2015-03-09T09:39:41Z</published>
    <title>Deep Learning and the Information Bottleneck Principle</title>
    <summary>  Deep Neural Networks (DNNs) are analyzed via the theoretical framework of the
information bottleneck (IB) principle. We first show that any DNN can be
quantified by the mutual information between the layers and the input and
output variables. Using this representation we can calculate the optimal
information theoretic limits of the DNN and obtain finite sample generalization
bounds. The advantage of getting closer to the theoretical limit is
quantifiable both by the generalization bound and by the network's simplicity.
We argue that both the optimal architecture, number of layers and
features/connections at each layer, are related to the bifurcation points of
the information bottleneck tradeoff, namely, relevant compression of the input
layer with respect to the output layer. The hierarchical representations at the
layered network naturally correspond to the structural phase transitions along
the information curve. We believe that this new insight can lead to new
optimality bounds and deep learning algorithms.
</summary>
    <author>
      <name>Naftali Tishby</name>
    </author>
    <author>
      <name>Noga Zaslavsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures, Invited paper to ITW 2015; 2015 IEEE Information
  Theory Workshop (ITW) (IEEE ITW 2015)</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.02406v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.02406v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.02531v1</id>
    <updated>2015-03-09T15:44:49Z</updated>
    <published>2015-03-09T15:44:49Z</published>
    <title>Distilling the Knowledge in a Neural Network</title>
    <summary>  A very simple way to improve the performance of almost any machine learning
algorithm is to train many different models on the same data and then to
average their predictions. Unfortunately, making predictions using a whole
ensemble of models is cumbersome and may be too computationally expensive to
allow deployment to a large number of users, especially if the individual
models are large neural nets. Caruana and his collaborators have shown that it
is possible to compress the knowledge in an ensemble into a single model which
is much easier to deploy and we develop this approach further using a different
compression technique. We achieve some surprising results on MNIST and we show
that we can significantly improve the acoustic model of a heavily used
commercial system by distilling the knowledge in an ensemble of models into a
single model. We also introduce a new type of ensemble composed of one or more
full models and many specialist models which learn to distinguish fine-grained
classes that the full models confuse. Unlike a mixture of experts, these
specialist models can be trained rapidly and in parallel.
</summary>
    <author>
      <name>Geoffrey Hinton</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Jeff Dean</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NIPS 2014 Deep Learning Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.02531v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.02531v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.02531v1</id>
    <updated>2015-03-09T15:44:49Z</updated>
    <published>2015-03-09T15:44:49Z</published>
    <title>Distilling the Knowledge in a Neural Network</title>
    <summary>  A very simple way to improve the performance of almost any machine learning
algorithm is to train many different models on the same data and then to
average their predictions. Unfortunately, making predictions using a whole
ensemble of models is cumbersome and may be too computationally expensive to
allow deployment to a large number of users, especially if the individual
models are large neural nets. Caruana and his collaborators have shown that it
is possible to compress the knowledge in an ensemble into a single model which
is much easier to deploy and we develop this approach further using a different
compression technique. We achieve some surprising results on MNIST and we show
that we can significantly improve the acoustic model of a heavily used
commercial system by distilling the knowledge in an ensemble of models into a
single model. We also introduce a new type of ensemble composed of one or more
full models and many specialist models which learn to distinguish fine-grained
classes that the full models confuse. Unlike a mixture of experts, these
specialist models can be trained rapidly and in parallel.
</summary>
    <author>
      <name>Geoffrey Hinton</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Jeff Dean</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NIPS 2014 Deep Learning Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.02531v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.02531v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.03585v8</id>
    <updated>2015-11-18T21:50:51Z</updated>
    <published>2015-03-12T04:51:37Z</published>
    <title>Deep Unsupervised Learning using Nonequilibrium Thermodynamics</title>
    <summary>  A central problem in machine learning involves modeling complex data-sets
using highly flexible families of probability distributions in which learning,
sampling, inference, and evaluation are still analytically or computationally
tractable. Here, we develop an approach that simultaneously achieves both
flexibility and tractability. The essential idea, inspired by non-equilibrium
statistical physics, is to systematically and slowly destroy structure in a
data distribution through an iterative forward diffusion process. We then learn
a reverse diffusion process that restores structure in data, yielding a highly
flexible and tractable generative model of the data. This approach allows us to
rapidly learn, sample from, and evaluate probabilities in deep generative
models with thousands of layers or time steps, as well as to compute
conditional and posterior probabilities under the learned model. We
additionally release an open source reference implementation of the algorithm.
</summary>
    <author>
      <name>Jascha Sohl-Dickstein</name>
    </author>
    <author>
      <name>Eric A. Weiss</name>
    </author>
    <author>
      <name>Niru Maheswaranathan</name>
    </author>
    <author>
      <name>Surya Ganguli</name>
    </author>
    <link href="http://arxiv.org/abs/1503.03585v8" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.03585v8" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.04069v2</id>
    <updated>2017-10-04T11:40:31Z</updated>
    <published>2015-03-13T14:01:38Z</published>
    <title>LSTM: A Search Space Odyssey</title>
    <summary>  Several variants of the Long Short-Term Memory (LSTM) architecture for
recurrent neural networks have been proposed since its inception in 1995. In
recent years, these networks have become the state-of-the-art models for a
variety of machine learning problems. This has led to a renewed interest in
understanding the role and utility of various computational components of
typical LSTM variants. In this paper, we present the first large-scale analysis
of eight LSTM variants on three representative tasks: speech recognition,
handwriting recognition, and polyphonic music modeling. The hyperparameters of
all LSTM variants for each task were optimized separately using random search,
and their importance was assessed using the powerful fANOVA framework. In
total, we summarize the results of 5400 experimental runs ($\approx 15$ years
of CPU time), which makes our study the largest of its kind on LSTM networks.
Our results show that none of the variants can improve upon the standard LSTM
architecture significantly, and demonstrate the forget gate and the output
activation function to be its most critical components. We further observe that
the studied hyperparameters are virtually independent and derive guidelines for
their efficient adjustment.
</summary>
    <author>
      <name>Klaus Greff</name>
    </author>
    <author>
      <name>Rupesh Kumar Srivastava</name>
    </author>
    <author>
      <name>Jan Koutník</name>
    </author>
    <author>
      <name>Bas R. Steunebrink</name>
    </author>
    <author>
      <name>Jürgen Schmidhuber</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TNNLS.2016.2582924</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TNNLS.2016.2582924" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 6 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Neural Networks and Learning Systems (
  Volume: 28, Issue: 10, Oct. 2017 ) Pages: 2222 - 2232</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1503.04069v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.04069v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T10" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.7; I.5.1; H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.05671v7</id>
    <updated>2020-06-08T01:28:58Z</updated>
    <published>2015-03-19T08:30:24Z</published>
    <title>Optimizing Neural Networks with Kronecker-factored Approximate Curvature</title>
    <summary>  We propose an efficient method for approximating natural gradient descent in
neural networks which we call Kronecker-Factored Approximate Curvature (K-FAC).
K-FAC is based on an efficiently invertible approximation of a neural network's
Fisher information matrix which is neither diagonal nor low-rank, and in some
cases is completely non-sparse. It is derived by approximating various large
blocks of the Fisher (corresponding to entire layers) as being the Kronecker
product of two much smaller matrices. While only several times more expensive
to compute than the plain stochastic gradient, the updates produced by K-FAC
make much more progress optimizing the objective, which results in an algorithm
that can be much faster than stochastic gradient descent with momentum in
practice. And unlike some previously proposed approximate
natural-gradient/Newton methods which use high-quality non-diagonal curvature
matrices (such as Hessian-free optimization), K-FAC works very well in highly
stochastic optimization regimes. This is because the cost of storing and
inverting K-FAC's approximation to the curvature matrix does not depend on the
amount of data used to estimate it, which is a feature typically associated
only with diagonal or low-rank approximations to the curvature matrix.
</summary>
    <author>
      <name>James Martens</name>
    </author>
    <author>
      <name>Roger Grosse</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Reduction ratio formula corrected. Removed incorrect claim about
  geodesics in footnote</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.05671v7" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.05671v7" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.00387v2</id>
    <updated>2015-11-03T18:15:15Z</updated>
    <published>2015-05-03T01:56:57Z</published>
    <title>Highway Networks</title>
    <summary>  There is plenty of theoretical and empirical evidence that depth of neural
networks is a crucial ingredient for their success. However, network training
becomes more difficult with increasing depth and training of very deep networks
remains an open problem. In this extended abstract, we introduce a new
architecture designed to ease gradient-based training of very deep networks. We
refer to networks with this architecture as highway networks, since they allow
unimpeded information flow across several layers on "information highways". The
architecture is characterized by the use of gating units which learn to
regulate the flow of information through a network. Highway networks with
hundreds of layers can be trained directly using stochastic gradient descent
and with a variety of activation functions, opening up the possibility of
studying extremely deep and efficient architectures.
</summary>
    <author>
      <name>Rupesh Kumar Srivastava</name>
    </author>
    <author>
      <name>Klaus Greff</name>
    </author>
    <author>
      <name>Jürgen Schmidhuber</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures. Presented at ICML 2015 Deep Learning workshop.
  Full paper is at arXiv:1507.06228</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.00387v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.00387v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T01" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; G.1.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.03540v3</id>
    <updated>2016-05-20T06:30:23Z</updated>
    <published>2015-05-13T20:06:21Z</published>
    <title>Brain Tumor Segmentation with Deep Neural Networks</title>
    <summary>  In this paper, we present a fully automatic brain tumor segmentation method
based on Deep Neural Networks (DNNs). The proposed networks are tailored to
glioblastomas (both low and high grade) pictured in MR images. By their very
nature, these tumors can appear anywhere in the brain and have almost any kind
of shape, size, and contrast. These reasons motivate our exploration of a
machine learning solution that exploits a flexible, high capacity DNN while
being extremely efficient. Here, we give a description of different model
choices that we've found to be necessary for obtaining competitive performance.
We explore in particular different architectures based on Convolutional Neural
Networks (CNN), i.e. DNNs specifically adapted to image data.
  We present a novel CNN architecture which differs from those traditionally
used in computer vision. Our CNN exploits both local features as well as more
global contextual features simultaneously. Also, different from most
traditional uses of CNNs, our networks use a final layer that is a
convolutional implementation of a fully connected layer which allows a 40 fold
speed up. We also describe a 2-phase training procedure that allows us to
tackle difficulties related to the imbalance of tumor labels. Finally, we
explore a cascade architecture in which the output of a basic CNN is treated as
an additional source of information for a subsequent CNN. Results reported on
the 2013 BRATS test dataset reveal that our architecture improves over the
currently published state-of-the-art while being over 30 times faster.
</summary>
    <author>
      <name>Mohammad Havaei</name>
    </author>
    <author>
      <name>Axel Davy</name>
    </author>
    <author>
      <name>David Warde-Farley</name>
    </author>
    <author>
      <name>Antoine Biard</name>
    </author>
    <author>
      <name>Aaron Courville</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <author>
      <name>Chris Pal</name>
    </author>
    <author>
      <name>Pierre-Marc Jodoin</name>
    </author>
    <author>
      <name>Hugo Larochelle</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.media.2016.05.004</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.media.2016.05.004" rel="related"/>
    <link href="http://arxiv.org/abs/1505.03540v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.03540v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.04366v1</id>
    <updated>2015-05-17T07:33:28Z</updated>
    <published>2015-05-17T07:33:28Z</published>
    <title>Learning Deconvolution Network for Semantic Segmentation</title>
    <summary>  We propose a novel semantic segmentation algorithm by learning a
deconvolution network. We learn the network on top of the convolutional layers
adopted from VGG 16-layer net. The deconvolution network is composed of
deconvolution and unpooling layers, which identify pixel-wise class labels and
predict segmentation masks. We apply the trained network to each proposal in an
input image, and construct the final semantic segmentation map by combining the
results from all proposals in a simple manner. The proposed algorithm mitigates
the limitations of the existing methods based on fully convolutional networks
by integrating deep deconvolution network and proposal-wise prediction; our
segmentation method typically identifies detailed structures and handles
objects in multiple scales naturally. Our network demonstrates outstanding
performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy
(72.5%) among the methods trained with no external data through ensemble with
the fully convolutional network.
</summary>
    <author>
      <name>Hyeonwoo Noh</name>
    </author>
    <author>
      <name>Seunghoon Hong</name>
    </author>
    <author>
      <name>Bohyung Han</name>
    </author>
    <link href="http://arxiv.org/abs/1505.04366v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.04366v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.04597v1</id>
    <updated>2015-05-18T11:28:37Z</updated>
    <published>2015-05-18T11:28:37Z</published>
    <title>U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
    <summary>  There is large consent that successful training of deep networks requires
many thousand annotated training samples. In this paper, we present a network
and training strategy that relies on the strong use of data augmentation to use
the available annotated samples more efficiently. The architecture consists of
a contracting path to capture context and a symmetric expanding path that
enables precise localization. We show that such a network can be trained
end-to-end from very few images and outperforms the prior best method (a
sliding-window convolutional network) on the ISBI challenge for segmentation of
neuronal structures in electron microscopic stacks. Using the same network
trained on transmitted light microscopy images (phase contrast and DIC) we won
the ISBI cell tracking challenge 2015 in these categories by a large margin.
Moreover, the network is fast. Segmentation of a 512x512 image takes less than
a second on a recent GPU. The full implementation (based on Caffe) and the
trained networks are available at
http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .
</summary>
    <author>
      <name>Olaf Ronneberger</name>
    </author>
    <author>
      <name>Philipp Fischer</name>
    </author>
    <author>
      <name>Thomas Brox</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">conditionally accepted at MICCAI 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.04597v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.04597v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.02078v2</id>
    <updated>2015-11-17T02:42:24Z</updated>
    <published>2015-06-05T22:33:04Z</published>
    <title>Visualizing and Understanding Recurrent Networks</title>
    <summary>  Recurrent Neural Networks (RNNs), and specifically a variant with Long
Short-Term Memory (LSTM), are enjoying renewed interest as a result of
successful applications in a wide range of machine learning problems that
involve sequential data. However, while LSTMs provide exceptional results in
practice, the source of their performance and their limitations remain rather
poorly understood. Using character-level language models as an interpretable
testbed, we aim to bridge this gap by providing an analysis of their
representations, predictions and error types. In particular, our experiments
reveal the existence of interpretable cells that keep track of long-range
dependencies such as line lengths, quotes and brackets. Moreover, our
comparative analysis with finite horizon n-gram models traces the source of the
LSTM improvements to long-range structural dependencies. Finally, we provide
analysis of the remaining errors and suggests areas for further study.
</summary>
    <author>
      <name>Andrej Karpathy</name>
    </author>
    <author>
      <name>Justin Johnson</name>
    </author>
    <author>
      <name>Li Fei-Fei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">changing style, adding references, minor changes to text</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.02078v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.02078v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.02142v6</id>
    <updated>2016-10-04T16:50:26Z</updated>
    <published>2015-06-06T12:30:43Z</published>
    <title>Dropout as a Bayesian Approximation: Representing Model Uncertainty in
  Deep Learning</title>
    <summary>  Deep learning tools have gained tremendous attention in applied machine
learning. However such tools for regression and classification do not capture
model uncertainty. In comparison, Bayesian models offer a mathematically
grounded framework to reason about model uncertainty, but usually come with a
prohibitive computational cost. In this paper we develop a new theoretical
framework casting dropout training in deep neural networks (NNs) as approximate
Bayesian inference in deep Gaussian processes. A direct result of this theory
gives us tools to model uncertainty with dropout NNs -- extracting information
from existing models that has been thrown away so far. This mitigates the
problem of representing uncertainty in deep learning without sacrificing either
computational complexity or test accuracy. We perform an extensive study of the
properties of dropout's uncertainty. Various network architectures and
non-linearities are assessed on tasks of regression and classification, using
MNIST as an example. We show a considerable improvement in predictive
log-likelihood and RMSE compared to existing state-of-the-art methods, and
finish by using dropout's uncertainty in deep reinforcement learning.
</summary>
    <author>
      <name>Yarin Gal</name>
    </author>
    <author>
      <name>Zoubin Ghahramani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 6 figures; fixed a mistake with standard error and added a
  new table with updated results (marked "Update [October 2016]"); Published in
  ICML 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.02142v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.02142v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.05869v3</id>
    <updated>2015-07-22T03:29:47Z</updated>
    <published>2015-06-19T02:52:23Z</published>
    <title>A Neural Conversational Model</title>
    <summary>  Conversational modeling is an important task in natural language
understanding and machine intelligence. Although previous approaches exist,
they are often restricted to specific domains (e.g., booking an airline ticket)
and require hand-crafted rules. In this paper, we present a simple approach for
this task which uses the recently proposed sequence to sequence framework. Our
model converses by predicting the next sentence given the previous sentence or
sentences in a conversation. The strength of our model is that it can be
trained end-to-end and thus requires much fewer hand-crafted rules. We find
that this straightforward model can generate simple conversations given a large
conversational training dataset. Our preliminary results suggest that, despite
optimizing the wrong objective function, the model is able to converse well. It
is able extract knowledge from both a domain specific dataset, and from a
large, noisy, and general domain dataset of movie subtitles. On a
domain-specific IT helpdesk dataset, the model can find a solution to a
technical problem via conversations. On a noisy open-domain movie transcript
dataset, the model can perform simple forms of common sense reasoning. As
expected, we also find that the lack of consistency is a common failure mode of
our model.
</summary>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Quoc Le</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML Deep Learning Workshop 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.05869v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.05869v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.06579v1</id>
    <updated>2015-06-22T12:57:15Z</updated>
    <published>2015-06-22T12:57:15Z</published>
    <title>Understanding Neural Networks Through Deep Visualization</title>
    <summary>  Recent years have produced great advances in training large, deep neural
networks (DNNs), including notable successes in training convolutional neural
networks (convnets) to recognize natural images. However, our understanding of
how these models work, especially what computations they perform at
intermediate layers, has lagged behind. Progress in the field will be further
accelerated by the development of better tools for visualizing and interpreting
neural nets. We introduce two such tools here. The first is a tool that
visualizes the activations produced on each layer of a trained convnet as it
processes an image or video (e.g. a live webcam stream). We have found that
looking at live activations that change in response to user input helps build
valuable intuitions about how convnets work. The second tool enables
visualizing features at each layer of a DNN via regularized optimization in
image space. Because previous versions of this idea produced less recognizable
images, here we introduce several new regularization methods that combine to
produce qualitatively clearer, more interpretable visualizations. Both tools
are open source and work on a pre-trained convnet with minimal setup.
</summary>
    <author>
      <name>Jason Yosinski</name>
    </author>
    <author>
      <name>Jeff Clune</name>
    </author>
    <author>
      <name>Anh Nguyen</name>
    </author>
    <author>
      <name>Thomas Fuchs</name>
    </author>
    <author>
      <name>Hod Lipson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages. To appear at ICML Deep Learning Workshop 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.06579v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.06579v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.07285v5</id>
    <updated>2016-03-05T20:18:55Z</updated>
    <published>2015-06-24T08:27:02Z</published>
    <title>Ask Me Anything: Dynamic Memory Networks for Natural Language Processing</title>
    <summary>  Most tasks in natural language processing can be cast into question answering
(QA) problems over language input. We introduce the dynamic memory network
(DMN), a neural network architecture which processes input sequences and
questions, forms episodic memories, and generates relevant answers. Questions
trigger an iterative attention process which allows the model to condition its
attention on the inputs and the result of previous iterations. These results
are then reasoned over in a hierarchical recurrent sequence model to generate
answers. The DMN can be trained end-to-end and obtains state-of-the-art results
on several types of tasks and datasets: question answering (Facebook's bAbI
dataset), text classification for sentiment analysis (Stanford Sentiment
Treebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The
training for these different tasks relies exclusively on trained word vector
representations and input-question-answer triplets.
</summary>
    <author>
      <name>Ankit Kumar</name>
    </author>
    <author>
      <name>Ozan Irsoy</name>
    </author>
    <author>
      <name>Peter Ondruska</name>
    </author>
    <author>
      <name>Mohit Iyyer</name>
    </author>
    <author>
      <name>James Bradbury</name>
    </author>
    <author>
      <name>Ishaan Gulrajani</name>
    </author>
    <author>
      <name>Victor Zhong</name>
    </author>
    <author>
      <name>Romain Paulus</name>
    </author>
    <author>
      <name>Richard Socher</name>
    </author>
    <link href="http://arxiv.org/abs/1506.07285v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.07285v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.04025v5</id>
    <updated>2015-09-20T08:25:52Z</updated>
    <published>2015-08-17T13:43:19Z</published>
    <title>Effective Approaches to Attention-based Neural Machine Translation</title>
    <summary>  An attentional mechanism has lately been used to improve neural machine
translation (NMT) by selectively focusing on parts of the source sentence
during translation. However, there has been little work exploring useful
architectures for attention-based NMT. This paper examines two simple and
effective classes of attentional mechanism: a global approach which always
attends to all source words and a local one that only looks at a subset of
source words at a time. We demonstrate the effectiveness of both approaches
over the WMT translation tasks between English and German in both directions.
With local attention, we achieve a significant gain of 5.0 BLEU points over
non-attentional systems which already incorporate known techniques such as
dropout. Our ensemble model using different attention architectures has
established a new state-of-the-art result in the WMT'15 English to German
translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over
the existing best system backed by NMT and an n-gram reranker.
</summary>
    <author>
      <name>Minh-Thang Luong</name>
    </author>
    <author>
      <name>Hieu Pham</name>
    </author>
    <author>
      <name>Christopher D. Manning</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 7 figures, EMNLP 2015 camera-ready version, more training
  details</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.04025v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.04025v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.04395v2</id>
    <updated>2016-03-14T23:07:20Z</updated>
    <published>2015-08-18T17:40:00Z</published>
    <title>End-to-End Attention-based Large Vocabulary Speech Recognition</title>
    <summary>  Many of the current state-of-the-art Large Vocabulary Continuous Speech
Recognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov
Models (HMMs). Most of these systems contain separate components that deal with
the acoustic modelling, language modelling and sequence decoding. We
investigate a more direct approach in which the HMM is replaced with a
Recurrent Neural Network (RNN) that performs sequence prediction directly at
the character level. Alignment between the input features and the desired
character sequence is learned automatically by an attention mechanism built
into the RNN. For each predicted character, the attention mechanism scans the
input sequence and chooses relevant frames. We propose two methods to speed up
this operation: limiting the scan to a subset of most promising frames and
pooling over time the information contained in neighboring frames, thereby
reducing source sequence length. Integrating an n-gram language model into the
decoding process yields recognition accuracies similar to other HMM-free
RNN-based approaches.
</summary>
    <author>
      <name>Dzmitry Bahdanau</name>
    </author>
    <author>
      <name>Jan Chorowski</name>
    </author>
    <author>
      <name>Dmitriy Serdyuk</name>
    </author>
    <author>
      <name>Philemon Brakel</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <link href="http://arxiv.org/abs/1508.04395v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.04395v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.06576v2</id>
    <updated>2015-09-02T08:24:59Z</updated>
    <published>2015-08-26T17:14:42Z</published>
    <title>A Neural Algorithm of Artistic Style</title>
    <summary>  In fine art, especially painting, humans have mastered the skill to create
unique visual experiences through composing a complex interplay between the
content and style of an image. Thus far the algorithmic basis of this process
is unknown and there exists no artificial system with similar capabilities.
However, in other key areas of visual perception such as object and face
recognition near-human performance was recently demonstrated by a class of
biologically inspired vision models called Deep Neural Networks. Here we
introduce an artificial system based on a Deep Neural Network that creates
artistic images of high perceptual quality. The system uses neural
representations to separate and recombine content and style of arbitrary
images, providing a neural algorithm for the creation of artistic images.
Moreover, in light of the striking similarities between performance-optimised
artificial neural networks and biological vision, our work offers a path
forward to an algorithmic understanding of how humans create and perceive
artistic imagery.
</summary>
    <author>
      <name>Leon A. Gatys</name>
    </author>
    <author>
      <name>Alexander S. Ecker</name>
    </author>
    <author>
      <name>Matthias Bethge</name>
    </author>
    <link href="http://arxiv.org/abs/1508.06576v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.06576v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.06615v4</id>
    <updated>2015-12-01T22:59:24Z</updated>
    <published>2015-08-26T19:25:34Z</published>
    <title>Character-Aware Neural Language Models</title>
    <summary>  We describe a simple neural language model that relies only on
character-level inputs. Predictions are still made at the word-level. Our model
employs a convolutional neural network (CNN) and a highway network over
characters, whose output is given to a long short-term memory (LSTM) recurrent
neural network language model (RNN-LM). On the English Penn Treebank the model
is on par with the existing state-of-the-art despite having 60% fewer
parameters. On languages with rich morphology (Arabic, Czech, French, German,
Spanish, Russian), the model outperforms word-level/morpheme-level LSTM
baselines, again with fewer parameters. The results suggest that on many
languages, character inputs are sufficient for language modeling. Analysis of
word representations obtained from the character composition part of the model
reveals that the model is able to encode, from characters only, both semantic
and orthographic information.
</summary>
    <author>
      <name>Yoon Kim</name>
    </author>
    <author>
      <name>Yacine Jernite</name>
    </author>
    <author>
      <name>David Sontag</name>
    </author>
    <author>
      <name>Alexander M. Rush</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AAAI 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.06615v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.06615v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.07909v5</id>
    <updated>2016-06-10T14:45:08Z</updated>
    <published>2015-08-31T16:37:31Z</published>
    <title>Neural Machine Translation of Rare Words with Subword Units</title>
    <summary>  Neural machine translation (NMT) models typically operate with a fixed
vocabulary, but translation is an open-vocabulary problem. Previous work
addresses the translation of out-of-vocabulary words by backing off to a
dictionary. In this paper, we introduce a simpler and more effective approach,
making the NMT model capable of open-vocabulary translation by encoding rare
and unknown words as sequences of subword units. This is based on the intuition
that various word classes are translatable via smaller units than words, for
instance names (via character copying or transliteration), compounds (via
compositional translation), and cognates and loanwords (via phonological and
morphological transformations). We discuss the suitability of different word
segmentation techniques, including simple character n-gram models and a
segmentation based on the byte pair encoding compression algorithm, and
empirically show that subword models improve over a back-off dictionary
baseline for the WMT 15 translation tasks English-German and English-Russian by
1.1 and 1.3 BLEU, respectively.
</summary>
    <author>
      <name>Rico Sennrich</name>
    </author>
    <author>
      <name>Barry Haddow</name>
    </author>
    <author>
      <name>Alexandra Birch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted at ACL 2016; new in this version: figure 3</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.07909v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.07909v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.02971v6</id>
    <updated>2019-07-05T10:47:27Z</updated>
    <published>2015-09-09T23:01:36Z</published>
    <title>Continuous control with deep reinforcement learning</title>
    <summary>  We adapt the ideas underlying the success of Deep Q-Learning to the
continuous action domain. We present an actor-critic, model-free algorithm
based on the deterministic policy gradient that can operate over continuous
action spaces. Using the same learning algorithm, network architecture and
hyper-parameters, our algorithm robustly solves more than 20 simulated physics
tasks, including classic problems such as cartpole swing-up, dexterous
manipulation, legged locomotion and car driving. Our algorithm is able to find
policies whose performance is competitive with those found by a planning
algorithm with full access to the dynamics of the domain and its derivatives.
We further demonstrate that for many of the tasks the algorithm can learn
policies end-to-end: directly from raw pixel inputs.
</summary>
    <author>
      <name>Timothy P. Lillicrap</name>
    </author>
    <author>
      <name>Jonathan J. Hunt</name>
    </author>
    <author>
      <name>Alexander Pritzel</name>
    </author>
    <author>
      <name>Nicolas Heess</name>
    </author>
    <author>
      <name>Tom Erez</name>
    </author>
    <author>
      <name>Yuval Tassa</name>
    </author>
    <author>
      <name>David Silver</name>
    </author>
    <author>
      <name>Daan Wierstra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages + supplementary</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.02971v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.02971v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.06461v3</id>
    <updated>2015-12-08T21:19:16Z</updated>
    <published>2015-09-22T04:40:22Z</published>
    <title>Deep Reinforcement Learning with Double Q-learning</title>
    <summary>  The popular Q-learning algorithm is known to overestimate action values under
certain conditions. It was not previously known whether, in practice, such
overestimations are common, whether they harm performance, and whether they can
generally be prevented. In this paper, we answer all these questions
affirmatively. In particular, we first show that the recent DQN algorithm,
which combines Q-learning with a deep neural network, suffers from substantial
overestimations in some games in the Atari 2600 domain. We then show that the
idea behind the Double Q-learning algorithm, which was introduced in a tabular
setting, can be generalized to work with large-scale function approximation. We
propose a specific adaptation to the DQN algorithm and show that the resulting
algorithm not only reduces the observed overestimations, as hypothesized, but
that this also leads to much better performance on several games.
</summary>
    <author>
      <name>Hado van Hasselt</name>
    </author>
    <author>
      <name>Arthur Guez</name>
    </author>
    <author>
      <name>David Silver</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AAAI 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.06461v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.06461v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.00149v5</id>
    <updated>2016-02-15T06:25:40Z</updated>
    <published>2015-10-01T09:03:44Z</published>
    <title>Deep Compression: Compressing Deep Neural Networks with Pruning, Trained
  Quantization and Huffman Coding</title>
    <summary>  Neural networks are both computationally intensive and memory intensive,
making them difficult to deploy on embedded systems with limited hardware
resources. To address this limitation, we introduce "deep compression", a three
stage pipeline: pruning, trained quantization and Huffman coding, that work
together to reduce the storage requirement of neural networks by 35x to 49x
without affecting their accuracy. Our method first prunes the network by
learning only the important connections. Next, we quantize the weights to
enforce weight sharing, finally, we apply Huffman coding. After the first two
steps we retrain the network to fine tune the remaining connections and the
quantized centroids. Pruning, reduces the number of connections by 9x to 13x;
Quantization then reduces the number of bits that represent each connection
from 32 to 5. On the ImageNet dataset, our method reduced the storage required
by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method
reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of
accuracy. This allows fitting the model into on-chip SRAM cache rather than
off-chip DRAM memory. Our compression method also facilitates the use of
complex neural networks in mobile applications where application size and
download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU,
compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy
efficiency.
</summary>
    <author>
      <name>Song Han</name>
    </author>
    <author>
      <name>Huizi Mao</name>
    </author>
    <author>
      <name>William J. Dally</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a conference paper at ICLR 2016 (oral)</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.00149v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.00149v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.06434v2</id>
    <updated>2016-01-07T23:09:39Z</updated>
    <published>2015-11-19T22:50:32Z</published>
    <title>Unsupervised Representation Learning with Deep Convolutional Generative
  Adversarial Networks</title>
    <summary>  In recent years, supervised learning with convolutional networks (CNNs) has
seen huge adoption in computer vision applications. Comparatively, unsupervised
learning with CNNs has received less attention. In this work we hope to help
bridge the gap between the success of CNNs for supervised learning and
unsupervised learning. We introduce a class of CNNs called deep convolutional
generative adversarial networks (DCGANs), that have certain architectural
constraints, and demonstrate that they are a strong candidate for unsupervised
learning. Training on various image datasets, we show convincing evidence that
our deep convolutional adversarial pair learns a hierarchy of representations
from object parts to scenes in both the generator and discriminator.
Additionally, we use the learned features for novel tasks - demonstrating their
applicability as general image representations.
</summary>
    <author>
      <name>Alec Radford</name>
    </author>
    <author>
      <name>Luke Metz</name>
    </author>
    <author>
      <name>Soumith Chintala</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under review as a conference paper at ICLR 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.06434v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.06434v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.07289v5</id>
    <updated>2016-02-22T07:02:58Z</updated>
    <published>2015-11-23T15:58:05Z</published>
    <title>Fast and Accurate Deep Network Learning by Exponential Linear Units
  (ELUs)</title>
    <summary>  We introduce the "exponential linear unit" (ELU) which speeds up learning in
deep neural networks and leads to higher classification accuracies. Like
rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs
(PReLUs), ELUs alleviate the vanishing gradient problem via the identity for
positive values. However, ELUs have improved learning characteristics compared
to the units with other activation functions. In contrast to ReLUs, ELUs have
negative values which allows them to push mean unit activations closer to zero
like batch normalization but with lower computational complexity. Mean shifts
toward zero speed up learning by bringing the normal gradient closer to the
unit natural gradient because of a reduced bias shift effect. While LReLUs and
PReLUs have negative values, too, they do not ensure a noise-robust
deactivation state. ELUs saturate to a negative value with smaller inputs and
thereby decrease the forward propagated variation and information. Therefore,
ELUs code the degree of presence of particular phenomena in the input, while
they do not quantitatively model the degree of their absence. In experiments,
ELUs lead not only to faster learning, but also to significantly better
generalization performance than ReLUs and LReLUs on networks with more than 5
layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with
batch normalization while batch normalization does not improve ELU networks.
ELU networks are among the top 10 reported CIFAR-10 results and yield the best
published result on CIFAR-100, without resorting to multi-view evaluation or
model averaging. On ImageNet, ELU networks considerably speed up learning
compared to a ReLU network with the same architecture, obtaining less than 10%
classification error for a single crop, single model network.
</summary>
    <author>
      <name>Djork-Arné Clevert</name>
    </author>
    <author>
      <name>Thomas Unterthiner</name>
    </author>
    <author>
      <name>Sepp Hochreiter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a conference paper at ICLR 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.07289v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.07289v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.02325v5</id>
    <updated>2016-12-29T19:05:11Z</updated>
    <published>2015-12-08T04:46:38Z</published>
    <title>SSD: Single Shot MultiBox Detector</title>
    <summary>  We present a method for detecting objects in images using a single deep
neural network. Our approach, named SSD, discretizes the output space of
bounding boxes into a set of default boxes over different aspect ratios and
scales per feature map location. At prediction time, the network generates
scores for the presence of each object category in each default box and
produces adjustments to the box to better match the object shape. Additionally,
the network combines predictions from multiple feature maps with different
resolutions to naturally handle objects of various sizes. Our SSD model is
simple relative to methods that require object proposals because it completely
eliminates proposal generation and subsequent pixel or feature resampling stage
and encapsulates all computation in a single network. This makes SSD easy to
train and straightforward to integrate into systems that require a detection
component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets
confirm that SSD has comparable accuracy to methods that utilize an additional
object proposal step and is much faster, while providing a unified framework
for both training and inference. Compared to other single stage methods, SSD
has much better accuracy, even with a smaller input image size. For $300\times
300$ input, SSD achieves 72.1% mAP on VOC2007 test at 58 FPS on a Nvidia Titan
X and for $500\times 500$ input, SSD achieves 75.1% mAP, outperforming a
comparable state of the art Faster R-CNN model. Code is available at
https://github.com/weiliu89/caffe/tree/ssd .
</summary>
    <author>
      <name>Wei Liu</name>
    </author>
    <author>
      <name>Dragomir Anguelov</name>
    </author>
    <author>
      <name>Dumitru Erhan</name>
    </author>
    <author>
      <name>Christian Szegedy</name>
    </author>
    <author>
      <name>Scott Reed</name>
    </author>
    <author>
      <name>Cheng-Yang Fu</name>
    </author>
    <author>
      <name>Alexander C. Berg</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-46448-0_2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-46448-0_2" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ECCV 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.02325v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.02325v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.02595v1</id>
    <updated>2015-12-08T19:13:50Z</updated>
    <published>2015-12-08T19:13:50Z</published>
    <title>Deep Speech 2: End-to-End Speech Recognition in English and Mandarin</title>
    <summary>  We show that an end-to-end deep learning approach can be used to recognize
either English or Mandarin Chinese speech--two vastly different languages.
Because it replaces entire pipelines of hand-engineered components with neural
networks, end-to-end learning allows us to handle a diverse variety of speech
including noisy environments, accents and different languages. Key to our
approach is our application of HPC techniques, resulting in a 7x speedup over
our previous system. Because of this efficiency, experiments that previously
took weeks now run in days. This enables us to iterate more quickly to identify
superior architectures and algorithms. As a result, in several cases, our
system is competitive with the transcription of human workers when benchmarked
on standard datasets. Finally, using a technique called Batch Dispatch with
GPUs in the data center, we show that our system can be inexpensively deployed
in an online setting, delivering low latency when serving users at scale.
</summary>
    <author>
      <name>Dario Amodei</name>
    </author>
    <author>
      <name>Rishita Anubhai</name>
    </author>
    <author>
      <name>Eric Battenberg</name>
    </author>
    <author>
      <name>Carl Case</name>
    </author>
    <author>
      <name>Jared Casper</name>
    </author>
    <author>
      <name>Bryan Catanzaro</name>
    </author>
    <author>
      <name>Jingdong Chen</name>
    </author>
    <author>
      <name>Mike Chrzanowski</name>
    </author>
    <author>
      <name>Adam Coates</name>
    </author>
    <author>
      <name>Greg Diamos</name>
    </author>
    <author>
      <name>Erich Elsen</name>
    </author>
    <author>
      <name>Jesse Engel</name>
    </author>
    <author>
      <name>Linxi Fan</name>
    </author>
    <author>
      <name>Christopher Fougner</name>
    </author>
    <author>
      <name>Tony Han</name>
    </author>
    <author>
      <name>Awni Hannun</name>
    </author>
    <author>
      <name>Billy Jun</name>
    </author>
    <author>
      <name>Patrick LeGresley</name>
    </author>
    <author>
      <name>Libby Lin</name>
    </author>
    <author>
      <name>Sharan Narang</name>
    </author>
    <author>
      <name>Andrew Ng</name>
    </author>
    <author>
      <name>Sherjil Ozair</name>
    </author>
    <author>
      <name>Ryan Prenger</name>
    </author>
    <author>
      <name>Jonathan Raiman</name>
    </author>
    <author>
      <name>Sanjeev Satheesh</name>
    </author>
    <author>
      <name>David Seetapun</name>
    </author>
    <author>
      <name>Shubho Sengupta</name>
    </author>
    <author>
      <name>Yi Wang</name>
    </author>
    <author>
      <name>Zhiqian Wang</name>
    </author>
    <author>
      <name>Chong Wang</name>
    </author>
    <author>
      <name>Bo Xiao</name>
    </author>
    <author>
      <name>Dani Yogatama</name>
    </author>
    <author>
      <name>Jun Zhan</name>
    </author>
    <author>
      <name>Zhenyao Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/1512.02595v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.02595v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.03385v1</id>
    <updated>2015-12-10T19:51:55Z</updated>
    <published>2015-12-10T19:51:55Z</published>
    <title>Deep Residual Learning for Image Recognition</title>
    <summary>  Deeper neural networks are more difficult to train. We present a residual
learning framework to ease the training of networks that are substantially
deeper than those used previously. We explicitly reformulate the layers as
learning residual functions with reference to the layer inputs, instead of
learning unreferenced functions. We provide comprehensive empirical evidence
showing that these residual networks are easier to optimize, and can gain
accuracy from considerably increased depth. On the ImageNet dataset we evaluate
residual nets with a depth of up to 152 layers---8x deeper than VGG nets but
still having lower complexity. An ensemble of these residual nets achieves
3.57% error on the ImageNet test set. This result won the 1st place on the
ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100
and 1000 layers.
  The depth of representations is of central importance for many visual
recognition tasks. Solely due to our extremely deep representations, we obtain
a 28% relative improvement on the COCO object detection dataset. Deep residual
nets are foundations of our submissions to ILSVRC &amp; COCO 2015 competitions,
where we also won the 1st places on the tasks of ImageNet detection, ImageNet
localization, COCO detection, and COCO segmentation.
</summary>
    <author>
      <name>Kaiming He</name>
    </author>
    <author>
      <name>Xiangyu Zhang</name>
    </author>
    <author>
      <name>Shaoqing Ren</name>
    </author>
    <author>
      <name>Jian Sun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Tech report</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.03385v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.03385v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.03385v1</id>
    <updated>2015-12-10T19:51:55Z</updated>
    <published>2015-12-10T19:51:55Z</published>
    <title>Deep Residual Learning for Image Recognition</title>
    <summary>  Deeper neural networks are more difficult to train. We present a residual
learning framework to ease the training of networks that are substantially
deeper than those used previously. We explicitly reformulate the layers as
learning residual functions with reference to the layer inputs, instead of
learning unreferenced functions. We provide comprehensive empirical evidence
showing that these residual networks are easier to optimize, and can gain
accuracy from considerably increased depth. On the ImageNet dataset we evaluate
residual nets with a depth of up to 152 layers---8x deeper than VGG nets but
still having lower complexity. An ensemble of these residual nets achieves
3.57% error on the ImageNet test set. This result won the 1st place on the
ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100
and 1000 layers.
  The depth of representations is of central importance for many visual
recognition tasks. Solely due to our extremely deep representations, we obtain
a 28% relative improvement on the COCO object detection dataset. Deep residual
nets are foundations of our submissions to ILSVRC &amp; COCO 2015 competitions,
where we also won the 1st places on the tasks of ImageNet detection, ImageNet
localization, COCO detection, and COCO segmentation.
</summary>
    <author>
      <name>Kaiming He</name>
    </author>
    <author>
      <name>Xiangyu Zhang</name>
    </author>
    <author>
      <name>Shaoqing Ren</name>
    </author>
    <author>
      <name>Jian Sun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Tech report</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.03385v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.03385v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.03385v1</id>
    <updated>2015-12-10T19:51:55Z</updated>
    <published>2015-12-10T19:51:55Z</published>
    <title>Deep Residual Learning for Image Recognition</title>
    <summary>  Deeper neural networks are more difficult to train. We present a residual
learning framework to ease the training of networks that are substantially
deeper than those used previously. We explicitly reformulate the layers as
learning residual functions with reference to the layer inputs, instead of
learning unreferenced functions. We provide comprehensive empirical evidence
showing that these residual networks are easier to optimize, and can gain
accuracy from considerably increased depth. On the ImageNet dataset we evaluate
residual nets with a depth of up to 152 layers---8x deeper than VGG nets but
still having lower complexity. An ensemble of these residual nets achieves
3.57% error on the ImageNet test set. This result won the 1st place on the
ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100
and 1000 layers.
  The depth of representations is of central importance for many visual
recognition tasks. Solely due to our extremely deep representations, we obtain
a 28% relative improvement on the COCO object detection dataset. Deep residual
nets are foundations of our submissions to ILSVRC &amp; COCO 2015 competitions,
where we also won the 1st places on the tasks of ImageNet detection, ImageNet
localization, COCO detection, and COCO segmentation.
</summary>
    <author>
      <name>Kaiming He</name>
    </author>
    <author>
      <name>Xiangyu Zhang</name>
    </author>
    <author>
      <name>Shaoqing Ren</name>
    </author>
    <author>
      <name>Jian Sun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Tech report</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.03385v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.03385v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.01705v4</id>
    <updated>2016-06-07T23:25:51Z</updated>
    <published>2016-01-07T21:21:59Z</published>
    <title>Learning to Compose Neural Networks for Question Answering</title>
    <summary>  We describe a question answering model that applies to both images and
structured knowledge bases. The model uses natural language strings to
automatically assemble neural networks from a collection of composable modules.
Parameters for these modules are learned jointly with network-assembly
parameters via reinforcement learning, with only (world, question, answer)
triples as supervision. Our approach, which we term a dynamic neural model
network, achieves state-of-the-art results on benchmark datasets in both visual
and structured domains.
</summary>
    <author>
      <name>Jacob Andreas</name>
    </author>
    <author>
      <name>Marcus Rohrbach</name>
    </author>
    <author>
      <name>Trevor Darrell</name>
    </author>
    <author>
      <name>Dan Klein</name>
    </author>
    <link href="http://arxiv.org/abs/1601.01705v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.01705v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.06759v2</id>
    <updated>2016-02-29T15:32:16Z</updated>
    <published>2016-01-25T20:34:24Z</published>
    <title>Pixel Recurrent Neural Networks</title>
    <summary>  Modeling the distribution of natural images is a landmark problem in
unsupervised learning. This task requires an image model that is at once
expressive, tractable and scalable. We present a deep neural network that
sequentially predicts the pixels in an image along the two spatial dimensions.
Our method models the discrete probability of the raw pixel values and encodes
the complete set of dependencies in the image. Architectural novelties include
fast two-dimensional recurrent layers and an effective use of residual
connections in deep recurrent networks. We achieve log-likelihood scores on
natural images that are considerably better than the previous state of the art.
Our main results also provide benchmarks on the diverse ImageNet dataset.
Samples generated from the model appear crisp, varied and globally coherent.
</summary>
    <author>
      <name>Aaron van den Oord</name>
    </author>
    <author>
      <name>Nal Kalchbrenner</name>
    </author>
    <author>
      <name>Koray Kavukcuoglu</name>
    </author>
    <link href="http://arxiv.org/abs/1601.06759v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.06759v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.01528v2</id>
    <updated>2016-05-03T04:27:02Z</updated>
    <published>2016-02-04T01:28:28Z</published>
    <title>EIE: Efficient Inference Engine on Compressed Deep Neural Network</title>
    <summary>  State-of-the-art deep neural networks (DNNs) have hundreds of millions of
connections and are both computationally and memory intensive, making them
difficult to deploy on embedded systems with limited hardware resources and
power budgets. While custom hardware helps the computation, fetching weights
from DRAM is two orders of magnitude more expensive than ALU operations, and
dominates the required power.
  Previously proposed 'Deep Compression' makes it possible to fit large DNNs
(AlexNet and VGGNet) fully in on-chip SRAM. This compression is achieved by
pruning the redundant connections and having multiple connections share the
same weight. We propose an energy efficient inference engine (EIE) that
performs inference on this compressed network model and accelerates the
resulting sparse matrix-vector multiplication with weight sharing. Going from
DRAM to SRAM gives EIE 120x energy saving; Exploiting sparsity saves 10x;
Weight sharing gives 8x; Skipping zero activations from ReLU saves another 3x.
Evaluated on nine DNN benchmarks, EIE is 189x and 13x faster when compared to
CPU and GPU implementations of the same DNN without compression. EIE has a
processing power of 102GOPS/s working directly on a compressed network,
corresponding to 3TOPS/s on an uncompressed network, and processes FC layers of
AlexNet at 1.88x10^4 frames/sec with a power dissipation of only 600mW. It is
24,000x and 3,400x more energy efficient than a CPU and GPU respectively.
Compared with DaDianNao, EIE has 2.9x, 19x and 3x better throughput, energy
efficiency and area efficiency.
</summary>
    <author>
      <name>Song Han</name>
    </author>
    <author>
      <name>Xingyu Liu</name>
    </author>
    <author>
      <name>Huizi Mao</name>
    </author>
    <author>
      <name>Jing Pu</name>
    </author>
    <author>
      <name>Ardavan Pedram</name>
    </author>
    <author>
      <name>Mark A. Horowitz</name>
    </author>
    <author>
      <name>William J. Dally</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">External Links: TheNextPlatform: http://goo.gl/f7qX0L ; O'Reilly:
  https://goo.gl/Id1HNT ; Hacker News: https://goo.gl/KM72SV ; Embedded-vision:
  http://goo.gl/joQNg8 ; Talk at NVIDIA GTC'16: http://goo.gl/6wJYvn ; Talk at
  Embedded Vision Summit: https://goo.gl/7abFNe ; Talk at Stanford University:
  https://goo.gl/6lwuer. Published as a conference paper in ISCA 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.01528v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.01528v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.02410v2</id>
    <updated>2016-02-11T23:01:48Z</updated>
    <published>2016-02-07T19:11:17Z</published>
    <title>Exploring the Limits of Language Modeling</title>
    <summary>  In this work we explore recent advances in Recurrent Neural Networks for
large scale Language Modeling, a task central to language understanding. We
extend current models to deal with two key challenges present in this task:
corpora and vocabulary sizes, and complex, long term structure of language. We
perform an exhaustive study on techniques such as character Convolutional
Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark.
Our best single model significantly improves state-of-the-art perplexity from
51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20),
while an ensemble of models sets a new record by improving perplexity from 41.0
down to 23.7. We also release these models for the NLP and ML community to
study and improve upon.
</summary>
    <author>
      <name>Rafal Jozefowicz</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Mike Schuster</name>
    </author>
    <author>
      <name>Noam Shazeer</name>
    </author>
    <author>
      <name>Yonghui Wu</name>
    </author>
    <link href="http://arxiv.org/abs/1602.02410v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.02410v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.02830v3</id>
    <updated>2016-03-17T14:54:25Z</updated>
    <published>2016-02-09T01:01:59Z</published>
    <title>Binarized Neural Networks: Training Deep Neural Networks with Weights
  and Activations Constrained to +1 or -1</title>
    <summary>  We introduce a method to train Binarized Neural Networks (BNNs) - neural
networks with binary weights and activations at run-time. At training-time the
binary weights and activations are used for computing the parameters gradients.
During the forward pass, BNNs drastically reduce memory size and accesses, and
replace most arithmetic operations with bit-wise operations, which is expected
to substantially improve power-efficiency. To validate the effectiveness of
BNNs we conduct two sets of experiments on the Torch7 and Theano frameworks. On
both, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10
and SVHN datasets. Last but not least, we wrote a binary matrix multiplication
GPU kernel with which it is possible to run our MNIST BNN 7 times faster than
with an unoptimized GPU kernel, without suffering any loss in classification
accuracy. The code for training and running our BNNs is available on-line.
</summary>
    <author>
      <name>Matthieu Courbariaux</name>
    </author>
    <author>
      <name>Itay Hubara</name>
    </author>
    <author>
      <name>Daniel Soudry</name>
    </author>
    <author>
      <name>Ran El-Yaniv</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages and 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.02830v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.02830v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.07261v2</id>
    <updated>2016-08-23T16:42:29Z</updated>
    <published>2016-02-23T18:44:39Z</published>
    <title>Inception-v4, Inception-ResNet and the Impact of Residual Connections on
  Learning</title>
    <summary>  Very deep convolutional networks have been central to the largest advances in
image recognition performance in recent years. One example is the Inception
architecture that has been shown to achieve very good performance at relatively
low computational cost. Recently, the introduction of residual connections in
conjunction with a more traditional architecture has yielded state-of-the-art
performance in the 2015 ILSVRC challenge; its performance was similar to the
latest generation Inception-v3 network. This raises the question of whether
there are any benefit in combining the Inception architecture with residual
connections. Here we give clear empirical evidence that training with residual
connections accelerates the training of Inception networks significantly. There
is also some evidence of residual Inception networks outperforming similarly
expensive Inception networks without residual connections by a thin margin. We
also present several new streamlined architectures for both residual and
non-residual Inception networks. These variations improve the single-frame
recognition performance on the ILSVRC 2012 classification task significantly.
We further demonstrate how proper activation scaling stabilizes the training of
very wide residual Inception networks. With an ensemble of three residual and
one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the
ImageNet classification (CLS) challenge
</summary>
    <author>
      <name>Christian Szegedy</name>
    </author>
    <author>
      <name>Sergey Ioffe</name>
    </author>
    <author>
      <name>Vincent Vanhoucke</name>
    </author>
    <author>
      <name>Alex Alemi</name>
    </author>
    <link href="http://arxiv.org/abs/1602.07261v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.07261v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.07360v4</id>
    <updated>2016-11-04T21:26:08Z</updated>
    <published>2016-02-24T00:09:45Z</published>
    <title>SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB
  model size</title>
    <summary>  Recent research on deep neural networks has focused primarily on improving
accuracy. For a given accuracy level, it is typically possible to identify
multiple DNN architectures that achieve that accuracy level. With equivalent
accuracy, smaller DNN architectures offer at least three advantages: (1)
Smaller DNNs require less communication across servers during distributed
training. (2) Smaller DNNs require less bandwidth to export a new model from
the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on
FPGAs and other hardware with limited memory. To provide all of these
advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet
achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters.
Additionally, with model compression techniques we are able to compress
SqueezeNet to less than 0.5MB (510x smaller than AlexNet).
  The SqueezeNet architecture is available for download here:
https://github.com/DeepScale/SqueezeNet
</summary>
    <author>
      <name>Forrest N. Iandola</name>
    </author>
    <author>
      <name>Song Han</name>
    </author>
    <author>
      <name>Matthew W. Moskewicz</name>
    </author>
    <author>
      <name>Khalid Ashraf</name>
    </author>
    <author>
      <name>William J. Dally</name>
    </author>
    <author>
      <name>Kurt Keutzer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In ICLR Format</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.07360v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.07360v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.02199v4</id>
    <updated>2016-08-28T23:32:37Z</updated>
    <published>2016-03-07T18:53:00Z</published>
    <title>Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning
  and Large-Scale Data Collection</title>
    <summary>  We describe a learning-based approach to hand-eye coordination for robotic
grasping from monocular images. To learn hand-eye coordination for grasping, we
trained a large convolutional neural network to predict the probability that
task-space motion of the gripper will result in successful grasps, using only
monocular camera images and independently of camera calibration or the current
robot pose. This requires the network to observe the spatial relationship
between the gripper and objects in the scene, thus learning hand-eye
coordination. We then use this network to servo the gripper in real time to
achieve successful grasps. To train our network, we collected over 800,000
grasp attempts over the course of two months, using between 6 and 14 robotic
manipulators at any given time, with differences in camera placement and
hardware. Our experimental evaluation demonstrates that our method achieves
effective real-time control, can successfully grasp novel objects, and corrects
mistakes by continuous servoing.
</summary>
    <author>
      <name>Sergey Levine</name>
    </author>
    <author>
      <name>Peter Pastor</name>
    </author>
    <author>
      <name>Alex Krizhevsky</name>
    </author>
    <author>
      <name>Deirdre Quillen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is an extended version of "Learning Hand-Eye Coordination for
  Robotic Grasping with Large-Scale Data Collection," ISER 2016. Draft modified
  to correct typo in Algorithm 1 and add a link to the publicly available
  dataset</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.02199v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.02199v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.02754v3</id>
    <updated>2016-06-10T23:23:51Z</updated>
    <published>2016-03-09T01:11:51Z</published>
    <title>XGBoost: A Scalable Tree Boosting System</title>
    <summary>  Tree boosting is a highly effective and widely used machine learning method.
In this paper, we describe a scalable end-to-end tree boosting system called
XGBoost, which is used widely by data scientists to achieve state-of-the-art
results on many machine learning challenges. We propose a novel sparsity-aware
algorithm for sparse data and weighted quantile sketch for approximate tree
learning. More importantly, we provide insights on cache access patterns, data
compression and sharding to build a scalable tree boosting system. By combining
these insights, XGBoost scales beyond billions of examples using far fewer
resources than existing systems.
</summary>
    <author>
      <name>Tianqi Chen</name>
    </author>
    <author>
      <name>Carlos Guestrin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2939672.2939785</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2939672.2939785" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">KDD'16 changed all figures to type1</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.02754v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.02754v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.04467v2</id>
    <updated>2016-03-16T16:57:12Z</updated>
    <published>2016-03-14T20:50:20Z</published>
    <title>TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed
  Systems</title>
    <summary>  TensorFlow is an interface for expressing machine learning algorithms, and an
implementation for executing such algorithms. A computation expressed using
TensorFlow can be executed with little or no change on a wide variety of
heterogeneous systems, ranging from mobile devices such as phones and tablets
up to large-scale distributed systems of hundreds of machines and thousands of
computational devices such as GPU cards. The system is flexible and can be used
to express a wide variety of algorithms, including training and inference
algorithms for deep neural network models, and it has been used for conducting
research and for deploying machine learning systems into production across more
than a dozen areas of computer science and other fields, including speech
recognition, computer vision, robotics, information retrieval, natural language
processing, geographic information extraction, and computational drug
discovery. This paper describes the TensorFlow interface and an implementation
of that interface that we have built at Google. The TensorFlow API and a
reference implementation were released as an open-source package under the
Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.
</summary>
    <author>
      <name>Martín Abadi</name>
    </author>
    <author>
      <name>Ashish Agarwal</name>
    </author>
    <author>
      <name>Paul Barham</name>
    </author>
    <author>
      <name>Eugene Brevdo</name>
    </author>
    <author>
      <name>Zhifeng Chen</name>
    </author>
    <author>
      <name>Craig Citro</name>
    </author>
    <author>
      <name>Greg S. Corrado</name>
    </author>
    <author>
      <name>Andy Davis</name>
    </author>
    <author>
      <name>Jeffrey Dean</name>
    </author>
    <author>
      <name>Matthieu Devin</name>
    </author>
    <author>
      <name>Sanjay Ghemawat</name>
    </author>
    <author>
      <name>Ian Goodfellow</name>
    </author>
    <author>
      <name>Andrew Harp</name>
    </author>
    <author>
      <name>Geoffrey Irving</name>
    </author>
    <author>
      <name>Michael Isard</name>
    </author>
    <author>
      <name>Yangqing Jia</name>
    </author>
    <author>
      <name>Rafal Jozefowicz</name>
    </author>
    <author>
      <name>Lukasz Kaiser</name>
    </author>
    <author>
      <name>Manjunath Kudlur</name>
    </author>
    <author>
      <name>Josh Levenberg</name>
    </author>
    <author>
      <name>Dan Mane</name>
    </author>
    <author>
      <name>Rajat Monga</name>
    </author>
    <author>
      <name>Sherry Moore</name>
    </author>
    <author>
      <name>Derek Murray</name>
    </author>
    <author>
      <name>Chris Olah</name>
    </author>
    <author>
      <name>Mike Schuster</name>
    </author>
    <author>
      <name>Jonathon Shlens</name>
    </author>
    <author>
      <name>Benoit Steiner</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Kunal Talwar</name>
    </author>
    <author>
      <name>Paul Tucker</name>
    </author>
    <author>
      <name>Vincent Vanhoucke</name>
    </author>
    <author>
      <name>Vijay Vasudevan</name>
    </author>
    <author>
      <name>Fernanda Viegas</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Pete Warden</name>
    </author>
    <author>
      <name>Martin Wattenberg</name>
    </author>
    <author>
      <name>Martin Wicke</name>
    </author>
    <author>
      <name>Yuan Yu</name>
    </author>
    <author>
      <name>Xiaoqiang Zheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Version 2 updates only the metadata, to correct the formatting of
  Mart\'in Abadi's name</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.04467v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.04467v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.05027v2</id>
    <updated>2016-04-12T09:40:08Z</updated>
    <published>2016-03-16T10:53:56Z</published>
    <title>Identity Mappings in Deep Residual Networks</title>
    <summary>  Deep residual networks have emerged as a family of extremely deep
architectures showing compelling accuracy and nice convergence behaviors. In
this paper, we analyze the propagation formulations behind the residual
building blocks, which suggest that the forward and backward signals can be
directly propagated from one block to any other block, when using identity
mappings as the skip connections and after-addition activation. A series of
ablation experiments support the importance of these identity mappings. This
motivates us to propose a new residual unit, which further makes training easy
and improves generalization. We report improved results using a 1001-layer
ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on
ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers.
</summary>
    <author>
      <name>Kaiming He</name>
    </author>
    <author>
      <name>Xiangyu Zhang</name>
    </author>
    <author>
      <name>Shaoqing Ren</name>
    </author>
    <author>
      <name>Jian Sun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Tech report</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.05027v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.05027v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06147v4</id>
    <updated>2016-06-21T01:12:22Z</updated>
    <published>2016-03-19T21:35:04Z</published>
    <title>A Character-Level Decoder without Explicit Segmentation for Neural
  Machine Translation</title>
    <summary>  The existing machine translation systems, whether phrase-based or neural,
have relied almost exclusively on word-level modelling with explicit
segmentation. In this paper, we ask a fundamental question: can neural machine
translation generate a character sequence without any explicit segmentation? To
answer this question, we evaluate an attention-based encoder-decoder with a
subword-level encoder and a character-level decoder on four language
pairs--En-Cs, En-De, En-Ru and En-Fi-- using the parallel corpora from WMT'15.
Our experiments show that the models with a character-level decoder outperform
the ones with a subword-level decoder on all of the four language pairs.
Furthermore, the ensembles of neural models with a character-level decoder
outperform the state-of-the-art non-neural machine translation systems on
En-Cs, En-De and En-Fi and perform comparably on En-Ru.
</summary>
    <author>
      <name>Junyoung Chung</name>
    </author>
    <author>
      <name>Kyunghyun Cho</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <link href="http://arxiv.org/abs/1603.06147v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06147v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06560v4</id>
    <updated>2018-06-18T23:01:43Z</updated>
    <published>2016-03-21T19:51:04Z</published>
    <title>Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization</title>
    <summary>  Performance of machine learning algorithms depends critically on identifying
a good set of hyperparameters. While recent approaches use Bayesian
optimization to adaptively select configurations, we focus on speeding up
random search through adaptive resource allocation and early-stopping. We
formulate hyperparameter optimization as a pure-exploration non-stochastic
infinite-armed bandit problem where a predefined resource like iterations, data
samples, or features is allocated to randomly sampled configurations. We
introduce a novel algorithm, Hyperband, for this framework and analyze its
theoretical properties, providing several desirable guarantees. Furthermore, we
compare Hyperband with popular Bayesian optimization methods on a suite of
hyperparameter optimization problems. We observe that Hyperband can provide
over an order-of-magnitude speedup over our competitor set on a variety of
deep-learning and kernel-based learning problems.
</summary>
    <author>
      <name>Lisha Li</name>
    </author>
    <author>
      <name>Kevin Jamieson</name>
    </author>
    <author>
      <name>Giulia DeSalvo</name>
    </author>
    <author>
      <name>Afshin Rostamizadeh</name>
    </author>
    <author>
      <name>Ameet Talwalkar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Changes: - Updated to JMLR version</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Machine Learning Research 18 (2018) 1-52</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1603.06560v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06560v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.08155v1</id>
    <updated>2016-03-27T01:04:27Z</updated>
    <published>2016-03-27T01:04:27Z</published>
    <title>Perceptual Losses for Real-Time Style Transfer and Super-Resolution</title>
    <summary>  We consider image transformation problems, where an input image is
transformed into an output image. Recent methods for such problems typically
train feed-forward convolutional neural networks using a \emph{per-pixel} loss
between the output and ground-truth images. Parallel work has shown that
high-quality images can be generated by defining and optimizing
\emph{perceptual} loss functions based on high-level features extracted from
pretrained networks. We combine the benefits of both approaches, and propose
the use of perceptual loss functions for training feed-forward networks for
image transformation tasks. We show results on image style transfer, where a
feed-forward network is trained to solve the optimization problem proposed by
Gatys et al in real-time. Compared to the optimization-based method, our
network gives similar qualitative results but is three orders of magnitude
faster. We also experiment with single-image super-resolution, where replacing
a per-pixel loss with a perceptual loss gives visually pleasing results.
</summary>
    <author>
      <name>Justin Johnson</name>
    </author>
    <author>
      <name>Alexandre Alahi</name>
    </author>
    <author>
      <name>Li Fei-Fei</name>
    </author>
    <link href="http://arxiv.org/abs/1603.08155v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.08155v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.08511v5</id>
    <updated>2016-10-05T18:01:05Z</updated>
    <published>2016-03-28T19:58:19Z</published>
    <title>Colorful Image Colorization</title>
    <summary>  Given a grayscale photograph as input, this paper attacks the problem of
hallucinating a plausible color version of the photograph. This problem is
clearly underconstrained, so previous approaches have either relied on
significant user interaction or resulted in desaturated colorizations. We
propose a fully automatic approach that produces vibrant and realistic
colorizations. We embrace the underlying uncertainty of the problem by posing
it as a classification task and use class-rebalancing at training time to
increase the diversity of colors in the result. The system is implemented as a
feed-forward pass in a CNN at test time and is trained on over a million color
images. We evaluate our algorithm using a "colorization Turing test," asking
human participants to choose between a generated and ground truth color image.
Our method successfully fools humans on 32% of the trials, significantly higher
than previous methods. Moreover, we show that colorization can be a powerful
pretext task for self-supervised feature learning, acting as a cross-channel
encoder. This approach results in state-of-the-art performance on several
feature learning benchmarks.
</summary>
    <author>
      <name>Richard Zhang</name>
    </author>
    <author>
      <name>Phillip Isola</name>
    </author>
    <author>
      <name>Alexei A. Efros</name>
    </author>
    <link href="http://arxiv.org/abs/1603.08511v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.08511v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.08983v6</id>
    <updated>2017-02-21T16:21:21Z</updated>
    <published>2016-03-29T22:09:00Z</published>
    <title>Adaptive Computation Time for Recurrent Neural Networks</title>
    <summary>  This paper introduces Adaptive Computation Time (ACT), an algorithm that
allows recurrent neural networks to learn how many computational steps to take
between receiving an input and emitting an output. ACT requires minimal changes
to the network architecture, is deterministic and differentiable, and does not
add any noise to the parameter gradients. Experimental results are provided for
four synthetic problems: determining the parity of binary vectors, applying
binary logic operations, adding integers, and sorting real numbers. Overall,
performance is dramatically improved by the use of ACT, which successfully
adapts the number of computational steps to the requirements of the problem. We
also present character-level language modelling results on the Hutter prize
Wikipedia dataset. In this case ACT does not yield large gains in performance;
however it does provide intriguing insight into the structure of the data, with
more computation allocated to harder-to-predict transitions, such as spaces
between words and ends of sentences. This suggests that ACT or other adaptive
computation methods could provide a generic method for inferring segment
boundaries in sequence data.
</summary>
    <author>
      <name>Alex Graves</name>
    </author>
    <link href="http://arxiv.org/abs/1603.08983v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.08983v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.09382v3</id>
    <updated>2016-07-28T23:24:16Z</updated>
    <published>2016-03-30T20:58:07Z</published>
    <title>Deep Networks with Stochastic Depth</title>
    <summary>  Very deep convolutional networks with hundreds of layers have led to
significant reductions in error on competitive benchmarks. Although the
unmatched expressiveness of the many layers can be highly desirable at test
time, training very deep networks comes with its own set of challenges. The
gradients can vanish, the forward flow often diminishes, and the training time
can be painfully slow. To address these problems, we propose stochastic depth,
a training procedure that enables the seemingly contradictory setup to train
short networks and use deep networks at test time. We start with very deep
networks but during training, for each mini-batch, randomly drop a subset of
layers and bypass them with the identity function. This simple approach
complements the recent success of residual networks. It reduces training time
substantially and improves the test error significantly on almost all data sets
that we used for evaluation. With stochastic depth we can increase the depth of
residual networks even beyond 1200 layers and still yield meaningful
improvements in test error (4.91% on CIFAR-10).
</summary>
    <author>
      <name>Gao Huang</name>
    </author>
    <author>
      <name>Yu Sun</name>
    </author>
    <author>
      <name>Zhuang Liu</name>
    </author>
    <author>
      <name>Daniel Sedra</name>
    </author>
    <author>
      <name>Kilian Weinberger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">first two authors contributed equally</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.09382v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.09382v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00788v2</id>
    <updated>2016-06-23T00:50:19Z</updated>
    <published>2016-04-04T09:30:54Z</published>
    <title>Achieving Open Vocabulary Neural Machine Translation with Hybrid
  Word-Character Models</title>
    <summary>  Nearly all previous work on neural machine translation (NMT) has used quite
restricted vocabularies, perhaps with a subsequent method to patch in unknown
words. This paper presents a novel word-character solution to achieving open
vocabulary NMT. We build hybrid systems that translate mostly at the word level
and consult the character components for rare words. Our character-level
recurrent neural networks compute source word representations and recover
unknown target words when needed. The twofold advantage of such a hybrid
approach is that it is much faster and easier to train than character-based
ones; at the same time, it never produces unknown words as in the case of
word-based models. On the WMT'15 English to Czech translation task, this hybrid
approach offers an addition boost of +2.1-11.4 BLEU points over models that
already handle unknown words. Our best system achieves a new state-of-the-art
result with 20.7 BLEU score. We demonstrate that our character models can
successfully learn to not only generate well-formed words for Czech, a
highly-inflected language with a very complex vocabulary, but also build
correct representations for English source words.
</summary>
    <author>
      <name>Minh-Thang Luong</name>
    </author>
    <author>
      <name>Christopher D. Manning</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11pages, 4 figures. ACL 2016 camera-ready version. SOTA WMT'15
  English-Czech 20.7 BLEU (+2.1-11.4 points)</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.00788v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00788v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.08610v2</id>
    <updated>2016-10-19T20:01:09Z</updated>
    <published>2016-04-28T20:23:15Z</published>
    <title>Artistic style transfer for videos</title>
    <summary>  In the past, manually re-drawing an image in a certain artistic style
required a professional artist and a long time. Doing this for a video sequence
single-handed was beyond imagination. Nowadays computers provide new
possibilities. We present an approach that transfers the style from one image
(for example, a painting) to a whole video sequence. We make use of recent
advances in style transfer in still images and propose new initializations and
loss functions applicable to videos. This allows us to generate consistent and
stable stylized video sequences, even in cases with large motion and strong
occlusion. We show that the proposed method clearly outperforms simpler
baselines both qualitatively and quantitatively.
</summary>
    <author>
      <name>Manuel Ruder</name>
    </author>
    <author>
      <name>Alexey Dosovitskiy</name>
    </author>
    <author>
      <name>Thomas Brox</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-45886-1_3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-45886-1_3" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">final version appeared in GCPR-2016; minor changes to improve the
  clarity</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">German Conference on Pattern Recognition (GCPR), LNCS 9796, pp.
  26-36 (2016)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1604.08610v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08610v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.09081v1</id>
    <updated>2016-05-30T00:50:39Z</updated>
    <published>2016-05-30T00:50:39Z</published>
    <title>Understanding Convolutional Neural Networks</title>
    <summary>  Convoulutional Neural Networks (CNNs) exhibit extraordinary performance on a
variety of machine learning tasks. However, their mathematical properties and
behavior are quite poorly understood. There is some work, in the form of a
framework, for analyzing the operations that they perform. The goal of this
project is to present key results from this theory, and provide intuition for
why CNNs work.
</summary>
    <author>
      <name>Jayanth Koushik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Statistical Machine Learning Course Project at Carnegie Mellon
  University</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.09081v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.09081v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00704v1</id>
    <updated>2016-06-02T14:43:37Z</updated>
    <published>2016-06-02T14:43:37Z</published>
    <title>Adversarially Learned Inference</title>
    <summary>  We introduce the adversarially learned inference (ALI) model, which jointly
learns a generation network and an inference network using an adversarial
process. The generation network maps samples from stochastic latent variables
to the data space while the inference network maps training examples in data
space to the space of latent variables. An adversarial game is cast between
these two networks and a discriminative network that is trained to distinguish
between joint latent/data-space samples from the generative network and joint
samples from the inference network. We illustrate the ability of the model to
learn mutually coherent inference and generation networks through the
inspections of model samples and reconstructions and confirm the usefulness of
the learned representations by obtaining a performance competitive with other
recent approaches on the semi-supervised SVHN task.
</summary>
    <author>
      <name>Vincent Dumoulin</name>
    </author>
    <author>
      <name>Ishmael Belghazi</name>
    </author>
    <author>
      <name>Ben Poole</name>
    </author>
    <author>
      <name>Alex Lamb</name>
    </author>
    <author>
      <name>Martin Arjovsky</name>
    </author>
    <author>
      <name>Olivier Mastropietro</name>
    </author>
    <author>
      <name>Aaron Courville</name>
    </author>
    <link href="http://arxiv.org/abs/1606.00704v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00704v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.01540v1</id>
    <updated>2016-06-05T17:54:48Z</updated>
    <published>2016-06-05T17:54:48Z</published>
    <title>OpenAI Gym</title>
    <summary>  OpenAI Gym is a toolkit for reinforcement learning research. It includes a
growing collection of benchmark problems that expose a common interface, and a
website where people can share their results and compare the performance of
algorithms. This whitepaper discusses the components of OpenAI Gym and the
design decisions that went into the software.
</summary>
    <author>
      <name>Greg Brockman</name>
    </author>
    <author>
      <name>Vicki Cheung</name>
    </author>
    <author>
      <name>Ludwig Pettersson</name>
    </author>
    <author>
      <name>Jonas Schneider</name>
    </author>
    <author>
      <name>John Schulman</name>
    </author>
    <author>
      <name>Jie Tang</name>
    </author>
    <author>
      <name>Wojciech Zaremba</name>
    </author>
    <link href="http://arxiv.org/abs/1606.01540v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.01540v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.01781v2</id>
    <updated>2017-01-27T12:49:11Z</updated>
    <published>2016-06-06T15:14:50Z</published>
    <title>Very Deep Convolutional Networks for Text Classification</title>
    <summary>  The dominant approach for many NLP tasks are recurrent neural networks, in
particular LSTMs, and convolutional neural networks. However, these
architectures are rather shallow in comparison to the deep convolutional
networks which have pushed the state-of-the-art in computer vision. We present
a new architecture (VDCNN) for text processing which operates directly at the
character level and uses only small convolutions and pooling operations. We are
able to show that the performance of this model increases with depth: using up
to 29 convolutional layers, we report improvements over the state-of-the-art on
several public text classification tasks. To the best of our knowledge, this is
the first time that very deep convolutional nets have been applied to text
processing.
</summary>
    <author>
      <name>Alexis Conneau</name>
    </author>
    <author>
      <name>Holger Schwenk</name>
    </author>
    <author>
      <name>Loïc Barrault</name>
    </author>
    <author>
      <name>Yann Lecun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, EACL 2017, camera-ready</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.01781v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.01781v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02858v2</id>
    <updated>2016-08-08T21:21:19Z</updated>
    <published>2016-06-09T08:19:16Z</published>
    <title>A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task</title>
    <summary>  Enabling a computer to understand a document so that it can answer
comprehension questions is a central, yet unsolved goal of NLP. A key factor
impeding its solution by machine learned systems is the limited availability of
human-annotated data. Hermann et al. (2015) seek to solve this problem by
creating over a million training examples by pairing CNN and Daily Mail news
articles with their summarized bullet points, and show that a neural network
can then be trained to give good performance on this task. In this paper, we
conduct a thorough examination of this new reading comprehension task. Our
primary aim is to understand what depth of language understanding is required
to do well on this task. We approach this from one side by doing a careful
hand-analysis of a small subset of the problems and from the other by showing
that simple, carefully designed systems can obtain accuracies of 73.6% and
76.6% on these two datasets, exceeding current state-of-the-art results by
7-10% and approaching what we believe is the ceiling for performance on this
task.
</summary>
    <author>
      <name>Danqi Chen</name>
    </author>
    <author>
      <name>Jason Bolton</name>
    </author>
    <author>
      <name>Christopher D. Manning</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACL 2016, updated results</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.02858v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02858v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04474v1</id>
    <updated>2016-06-14T17:49:32Z</updated>
    <published>2016-06-14T17:49:32Z</published>
    <title>Learning to learn by gradient descent by gradient descent</title>
    <summary>  The move from hand-designed features to learned features in machine learning
has been wildly successful. In spite of this, optimization algorithms are still
designed by hand. In this paper we show how the design of an optimization
algorithm can be cast as a learning problem, allowing the algorithm to learn to
exploit structure in the problems of interest in an automatic way. Our learned
algorithms, implemented by LSTMs, outperform generic, hand-designed competitors
on the tasks for which they are trained, and also generalize well to new tasks
with similar structure. We demonstrate this on a number of tasks, including
simple convex problems, training neural networks, and styling images with
neural art.
</summary>
    <author>
      <name>Marcin Andrychowicz</name>
    </author>
    <author>
      <name>Misha Denil</name>
    </author>
    <author>
      <name>Sergio Gomez</name>
    </author>
    <author>
      <name>Matthew W. Hoffman</name>
    </author>
    <author>
      <name>David Pfau</name>
    </author>
    <author>
      <name>Tom Schaul</name>
    </author>
    <author>
      <name>Nando de Freitas</name>
    </author>
    <link href="http://arxiv.org/abs/1606.04474v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04474v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.05250v3</id>
    <updated>2016-10-11T02:42:36Z</updated>
    <published>2016-06-16T16:36:00Z</published>
    <title>SQuAD: 100,000+ Questions for Machine Comprehension of Text</title>
    <summary>  We present the Stanford Question Answering Dataset (SQuAD), a new reading
comprehension dataset consisting of 100,000+ questions posed by crowdworkers on
a set of Wikipedia articles, where the answer to each question is a segment of
text from the corresponding reading passage. We analyze the dataset to
understand the types of reasoning required to answer the questions, leaning
heavily on dependency and constituency trees. We build a strong logistic
regression model, which achieves an F1 score of 51.0%, a significant
improvement over a simple baseline (20%). However, human performance (86.8%) is
much higher, indicating that the dataset presents a good challenge problem for
future research.
  The dataset is freely available at https://stanford-qa.com
</summary>
    <author>
      <name>Pranav Rajpurkar</name>
    </author>
    <author>
      <name>Jian Zhang</name>
    </author>
    <author>
      <name>Konstantin Lopyrev</name>
    </author>
    <author>
      <name>Percy Liang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Proceedings of the 2016 Conference on Empirical Methods
  in Natural Language Processing (EMNLP)</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.05250v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.05250v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.05908v3</id>
    <updated>2021-01-03T16:56:46Z</updated>
    <published>2016-06-19T21:02:30Z</published>
    <title>Tutorial on Variational Autoencoders</title>
    <summary>  In just three years, Variational Autoencoders (VAEs) have emerged as one of
the most popular approaches to unsupervised learning of complicated
distributions. VAEs are appealing because they are built on top of standard
function approximators (neural networks), and can be trained with stochastic
gradient descent. VAEs have already shown promise in generating many kinds of
complicated data, including handwritten digits, faces, house numbers, CIFAR
images, physical models of scenes, segmentation, and predicting the future from
static images. This tutorial introduces the intuitions behind VAEs, explains
the mathematics behind them, and describes some empirical behavior. No prior
knowledge of variational Bayesian methods is assumed.
</summary>
    <author>
      <name>Carl Doersch</name>
    </author>
    <link href="http://arxiv.org/abs/1606.05908v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.05908v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.08415v4</id>
    <updated>2020-07-08T16:41:42Z</updated>
    <published>2016-06-27T19:20:40Z</published>
    <title>Gaussian Error Linear Units (GELUs)</title>
    <summary>  We propose the Gaussian Error Linear Unit (GELU), a high-performing neural
network activation function. The GELU activation function is $x\Phi(x)$, where
$\Phi(x)$ the standard Gaussian cumulative distribution function. The GELU
nonlinearity weights inputs by their value, rather than gates inputs by their
sign as in ReLUs ($x\mathbf{1}_{x&gt;0}$). We perform an empirical evaluation of
the GELU nonlinearity against the ReLU and ELU activations and find performance
improvements across all considered computer vision, natural language
processing, and speech tasks.
</summary>
    <author>
      <name>Dan Hendrycks</name>
    </author>
    <author>
      <name>Kevin Gimpel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Trimmed version of 2016 draft; add exact formula</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.08415v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.08415v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.01759v3</id>
    <updated>2016-08-09T17:38:43Z</updated>
    <published>2016-07-06T19:40:15Z</published>
    <title>Bag of Tricks for Efficient Text Classification</title>
    <summary>  This paper explores a simple and efficient baseline for text classification.
Our experiments show that our fast text classifier fastText is often on par
with deep learning classifiers in terms of accuracy, and many orders of
magnitude faster for training and evaluation. We can train fastText on more
than one billion words in less than ten minutes using a standard multicore~CPU,
and classify half a million sentences among~312K classes in less than a minute.
</summary>
    <author>
      <name>Armand Joulin</name>
    </author>
    <author>
      <name>Edouard Grave</name>
    </author>
    <author>
      <name>Piotr Bojanowski</name>
    </author>
    <author>
      <name>Tomas Mikolov</name>
    </author>
    <link href="http://arxiv.org/abs/1607.01759v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.01759v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.06450v1</id>
    <updated>2016-07-21T19:57:52Z</updated>
    <published>2016-07-21T19:57:52Z</published>
    <title>Layer Normalization</title>
    <summary>  Training state-of-the-art, deep neural networks is computationally expensive.
One way to reduce the training time is to normalize the activities of the
neurons. A recently introduced technique called batch normalization uses the
distribution of the summed input to a neuron over a mini-batch of training
cases to compute a mean and variance which are then used to normalize the
summed input to that neuron on each training case. This significantly reduces
the training time in feed-forward neural networks. However, the effect of batch
normalization is dependent on the mini-batch size and it is not obvious how to
apply it to recurrent neural networks. In this paper, we transpose batch
normalization into layer normalization by computing the mean and variance used
for normalization from all of the summed inputs to the neurons in a layer on a
single training case. Like batch normalization, we also give each neuron its
own adaptive bias and gain which are applied after the normalization but before
the non-linearity. Unlike batch normalization, layer normalization performs
exactly the same computation at training and test times. It is also
straightforward to apply to recurrent neural networks by computing the
normalization statistics separately at each time step. Layer normalization is
very effective at stabilizing the hidden state dynamics in recurrent networks.
Empirically, we show that layer normalization can substantially reduce the
training time compared with previously published techniques.
</summary>
    <author>
      <name>Jimmy Lei Ba</name>
    </author>
    <author>
      <name>Jamie Ryan Kiros</name>
    </author>
    <author>
      <name>Geoffrey E. Hinton</name>
    </author>
    <link href="http://arxiv.org/abs/1607.06450v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.06450v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.00060v7</id>
    <updated>2024-11-03T18:37:44Z</updated>
    <published>2016-07-30T01:58:04Z</published>
    <title>Double/Debiased Machine Learning for Treatment and Causal Parameters</title>
    <summary>  Most modern supervised statistical/machine learning (ML) methods are
explicitly designed to solve prediction problems very well. Achieving this goal
does not imply that these methods automatically deliver good estimators of
causal parameters. Examples of such parameters include individual regression
coefficients, average treatment effects, average lifts, and demand or supply
elasticities. In fact, estimates of such causal parameters obtained via naively
plugging ML estimators into estimating equations for such parameters can behave
very poorly due to the regularization bias. Fortunately, this regularization
bias can be removed by solving auxiliary prediction problems via ML tools.
Specifically, we can form an orthogonal score for the target low-dimensional
parameter by combining auxiliary and main ML predictions. The score is then
used to build a de-biased estimator of the target parameter which typically
will converge at the fastest possible 1/root(n) rate and be approximately
unbiased and normal, and from which valid confidence intervals for these
parameters of interest may be constructed. The resulting method thus could be
called a "double ML" method because it relies on estimating primary and
auxiliary predictive models. In order to avoid overfitting, our construction
also makes use of the K-fold sample splitting, which we call cross-fitting.
This allows us to use a very broad set of ML predictive methods in solving the
auxiliary and main prediction problems, such as random forest, lasso, ridge,
deep neural nets, boosted trees, as well as various hybrids and aggregators of
these methods.
</summary>
    <author>
      <name>Victor Chernozhukov</name>
    </author>
    <author>
      <name>Denis Chetverikov</name>
    </author>
    <author>
      <name>Mert Demirer</name>
    </author>
    <author>
      <name>Esther Duflo</name>
    </author>
    <author>
      <name>Christian Hansen</name>
    </author>
    <author>
      <name>Whitney Newey</name>
    </author>
    <author>
      <name>James Robins</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">71 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.00060v7" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.00060v7" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.06993v1</id>
    <updated>2016-08-25T00:44:55Z</updated>
    <published>2016-08-25T00:44:55Z</published>
    <title>Densely Connected Convolutional Networks</title>
    <summary>  Recent work has shown that convolutional networks can be substantially
deeper, more accurate and efficient to train if they contain shorter
connections between layers close to the input and those close to the output. In
this paper we embrace this observation and introduce the Dense Convolutional
Network (DenseNet), where each layer is directly connected to every other layer
in a feed-forward fashion. Whereas traditional convolutional networks with L
layers have L connections, one between each layer and its subsequent layer
(treating the input as layer 0), our network has L(L+1)/2 direct connections.
For each layer, the feature maps of all preceding layers are treated as
separate inputs whereas its own feature maps are passed on as inputs to all
subsequent layers. Our proposed connectivity pattern has several compelling
advantages: it alleviates the vanishing gradient problem and strengthens
feature propagation; despite the increase in connections, it encourages feature
reuse and leads to a substantial reduction of parameters; its models tend to
generalize surprisingly well. We evaluate our proposed architecture on five
highly competitive object recognition benchmark tasks. The DenseNet obtains
significant improvements over the state-of-the-art on all five of them (e.g.,
yielding 3.74% test error on CIFAR-10, 19.25% on CIFAR-100 and 1.59% on SVHN).
</summary>
    <author>
      <name>Gao Huang</name>
    </author>
    <author>
      <name>Zhuang Liu</name>
    </author>
    <author>
      <name>Kilian Q. Weinberger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.06993v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.06993v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03499v2</id>
    <updated>2016-09-19T18:04:35Z</updated>
    <published>2016-09-12T17:29:40Z</published>
    <title>WaveNet: A Generative Model for Raw Audio</title>
    <summary>  This paper introduces WaveNet, a deep neural network for generating raw audio
waveforms. The model is fully probabilistic and autoregressive, with the
predictive distribution for each audio sample conditioned on all previous ones;
nonetheless we show that it can be efficiently trained on data with tens of
thousands of samples per second of audio. When applied to text-to-speech, it
yields state-of-the-art performance, with human listeners rating it as
significantly more natural sounding than the best parametric and concatenative
systems for both English and Mandarin. A single WaveNet can capture the
characteristics of many different speakers with equal fidelity, and can switch
between them by conditioning on the speaker identity. When trained to model
music, we find that it generates novel and often highly realistic musical
fragments. We also show that it can be employed as a discriminative model,
returning promising results for phoneme recognition.
</summary>
    <author>
      <name>Aaron van den Oord</name>
    </author>
    <author>
      <name>Sander Dieleman</name>
    </author>
    <author>
      <name>Heiga Zen</name>
    </author>
    <author>
      <name>Karen Simonyan</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Alex Graves</name>
    </author>
    <author>
      <name>Nal Kalchbrenner</name>
    </author>
    <author>
      <name>Andrew Senior</name>
    </author>
    <author>
      <name>Koray Kavukcuoglu</name>
    </author>
    <link href="http://arxiv.org/abs/1609.03499v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03499v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03499v2</id>
    <updated>2016-09-19T18:04:35Z</updated>
    <published>2016-09-12T17:29:40Z</published>
    <title>WaveNet: A Generative Model for Raw Audio</title>
    <summary>  This paper introduces WaveNet, a deep neural network for generating raw audio
waveforms. The model is fully probabilistic and autoregressive, with the
predictive distribution for each audio sample conditioned on all previous ones;
nonetheless we show that it can be efficiently trained on data with tens of
thousands of samples per second of audio. When applied to text-to-speech, it
yields state-of-the-art performance, with human listeners rating it as
significantly more natural sounding than the best parametric and concatenative
systems for both English and Mandarin. A single WaveNet can capture the
characteristics of many different speakers with equal fidelity, and can switch
between them by conditioning on the speaker identity. When trained to model
music, we find that it generates novel and often highly realistic musical
fragments. We also show that it can be employed as a discriminative model,
returning promising results for phoneme recognition.
</summary>
    <author>
      <name>Aaron van den Oord</name>
    </author>
    <author>
      <name>Sander Dieleman</name>
    </author>
    <author>
      <name>Heiga Zen</name>
    </author>
    <author>
      <name>Karen Simonyan</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Alex Graves</name>
    </author>
    <author>
      <name>Nal Kalchbrenner</name>
    </author>
    <author>
      <name>Andrew Senior</name>
    </author>
    <author>
      <name>Koray Kavukcuoglu</name>
    </author>
    <link href="http://arxiv.org/abs/1609.03499v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03499v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03552v3</id>
    <updated>2018-12-16T22:00:59Z</updated>
    <published>2016-09-12T19:46:08Z</published>
    <title>Generative Visual Manipulation on the Natural Image Manifold</title>
    <summary>  Realistic image manipulation is challenging because it requires modifying the
image appearance in a user-controlled way, while preserving the realism of the
result. Unless the user has considerable artistic skill, it is easy to "fall
off" the manifold of natural images while editing. In this paper, we propose to
learn the natural image manifold directly from data using a generative
adversarial neural network. We then define a class of image editing operations,
and constrain their output to lie on that learned manifold at all times. The
model automatically adjusts the output keeping all edits as realistic as
possible. All our manipulations are expressed in terms of constrained
optimization and are applied in near-real time. We evaluate our algorithm on
the task of realistic photo manipulation of shape and color. The presented
method can further be used for changing one image to look like the other, as
well as generating novel imagery from scratch based on user's scribbles.
</summary>
    <author>
      <name>Jun-Yan Zhu</name>
    </author>
    <author>
      <name>Philipp Krähenbühl</name>
    </author>
    <author>
      <name>Eli Shechtman</name>
    </author>
    <author>
      <name>Alexei A. Efros</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In European Conference on Computer Vision (ECCV 2016)</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.03552v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03552v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.08144v2</id>
    <updated>2016-10-08T19:10:41Z</updated>
    <published>2016-09-26T19:59:55Z</published>
    <title>Google's Neural Machine Translation System: Bridging the Gap between
  Human and Machine Translation</title>
    <summary>  Neural Machine Translation (NMT) is an end-to-end learning approach for
automated translation, with the potential to overcome many of the weaknesses of
conventional phrase-based translation systems. Unfortunately, NMT systems are
known to be computationally expensive both in training and in translation
inference. Also, most NMT systems have difficulty with rare words. These issues
have hindered NMT's use in practical deployments and services, where both
accuracy and speed are essential. In this work, we present GNMT, Google's
Neural Machine Translation system, which attempts to address many of these
issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder
layers using attention and residual connections. To improve parallelism and
therefore decrease training time, our attention mechanism connects the bottom
layer of the decoder to the top layer of the encoder. To accelerate the final
translation speed, we employ low-precision arithmetic during inference
computations. To improve handling of rare words, we divide words into a limited
set of common sub-word units ("wordpieces") for both input and output. This
method provides a good balance between the flexibility of "character"-delimited
models and the efficiency of "word"-delimited models, naturally handles
translation of rare words, and ultimately improves the overall accuracy of the
system. Our beam search technique employs a length-normalization procedure and
uses a coverage penalty, which encourages generation of an output sentence that
is most likely to cover all the words in the source sentence. On the WMT'14
English-to-French and English-to-German benchmarks, GNMT achieves competitive
results to state-of-the-art. Using a human side-by-side evaluation on a set of
isolated simple sentences, it reduces translation errors by an average of 60%
compared to Google's phrase-based production system.
</summary>
    <author>
      <name>Yonghui Wu</name>
    </author>
    <author>
      <name>Mike Schuster</name>
    </author>
    <author>
      <name>Zhifeng Chen</name>
    </author>
    <author>
      <name>Quoc V. Le</name>
    </author>
    <author>
      <name>Mohammad Norouzi</name>
    </author>
    <author>
      <name>Wolfgang Macherey</name>
    </author>
    <author>
      <name>Maxim Krikun</name>
    </author>
    <author>
      <name>Yuan Cao</name>
    </author>
    <author>
      <name>Qin Gao</name>
    </author>
    <author>
      <name>Klaus Macherey</name>
    </author>
    <author>
      <name>Jeff Klingner</name>
    </author>
    <author>
      <name>Apurva Shah</name>
    </author>
    <author>
      <name>Melvin Johnson</name>
    </author>
    <author>
      <name>Xiaobing Liu</name>
    </author>
    <author>
      <name>Łukasz Kaiser</name>
    </author>
    <author>
      <name>Stephan Gouws</name>
    </author>
    <author>
      <name>Yoshikiyo Kato</name>
    </author>
    <author>
      <name>Taku Kudo</name>
    </author>
    <author>
      <name>Hideto Kazawa</name>
    </author>
    <author>
      <name>Keith Stevens</name>
    </author>
    <author>
      <name>George Kurian</name>
    </author>
    <author>
      <name>Nishant Patil</name>
    </author>
    <author>
      <name>Wei Wang</name>
    </author>
    <author>
      <name>Cliff Young</name>
    </author>
    <author>
      <name>Jason Smith</name>
    </author>
    <author>
      <name>Jason Riesa</name>
    </author>
    <author>
      <name>Alex Rudnick</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Greg Corrado</name>
    </author>
    <author>
      <name>Macduff Hughes</name>
    </author>
    <author>
      <name>Jeffrey Dean</name>
    </author>
    <link href="http://arxiv.org/abs/1609.08144v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.08144v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.08144v2</id>
    <updated>2016-10-08T19:10:41Z</updated>
    <published>2016-09-26T19:59:55Z</published>
    <title>Google's Neural Machine Translation System: Bridging the Gap between
  Human and Machine Translation</title>
    <summary>  Neural Machine Translation (NMT) is an end-to-end learning approach for
automated translation, with the potential to overcome many of the weaknesses of
conventional phrase-based translation systems. Unfortunately, NMT systems are
known to be computationally expensive both in training and in translation
inference. Also, most NMT systems have difficulty with rare words. These issues
have hindered NMT's use in practical deployments and services, where both
accuracy and speed are essential. In this work, we present GNMT, Google's
Neural Machine Translation system, which attempts to address many of these
issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder
layers using attention and residual connections. To improve parallelism and
therefore decrease training time, our attention mechanism connects the bottom
layer of the decoder to the top layer of the encoder. To accelerate the final
translation speed, we employ low-precision arithmetic during inference
computations. To improve handling of rare words, we divide words into a limited
set of common sub-word units ("wordpieces") for both input and output. This
method provides a good balance between the flexibility of "character"-delimited
models and the efficiency of "word"-delimited models, naturally handles
translation of rare words, and ultimately improves the overall accuracy of the
system. Our beam search technique employs a length-normalization procedure and
uses a coverage penalty, which encourages generation of an output sentence that
is most likely to cover all the words in the source sentence. On the WMT'14
English-to-French and English-to-German benchmarks, GNMT achieves competitive
results to state-of-the-art. Using a human side-by-side evaluation on a set of
isolated simple sentences, it reduces translation errors by an average of 60%
compared to Google's phrase-based production system.
</summary>
    <author>
      <name>Yonghui Wu</name>
    </author>
    <author>
      <name>Mike Schuster</name>
    </author>
    <author>
      <name>Zhifeng Chen</name>
    </author>
    <author>
      <name>Quoc V. Le</name>
    </author>
    <author>
      <name>Mohammad Norouzi</name>
    </author>
    <author>
      <name>Wolfgang Macherey</name>
    </author>
    <author>
      <name>Maxim Krikun</name>
    </author>
    <author>
      <name>Yuan Cao</name>
    </author>
    <author>
      <name>Qin Gao</name>
    </author>
    <author>
      <name>Klaus Macherey</name>
    </author>
    <author>
      <name>Jeff Klingner</name>
    </author>
    <author>
      <name>Apurva Shah</name>
    </author>
    <author>
      <name>Melvin Johnson</name>
    </author>
    <author>
      <name>Xiaobing Liu</name>
    </author>
    <author>
      <name>Łukasz Kaiser</name>
    </author>
    <author>
      <name>Stephan Gouws</name>
    </author>
    <author>
      <name>Yoshikiyo Kato</name>
    </author>
    <author>
      <name>Taku Kudo</name>
    </author>
    <author>
      <name>Hideto Kazawa</name>
    </author>
    <author>
      <name>Keith Stevens</name>
    </author>
    <author>
      <name>George Kurian</name>
    </author>
    <author>
      <name>Nishant Patil</name>
    </author>
    <author>
      <name>Wei Wang</name>
    </author>
    <author>
      <name>Cliff Young</name>
    </author>
    <author>
      <name>Jason Smith</name>
    </author>
    <author>
      <name>Jason Riesa</name>
    </author>
    <author>
      <name>Alex Rudnick</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Greg Corrado</name>
    </author>
    <author>
      <name>Macduff Hughes</name>
    </author>
    <author>
      <name>Jeffrey Dean</name>
    </author>
    <link href="http://arxiv.org/abs/1609.08144v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.08144v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.09038v1</id>
    <updated>2016-10-27T23:54:31Z</updated>
    <published>2016-10-27T23:54:31Z</published>
    <title>Professor Forcing: A New Algorithm for Training Recurrent Networks</title>
    <summary>  The Teacher Forcing algorithm trains recurrent networks by supplying observed
sequence values as inputs during training and using the network's own
one-step-ahead predictions to do multi-step sampling. We introduce the
Professor Forcing algorithm, which uses adversarial domain adaptation to
encourage the dynamics of the recurrent network to be the same when training
the network and when sampling from the network over multiple time steps. We
apply Professor Forcing to language modeling, vocal synthesis on raw waveforms,
handwriting generation, and image generation. Empirically we find that
Professor Forcing acts as a regularizer, improving test likelihood on character
level Penn Treebank and sequential MNIST. We also find that the model
qualitatively improves samples, especially when sampling for a large number of
time steps. This is supported by human evaluation of sample quality. Trade-offs
between Professor Forcing and Scheduled Sampling are discussed. We produce
T-SNEs showing that Professor Forcing successfully makes the dynamics of the
network during training and sampling more similar.
</summary>
    <author>
      <name>Alex Lamb</name>
    </author>
    <author>
      <name>Anirudh Goyal</name>
    </author>
    <author>
      <name>Ying Zhang</name>
    </author>
    <author>
      <name>Saizheng Zhang</name>
    </author>
    <author>
      <name>Aaron Courville</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NIPS 2016 Accepted Paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.09038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.09038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.01144v5</id>
    <updated>2017-08-05T22:45:19Z</updated>
    <published>2016-11-03T19:48:08Z</published>
    <title>Categorical Reparameterization with Gumbel-Softmax</title>
    <summary>  Categorical variables are a natural choice for representing discrete
structure in the world. However, stochastic neural networks rarely use
categorical latent variables due to the inability to backpropagate through
samples. In this work, we present an efficient gradient estimator that replaces
the non-differentiable sample from a categorical distribution with a
differentiable sample from a novel Gumbel-Softmax distribution. This
distribution has the essential property that it can be smoothly annealed into a
categorical distribution. We show that our Gumbel-Softmax estimator outperforms
state-of-the-art gradient estimators on structured output prediction and
unsupervised generative modeling tasks with categorical latent variables, and
enables large speedups on semi-supervised classification.
</summary>
    <author>
      <name>Eric Jang</name>
    </author>
    <author>
      <name>Shixiang Gu</name>
    </author>
    <author>
      <name>Ben Poole</name>
    </author>
    <link href="http://arxiv.org/abs/1611.01144v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.01144v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.03530v2</id>
    <updated>2017-02-26T19:36:40Z</updated>
    <published>2016-11-10T22:02:36Z</published>
    <title>Understanding deep learning requires rethinking generalization</title>
    <summary>  Despite their massive size, successful deep artificial neural networks can
exhibit a remarkably small difference between training and test performance.
Conventional wisdom attributes small generalization error either to properties
of the model family, or to the regularization techniques used during training.
  Through extensive systematic experiments, we show how these traditional
approaches fail to explain why large neural networks generalize well in
practice. Specifically, our experiments establish that state-of-the-art
convolutional networks for image classification trained with stochastic
gradient methods easily fit a random labeling of the training data. This
phenomenon is qualitatively unaffected by explicit regularization, and occurs
even if we replace the true images by completely unstructured random noise. We
corroborate these experimental findings with a theoretical construction showing
that simple depth two neural networks already have perfect finite sample
expressivity as soon as the number of parameters exceeds the number of data
points as it usually does in practice.
  We interpret our experimental findings by comparison with traditional models.
</summary>
    <author>
      <name>Chiyuan Zhang</name>
    </author>
    <author>
      <name>Samy Bengio</name>
    </author>
    <author>
      <name>Moritz Hardt</name>
    </author>
    <author>
      <name>Benjamin Recht</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in ICLR 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.03530v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.03530v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.04076v2</id>
    <updated>2017-02-24T07:50:53Z</updated>
    <published>2016-11-13T03:38:28Z</published>
    <title>Least Squares Generative Adversarial Networks</title>
    <summary>  Unsupervised learning with generative adversarial networks (GANs) has proven
hugely successful. Regular GANs hypothesize the discriminator as a classifier
with the sigmoid cross entropy loss function. This loss function, however, may
lead to the vanishing gradient problem during the learning process. To overcome
such problem, here we propose the Least Squares Generative Adversarial Networks
(LSGANs) that adopt the least squares loss function for the discriminator. We
show that minimizing the objective function of LSGAN yields minimizing the
Pearson $\chi^2$ divergence. There are two benefits of LSGANs over regular
GANs. First, LSGANs are able to generate higher quality images than regular
GANs. Second, LSGANs performs more stable during the learning process. We
evaluate the LSGANs on five scene datasets and the experimental results
demonstrate that the generated images by LSGANs look more realistic than the
ones generated by regular GANs. We also conduct two comparison experiments
between LSGANs and regular GANs to illustrate the stability of LSGANs.
</summary>
    <author>
      <name>Xudong Mao</name>
    </author>
    <author>
      <name>Qing Li</name>
    </author>
    <author>
      <name>Haoran Xie</name>
    </author>
    <author>
      <name>Raymond Y. K. Lau</name>
    </author>
    <author>
      <name>Zhen Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1611.04076v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.04076v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.03144v2</id>
    <updated>2017-04-19T22:46:32Z</updated>
    <published>2016-12-09T19:55:54Z</published>
    <title>Feature Pyramid Networks for Object Detection</title>
    <summary>  Feature pyramids are a basic component in recognition systems for detecting
objects at different scales. But recent deep learning object detectors have
avoided pyramid representations, in part because they are compute and memory
intensive. In this paper, we exploit the inherent multi-scale, pyramidal
hierarchy of deep convolutional networks to construct feature pyramids with
marginal extra cost. A top-down architecture with lateral connections is
developed for building high-level semantic feature maps at all scales. This
architecture, called a Feature Pyramid Network (FPN), shows significant
improvement as a generic feature extractor in several applications. Using FPN
in a basic Faster R-CNN system, our method achieves state-of-the-art
single-model results on the COCO detection benchmark without bells and
whistles, surpassing all existing single-model entries including those from the
COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU
and thus is a practical and accurate solution to multi-scale object detection.
Code will be made publicly available.
</summary>
    <author>
      <name>Tsung-Yi Lin</name>
    </author>
    <author>
      <name>Piotr Dollár</name>
    </author>
    <author>
      <name>Ross Girshick</name>
    </author>
    <author>
      <name>Kaiming He</name>
    </author>
    <author>
      <name>Bharath Hariharan</name>
    </author>
    <author>
      <name>Serge Belongie</name>
    </author>
    <link href="http://arxiv.org/abs/1612.03144v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.03144v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.07274v2</id>
    <updated>2017-01-26T16:38:08Z</updated>
    <published>2017-01-25T11:52:11Z</published>
    <title>Deep Reinforcement Learning: An Overview</title>
    <summary>  We give an overview of recent exciting achievements of deep reinforcement
learning (RL). We start with background of deep learning and reinforcement
learning, as well as introduction of testbeds. Next we discuss Deep Q-Network
(DQN) and its extensions, asynchronous methods, policy optimization, reward,
and planning. After that, we talk about attention and memory, unsupervised
learning, and learning to learn. Then we discuss various applications of RL,
including games, in particular, AlphaGo, robotics, spoken dialogue systems
(a.k.a. chatbot), machine translation, text sequence prediction, neural
architecture design, personalized web services, healthcare, finance, and music
generation. We mention topics/papers not reviewed yet. After listing a
collection of RL resources, we close with discussions.
</summary>
    <author>
      <name>Yuxi Li</name>
    </author>
    <link href="http://arxiv.org/abs/1701.07274v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.07274v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.07875v1</id>
    <updated>2017-01-26T21:10:29Z</updated>
    <published>2017-01-26T21:10:29Z</published>
    <title>Wasserstein GAN</title>
    <summary>  We introduce a new algorithm named WGAN, an alternative to traditional GAN
training. In this new model, we show that we can improve the stability of
learning, get rid of problems like mode collapse, and provide meaningful
learning curves useful for debugging and hyperparameter searches. Furthermore,
we show that the corresponding optimization problem is sound, and provide
extensive theoretical work highlighting the deep connections to other distances
between distributions.
</summary>
    <author>
      <name>Martin Arjovsky</name>
    </author>
    <author>
      <name>Soumith Chintala</name>
    </author>
    <author>
      <name>Léon Bottou</name>
    </author>
    <link href="http://arxiv.org/abs/1701.07875v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.07875v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.01932v2</id>
    <updated>2018-11-15T19:04:48Z</updated>
    <published>2017-02-07T09:16:46Z</published>
    <title>A Knowledge-Grounded Neural Conversation Model</title>
    <summary>  Neural network models are capable of generating extremely natural sounding
conversational interactions. Nevertheless, these models have yet to demonstrate
that they can incorporate content in the form of factual information or
entity-grounded opinion that would enable them to serve in more task-oriented
conversational applications. This paper presents a novel, fully data-driven,
and knowledge-grounded neural conversation model aimed at producing more
contentful responses without slot filling. We generalize the widely-used
Seq2Seq approach by conditioning responses on both conversation history and
external "facts", allowing the model to be versatile and applicable in an
open-domain setting. Our approach yields significant improvements over a
competitive Seq2Seq baseline. Human judges found that our outputs are
significantly more informative.
</summary>
    <author>
      <name>Marjan Ghazvininejad</name>
    </author>
    <author>
      <name>Chris Brockett</name>
    </author>
    <author>
      <name>Ming-Wei Chang</name>
    </author>
    <author>
      <name>Bill Dolan</name>
    </author>
    <author>
      <name>Jianfeng Gao</name>
    </author>
    <author>
      <name>Wen-tau Yih</name>
    </author>
    <author>
      <name>Michel Galley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AAAI 2018 (9 pages)</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.01932v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.01932v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.03275v2</id>
    <updated>2017-03-30T17:58:32Z</updated>
    <published>2017-02-10T18:27:17Z</published>
    <title>Batch Renormalization: Towards Reducing Minibatch Dependence in
  Batch-Normalized Models</title>
    <summary>  Batch Normalization is quite effective at accelerating and improving the
training of deep models. However, its effectiveness diminishes when the
training minibatches are small, or do not consist of independent samples. We
hypothesize that this is due to the dependence of model layer inputs on all the
examples in the minibatch, and different activations being produced between
training and inference. We propose Batch Renormalization, a simple and
effective extension to ensure that the training and inference models generate
the same outputs that depend on individual examples rather than the entire
minibatch. Models trained with Batch Renormalization perform substantially
better than batchnorm when training with small or non-i.i.d. minibatches. At
the same time, Batch Renormalization retains the benefits of batchnorm such as
insensitivity to initialization and training efficiency.
</summary>
    <author>
      <name>Sergey Ioffe</name>
    </author>
    <link href="http://arxiv.org/abs/1702.03275v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.03275v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.06506v1</id>
    <updated>2017-02-21T18:20:30Z</updated>
    <published>2017-02-21T18:20:30Z</published>
    <title>PixelNet: Representation of the pixels, by the pixels, and for the
  pixels</title>
    <summary>  We explore design principles for general pixel-level prediction problems,
from low-level edge detection to mid-level surface normal estimation to
high-level semantic segmentation. Convolutional predictors, such as the
fully-convolutional network (FCN), have achieved remarkable success by
exploiting the spatial redundancy of neighboring pixels through convolutional
processing. Though computationally efficient, we point out that such approaches
are not statistically efficient during learning precisely because spatial
redundancy limits the information learned from neighboring pixels. We
demonstrate that stratified sampling of pixels allows one to (1) add diversity
during batch updates, speeding up learning; (2) explore complex nonlinear
predictors, improving accuracy; and (3) efficiently train state-of-the-art
models tabula rasa (i.e., "from scratch") for diverse pixel-labeling tasks. Our
single architecture produces state-of-the-art results for semantic segmentation
on PASCAL-Context dataset, surface normal estimation on NYUDv2 depth dataset,
and edge detection on BSDS.
</summary>
    <author>
      <name>Aayush Bansal</name>
    </author>
    <author>
      <name>Xinlei Chen</name>
    </author>
    <author>
      <name>Bryan Russell</name>
    </author>
    <author>
      <name>Abhinav Gupta</name>
    </author>
    <author>
      <name>Deva Ramanan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Page: http://www.cs.cmu.edu/~aayushb/pixelNet/. arXiv admin
  note: substantial text overlap with arXiv:1609.06694</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.06506v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.06506v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.07800v4</id>
    <updated>2017-03-03T03:03:32Z</updated>
    <published>2017-02-24T23:30:08Z</published>
    <title>On the Origin of Deep Learning</title>
    <summary>  This paper is a review of the evolutionary history of deep learning models.
It covers from the genesis of neural networks when associationism modeling of
the brain is studied, to the models that dominate the last decade of research
in deep learning like convolutional neural networks, deep belief networks, and
recurrent neural networks. In addition to a review of these models, this paper
primarily focuses on the precedents of the models above, examining how the
initial ideas are assembled to construct the early models and how these
preliminary models are developed into their current forms. Many of these
evolutionary paths last more than half a century and have a diversity of
directions. For example, CNN is built on prior knowledge of biological vision
system; DBN is evolved from a trade-off of modeling power and computation
complexity of graphical models and many nowadays models are neural counterparts
of ancient linear models. This paper reviews these evolutionary paths and
offers a concise thought flow of how these models are developed, and aims to
provide a thorough background for deep learning. More importantly, along with
the path, this paper summarizes the gist behind these milestones and proposes
many directions to guide the future research of deep learning.
</summary>
    <author>
      <name>Haohan Wang</name>
    </author>
    <author>
      <name>Bhiksha Raj</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">70 pages, 200 references</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.07800v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.07800v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.07825v2</id>
    <updated>2017-03-07T23:09:23Z</updated>
    <published>2017-02-25T03:11:04Z</published>
    <title>Deep Voice: Real-time Neural Text-to-Speech</title>
    <summary>  We present Deep Voice, a production-quality text-to-speech system constructed
entirely from deep neural networks. Deep Voice lays the groundwork for truly
end-to-end neural speech synthesis. The system comprises five major building
blocks: a segmentation model for locating phoneme boundaries, a
grapheme-to-phoneme conversion model, a phoneme duration prediction model, a
fundamental frequency prediction model, and an audio synthesis model. For the
segmentation model, we propose a novel way of performing phoneme boundary
detection with deep neural networks using connectionist temporal classification
(CTC) loss. For the audio synthesis model, we implement a variant of WaveNet
that requires fewer parameters and trains faster than the original. By using a
neural network for each component, our system is simpler and more flexible than
traditional text-to-speech systems, where each component requires laborious
feature engineering and extensive domain expertise. Finally, we show that
inference with our system can be performed faster than real time and describe
optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x
speedups over existing implementations.
</summary>
    <author>
      <name>Sercan O. Arik</name>
    </author>
    <author>
      <name>Mike Chrzanowski</name>
    </author>
    <author>
      <name>Adam Coates</name>
    </author>
    <author>
      <name>Gregory Diamos</name>
    </author>
    <author>
      <name>Andrew Gibiansky</name>
    </author>
    <author>
      <name>Yongguo Kang</name>
    </author>
    <author>
      <name>Xian Li</name>
    </author>
    <author>
      <name>John Miller</name>
    </author>
    <author>
      <name>Andrew Ng</name>
    </author>
    <author>
      <name>Jonathan Raiman</name>
    </author>
    <author>
      <name>Shubho Sengupta</name>
    </author>
    <author>
      <name>Mohammad Shoeybi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ICML 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.07825v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.07825v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.01619v1</id>
    <updated>2017-03-05T16:10:11Z</updated>
    <published>2017-03-05T16:10:11Z</published>
    <title>Neural Machine Translation and Sequence-to-sequence Models: A Tutorial</title>
    <summary>  This tutorial introduces a new and powerful set of techniques variously
called "neural machine translation" or "neural sequence-to-sequence models".
These techniques have been used in a number of tasks regarding the handling of
human language, and can be a powerful tool in the toolbox of anyone who wants
to model sequential data of some sort. The tutorial assumes that the reader
knows the basics of math and programming, but does not assume any particular
experience with neural networks or natural language processing. It attempts to
explain the intuition behind the various methods covered, then delves into them
with enough mathematical detail to understand them concretely, and culiminates
with a suggestion for an implementation exercise, where readers can test that
they understood the content in practice.
</summary>
    <author>
      <name>Graham Neubig</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">65 Pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.01619v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.01619v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.03130v1</id>
    <updated>2017-03-09T04:42:30Z</updated>
    <published>2017-03-09T04:42:30Z</published>
    <title>A Structured Self-attentive Sentence Embedding</title>
    <summary>  This paper proposes a new model for extracting an interpretable sentence
embedding by introducing self-attention. Instead of using a vector, we use a
2-D matrix to represent the embedding, with each row of the matrix attending on
a different part of the sentence. We also propose a self-attention mechanism
and a special regularization term for the model. As a side effect, the
embedding comes with an easy way of visualizing what specific parts of the
sentence are encoded into the embedding. We evaluate our model on 3 different
tasks: author profiling, sentiment classification, and textual entailment.
Results show that our model yields a significant performance gain compared to
other sentence embedding methods in all of the 3 tasks.
</summary>
    <author>
      <name>Zhouhan Lin</name>
    </author>
    <author>
      <name>Minwei Feng</name>
    </author>
    <author>
      <name>Cicero Nogueira dos Santos</name>
    </author>
    <author>
      <name>Mo Yu</name>
    </author>
    <author>
      <name>Bing Xiang</name>
    </author>
    <author>
      <name>Bowen Zhou</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages with appendix, 7 figures, 4 tables. Conference paper in 5th
  International Conference on Learning Representations (ICLR 2017)</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.03130v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.03130v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.03864v1</id>
    <updated>2017-03-10T23:02:19Z</updated>
    <published>2017-03-10T23:02:19Z</published>
    <title>Evolution Strategies as a Scalable Alternative to Reinforcement Learning</title>
    <summary>  We explore the use of Evolution Strategies, a class of black box optimization
algorithms, as an alternative to popular RL techniques such as Q-learning and
Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable
solution strategy that scales extremely well with the number of CPUs available:
By using hundreds to thousands of parallel workers, ES can solve 3D humanoid
walking in 10 minutes and obtain competitive results on most Atari games after
one hour of training time. In addition, we highlight several advantages of ES
as a black box optimization technique: it is invariant to action frequency and
delayed rewards, tolerant of extremely long horizons, and does not need
temporal discounting or value function approximation.
</summary>
    <author>
      <name>Tim Salimans</name>
    </author>
    <author>
      <name>Jonathan Ho</name>
    </author>
    <author>
      <name>Xi Chen</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <link href="http://arxiv.org/abs/1703.03864v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.03864v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.05192v1</id>
    <updated>2017-03-15T14:53:15Z</updated>
    <published>2017-03-15T14:53:15Z</published>
    <title>Learning to Discover Cross-Domain Relations with Generative Adversarial
  Networks</title>
    <summary>  While humans easily recognize relations between data from different domains
without any supervision, learning to automatically discover them is in general
very challenging and needs many ground-truth pairs that illustrate the
relations. To avoid costly pairing, we address the task of discovering
cross-domain relations given unpaired data. We propose a method based on
generative adversarial networks that learns to discover relations between
different domains (DiscoGAN). Using the discovered relations, our proposed
network successfully transfers style from one domain to another while
preserving key attributes such as orientation and face identity.
</summary>
    <author>
      <name>Taeksoo Kim</name>
    </author>
    <author>
      <name>Moonsu Cha</name>
    </author>
    <author>
      <name>Hyunsoo Kim</name>
    </author>
    <author>
      <name>Jungkwon Lee</name>
    </author>
    <author>
      <name>Jiwon Kim</name>
    </author>
    <link href="http://arxiv.org/abs/1703.05192v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.05192v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.06211v2</id>
    <updated>2017-03-22T12:39:32Z</updated>
    <published>2017-03-17T21:58:20Z</published>
    <title>Deformable Convolutional Networks</title>
    <summary>  Convolutional neural networks (CNNs) are inherently limited to model
geometric transformations due to the fixed geometric structures in its building
modules. In this work, we introduce two new modules to enhance the
transformation modeling capacity of CNNs, namely, deformable convolution and
deformable RoI pooling. Both are based on the idea of augmenting the spatial
sampling locations in the modules with additional offsets and learning the
offsets from target tasks, without additional supervision. The new modules can
readily replace their plain counterparts in existing CNNs and can be easily
trained end-to-end by standard back-propagation, giving rise to deformable
convolutional networks. Extensive experiments validate the effectiveness of our
approach on sophisticated vision tasks of object detection and semantic
segmentation. The code would be released.
</summary>
    <author>
      <name>Jifeng Dai</name>
    </author>
    <author>
      <name>Haozhi Qi</name>
    </author>
    <author>
      <name>Yuwen Xiong</name>
    </author>
    <author>
      <name>Yi Li</name>
    </author>
    <author>
      <name>Guodong Zhang</name>
    </author>
    <author>
      <name>Han Hu</name>
    </author>
    <author>
      <name>Yichen Wei</name>
    </author>
    <link href="http://arxiv.org/abs/1703.06211v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.06211v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.06868v2</id>
    <updated>2017-07-30T09:32:17Z</updated>
    <published>2017-03-20T17:51:31Z</published>
    <title>Arbitrary Style Transfer in Real-time with Adaptive Instance
  Normalization</title>
    <summary>  Gatys et al. recently introduced a neural algorithm that renders a content
image in the style of another image, achieving so-called style transfer.
However, their framework requires a slow iterative optimization process, which
limits its practical application. Fast approximations with feed-forward neural
networks have been proposed to speed up neural style transfer. Unfortunately,
the speed improvement comes at a cost: the network is usually tied to a fixed
set of styles and cannot adapt to arbitrary new styles. In this paper, we
present a simple yet effective approach that for the first time enables
arbitrary style transfer in real-time. At the heart of our method is a novel
adaptive instance normalization (AdaIN) layer that aligns the mean and variance
of the content features with those of the style features. Our method achieves
speed comparable to the fastest existing approach, without the restriction to a
pre-defined set of styles. In addition, our approach allows flexible user
controls such as content-style trade-off, style interpolation, color &amp; spatial
controls, all using a single feed-forward neural network.
</summary>
    <author>
      <name>Xun Huang</name>
    </author>
    <author>
      <name>Serge Belongie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICCV 2017. Code is available:
  https://github.com/xunhuang1995/AdaIN-style</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.06868v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.06868v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.06870v3</id>
    <updated>2018-01-24T07:54:08Z</updated>
    <published>2017-03-20T17:53:38Z</published>
    <title>Mask R-CNN</title>
    <summary>  We present a conceptually simple, flexible, and general framework for object
instance segmentation. Our approach efficiently detects objects in an image
while simultaneously generating a high-quality segmentation mask for each
instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a
branch for predicting an object mask in parallel with the existing branch for
bounding box recognition. Mask R-CNN is simple to train and adds only a small
overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to
generalize to other tasks, e.g., allowing us to estimate human poses in the
same framework. We show top results in all three tracks of the COCO suite of
challenges, including instance segmentation, bounding-box object detection, and
person keypoint detection. Without bells and whistles, Mask R-CNN outperforms
all existing, single-model entries on every task, including the COCO 2016
challenge winners. We hope our simple and effective approach will serve as a
solid baseline and help ease future research in instance-level recognition.
Code has been made available at: https://github.com/facebookresearch/Detectron
</summary>
    <author>
      <name>Kaiming He</name>
    </author>
    <author>
      <name>Georgia Gkioxari</name>
    </author>
    <author>
      <name>Piotr Dollár</name>
    </author>
    <author>
      <name>Ross Girshick</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">open source; appendix on more results</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.06870v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.06870v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.07511v1</id>
    <updated>2017-03-22T04:21:41Z</updated>
    <published>2017-03-22T04:21:41Z</published>
    <title>Deep Photo Style Transfer</title>
    <summary>  This paper introduces a deep-learning approach to photographic style transfer
that handles a large variety of image content while faithfully transferring the
reference style. Our approach builds upon recent work on painterly transfer
that separates style from the content of an image by considering different
layers of a neural network. However, as is, this approach is not suitable for
photorealistic style transfer. Even when both the input and reference images
are photographs, the output still exhibits distortions reminiscent of a
painting. Our contribution is to constrain the transformation from the input to
the output to be locally affine in colorspace, and to express this constraint
as a custom CNN layer through which we can backpropagate. We show that this
approach successfully suppresses distortion and yields satisfying
photorealistic style transfers in a broad variety of scenarios, including
transfer of the time of day, weather, season, and artistic edits.
</summary>
    <author>
      <name>Fujun Luan</name>
    </author>
    <author>
      <name>Sylvain Paris</name>
    </author>
    <author>
      <name>Eli Shechtman</name>
    </author>
    <author>
      <name>Kavita Bala</name>
    </author>
    <link href="http://arxiv.org/abs/1703.07511v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.07511v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.10135v2</id>
    <updated>2017-04-06T21:20:34Z</updated>
    <published>2017-03-29T16:55:13Z</published>
    <title>Tacotron: Towards End-to-End Speech Synthesis</title>
    <summary>  A text-to-speech synthesis system typically consists of multiple stages, such
as a text analysis frontend, an acoustic model and an audio synthesis module.
Building these components often requires extensive domain expertise and may
contain brittle design choices. In this paper, we present Tacotron, an
end-to-end generative text-to-speech model that synthesizes speech directly
from characters. Given &lt;text, audio&gt; pairs, the model can be trained completely
from scratch with random initialization. We present several key techniques to
make the sequence-to-sequence framework perform well for this challenging task.
Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English,
outperforming a production parametric system in terms of naturalness. In
addition, since Tacotron generates speech at the frame level, it's
substantially faster than sample-level autoregressive methods.
</summary>
    <author>
      <name>Yuxuan Wang</name>
    </author>
    <author>
      <name>RJ Skerry-Ryan</name>
    </author>
    <author>
      <name>Daisy Stanton</name>
    </author>
    <author>
      <name>Yonghui Wu</name>
    </author>
    <author>
      <name>Ron J. Weiss</name>
    </author>
    <author>
      <name>Navdeep Jaitly</name>
    </author>
    <author>
      <name>Zongheng Yang</name>
    </author>
    <author>
      <name>Ying Xiao</name>
    </author>
    <author>
      <name>Zhifeng Chen</name>
    </author>
    <author>
      <name>Samy Bengio</name>
    </author>
    <author>
      <name>Quoc Le</name>
    </author>
    <author>
      <name>Yannis Agiomyrgiannakis</name>
    </author>
    <author>
      <name>Rob Clark</name>
    </author>
    <author>
      <name>Rif A. Saurous</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Interspeech 2017. v2 changed paper title to be
  consistent with our conference submission (no content change other than typo
  fixes)</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.10135v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.10135v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.00805v4</id>
    <updated>2018-08-21T00:02:44Z</updated>
    <published>2017-04-03T20:50:29Z</published>
    <title>On the Properties of the Softmax Function with Application in Game
  Theory and Reinforcement Learning</title>
    <summary>  In this paper, we utilize results from convex analysis and monotone operator
theory to derive additional properties of the softmax function that have not
yet been covered in the existing literature. In particular, we show that the
softmax function is the monotone gradient map of the log-sum-exp function. By
exploiting this connection, we show that the inverse temperature parameter
determines the Lipschitz and co-coercivity properties of the softmax function.
We then demonstrate the usefulness of these properties through an application
in game-theoretic reinforcement learning.
</summary>
    <author>
      <name>Bolin Gao</name>
    </author>
    <author>
      <name>Lacra Pavel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures. Comments are welcome</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.00805v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.00805v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.04289v2</id>
    <updated>2018-01-19T21:07:09Z</updated>
    <published>2017-04-13T22:17:30Z</published>
    <title>Stochastic Gradient Descent as Approximate Bayesian Inference</title>
    <summary>  Stochastic Gradient Descent with a constant learning rate (constant SGD)
simulates a Markov chain with a stationary distribution. With this perspective,
we derive several new results. (1) We show that constant SGD can be used as an
approximate Bayesian posterior inference algorithm. Specifically, we show how
to adjust the tuning parameters of constant SGD to best match the stationary
distribution to a posterior, minimizing the Kullback-Leibler divergence between
these two distributions. (2) We demonstrate that constant SGD gives rise to a
new variational EM algorithm that optimizes hyperparameters in complex
probabilistic models. (3) We also propose SGD with momentum for sampling and
show how to adjust the damping coefficient accordingly. (4) We analyze MCMC
algorithms. For Langevin Dynamics and Stochastic Gradient Fisher Scoring, we
quantify the approximation errors due to finite learning rates. Finally (5), we
use the stochastic process perspective to give a short proof of why Polyak
averaging is optimal. Based on this idea, we propose a scalable approximate
MCMC algorithm, the Averaged Stochastic Gradient Sampler.
</summary>
    <author>
      <name>Stephan Mandt</name>
    </author>
    <author>
      <name>Matthew D. Hoffman</name>
    </author>
    <author>
      <name>David M. Blei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages, published version (JMLR 2017)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Machine Learning Research 18 (2017) 1-35</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1704.04289v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.04289v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.04861v1</id>
    <updated>2017-04-17T03:57:34Z</updated>
    <published>2017-04-17T03:57:34Z</published>
    <title>MobileNets: Efficient Convolutional Neural Networks for Mobile Vision
  Applications</title>
    <summary>  We present a class of efficient models called MobileNets for mobile and
embedded vision applications. MobileNets are based on a streamlined
architecture that uses depth-wise separable convolutions to build light weight
deep neural networks. We introduce two simple global hyper-parameters that
efficiently trade off between latency and accuracy. These hyper-parameters
allow the model builder to choose the right sized model for their application
based on the constraints of the problem. We present extensive experiments on
resource and accuracy tradeoffs and show strong performance compared to other
popular models on ImageNet classification. We then demonstrate the
effectiveness of MobileNets across a wide range of applications and use cases
including object detection, finegrain classification, face attributes and large
scale geo-localization.
</summary>
    <author>
      <name>Andrew G. Howard</name>
    </author>
    <author>
      <name>Menglong Zhu</name>
    </author>
    <author>
      <name>Bo Chen</name>
    </author>
    <author>
      <name>Dmitry Kalenichenko</name>
    </author>
    <author>
      <name>Weijun Wang</name>
    </author>
    <author>
      <name>Tobias Weyand</name>
    </author>
    <author>
      <name>Marco Andreetto</name>
    </author>
    <author>
      <name>Hartwig Adam</name>
    </author>
    <link href="http://arxiv.org/abs/1704.04861v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.04861v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.03122v3</id>
    <updated>2017-07-25T01:40:57Z</updated>
    <published>2017-05-08T23:25:30Z</published>
    <title>Convolutional Sequence to Sequence Learning</title>
    <summary>  The prevalent approach to sequence to sequence learning maps an input
sequence to a variable length output sequence via recurrent neural networks. We
introduce an architecture based entirely on convolutional neural networks.
Compared to recurrent models, computations over all elements can be fully
parallelized during training and optimization is easier since the number of
non-linearities is fixed and independent of the input length. Our use of gated
linear units eases gradient propagation and we equip each decoder layer with a
separate attention module. We outperform the accuracy of the deep LSTM setup of
Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French
translation at an order of magnitude faster speed, both on GPU and CPU.
</summary>
    <author>
      <name>Jonas Gehring</name>
    </author>
    <author>
      <name>Michael Auli</name>
    </author>
    <author>
      <name>David Grangier</name>
    </author>
    <author>
      <name>Denis Yarats</name>
    </author>
    <author>
      <name>Yann N. Dauphin</name>
    </author>
    <link href="http://arxiv.org/abs/1705.03122v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.03122v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.03762v7</id>
    <updated>2023-08-02T00:41:18Z</updated>
    <published>2017-06-12T17:57:34Z</published>
    <title>Attention Is All You Need</title>
    <summary>  The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks in an encoder-decoder configuration. The best
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new simple network architecture, the Transformer, based
solely on attention mechanisms, dispensing with recurrence and convolutions
entirely. Experiments on two machine translation tasks show these models to be
superior in quality while being more parallelizable and requiring significantly
less time to train. Our model achieves 28.4 BLEU on the WMT 2014
English-to-German translation task, improving over the existing best results,
including ensembles by over 2 BLEU. On the WMT 2014 English-to-French
translation task, our model establishes a new single-model state-of-the-art
BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction
of the training costs of the best models from the literature. We show that the
Transformer generalizes well to other tasks by applying it successfully to
English constituency parsing both with large and limited training data.
</summary>
    <author>
      <name>Ashish Vaswani</name>
    </author>
    <author>
      <name>Noam Shazeer</name>
    </author>
    <author>
      <name>Niki Parmar</name>
    </author>
    <author>
      <name>Jakob Uszkoreit</name>
    </author>
    <author>
      <name>Llion Jones</name>
    </author>
    <author>
      <name>Aidan N. Gomez</name>
    </author>
    <author>
      <name>Lukasz Kaiser</name>
    </author>
    <author>
      <name>Illia Polosukhin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.03762v7" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.03762v7" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.06859v1</id>
    <updated>2017-06-20T04:19:57Z</updated>
    <published>2017-06-20T04:19:57Z</published>
    <title>Analysis of dropout learning regarded as ensemble learning</title>
    <summary>  Deep learning is the state-of-the-art in fields such as visual object
recognition and speech recognition. This learning uses a large number of
layers, huge number of units, and connections. Therefore, overfitting is a
serious problem. To avoid this problem, dropout learning is proposed. Dropout
learning neglects some inputs and hidden units in the learning process with a
probability, p, and then, the neglected inputs and hidden units are combined
with the learned network to express the final output. We find that the process
of combining the neglected hidden units with the learned network can be
regarded as ensemble learning, so we analyze dropout learning from this point
of view.
</summary>
    <author>
      <name>Kazuyuki Hara</name>
    </author>
    <author>
      <name>Daisuke Saitoh</name>
    </author>
    <author>
      <name>Hayaru Shouno</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-44781-0_9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-44781-0_9" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 8 figures, submitted to Conference</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">A. E. P. VIlla et al. (Eds.): ICANN 2016 ( Part II, LNCS 9887, pp.
  1-8, 2016)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.06859v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.06859v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.08500v6</id>
    <updated>2018-01-12T14:05:44Z</updated>
    <published>2017-06-26T17:45:23Z</published>
    <title>GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash
  Equilibrium</title>
    <summary>  Generative Adversarial Networks (GANs) excel at creating realistic images
with complex models for which maximum likelihood is infeasible. However, the
convergence of GAN training has still not been proved. We propose a two
time-scale update rule (TTUR) for training GANs with stochastic gradient
descent on arbitrary GAN loss functions. TTUR has an individual learning rate
for both the discriminator and the generator. Using the theory of stochastic
approximation, we prove that the TTUR converges under mild assumptions to a
stationary local Nash equilibrium. The convergence carries over to the popular
Adam optimization, for which we prove that it follows the dynamics of a heavy
ball with friction and thus prefers flat minima in the objective landscape. For
the evaluation of the performance of GANs at image generation, we introduce the
"Fr\'echet Inception Distance" (FID) which captures the similarity of generated
images to real ones better than the Inception Score. In experiments, TTUR
improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP)
outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN
Bedrooms, and the One Billion Word Benchmark.
</summary>
    <author>
      <name>Martin Heusel</name>
    </author>
    <author>
      <name>Hubert Ramsauer</name>
    </author>
    <author>
      <name>Thomas Unterthiner</name>
    </author>
    <author>
      <name>Bernhard Nessler</name>
    </author>
    <author>
      <name>Sepp Hochreiter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Implementations are available at: https://github.com/bioinf-jku/TTUR</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Advances in Neural Information Processing Systems 30 (NIPS 2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.08500v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.08500v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.06347v2</id>
    <updated>2017-08-28T09:20:06Z</updated>
    <published>2017-07-20T02:32:33Z</published>
    <title>Proximal Policy Optimization Algorithms</title>
    <summary>  We propose a new family of policy gradient methods for reinforcement
learning, which alternate between sampling data through interaction with the
environment, and optimizing a "surrogate" objective function using stochastic
gradient ascent. Whereas standard policy gradient methods perform one gradient
update per data sample, we propose a novel objective function that enables
multiple epochs of minibatch updates. The new methods, which we call proximal
policy optimization (PPO), have some of the benefits of trust region policy
optimization (TRPO), but they are much simpler to implement, more general, and
have better sample complexity (empirically). Our experiments test PPO on a
collection of benchmark tasks, including simulated robotic locomotion and Atari
game playing, and we show that PPO outperforms other online policy gradient
methods, and overall strikes a favorable balance between sample complexity,
simplicity, and wall-time.
</summary>
    <author>
      <name>John Schulman</name>
    </author>
    <author>
      <name>Filip Wolski</name>
    </author>
    <author>
      <name>Prafulla Dhariwal</name>
    </author>
    <author>
      <name>Alec Radford</name>
    </author>
    <author>
      <name>Oleg Klimov</name>
    </author>
    <link href="http://arxiv.org/abs/1707.06347v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.06347v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.05344v1</id>
    <updated>2017-08-17T16:03:33Z</updated>
    <published>2017-08-17T16:03:33Z</published>
    <title>SMASH: One-Shot Model Architecture Search through HyperNetworks</title>
    <summary>  Designing architectures for deep neural networks requires expert knowledge
and substantial computation time. We propose a technique to accelerate
architecture selection by learning an auxiliary HyperNet that generates the
weights of a main model conditioned on that model's architecture. By comparing
the relative validation performance of networks with HyperNet-generated
weights, we can effectively search over a wide range of architectures at the
cost of a single training run. To facilitate this search, we develop a flexible
mechanism based on memory read-writes that allows us to define a wide range of
network connectivity patterns, with ResNet, DenseNet, and FractalNet blocks as
special cases. We validate our method (SMASH) on CIFAR-10 and CIFAR-100,
STL-10, ModelNet10, and Imagenet32x32, achieving competitive performance with
similarly-sized hand-designed networks. Our code is available at
https://github.com/ajbrock/SMASH
</summary>
    <author>
      <name>Andrew Brock</name>
    </author>
    <author>
      <name>Theodore Lim</name>
    </author>
    <author>
      <name>J. M. Ritchie</name>
    </author>
    <author>
      <name>Nick Weston</name>
    </author>
    <link href="http://arxiv.org/abs/1708.05344v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.05344v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.01507v4</id>
    <updated>2019-05-16T05:32:17Z</updated>
    <published>2017-09-05T17:42:13Z</published>
    <title>Squeeze-and-Excitation Networks</title>
    <summary>  The central building block of convolutional neural networks (CNNs) is the
convolution operator, which enables networks to construct informative features
by fusing both spatial and channel-wise information within local receptive
fields at each layer. A broad range of prior research has investigated the
spatial component of this relationship, seeking to strengthen the
representational power of a CNN by enhancing the quality of spatial encodings
throughout its feature hierarchy. In this work, we focus instead on the channel
relationship and propose a novel architectural unit, which we term the
"Squeeze-and-Excitation" (SE) block, that adaptively recalibrates channel-wise
feature responses by explicitly modelling interdependencies between channels.
We show that these blocks can be stacked together to form SENet architectures
that generalise extremely effectively across different datasets. We further
demonstrate that SE blocks bring significant improvements in performance for
existing state-of-the-art CNNs at slight additional computational cost.
Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017
classification submission which won first place and reduced the top-5 error to
2.251%, surpassing the winning entry of 2016 by a relative improvement of ~25%.
Models and code are available at https://github.com/hujie-frank/SENet.
</summary>
    <author>
      <name>Jie Hu</name>
    </author>
    <author>
      <name>Li Shen</name>
    </author>
    <author>
      <name>Samuel Albanie</name>
    </author>
    <author>
      <name>Gang Sun</name>
    </author>
    <author>
      <name>Enhua Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">journal version of the CVPR 2018 paper, accepted by TPAMI</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.01507v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.01507v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.00937v2</id>
    <updated>2018-05-30T14:58:27Z</updated>
    <published>2017-11-02T21:14:44Z</published>
    <title>Neural Discrete Representation Learning</title>
    <summary>  Learning useful representations without supervision remains a key challenge
in machine learning. In this paper, we propose a simple yet powerful generative
model that learns such discrete representations. Our model, the Vector
Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways:
the encoder network outputs discrete, rather than continuous, codes; and the
prior is learnt rather than static. In order to learn a discrete latent
representation, we incorporate ideas from vector quantisation (VQ). Using the
VQ method allows the model to circumvent issues of "posterior collapse" --
where the latents are ignored when they are paired with a powerful
autoregressive decoder -- typically observed in the VAE framework. Pairing
these representations with an autoregressive prior, the model can generate high
quality images, videos, and speech as well as doing high quality speaker
conversion and unsupervised learning of phonemes, providing further evidence of
the utility of the learnt representations.
</summary>
    <author>
      <name>Aaron van den Oord</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Koray Kavukcuoglu</name>
    </author>
    <link href="http://arxiv.org/abs/1711.00937v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.00937v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.11053v2</id>
    <updated>2018-06-28T17:54:39Z</updated>
    <published>2017-11-29T19:01:32Z</published>
    <title>A Multi-Horizon Quantile Recurrent Forecaster</title>
    <summary>  We propose a framework for general probabilistic multi-step time series
regression. Specifically, we exploit the expressiveness and temporal nature of
Sequence-to-Sequence Neural Networks (e.g. recurrent and convolutional
structures), the nonparametric nature of Quantile Regression and the efficiency
of Direct Multi-Horizon Forecasting. A new training scheme,
*forking-sequences*, is designed for sequential nets to boost stability and
performance. We show that the approach accommodates both temporal and static
covariates, learning across multiple related series, shifting seasonality,
future planned event spikes and cold-starts in real life large-scale
forecasting. The performance of the framework is demonstrated in an application
to predict the future demand of items sold on Amazon.com, and in a public
probabilistic forecasting competition to predict electricity price and load.
</summary>
    <author>
      <name>Ruofeng Wen</name>
    </author>
    <author>
      <name>Kari Torkkola</name>
    </author>
    <author>
      <name>Balakrishnan Narayanaswamy</name>
    </author>
    <author>
      <name>Dhruv Madeka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published @ 31st Conference on Neural Information Processing Systems
  (NIPS 2017), Time Series Workshop. Long Beach, CA, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.11053v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.11053v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.04381v4</id>
    <updated>2019-03-21T19:44:34Z</updated>
    <published>2018-01-13T04:46:26Z</published>
    <title>MobileNetV2: Inverted Residuals and Linear Bottlenecks</title>
    <summary>  In this paper we describe a new mobile architecture, MobileNetV2, that
improves the state of the art performance of mobile models on multiple tasks
and benchmarks as well as across a spectrum of different model sizes. We also
describe efficient ways of applying these mobile models to object detection in
a novel framework we call SSDLite. Additionally, we demonstrate how to build
mobile semantic segmentation models through a reduced form of DeepLabv3 which
we call Mobile DeepLabv3.
  The MobileNetV2 architecture is based on an inverted residual structure where
the input and output of the residual block are thin bottleneck layers opposite
to traditional residual models which use expanded representations in the input
an MobileNetV2 uses lightweight depthwise convolutions to filter features in
the intermediate expansion layer. Additionally, we find that it is important to
remove non-linearities in the narrow layers in order to maintain
representational power. We demonstrate that this improves performance and
provide an intuition that led to this design. Finally, our approach allows
decoupling of the input/output domains from the expressiveness of the
transformation, which provides a convenient framework for further analysis. We
measure our performance on Imagenet classification, COCO object detection, VOC
image segmentation. We evaluate the trade-offs between accuracy, and number of
operations measured by multiply-adds (MAdd), as well as the number of
parameters
</summary>
    <author>
      <name>Mark Sandler</name>
    </author>
    <author>
      <name>Andrew Howard</name>
    </author>
    <author>
      <name>Menglong Zhu</name>
    </author>
    <author>
      <name>Andrey Zhmoginov</name>
    </author>
    <author>
      <name>Liang-Chieh Chen</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR), 2018, pp. 4510-4520</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1801.04381v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.04381v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.05134v1</id>
    <updated>2018-01-16T06:47:59Z</updated>
    <published>2018-01-16T06:47:59Z</published>
    <title>Understanding the Disharmony between Dropout and Batch Normalization by
  Variance Shift</title>
    <summary>  This paper first answers the question "why do the two most powerful
techniques Dropout and Batch Normalization (BN) often lead to a worse
performance when they are combined together?" in both theoretical and
statistical aspects. Theoretically, we find that Dropout would shift the
variance of a specific neural unit when we transfer the state of that network
from train to test. However, BN would maintain its statistical variance, which
is accumulated from the entire learning procedure, in the test phase. The
inconsistency of that variance (we name this scheme as "variance shift") causes
the unstable numerical behavior in inference that leads to more erroneous
predictions finally, when applying Dropout before BN. Thorough experiments on
DenseNet, ResNet, ResNeXt and Wide ResNet confirm our findings. According to
the uncovered mechanism, we next explore several strategies that modifies
Dropout and try to overcome the limitations of their combination by avoiding
the variance shift risks.
</summary>
    <author>
      <name>Xiang Li</name>
    </author>
    <author>
      <name>Shuo Chen</name>
    </author>
    <author>
      <name>Xiaolin Hu</name>
    </author>
    <author>
      <name>Jian Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.05134v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.05134v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.06146v5</id>
    <updated>2018-05-23T09:23:47Z</updated>
    <published>2018-01-18T17:54:52Z</published>
    <title>Universal Language Model Fine-tuning for Text Classification</title>
    <summary>  Inductive transfer learning has greatly impacted computer vision, but
existing approaches in NLP still require task-specific modifications and
training from scratch. We propose Universal Language Model Fine-tuning
(ULMFiT), an effective transfer learning method that can be applied to any task
in NLP, and introduce techniques that are key for fine-tuning a language model.
Our method significantly outperforms the state-of-the-art on six text
classification tasks, reducing the error by 18-24% on the majority of datasets.
Furthermore, with only 100 labeled examples, it matches the performance of
training from scratch on 100x more data. We open-source our pretrained models
and code.
</summary>
    <author>
      <name>Jeremy Howard</name>
    </author>
    <author>
      <name>Sebastian Ruder</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACL 2018, fixed denominator in Equation 3, line 3</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.06146v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.06146v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.03426v3</id>
    <updated>2020-09-18T01:56:41Z</updated>
    <published>2018-02-09T19:39:33Z</published>
    <title>UMAP: Uniform Manifold Approximation and Projection for Dimension
  Reduction</title>
    <summary>  UMAP (Uniform Manifold Approximation and Projection) is a novel manifold
learning technique for dimension reduction. UMAP is constructed from a
theoretical framework based in Riemannian geometry and algebraic topology. The
result is a practical scalable algorithm that applies to real world data. The
UMAP algorithm is competitive with t-SNE for visualization quality, and
arguably preserves more of the global structure with superior run time
performance. Furthermore, UMAP has no computational restrictions on embedding
dimension, making it viable as a general purpose dimension reduction technique
for machine learning.
</summary>
    <author>
      <name>Leland McInnes</name>
    </author>
    <author>
      <name>John Healy</name>
    </author>
    <author>
      <name>James Melville</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Reference implementation available at http://github.com/lmcinnes/umap</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.03426v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.03426v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.03635v5</id>
    <updated>2019-03-04T15:51:11Z</updated>
    <published>2018-03-09T18:51:28Z</published>
    <title>The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks</title>
    <summary>  Neural network pruning techniques can reduce the parameter counts of trained
networks by over 90%, decreasing storage requirements and improving
computational performance of inference without compromising accuracy. However,
contemporary experience is that the sparse architectures produced by pruning
are difficult to train from the start, which would similarly improve training
performance.
  We find that a standard pruning technique naturally uncovers subnetworks
whose initializations made them capable of training effectively. Based on these
results, we articulate the "lottery ticket hypothesis:" dense,
randomly-initialized, feed-forward networks contain subnetworks ("winning
tickets") that - when trained in isolation - reach test accuracy comparable to
the original network in a similar number of iterations. The winning tickets we
find have won the initialization lottery: their connections have initial
weights that make training particularly effective.
  We present an algorithm to identify winning tickets and a series of
experiments that support the lottery ticket hypothesis and the importance of
these fortuitous initializations. We consistently find winning tickets that are
less than 10-20% of the size of several fully-connected and convolutional
feed-forward architectures for MNIST and CIFAR10. Above this size, the winning
tickets that we find learn faster than the original network and reach higher
test accuracy.
</summary>
    <author>
      <name>Jonathan Frankle</name>
    </author>
    <author>
      <name>Michael Carbin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICLR camera ready</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ICLR 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1803.03635v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.03635v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01261v3</id>
    <updated>2018-10-17T17:51:36Z</updated>
    <published>2018-06-04T17:58:18Z</published>
    <title>Relational inductive biases, deep learning, and graph networks</title>
    <summary>  Artificial intelligence (AI) has undergone a renaissance recently, making
major progress in key domains such as vision, language, control, and
decision-making. This has been due, in part, to cheap data and cheap compute
resources, which have fit the natural strengths of deep learning. However, many
defining characteristics of human intelligence, which developed under much
different pressures, remain out of reach for current approaches. In particular,
generalizing beyond one's experiences--a hallmark of human intelligence from
infancy--remains a formidable challenge for modern AI.
  The following is part position paper, part review, and part unification. We
argue that combinatorial generalization must be a top priority for AI to
achieve human-like abilities, and that structured representations and
computations are key to realizing this objective. Just as biology uses nature
and nurture cooperatively, we reject the false choice between
"hand-engineering" and "end-to-end" learning, and instead advocate for an
approach which benefits from their complementary strengths. We explore how
using relational inductive biases within deep learning architectures can
facilitate learning about entities, relations, and rules for composing them. We
present a new building block for the AI toolkit with a strong relational
inductive bias--the graph network--which generalizes and extends various
approaches for neural networks that operate on graphs, and provides a
straightforward interface for manipulating structured knowledge and producing
structured behaviors. We discuss how graph networks can support relational
reasoning and combinatorial generalization, laying the foundation for more
sophisticated, interpretable, and flexible patterns of reasoning. As a
companion to this paper, we have released an open-source software library for
building graph networks, with demonstrations of how to use them in practice.
</summary>
    <author>
      <name>Peter W. Battaglia</name>
    </author>
    <author>
      <name>Jessica B. Hamrick</name>
    </author>
    <author>
      <name>Victor Bapst</name>
    </author>
    <author>
      <name>Alvaro Sanchez-Gonzalez</name>
    </author>
    <author>
      <name>Vinicius Zambaldi</name>
    </author>
    <author>
      <name>Mateusz Malinowski</name>
    </author>
    <author>
      <name>Andrea Tacchetti</name>
    </author>
    <author>
      <name>David Raposo</name>
    </author>
    <author>
      <name>Adam Santoro</name>
    </author>
    <author>
      <name>Ryan Faulkner</name>
    </author>
    <author>
      <name>Caglar Gulcehre</name>
    </author>
    <author>
      <name>Francis Song</name>
    </author>
    <author>
      <name>Andrew Ballard</name>
    </author>
    <author>
      <name>Justin Gilmer</name>
    </author>
    <author>
      <name>George Dahl</name>
    </author>
    <author>
      <name>Ashish Vaswani</name>
    </author>
    <author>
      <name>Kelsey Allen</name>
    </author>
    <author>
      <name>Charles Nash</name>
    </author>
    <author>
      <name>Victoria Langston</name>
    </author>
    <author>
      <name>Chris Dyer</name>
    </author>
    <author>
      <name>Nicolas Heess</name>
    </author>
    <author>
      <name>Daan Wierstra</name>
    </author>
    <author>
      <name>Pushmeet Kohli</name>
    </author>
    <author>
      <name>Matt Botvinick</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Yujia Li</name>
    </author>
    <author>
      <name>Razvan Pascanu</name>
    </author>
    <link href="http://arxiv.org/abs/1806.01261v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01261v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.07572v4</id>
    <updated>2020-02-10T08:39:09Z</updated>
    <published>2018-06-20T06:35:46Z</published>
    <title>Neural Tangent Kernel: Convergence and Generalization in Neural Networks</title>
    <summary>  At initialization, artificial neural networks (ANNs) are equivalent to
Gaussian processes in the infinite-width limit, thus connecting them to kernel
methods. We prove that the evolution of an ANN during training can also be
described by a kernel: during gradient descent on the parameters of an ANN, the
network function $f_\theta$ (which maps input vectors to output vectors)
follows the kernel gradient of the functional cost (which is convex, in
contrast to the parameter cost) w.r.t. a new kernel: the Neural Tangent Kernel
(NTK). This kernel is central to describe the generalization features of ANNs.
While the NTK is random at initialization and varies during training, in the
infinite-width limit it converges to an explicit limiting kernel and it stays
constant during training. This makes it possible to study the training of ANNs
in function space instead of parameter space. Convergence of the training can
then be related to the positive-definiteness of the limiting NTK. We prove the
positive-definiteness of the limiting NTK when the data is supported on the
sphere and the non-linearity is non-polynomial. We then focus on the setting of
least-squares regression and show that in the infinite-width limit, the network
function $f_\theta$ follows a linear differential equation during training. The
convergence is fastest along the largest kernel principal components of the
input data with respect to the NTK, hence suggesting a theoretical motivation
for early stopping. Finally we study the NTK numerically, observe its behavior
for wide networks, and compare it to the infinite-width limit.
</summary>
    <author>
      <name>Arthur Jacot</name>
    </author>
    <author>
      <name>Franck Gabriel</name>
    </author>
    <author>
      <name>Clément Hongler</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Advances in neural information processing systems (pp.
  8571-8580) 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1806.07572v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.07572v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.09594v2</id>
    <updated>2018-07-27T22:38:39Z</updated>
    <published>2018-06-25T17:44:40Z</published>
    <title>Tracking Emerges by Colorizing Videos</title>
    <summary>  We use large amounts of unlabeled video to learn models for visual tracking
without manual human supervision. We leverage the natural temporal coherency of
color to create a model that learns to colorize gray-scale videos by copying
colors from a reference frame. Quantitative and qualitative experiments suggest
that this task causes the model to automatically learn to track visual regions.
Although the model is trained without any ground-truth labels, our method
learns to track well enough to outperform the latest methods based on optical
flow. Moreover, our results suggest that failures to track are correlated with
failures to colorize, indicating that advancing video colorization may further
improve self-supervised visual tracking.
</summary>
    <author>
      <name>Carl Vondrick</name>
    </author>
    <author>
      <name>Abhinav Shrivastava</name>
    </author>
    <author>
      <name>Alireza Fathi</name>
    </author>
    <author>
      <name>Sergio Guadarrama</name>
    </author>
    <author>
      <name>Kevin Murphy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ECCV 2018. Blog post:
  https://ai.googleblog.com/2018/06/self-supervised-tracking-via-video.html</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.09594v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.09594v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.03748v2</id>
    <updated>2019-01-22T18:47:12Z</updated>
    <published>2018-07-10T16:52:11Z</published>
    <title>Representation Learning with Contrastive Predictive Coding</title>
    <summary>  While supervised learning has enabled great progress in many applications,
unsupervised learning has not seen such widespread adoption, and remains an
important and challenging endeavor for artificial intelligence. In this work,
we propose a universal unsupervised learning approach to extract useful
representations from high-dimensional data, which we call Contrastive
Predictive Coding. The key insight of our model is to learn such
representations by predicting the future in latent space by using powerful
autoregressive models. We use a probabilistic contrastive loss which induces
the latent space to capture information that is maximally useful to predict
future samples. It also makes the model tractable by using negative sampling.
While most prior work has focused on evaluating representations for a
particular modality, we demonstrate that our approach is able to learn useful
representations achieving strong performance on four distinct domains: speech,
images, text and reinforcement learning in 3D environments.
</summary>
    <author>
      <name>Aaron van den Oord</name>
    </author>
    <author>
      <name>Yazhe Li</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <link href="http://arxiv.org/abs/1807.03748v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03748v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.04805v2</id>
    <updated>2019-05-24T20:37:26Z</updated>
    <published>2018-10-11T00:50:01Z</published>
    <title>BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding</title>
    <summary>  We introduce a new language representation model called BERT, which stands
for Bidirectional Encoder Representations from Transformers. Unlike recent
language representation models, BERT is designed to pre-train deep
bidirectional representations from unlabeled text by jointly conditioning on
both left and right context in all layers. As a result, the pre-trained BERT
model can be fine-tuned with just one additional output layer to create
state-of-the-art models for a wide range of tasks, such as question answering
and language inference, without substantial task-specific architecture
modifications.
  BERT is conceptually simple and empirically powerful. It obtains new
state-of-the-art results on eleven natural language processing tasks, including
pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI
accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering
Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1
(5.1 point absolute improvement).
</summary>
    <author>
      <name>Jacob Devlin</name>
    </author>
    <author>
      <name>Ming-Wei Chang</name>
    </author>
    <author>
      <name>Kenton Lee</name>
    </author>
    <author>
      <name>Kristina Toutanova</name>
    </author>
    <link href="http://arxiv.org/abs/1810.04805v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.04805v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.04805v2</id>
    <updated>2019-05-24T20:37:26Z</updated>
    <published>2018-10-11T00:50:01Z</published>
    <title>BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding</title>
    <summary>  We introduce a new language representation model called BERT, which stands
for Bidirectional Encoder Representations from Transformers. Unlike recent
language representation models, BERT is designed to pre-train deep
bidirectional representations from unlabeled text by jointly conditioning on
both left and right context in all layers. As a result, the pre-trained BERT
model can be fine-tuned with just one additional output layer to create
state-of-the-art models for a wide range of tasks, such as question answering
and language inference, without substantial task-specific architecture
modifications.
  BERT is conceptually simple and empirically powerful. It obtains new
state-of-the-art results on eleven natural language processing tasks, including
pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI
accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering
Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1
(5.1 point absolute improvement).
</summary>
    <author>
      <name>Jacob Devlin</name>
    </author>
    <author>
      <name>Ming-Wei Chang</name>
    </author>
    <author>
      <name>Kenton Lee</name>
    </author>
    <author>
      <name>Kristina Toutanova</name>
    </author>
    <link href="http://arxiv.org/abs/1810.04805v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.04805v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.03828v2</id>
    <updated>2019-04-30T14:43:13Z</updated>
    <published>2018-12-10T14:36:52Z</published>
    <title>Occupancy Networks: Learning 3D Reconstruction in Function Space</title>
    <summary>  With the advent of deep neural networks, learning-based approaches for 3D
reconstruction have gained popularity. However, unlike for images, in 3D there
is no canonical representation which is both computationally and memory
efficient yet allows for representing high-resolution geometry of arbitrary
topology. Many of the state-of-the-art learning-based 3D reconstruction
approaches can hence only represent very coarse 3D geometry or are limited to a
restricted domain. In this paper, we propose Occupancy Networks, a new
representation for learning-based 3D reconstruction methods. Occupancy networks
implicitly represent the 3D surface as the continuous decision boundary of a
deep neural network classifier. In contrast to existing approaches, our
representation encodes a description of the 3D output at infinite resolution
without excessive memory footprint. We validate that our representation can
efficiently encode 3D structure and can be inferred from various kinds of
input. Our experiments demonstrate competitive results, both qualitatively and
quantitatively, for the challenging tasks of 3D reconstruction from single
images, noisy point clouds and coarse discrete voxel grids. We believe that
occupancy networks will become a useful tool in a wide variety of
learning-based 3D tasks.
</summary>
    <author>
      <name>Lars Mescheder</name>
    </author>
    <author>
      <name>Michael Oechsle</name>
    </author>
    <author>
      <name>Michael Niemeyer</name>
    </author>
    <author>
      <name>Sebastian Nowozin</name>
    </author>
    <author>
      <name>Andreas Geiger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be presented at CVPR 2019. Supplementary material and code is
  available at http://avg.is.tuebingen.mpg.de/publications/occupancy-networks</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.03828v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.03828v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.00446v1</id>
    <updated>2019-06-02T16:46:42Z</updated>
    <published>2019-06-02T16:46:42Z</published>
    <title>Generating Diverse High-Fidelity Images with VQ-VAE-2</title>
    <summary>  We explore the use of Vector Quantized Variational AutoEncoder (VQ-VAE)
models for large scale image generation. To this end, we scale and enhance the
autoregressive priors used in VQ-VAE to generate synthetic samples of much
higher coherence and fidelity than possible before. We use simple feed-forward
encoder and decoder networks, making our model an attractive candidate for
applications where the encoding and/or decoding speed is critical.
Additionally, VQ-VAE requires sampling an autoregressive model only in the
compressed latent space, which is an order of magnitude faster than sampling in
the pixel space, especially for large images. We demonstrate that a multi-scale
hierarchical organization of VQ-VAE, augmented with powerful priors over the
latent codes, is able to generate samples with quality that rivals that of
state of the art Generative Adversarial Networks on multifaceted datasets such
as ImageNet, while not suffering from GAN's known shortcomings such as mode
collapse and lack of diversity.
</summary>
    <author>
      <name>Ali Razavi</name>
    </author>
    <author>
      <name>Aaron van den Oord</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <link href="http://arxiv.org/abs/1906.00446v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.00446v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.05355v5</id>
    <updated>2020-12-11T04:06:17Z</updated>
    <published>2019-08-14T21:23:40Z</published>
    <title>The generalization error of random features regression: Precise
  asymptotics and double descent curve</title>
    <summary>  Deep learning methods operate in regimes that defy the traditional
statistical mindset. Neural network architectures often contain more parameters
than training samples, and are so rich that they can interpolate the observed
labels, even if the latter are replaced by pure noise. Despite their huge
complexity, the same architectures achieve small generalization error on real
data.
  This phenomenon has been rationalized in terms of a so-called `double
descent' curve. As the model complexity increases, the test error follows the
usual U-shaped curve at the beginning, first decreasing and then peaking around
the interpolation threshold (when the model achieves vanishing training error).
However, it descends again as model complexity exceeds this threshold. The
global minimum of the test error is found above the interpolation threshold,
often in the extreme overparametrization regime in which the number of
parameters is much larger than the number of samples. Far from being a peculiar
property of deep neural networks, elements of this behavior have been
demonstrated in much simpler settings, including linear regression with random
covariates.
  In this paper we consider the problem of learning an unknown function over
the $d$-dimensional sphere $\mathbb S^{d-1}$, from $n$ i.i.d. samples
$(\boldsymbol x_i, y_i)\in \mathbb S^{d-1} \times \mathbb R$, $i\le n$. We
perform ridge regression on $N$ random features of the form $\sigma(\boldsymbol
w_a^{\mathsf T} \boldsymbol x)$, $a\le N$. This can be equivalently described
as a two-layers neural network with random first-layer weights. We compute the
precise asymptotics of the test error, in the limit $N,n,d\to \infty$ with
$N/d$ and $n/d$ fixed. This provides the first analytically tractable model
that captures all the features of the double descent phenomenon without
assuming ad hoc misspecification structures.
</summary>
    <author>
      <name>Song Mei</name>
    </author>
    <author>
      <name>Andrea Montanari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">We reorganized the proofs of the main theorem</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.05355v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.05355v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62J99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.02054v3</id>
    <updated>2020-05-13T06:45:15Z</updated>
    <published>2019-10-04T17:29:39Z</published>
    <title>ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</title>
    <summary>  Large deep learning models offer significant accuracy gains, but training
billions to trillions of parameters is challenging. Existing solutions such as
data and model parallelisms exhibit fundamental limitations to fit these models
into limited device memory, while obtaining computation, communication and
development efficiency. We develop a novel solution, Zero Redundancy Optimizer
(ZeRO), to optimize memory, vastly improving training speed while increasing
the model size that can be efficiently trained. ZeRO eliminates memory
redundancies in data- and model-parallel training while retaining low
communication volume and high computational granularity, allowing us to scale
the model size proportional to the number of devices with sustained high
efficiency. Our analysis on memory requirements and communication volume
demonstrates: ZeRO has the potential to scale beyond 1 Trillion parameters
using today's hardware.
  We implement and evaluate ZeRO: it trains large models of over 100B parameter
with super-linear speedup on 400 GPUs, achieving throughput of 15 Petaflops.
This represents an 8x increase in model size and 10x increase in achievable
performance over state-of-the-art. In terms of usability, ZeRO can train large
models of up to 13B parameters (e.g., larger than Megatron GPT 8.3B and T5 11B)
without requiring model parallelism which is harder for scientists to apply.
Last but not the least, researchers have used the system breakthroughs of ZeRO
to create the world's largest language model (Turing-NLG, 17B parameters) with
record breaking accuracy.
</summary>
    <author>
      <name>Samyam Rajbhandari</name>
    </author>
    <author>
      <name>Jeff Rasley</name>
    </author>
    <author>
      <name>Olatunji Ruwase</name>
    </author>
    <author>
      <name>Yuxiong He</name>
    </author>
    <link href="http://arxiv.org/abs/1910.02054v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.02054v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.01547v2</id>
    <updated>2019-11-25T13:02:04Z</updated>
    <published>2019-11-05T00:31:38Z</published>
    <title>On the Measure of Intelligence</title>
    <summary>  To make deliberate progress towards more intelligent and more human-like
artificial systems, we need to be following an appropriate feedback signal: we
need to be able to define and evaluate intelligence in a way that enables
comparisons between two systems, as well as comparisons with humans. Over the
past hundred years, there has been an abundance of attempts to define and
measure intelligence, across both the fields of psychology and AI. We summarize
and critically assess these definitions and evaluation approaches, while making
apparent the two historical conceptions of intelligence that have implicitly
guided them. We note that in practice, the contemporary AI community still
gravitates towards benchmarking intelligence by comparing the skill exhibited
by AIs and humans at specific tasks such as board games and video games. We
argue that solely measuring skill at any given task falls short of measuring
intelligence, because skill is heavily modulated by prior knowledge and
experience: unlimited priors or unlimited training data allow experimenters to
"buy" arbitrary levels of skills for a system, in a way that masks the system's
own generalization power. We then articulate a new formal definition of
intelligence based on Algorithmic Information Theory, describing intelligence
as skill-acquisition efficiency and highlighting the concepts of scope,
generalization difficulty, priors, and experience. Using this definition, we
propose a set of guidelines for what a general AI benchmark should look like.
Finally, we present a benchmark closely following these guidelines, the
Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors
designed to be as close as possible to innate human priors. We argue that ARC
can be used to measure a human-like form of general fluid intelligence and that
it enables fair general intelligence comparisons between AI systems and humans.
</summary>
    <author>
      <name>François Chollet</name>
    </author>
    <link href="http://arxiv.org/abs/1911.01547v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.01547v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.02292v1</id>
    <updated>2019-12-04T22:47:31Z</updated>
    <published>2019-12-04T22:47:31Z</published>
    <title>Deep Double Descent: Where Bigger Models and More Data Hurt</title>
    <summary>  We show that a variety of modern deep learning tasks exhibit a
"double-descent" phenomenon where, as we increase model size, performance first
gets worse and then gets better. Moreover, we show that double descent occurs
not just as a function of model size, but also as a function of the number of
training epochs. We unify the above phenomena by defining a new complexity
measure we call the effective model complexity and conjecture a generalized
double descent with respect to this measure. Furthermore, our notion of model
complexity allows us to identify certain regimes where increasing (even
quadrupling) the number of train samples actually hurts test performance.
</summary>
    <author>
      <name>Preetum Nakkiran</name>
    </author>
    <author>
      <name>Gal Kaplun</name>
    </author>
    <author>
      <name>Yamini Bansal</name>
    </author>
    <author>
      <name>Tristan Yang</name>
    </author>
    <author>
      <name>Boaz Barak</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">G.K. and Y.B. contributed equally</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.02292v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.02292v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.08934v2</id>
    <updated>2020-08-03T22:17:31Z</updated>
    <published>2020-03-19T17:57:23Z</published>
    <title>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</title>
    <summary>  We present a method that achieves state-of-the-art results for synthesizing
novel views of complex scenes by optimizing an underlying continuous volumetric
scene function using a sparse set of input views. Our algorithm represents a
scene using a fully-connected (non-convolutional) deep network, whose input is
a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing
direction $(\theta, \phi)$) and whose output is the volume density and
view-dependent emitted radiance at that spatial location. We synthesize views
by querying 5D coordinates along camera rays and use classic volume rendering
techniques to project the output colors and densities into an image. Because
volume rendering is naturally differentiable, the only input required to
optimize our representation is a set of images with known camera poses. We
describe how to effectively optimize neural radiance fields to render
photorealistic novel views of scenes with complicated geometry and appearance,
and demonstrate results that outperform prior work on neural rendering and view
synthesis. View synthesis results are best viewed as videos, so we urge readers
to view our supplementary video for convincing comparisons.
</summary>
    <author>
      <name>Ben Mildenhall</name>
    </author>
    <author>
      <name>Pratul P. Srinivasan</name>
    </author>
    <author>
      <name>Matthew Tancik</name>
    </author>
    <author>
      <name>Jonathan T. Barron</name>
    </author>
    <author>
      <name>Ravi Ramamoorthi</name>
    </author>
    <author>
      <name>Ren Ng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ECCV 2020 (oral). Project page with videos and code:
  http://tancik.com/nerf</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.08934v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.08934v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.14165v4</id>
    <updated>2020-07-22T19:47:17Z</updated>
    <published>2020-05-28T17:29:03Z</published>
    <title>Language Models are Few-Shot Learners</title>
    <summary>  Recent work has demonstrated substantial gains on many NLP tasks and
benchmarks by pre-training on a large corpus of text followed by fine-tuning on
a specific task. While typically task-agnostic in architecture, this method
still requires task-specific fine-tuning datasets of thousands or tens of
thousands of examples. By contrast, humans can generally perform a new language
task from only a few examples or from simple instructions - something which
current NLP systems still largely struggle to do. Here we show that scaling up
language models greatly improves task-agnostic, few-shot performance, sometimes
even reaching competitiveness with prior state-of-the-art fine-tuning
approaches. Specifically, we train GPT-3, an autoregressive language model with
175 billion parameters, 10x more than any previous non-sparse language model,
and test its performance in the few-shot setting. For all tasks, GPT-3 is
applied without any gradient updates or fine-tuning, with tasks and few-shot
demonstrations specified purely via text interaction with the model. GPT-3
achieves strong performance on many NLP datasets, including translation,
question-answering, and cloze tasks, as well as several tasks that require
on-the-fly reasoning or domain adaptation, such as unscrambling words, using a
novel word in a sentence, or performing 3-digit arithmetic. At the same time,
we also identify some datasets where GPT-3's few-shot learning still struggles,
as well as some datasets where GPT-3 faces methodological issues related to
training on large web corpora. Finally, we find that GPT-3 can generate samples
of news articles which human evaluators have difficulty distinguishing from
articles written by humans. We discuss broader societal impacts of this finding
and of GPT-3 in general.
</summary>
    <author>
      <name>Tom B. Brown</name>
    </author>
    <author>
      <name>Benjamin Mann</name>
    </author>
    <author>
      <name>Nick Ryder</name>
    </author>
    <author>
      <name>Melanie Subbiah</name>
    </author>
    <author>
      <name>Jared Kaplan</name>
    </author>
    <author>
      <name>Prafulla Dhariwal</name>
    </author>
    <author>
      <name>Arvind Neelakantan</name>
    </author>
    <author>
      <name>Pranav Shyam</name>
    </author>
    <author>
      <name>Girish Sastry</name>
    </author>
    <author>
      <name>Amanda Askell</name>
    </author>
    <author>
      <name>Sandhini Agarwal</name>
    </author>
    <author>
      <name>Ariel Herbert-Voss</name>
    </author>
    <author>
      <name>Gretchen Krueger</name>
    </author>
    <author>
      <name>Tom Henighan</name>
    </author>
    <author>
      <name>Rewon Child</name>
    </author>
    <author>
      <name>Aditya Ramesh</name>
    </author>
    <author>
      <name>Daniel M. Ziegler</name>
    </author>
    <author>
      <name>Jeffrey Wu</name>
    </author>
    <author>
      <name>Clemens Winter</name>
    </author>
    <author>
      <name>Christopher Hesse</name>
    </author>
    <author>
      <name>Mark Chen</name>
    </author>
    <author>
      <name>Eric Sigler</name>
    </author>
    <author>
      <name>Mateusz Litwin</name>
    </author>
    <author>
      <name>Scott Gray</name>
    </author>
    <author>
      <name>Benjamin Chess</name>
    </author>
    <author>
      <name>Jack Clark</name>
    </author>
    <author>
      <name>Christopher Berner</name>
    </author>
    <author>
      <name>Sam McCandlish</name>
    </author>
    <author>
      <name>Alec Radford</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Dario Amodei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">40+32 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.14165v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.14165v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.09661v1</id>
    <updated>2020-06-17T05:13:33Z</updated>
    <published>2020-06-17T05:13:33Z</published>
    <title>Implicit Neural Representations with Periodic Activation Functions</title>
    <summary>  Implicitly defined, continuous, differentiable signal representations
parameterized by neural networks have emerged as a powerful paradigm, offering
many possible benefits over conventional representations. However, current
network architectures for such implicit neural representations are incapable of
modeling signals with fine detail, and fail to represent a signal's spatial and
temporal derivatives, despite the fact that these are essential to many
physical signals defined implicitly as the solution to partial differential
equations. We propose to leverage periodic activation functions for implicit
neural representations and demonstrate that these networks, dubbed sinusoidal
representation networks or Sirens, are ideally suited for representing complex
natural signals and their derivatives. We analyze Siren activation statistics
to propose a principled initialization scheme and demonstrate the
representation of images, wavefields, video, sound, and their derivatives.
Further, we show how Sirens can be leveraged to solve challenging boundary
value problems, such as particular Eikonal equations (yielding signed distance
functions), the Poisson equation, and the Helmholtz and wave equations. Lastly,
we combine Sirens with hypernetworks to learn priors over the space of Siren
functions.
</summary>
    <author>
      <name>Vincent Sitzmann</name>
    </author>
    <author>
      <name>Julien N. P. Martel</name>
    </author>
    <author>
      <name>Alexander W. Bergman</name>
    </author>
    <author>
      <name>David B. Lindell</name>
    </author>
    <author>
      <name>Gordon Wetzstein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project website: https://vsitzmann.github.io/siren/ Project video:
  https://youtu.be/Q2fLWGBeaiI</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.09661v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.09661v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.10739v1</id>
    <updated>2020-06-18T17:59:11Z</updated>
    <published>2020-06-18T17:59:11Z</published>
    <title>Fourier Features Let Networks Learn High Frequency Functions in Low
  Dimensional Domains</title>
    <summary>  We show that passing input points through a simple Fourier feature mapping
enables a multilayer perceptron (MLP) to learn high-frequency functions in
low-dimensional problem domains. These results shed light on recent advances in
computer vision and graphics that achieve state-of-the-art results by using
MLPs to represent complex 3D objects and scenes. Using tools from the neural
tangent kernel (NTK) literature, we show that a standard MLP fails to learn
high frequencies both in theory and in practice. To overcome this spectral
bias, we use a Fourier feature mapping to transform the effective NTK into a
stationary kernel with a tunable bandwidth. We suggest an approach for
selecting problem-specific Fourier features that greatly improves the
performance of MLPs for low-dimensional regression tasks relevant to the
computer vision and graphics communities.
</summary>
    <author>
      <name>Matthew Tancik</name>
    </author>
    <author>
      <name>Pratul P. Srinivasan</name>
    </author>
    <author>
      <name>Ben Mildenhall</name>
    </author>
    <author>
      <name>Sara Fridovich-Keil</name>
    </author>
    <author>
      <name>Nithin Raghavan</name>
    </author>
    <author>
      <name>Utkarsh Singhal</name>
    </author>
    <author>
      <name>Ravi Ramamoorthi</name>
    </author>
    <author>
      <name>Jonathan T. Barron</name>
    </author>
    <author>
      <name>Ren Ng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page: https://people.eecs.berkeley.edu/~bmild/fourfeat/</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.10739v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.10739v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.11239v2</id>
    <updated>2020-12-16T21:15:05Z</updated>
    <published>2020-06-19T17:24:44Z</published>
    <title>Denoising Diffusion Probabilistic Models</title>
    <summary>  We present high quality image synthesis results using diffusion probabilistic
models, a class of latent variable models inspired by considerations from
nonequilibrium thermodynamics. Our best results are obtained by training on a
weighted variational bound designed according to a novel connection between
diffusion probabilistic models and denoising score matching with Langevin
dynamics, and our models naturally admit a progressive lossy decompression
scheme that can be interpreted as a generalization of autoregressive decoding.
On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and
a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality
similar to ProgressiveGAN. Our implementation is available at
https://github.com/hojonathanho/diffusion
</summary>
    <author>
      <name>Jonathan Ho</name>
    </author>
    <author>
      <name>Ajay Jain</name>
    </author>
    <author>
      <name>Pieter Abbeel</name>
    </author>
    <link href="http://arxiv.org/abs/2006.11239v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.11239v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.11929v2</id>
    <updated>2021-06-03T13:08:56Z</updated>
    <published>2020-10-22T17:55:59Z</published>
    <title>An Image is Worth 16x16 Words: Transformers for Image Recognition at
  Scale</title>
    <summary>  While the Transformer architecture has become the de-facto standard for
natural language processing tasks, its applications to computer vision remain
limited. In vision, attention is either applied in conjunction with
convolutional networks, or used to replace certain components of convolutional
networks while keeping their overall structure in place. We show that this
reliance on CNNs is not necessary and a pure transformer applied directly to
sequences of image patches can perform very well on image classification tasks.
When pre-trained on large amounts of data and transferred to multiple mid-sized
or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision
Transformer (ViT) attains excellent results compared to state-of-the-art
convolutional networks while requiring substantially fewer computational
resources to train.
</summary>
    <author>
      <name>Alexey Dosovitskiy</name>
    </author>
    <author>
      <name>Lucas Beyer</name>
    </author>
    <author>
      <name>Alexander Kolesnikov</name>
    </author>
    <author>
      <name>Dirk Weissenborn</name>
    </author>
    <author>
      <name>Xiaohua Zhai</name>
    </author>
    <author>
      <name>Thomas Unterthiner</name>
    </author>
    <author>
      <name>Mostafa Dehghani</name>
    </author>
    <author>
      <name>Matthias Minderer</name>
    </author>
    <author>
      <name>Georg Heigold</name>
    </author>
    <author>
      <name>Sylvain Gelly</name>
    </author>
    <author>
      <name>Jakob Uszkoreit</name>
    </author>
    <author>
      <name>Neil Houlsby</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Fine-tuning code and pre-trained models are available at
  https://github.com/google-research/vision_transformer. ICLR camera-ready
  version with 2 small modifications: 1) Added a discussion of CLS vs GAP
  classifier in the appendix, 2) Fixed an error in exaFLOPs computation in
  Figure 5 and Table 6 (relative performance of models is basically not
  affected)</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.11929v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.11929v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.01808v1</id>
    <updated>2020-11-03T15:59:50Z</updated>
    <published>2020-11-03T15:59:50Z</published>
    <title>Bayesian Workflow</title>
    <summary>  The Bayesian approach to data analysis provides a powerful way to handle
uncertainty in all observations, model parameters, and model structure using
probability theory. Probabilistic programming languages make it easier to
specify and fit Bayesian models, but this still leaves us with many options
regarding constructing, evaluating, and using these models, along with many
remaining challenges in computation. Using Bayesian inference to solve
real-world problems requires not only statistical skills, subject matter
knowledge, and programming, but also awareness of the decisions made in the
process of data analysis. All of these aspects can be understood as part of a
tangled workflow of applied Bayesian statistics. Beyond inference, the workflow
also includes iterative model building, model checking, validation and
troubleshooting of computational problems, model understanding, and model
comparison. We review all these aspects of workflow in the context of several
examples, keeping in mind that in practice we will be fitting many models for
any given problem, even if only a subset of them will ultimately be relevant
for our conclusions.
</summary>
    <author>
      <name>Andrew Gelman</name>
    </author>
    <author>
      <name>Aki Vehtari</name>
    </author>
    <author>
      <name>Daniel Simpson</name>
    </author>
    <author>
      <name>Charles C. Margossian</name>
    </author>
    <author>
      <name>Bob Carpenter</name>
    </author>
    <author>
      <name>Yuling Yao</name>
    </author>
    <author>
      <name>Lauren Kennedy</name>
    </author>
    <author>
      <name>Jonah Gabry</name>
    </author>
    <author>
      <name>Paul-Christian Bürkner</name>
    </author>
    <author>
      <name>Martin Modrák</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">77 pages, 35 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.01808v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.01808v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.09841v3</id>
    <updated>2021-06-23T16:07:21Z</updated>
    <published>2020-12-17T18:57:28Z</published>
    <title>Taming Transformers for High-Resolution Image Synthesis</title>
    <summary>  Designed to learn long-range interactions on sequential data, transformers
continue to show state-of-the-art results on a wide variety of tasks. In
contrast to CNNs, they contain no inductive bias that prioritizes local
interactions. This makes them expressive, but also computationally infeasible
for long sequences, such as high-resolution images. We demonstrate how
combining the effectiveness of the inductive bias of CNNs with the expressivity
of transformers enables them to model and thereby synthesize high-resolution
images. We show how to (i) use CNNs to learn a context-rich vocabulary of image
constituents, and in turn (ii) utilize transformers to efficiently model their
composition within high-resolution images. Our approach is readily applied to
conditional synthesis tasks, where both non-spatial information, such as object
classes, and spatial information, such as segmentations, can control the
generated image. In particular, we present the first results on
semantically-guided synthesis of megapixel images with transformers and obtain
the state of the art among autoregressive models on class-conditional ImageNet.
Code and pretrained models can be found at
https://github.com/CompVis/taming-transformers .
</summary>
    <author>
      <name>Patrick Esser</name>
    </author>
    <author>
      <name>Robin Rombach</name>
    </author>
    <author>
      <name>Björn Ommer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Changelog can be found in the supplementary</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.09841v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.09841v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.13255v1</id>
    <updated>2020-12-22T07:42:30Z</updated>
    <published>2020-12-22T07:42:30Z</published>
    <title>Intrinsic Dimensionality Explains the Effectiveness of Language Model
  Fine-Tuning</title>
    <summary>  Although pretrained language models can be fine-tuned to produce
state-of-the-art results for a very wide range of language understanding tasks,
the dynamics of this process are not well understood, especially in the low
data regime. Why can we use relatively vanilla gradient descent algorithms
(e.g., without strong regularization) to tune a model with hundreds of millions
of parameters on datasets with only hundreds or thousands of labeled examples?
In this paper, we argue that analyzing fine-tuning through the lens of
intrinsic dimension provides us with empirical and theoretical intuitions to
explain this remarkable phenomenon. We empirically show that common pre-trained
models have a very low intrinsic dimension; in other words, there exists a low
dimension reparameterization that is as effective for fine-tuning as the full
parameter space. For example, by optimizing only 200 trainable parameters
randomly projected back into the full space, we can tune a RoBERTa model to
achieve 90\% of the full parameter performance levels on MRPC. Furthermore, we
empirically show that pre-training implicitly minimizes intrinsic dimension
and, perhaps surprisingly, larger models tend to have lower intrinsic dimension
after a fixed number of pre-training updates, at least in part explaining their
extreme effectiveness. Lastly, we connect intrinsic dimensionality with low
dimensional task representations and compression based generalization bounds to
provide intrinsic-dimension-based generalization bounds that are independent of
the full parameter count.
</summary>
    <author>
      <name>Armen Aghajanyan</name>
    </author>
    <author>
      <name>Luke Zettlemoyer</name>
    </author>
    <author>
      <name>Sonal Gupta</name>
    </author>
    <link href="http://arxiv.org/abs/2012.13255v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.13255v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.00020v1</id>
    <updated>2021-02-26T19:04:58Z</updated>
    <published>2021-02-26T19:04:58Z</published>
    <title>Learning Transferable Visual Models From Natural Language Supervision</title>
    <summary>  State-of-the-art computer vision systems are trained to predict a fixed set
of predetermined object categories. This restricted form of supervision limits
their generality and usability since additional labeled data is needed to
specify any other visual concept. Learning directly from raw text about images
is a promising alternative which leverages a much broader source of
supervision. We demonstrate that the simple pre-training task of predicting
which caption goes with which image is an efficient and scalable way to learn
SOTA image representations from scratch on a dataset of 400 million (image,
text) pairs collected from the internet. After pre-training, natural language
is used to reference learned visual concepts (or describe new ones) enabling
zero-shot transfer of the model to downstream tasks. We study the performance
of this approach by benchmarking on over 30 different existing computer vision
datasets, spanning tasks such as OCR, action recognition in videos,
geo-localization, and many types of fine-grained object classification. The
model transfers non-trivially to most tasks and is often competitive with a
fully supervised baseline without the need for any dataset specific training.
For instance, we match the accuracy of the original ResNet-50 on ImageNet
zero-shot without needing to use any of the 1.28 million training examples it
was trained on. We release our code and pre-trained model weights at
https://github.com/OpenAI/CLIP.
</summary>
    <author>
      <name>Alec Radford</name>
    </author>
    <author>
      <name>Jong Wook Kim</name>
    </author>
    <author>
      <name>Chris Hallacy</name>
    </author>
    <author>
      <name>Aditya Ramesh</name>
    </author>
    <author>
      <name>Gabriel Goh</name>
    </author>
    <author>
      <name>Sandhini Agarwal</name>
    </author>
    <author>
      <name>Girish Sastry</name>
    </author>
    <author>
      <name>Amanda Askell</name>
    </author>
    <author>
      <name>Pamela Mishkin</name>
    </author>
    <author>
      <name>Jack Clark</name>
    </author>
    <author>
      <name>Gretchen Krueger</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <link href="http://arxiv.org/abs/2103.00020v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.00020v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.00564v1</id>
    <updated>2021-02-28T16:57:41Z</updated>
    <published>2021-02-28T16:57:41Z</published>
    <title>An Introduction to Johnson-Lindenstrauss Transforms</title>
    <summary>  Johnson--Lindenstrauss Transforms are powerful tools for reducing the
dimensionality of data while preserving key characteristics of that data, and
they have found use in many fields from machine learning to differential
privacy and more. This note explains what they are; it gives an overview of
their use and their development since they were introduced in the 1980s; and it
provides many references should the reader wish to explore these topics more
deeply.
</summary>
    <author>
      <name>Casper Benjamin Freksen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The text was previously a main part of the introduction of my PhD
  thesis, but it has been adapted to be self contained and serve as a
  (hopefully good) starting point for readers interested in the topic</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.00564v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.00564v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; A.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.13478v2</id>
    <updated>2021-05-02T16:16:03Z</updated>
    <published>2021-04-27T21:09:51Z</published>
    <title>Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges</title>
    <summary>  The last decade has witnessed an experimental revolution in data science and
machine learning, epitomised by deep learning methods. Indeed, many
high-dimensional learning tasks previously thought to be beyond reach -- such
as computer vision, playing Go, or protein folding -- are in fact feasible with
appropriate computational scale. Remarkably, the essence of deep learning is
built from two simple algorithmic principles: first, the notion of
representation or feature learning, whereby adapted, often hierarchical,
features capture the appropriate notion of regularity for each task, and
second, learning by local gradient-descent type methods, typically implemented
as backpropagation.
  While learning generic functions in high dimensions is a cursed estimation
problem, most tasks of interest are not generic, and come with essential
pre-defined regularities arising from the underlying low-dimensionality and
structure of the physical world. This text is concerned with exposing these
regularities through unified geometric principles that can be applied
throughout a wide spectrum of applications.
  Such a 'geometric unification' endeavour, in the spirit of Felix Klein's
Erlangen Program, serves a dual purpose: on one hand, it provides a common
mathematical framework to study the most successful neural network
architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand,
it gives a constructive procedure to incorporate prior physical knowledge into
neural architectures and provide principled way to build future architectures
yet to be invented.
</summary>
    <author>
      <name>Michael M. Bronstein</name>
    </author>
    <author>
      <name>Joan Bruna</name>
    </author>
    <author>
      <name>Taco Cohen</name>
    </author>
    <author>
      <name>Petar Veličković</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">156 pages. Work in progress -- comments welcome!</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.13478v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.13478v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.14294v2</id>
    <updated>2021-05-24T17:49:18Z</updated>
    <published>2021-04-29T12:28:51Z</published>
    <title>Emerging Properties in Self-Supervised Vision Transformers</title>
    <summary>  In this paper, we question if self-supervised learning provides new
properties to Vision Transformer (ViT) that stand out compared to convolutional
networks (convnets). Beyond the fact that adapting self-supervised methods to
this architecture works particularly well, we make the following observations:
first, self-supervised ViT features contain explicit information about the
semantic segmentation of an image, which does not emerge as clearly with
supervised ViTs, nor with convnets. Second, these features are also excellent
k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study
also underlines the importance of momentum encoder, multi-crop training, and
the use of small patches with ViTs. We implement our findings into a simple
self-supervised method, called DINO, which we interpret as a form of
self-distillation with no labels. We show the synergy between DINO and ViTs by
achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base.
</summary>
    <author>
      <name>Mathilde Caron</name>
    </author>
    <author>
      <name>Hugo Touvron</name>
    </author>
    <author>
      <name>Ishan Misra</name>
    </author>
    <author>
      <name>Hervé Jégou</name>
    </author>
    <author>
      <name>Julien Mairal</name>
    </author>
    <author>
      <name>Piotr Bojanowski</name>
    </author>
    <author>
      <name>Armand Joulin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.14294v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.14294v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.09685v2</id>
    <updated>2021-10-16T18:40:34Z</updated>
    <published>2021-06-17T17:37:18Z</published>
    <title>LoRA: Low-Rank Adaptation of Large Language Models</title>
    <summary>  An important paradigm of natural language processing consists of large-scale
pre-training on general domain data and adaptation to particular tasks or
domains. As we pre-train larger models, full fine-tuning, which retrains all
model parameters, becomes less feasible. Using GPT-3 175B as an example --
deploying independent instances of fine-tuned models, each with 175B
parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or
LoRA, which freezes the pre-trained model weights and injects trainable rank
decomposition matrices into each layer of the Transformer architecture, greatly
reducing the number of trainable parameters for downstream tasks. Compared to
GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable
parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA
performs on-par or better than fine-tuning in model quality on RoBERTa,
DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher
training throughput, and, unlike adapters, no additional inference latency. We
also provide an empirical investigation into rank-deficiency in language model
adaptation, which sheds light on the efficacy of LoRA. We release a package
that facilitates the integration of LoRA with PyTorch models and provide our
implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at
https://github.com/microsoft/LoRA.
</summary>
    <author>
      <name>Edward J. Hu</name>
    </author>
    <author>
      <name>Yelong Shen</name>
    </author>
    <author>
      <name>Phillip Wallis</name>
    </author>
    <author>
      <name>Zeyuan Allen-Zhu</name>
    </author>
    <author>
      <name>Yuanzhi Li</name>
    </author>
    <author>
      <name>Shean Wang</name>
    </author>
    <author>
      <name>Lu Wang</name>
    </author>
    <author>
      <name>Weizhu Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Draft V2 includes better baselines, experiments on GLUE, and more on
  adapter latency</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.09685v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.09685v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.01073v2</id>
    <updated>2022-01-05T00:07:35Z</updated>
    <published>2021-08-02T17:59:47Z</published>
    <title>SDEdit: Guided Image Synthesis and Editing with Stochastic Differential
  Equations</title>
    <summary>  Guided image synthesis enables everyday users to create and edit
photo-realistic images with minimum effort. The key challenge is balancing
faithfulness to the user input (e.g., hand-drawn colored strokes) and realism
of the synthesized image. Existing GAN-based methods attempt to achieve such
balance using either conditional GANs or GAN inversions, which are challenging
and often require additional training data or loss functions for individual
applications. To address these issues, we introduce a new image synthesis and
editing method, Stochastic Differential Editing (SDEdit), based on a diffusion
model generative prior, which synthesizes realistic images by iteratively
denoising through a stochastic differential equation (SDE). Given an input
image with user guide of any type, SDEdit first adds noise to the input, then
subsequently denoises the resulting image through the SDE prior to increase its
realism. SDEdit does not require task-specific training or inversions and can
naturally achieve the balance between realism and faithfulness. SDEdit
significantly outperforms state-of-the-art GAN-based methods by up to 98.09% on
realism and 91.72% on overall satisfaction scores, according to a human
perception study, on multiple tasks, including stroke-based image synthesis and
editing as well as image compositing.
</summary>
    <author>
      <name>Chenlin Meng</name>
    </author>
    <author>
      <name>Yutong He</name>
    </author>
    <author>
      <name>Yang Song</name>
    </author>
    <author>
      <name>Jiaming Song</name>
    </author>
    <author>
      <name>Jiajun Wu</name>
    </author>
    <author>
      <name>Jun-Yan Zhu</name>
    </author>
    <author>
      <name>Stefano Ermon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">https://sde-image-editing.github.io/</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.01073v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.01073v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.01652v5</id>
    <updated>2022-02-08T20:26:45Z</updated>
    <published>2021-09-03T17:55:52Z</published>
    <title>Finetuned Language Models Are Zero-Shot Learners</title>
    <summary>  This paper explores a simple method for improving the zero-shot learning
abilities of language models. We show that instruction tuning -- finetuning
language models on a collection of tasks described via instructions --
substantially improves zero-shot performance on unseen tasks.
  We take a 137B parameter pretrained language model and instruction-tune it on
over 60 NLP tasks verbalized via natural language instruction templates. We
evaluate this instruction-tuned model, which we call FLAN, on unseen task
types. FLAN substantially improves the performance of its unmodified
counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we
evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE,
BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number
of finetuning datasets, model scale, and natural language instructions are key
to the success of instruction tuning.
</summary>
    <author>
      <name>Jason Wei</name>
    </author>
    <author>
      <name>Maarten Bosma</name>
    </author>
    <author>
      <name>Vincent Y. Zhao</name>
    </author>
    <author>
      <name>Kelvin Guu</name>
    </author>
    <author>
      <name>Adams Wei Yu</name>
    </author>
    <author>
      <name>Brian Lester</name>
    </author>
    <author>
      <name>Nan Du</name>
    </author>
    <author>
      <name>Andrew M. Dai</name>
    </author>
    <author>
      <name>Quoc V. Le</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Version 5. Find list of changes in Appendix F (page 35)</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.01652v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.01652v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.02355v1</id>
    <updated>2021-09-06T10:48:40Z</updated>
    <published>2021-09-06T10:48:40Z</published>
    <title>A Farewell to the Bias-Variance Tradeoff? An Overview of the Theory of
  Overparameterized Machine Learning</title>
    <summary>  The rapid recent progress in machine learning (ML) has raised a number of
scientific questions that challenge the longstanding dogma of the field. One of
the most important riddles is the good empirical generalization of
overparameterized models. Overparameterized models are excessively complex with
respect to the size of the training dataset, which results in them perfectly
fitting (i.e., interpolating) the training data, which is usually noisy. Such
interpolation of noisy data is traditionally associated with detrimental
overfitting, and yet a wide range of interpolating models -- from simple linear
models to deep neural networks -- have recently been observed to generalize
extremely well on fresh test data. Indeed, the recently discovered double
descent phenomenon has revealed that highly overparameterized models often
improve over the best underparameterized model in test performance.
  Understanding learning in this overparameterized regime requires new theory
and foundational empirical studies, even for the simplest case of the linear
model. The underpinnings of this understanding have been laid in very recent
analyses of overparameterized linear regression and related statistical
learning tasks, which resulted in precise analytic characterizations of double
descent. This paper provides a succinct overview of this emerging theory of
overparameterized ML (henceforth abbreviated as TOPML) that explains these
recent findings through a statistical signal processing perspective. We
emphasize the unique aspects that define the TOPML research area as a subfield
of modern ML theory and outline interesting open questions that remain.
</summary>
    <author>
      <name>Yehuda Dar</name>
    </author>
    <author>
      <name>Vidya Muthukumar</name>
    </author>
    <author>
      <name>Richard G. Baraniuk</name>
    </author>
    <link href="http://arxiv.org/abs/2109.02355v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.02355v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.11978v3</id>
    <updated>2022-08-19T07:52:32Z</updated>
    <published>2021-09-24T14:04:19Z</published>
    <title>Learning to Walk in Minutes Using Massively Parallel Deep Reinforcement
  Learning</title>
    <summary>  In this work, we present and study a training set-up that achieves fast
policy generation for real-world robotic tasks by using massive parallelism on
a single workstation GPU. We analyze and discuss the impact of different
training algorithm components in the massively parallel regime on the final
policy performance and training times. In addition, we present a novel
game-inspired curriculum that is well suited for training with thousands of
simulated robots in parallel. We evaluate the approach by training the
quadrupedal robot ANYmal to walk on challenging terrain. The parallel approach
allows training policies for flat terrain in under four minutes, and in twenty
minutes for uneven terrain. This represents a speedup of multiple orders of
magnitude compared to previous work. Finally, we transfer the policies to the
real robot to validate the approach. We open-source our training code to help
accelerate further research in the field of learned legged locomotion.
</summary>
    <author>
      <name>Nikita Rudin</name>
    </author>
    <author>
      <name>David Hoeller</name>
    </author>
    <author>
      <name>Philipp Reist</name>
    </author>
    <author>
      <name>Marco Hutter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CoRL 2021 Project website: :
  https://leggedrobotics.github.io/legged_gym/ Video:
  https://youtu.be/8sO7VS3q8d0</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.11978v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.11978v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.13226v3</id>
    <updated>2022-07-21T18:43:03Z</updated>
    <published>2021-09-27T17:59:19Z</published>
    <title>BigSSL: Exploring the Frontier of Large-Scale Semi-Supervised Learning
  for Automatic Speech Recognition</title>
    <summary>  We summarize the results of a host of efforts using giant automatic speech
recognition (ASR) models pre-trained using large, diverse unlabeled datasets
containing approximately a million hours of audio. We find that the combination
of pre-training, self-training and scaling up model size greatly increases data
efficiency, even for extremely large tasks with tens of thousands of hours of
labeled data. In particular, on an ASR task with 34k hours of labeled data, by
fine-tuning an 8 billion parameter pre-trained Conformer model we can match
state-of-the-art (SoTA) performance with only 3% of the training data and
significantly improve SoTA with the full training set. We also report on the
universal benefits gained from using big pre-trained and self-trained models
for a large set of downstream tasks that cover a wide range of speech domains
and span multiple orders of magnitudes of dataset sizes, including obtaining
SoTA performance on many public benchmarks. In addition, we utilize the learned
representation of pre-trained networks to achieve SoTA results on non-ASR
tasks.
</summary>
    <author>
      <name>Yu Zhang</name>
    </author>
    <author>
      <name>Daniel S. Park</name>
    </author>
    <author>
      <name>Wei Han</name>
    </author>
    <author>
      <name>James Qin</name>
    </author>
    <author>
      <name>Anmol Gulati</name>
    </author>
    <author>
      <name>Joel Shor</name>
    </author>
    <author>
      <name>Aren Jansen</name>
    </author>
    <author>
      <name>Yuanzhong Xu</name>
    </author>
    <author>
      <name>Yanping Huang</name>
    </author>
    <author>
      <name>Shibo Wang</name>
    </author>
    <author>
      <name>Zongwei Zhou</name>
    </author>
    <author>
      <name>Bo Li</name>
    </author>
    <author>
      <name>Min Ma</name>
    </author>
    <author>
      <name>William Chan</name>
    </author>
    <author>
      <name>Jiahui Yu</name>
    </author>
    <author>
      <name>Yongqiang Wang</name>
    </author>
    <author>
      <name>Liangliang Cao</name>
    </author>
    <author>
      <name>Khe Chai Sim</name>
    </author>
    <author>
      <name>Bhuvana Ramabhadran</name>
    </author>
    <author>
      <name>Tara N. Sainath</name>
    </author>
    <author>
      <name>Françoise Beaufays</name>
    </author>
    <author>
      <name>Zhifeng Chen</name>
    </author>
    <author>
      <name>Quoc V. Le</name>
    </author>
    <author>
      <name>Chung-Cheng Chiu</name>
    </author>
    <author>
      <name>Ruoming Pang</name>
    </author>
    <author>
      <name>Yonghui Wu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/JSTSP.2022.3182537</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/JSTSP.2022.3182537" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 7 figures, 13 tables; v2: minor corrections, reference
  baselines and bibliography updated; v3: corrections based on reviewer
  feedback, bibliography updated</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.13226v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.13226v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.05069v3</id>
    <updated>2022-03-29T12:25:02Z</updated>
    <published>2021-10-11T08:07:50Z</published>
    <title>Efficient Training of Audio Transformers with Patchout</title>
    <summary>  The great success of transformer-based models in natural language processing
(NLP) has led to various attempts at adapting these architectures to other
domains such as vision and audio. Recent work has shown that transformers can
outperform Convolutional Neural Networks (CNNs) on vision and audio tasks.
However, one of the main shortcomings of transformer models, compared to the
well-established CNNs, is the computational complexity. In transformers, the
compute and memory complexity is known to grow quadratically with the input
length. Therefore, there has been extensive work on optimizing transformers,
but often at the cost of degrading predictive performance. In this work, we
propose a novel method to optimize and regularize transformers on audio
spectrograms. Our proposed models achieve a new state-of-the-art performance on
Audioset and can be trained on a single consumer-grade GPU. Furthermore, we
propose a transformer model that outperforms CNNs in terms of both performance
and training speed. Source code: https://github.com/kkoutini/PaSST
</summary>
    <author>
      <name>Khaled Koutini</name>
    </author>
    <author>
      <name>Jan Schlüter</name>
    </author>
    <author>
      <name>Hamid Eghbal-zadeh</name>
    </author>
    <author>
      <name>Gerhard Widmer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.21437/Interspeech.2022-227</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.21437/Interspeech.2022-227" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Interspeech 2022. Source code:
  https://github.com/kkoutini/PaSST</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.05069v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.05069v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.07205v3</id>
    <updated>2022-05-24T08:18:31Z</updated>
    <published>2021-10-14T07:59:27Z</published>
    <title>SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language
  Processing</title>
    <summary>  Motivated by the success of T5 (Text-To-Text Transfer Transformer) in
pre-trained natural language processing models, we propose a unified-modal
SpeechT5 framework that explores the encoder-decoder pre-training for
self-supervised speech/text representation learning. The SpeechT5 framework
consists of a shared encoder-decoder network and six modal-specific
(speech/text) pre/post-nets. After preprocessing the input speech/text through
the pre-nets, the shared encoder-decoder network models the
sequence-to-sequence transformation, and then the post-nets generate the output
in the speech/text modality based on the output of the decoder. Leveraging
large-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a
unified-modal representation, hoping to improve the modeling capability for
both speech and text. To align the textual and speech information into this
unified semantic space, we propose a cross-modal vector quantization approach
that randomly mixes up speech/text states with latent units as the interface
between encoder and decoder. Extensive evaluations show the superiority of the
proposed SpeechT5 framework on a wide variety of spoken language processing
tasks, including automatic speech recognition, speech synthesis, speech
translation, voice conversion, speech enhancement, and speaker identification.
We release our code and model at https://github.com/microsoft/SpeechT5.
</summary>
    <author>
      <name>Junyi Ao</name>
    </author>
    <author>
      <name>Rui Wang</name>
    </author>
    <author>
      <name>Long Zhou</name>
    </author>
    <author>
      <name>Chengyi Wang</name>
    </author>
    <author>
      <name>Shuo Ren</name>
    </author>
    <author>
      <name>Yu Wu</name>
    </author>
    <author>
      <name>Shujie Liu</name>
    </author>
    <author>
      <name>Tom Ko</name>
    </author>
    <author>
      <name>Qing Li</name>
    </author>
    <author>
      <name>Yu Zhang</name>
    </author>
    <author>
      <name>Zhihua Wei</name>
    </author>
    <author>
      <name>Yao Qian</name>
    </author>
    <author>
      <name>Jinyu Li</name>
    </author>
    <author>
      <name>Furu Wei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ACL 2022 main conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.07205v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.07205v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.08207v3</id>
    <updated>2022-03-17T17:53:01Z</updated>
    <published>2021-10-15T17:08:57Z</published>
    <title>Multitask Prompted Training Enables Zero-Shot Task Generalization</title>
    <summary>  Large language models have recently been shown to attain reasonable zero-shot
generalization on a diverse set of tasks (Brown et al., 2020). It has been
hypothesized that this is a consequence of implicit multitask learning in
language models' pretraining (Radford et al., 2019). Can zero-shot
generalization instead be directly induced by explicit multitask learning? To
test this question at scale, we develop a system for easily mapping any natural
language tasks into a human-readable prompted form. We convert a large set of
supervised datasets, each with multiple prompts with diverse wording. These
prompted datasets allow for benchmarking the ability of a model to perform
completely held-out tasks. We fine-tune a pretrained encoder-decoder model
(Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a
wide variety of tasks. The model attains strong zero-shot performance on
several standard datasets, often outperforming models up to 16x its size.
Further, our approach attains strong performance on a subset of tasks from the
BIG-bench benchmark, outperforming models up to 6x its size. All trained models
are available at https://github.com/bigscience-workshop/t-zero and all prompts
are available at https://github.com/bigscience-workshop/promptsource.
</summary>
    <author>
      <name>Victor Sanh</name>
    </author>
    <author>
      <name>Albert Webson</name>
    </author>
    <author>
      <name>Colin Raffel</name>
    </author>
    <author>
      <name>Stephen H. Bach</name>
    </author>
    <author>
      <name>Lintang Sutawika</name>
    </author>
    <author>
      <name>Zaid Alyafeai</name>
    </author>
    <author>
      <name>Antoine Chaffin</name>
    </author>
    <author>
      <name>Arnaud Stiegler</name>
    </author>
    <author>
      <name>Teven Le Scao</name>
    </author>
    <author>
      <name>Arun Raja</name>
    </author>
    <author>
      <name>Manan Dey</name>
    </author>
    <author>
      <name>M Saiful Bari</name>
    </author>
    <author>
      <name>Canwen Xu</name>
    </author>
    <author>
      <name>Urmish Thakker</name>
    </author>
    <author>
      <name>Shanya Sharma Sharma</name>
    </author>
    <author>
      <name>Eliza Szczechla</name>
    </author>
    <author>
      <name>Taewoon Kim</name>
    </author>
    <author>
      <name>Gunjan Chhablani</name>
    </author>
    <author>
      <name>Nihal Nayak</name>
    </author>
    <author>
      <name>Debajyoti Datta</name>
    </author>
    <author>
      <name>Jonathan Chang</name>
    </author>
    <author>
      <name>Mike Tian-Jian Jiang</name>
    </author>
    <author>
      <name>Han Wang</name>
    </author>
    <author>
      <name>Matteo Manica</name>
    </author>
    <author>
      <name>Sheng Shen</name>
    </author>
    <author>
      <name>Zheng Xin Yong</name>
    </author>
    <author>
      <name>Harshit Pandey</name>
    </author>
    <author>
      <name>Rachel Bawden</name>
    </author>
    <author>
      <name>Thomas Wang</name>
    </author>
    <author>
      <name>Trishala Neeraj</name>
    </author>
    <author>
      <name>Jos Rozen</name>
    </author>
    <author>
      <name>Abheesht Sharma</name>
    </author>
    <author>
      <name>Andrea Santilli</name>
    </author>
    <author>
      <name>Thibault Fevry</name>
    </author>
    <author>
      <name>Jason Alan Fries</name>
    </author>
    <author>
      <name>Ryan Teehan</name>
    </author>
    <author>
      <name>Tali Bers</name>
    </author>
    <author>
      <name>Stella Biderman</name>
    </author>
    <author>
      <name>Leo Gao</name>
    </author>
    <author>
      <name>Thomas Wolf</name>
    </author>
    <author>
      <name>Alexander M. Rush</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICLR 2022 Spotlight (with extended discussion)</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.08207v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.08207v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.09485v2</id>
    <updated>2021-10-29T20:48:06Z</updated>
    <published>2021-10-18T17:32:25Z</published>
    <title>Learning in High Dimension Always Amounts to Extrapolation</title>
    <summary>  The notion of interpolation and extrapolation is fundamental in various
fields from deep learning to function approximation. Interpolation occurs for a
sample $x$ whenever this sample falls inside or on the boundary of the given
dataset's convex hull. Extrapolation occurs when $x$ falls outside of that
convex hull. One fundamental (mis)conception is that state-of-the-art
algorithms work so well because of their ability to correctly interpolate
training data. A second (mis)conception is that interpolation happens
throughout tasks and datasets, in fact, many intuitions and theories rely on
that assumption. We empirically and theoretically argue against those two
points and demonstrate that on any high-dimensional ($&gt;$100) dataset,
interpolation almost surely never happens. Those results challenge the validity
of our current interpolation/extrapolation definition as an indicator of
generalization performances.
</summary>
    <author>
      <name>Randall Balestriero</name>
    </author>
    <author>
      <name>Jerome Pesenti</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
    <link href="http://arxiv.org/abs/2110.09485v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.09485v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.02358v2</id>
    <updated>2022-05-27T15:44:26Z</updated>
    <published>2021-11-03T17:20:36Z</published>
    <title>VLMo: Unified Vision-Language Pre-Training with
  Mixture-of-Modality-Experts</title>
    <summary>  We present a unified Vision-Language pretrained Model (VLMo) that jointly
learns a dual encoder and a fusion encoder with a modular Transformer network.
Specifically, we introduce Mixture-of-Modality-Experts (MoME) Transformer,
where each block contains a pool of modality-specific experts and a shared
self-attention layer. Because of the modeling flexibility of MoME, pretrained
VLMo can be fine-tuned as a fusion encoder for vision-language classification
tasks, or used as a dual encoder for efficient image-text retrieval. Moreover,
we propose a stagewise pre-training strategy, which effectively leverages
large-scale image-only and text-only data besides image-text pairs.
Experimental results show that VLMo achieves state-of-the-art results on
various vision-language tasks, including VQA, NLVR2 and image-text retrieval.
The code and pretrained models are available at https://aka.ms/vlmo.
</summary>
    <author>
      <name>Hangbo Bao</name>
    </author>
    <author>
      <name>Wenhui Wang</name>
    </author>
    <author>
      <name>Li Dong</name>
    </author>
    <author>
      <name>Qiang Liu</name>
    </author>
    <author>
      <name>Owais Khan Mohammed</name>
    </author>
    <author>
      <name>Kriti Aggarwal</name>
    </author>
    <author>
      <name>Subhojit Som</name>
    </author>
    <author>
      <name>Furu Wei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Work in progress</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.02358v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.02358v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.06825v2</id>
    <updated>2022-03-24T17:33:07Z</updated>
    <published>2021-12-13T17:35:26Z</published>
    <title>VL-Adapter: Parameter-Efficient Transfer Learning for
  Vision-and-Language Tasks</title>
    <summary>  Recently, fine-tuning language models pre-trained on large text corpora have
provided huge improvements on vision-and-language (V&amp;L) tasks as well as on
pure language tasks. However, fine-tuning the entire parameter set of
pre-trained models becomes impractical since the model size is growing rapidly.
Hence, in this paper, we introduce adapter-based parameter-efficient transfer
learning techniques to V&amp;L models such as VL-BART and VLT5. We evaluate our
methods in a unified multi-task setup on both image-text and video-text
benchmarks. For the image-text tasks, we use four diverse V&amp;L datasets: VQAv2,
GQA, NLVR2 , and MSCOCO image captioning. For video-text tasks, we use TVQA,
How2QA, TVC, and YC2C. With careful training and thorough experiments, we
benchmark three popular adapter-based methods (Adapter, Hyperformer, Compacter)
against the standard full fine-tuning and the recently proposed prompt-tuning
approach. We also enhance the efficiency and performance of adapters by sharing
their weights to attain knowledge across tasks. Our results demonstrate that
training the adapter with the weight-sharing technique (4.18% of total
parameters for image-text tasks and 3.39% for video-text tasks) can match the
performance of fine-tuning the entire model. Lastly, we present a comprehensive
analysis including the combination of adapter and task-specific prompts and the
impact of V&amp;L pre-training on adapters. Our code is available at:
https://github.com/ylsung/VL_adapter.
</summary>
    <author>
      <name>Yi-Lin Sung</name>
    </author>
    <author>
      <name>Jaemin Cho</name>
    </author>
    <author>
      <name>Mohit Bansal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2022 (15 pages; with new video-text and CLIP-ViL experiments)</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.06825v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.06825v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.10752v2</id>
    <updated>2022-04-13T11:38:44Z</updated>
    <published>2021-12-20T18:55:25Z</published>
    <title>High-Resolution Image Synthesis with Latent Diffusion Models</title>
    <summary>  By decomposing the image formation process into a sequential application of
denoising autoencoders, diffusion models (DMs) achieve state-of-the-art
synthesis results on image data and beyond. Additionally, their formulation
allows for a guiding mechanism to control the image generation process without
retraining. However, since these models typically operate directly in pixel
space, optimization of powerful DMs often consumes hundreds of GPU days and
inference is expensive due to sequential evaluations. To enable DM training on
limited computational resources while retaining their quality and flexibility,
we apply them in the latent space of powerful pretrained autoencoders. In
contrast to previous work, training diffusion models on such a representation
allows for the first time to reach a near-optimal point between complexity
reduction and detail preservation, greatly boosting visual fidelity. By
introducing cross-attention layers into the model architecture, we turn
diffusion models into powerful and flexible generators for general conditioning
inputs such as text or bounding boxes and high-resolution synthesis becomes
possible in a convolutional manner. Our latent diffusion models (LDMs) achieve
a new state of the art for image inpainting and highly competitive performance
on various tasks, including unconditional image generation, semantic scene
synthesis, and super-resolution, while significantly reducing computational
requirements compared to pixel-based DMs. Code is available at
https://github.com/CompVis/latent-diffusion .
</summary>
    <author>
      <name>Robin Rombach</name>
    </author>
    <author>
      <name>Andreas Blattmann</name>
    </author>
    <author>
      <name>Dominik Lorenz</name>
    </author>
    <author>
      <name>Patrick Esser</name>
    </author>
    <author>
      <name>Björn Ommer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.10752v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.10752v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.02177v1</id>
    <updated>2022-01-06T18:43:37Z</updated>
    <published>2022-01-06T18:43:37Z</published>
    <title>Grokking: Generalization Beyond Overfitting on Small Algorithmic
  Datasets</title>
    <summary>  In this paper we propose to study generalization of neural networks on small
algorithmically generated datasets. In this setting, questions about data
efficiency, memorization, generalization, and speed of learning can be studied
in great detail. In some situations we show that neural networks learn through
a process of "grokking" a pattern in the data, improving generalization
performance from random chance level to perfect generalization, and that this
improvement in generalization can happen well past the point of overfitting. We
also study generalization as a function of dataset size and find that smaller
datasets require increasing amounts of optimization for generalization. We
argue that these datasets provide a fertile ground for studying a poorly
understood aspect of deep learning: generalization of overparametrized neural
networks beyond memorization of the finite training dataset.
</summary>
    <author>
      <name>Alethea Power</name>
    </author>
    <author>
      <name>Yuri Burda</name>
    </author>
    <author>
      <name>Harri Edwards</name>
    </author>
    <author>
      <name>Igor Babuschkin</name>
    </author>
    <author>
      <name>Vedant Misra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Correspondence to alethea@openai.com. Code available at:
  https://github.com/openai/grok</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.02177v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.02177v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.03545v2</id>
    <updated>2022-03-02T15:08:16Z</updated>
    <published>2022-01-10T18:59:10Z</published>
    <title>A ConvNet for the 2020s</title>
    <summary>  The "Roaring 20s" of visual recognition began with the introduction of Vision
Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art
image classification model. A vanilla ViT, on the other hand, faces
difficulties when applied to general computer vision tasks such as object
detection and semantic segmentation. It is the hierarchical Transformers (e.g.,
Swin Transformers) that reintroduced several ConvNet priors, making
Transformers practically viable as a generic vision backbone and demonstrating
remarkable performance on a wide variety of vision tasks. However, the
effectiveness of such hybrid approaches is still largely credited to the
intrinsic superiority of Transformers, rather than the inherent inductive
biases of convolutions. In this work, we reexamine the design spaces and test
the limits of what a pure ConvNet can achieve. We gradually "modernize" a
standard ResNet toward the design of a vision Transformer, and discover several
key components that contribute to the performance difference along the way. The
outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt.
Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably
with Transformers in terms of accuracy and scalability, achieving 87.8%
ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection
and ADE20K segmentation, while maintaining the simplicity and efficiency of
standard ConvNets.
</summary>
    <author>
      <name>Zhuang Liu</name>
    </author>
    <author>
      <name>Hanzi Mao</name>
    </author>
    <author>
      <name>Chao-Yuan Wu</name>
    </author>
    <author>
      <name>Christoph Feichtenhofer</name>
    </author>
    <author>
      <name>Trevor Darrell</name>
    </author>
    <author>
      <name>Saining Xie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2022; Code: https://github.com/facebookresearch/ConvNeXt</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.03545v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.03545v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.08239v3</id>
    <updated>2022-02-10T16:30:11Z</updated>
    <published>2022-01-20T15:44:37Z</published>
    <title>LaMDA: Language Models for Dialog Applications</title>
    <summary>  We present LaMDA: Language Models for Dialog Applications. LaMDA is a family
of Transformer-based neural language models specialized for dialog, which have
up to 137B parameters and are pre-trained on 1.56T words of public dialog data
and web text. While model scaling alone can improve quality, it shows less
improvements on safety and factual grounding. We demonstrate that fine-tuning
with annotated data and enabling the model to consult external knowledge
sources can lead to significant improvements towards the two key challenges of
safety and factual grounding. The first challenge, safety, involves ensuring
that the model's responses are consistent with a set of human values, such as
preventing harmful suggestions and unfair bias. We quantify safety using a
metric based on an illustrative set of human values, and we find that filtering
candidate responses using a LaMDA classifier fine-tuned with a small amount of
crowdworker-annotated data offers a promising approach to improving model
safety. The second challenge, factual grounding, involves enabling the model to
consult external knowledge sources, such as an information retrieval system, a
language translator, and a calculator. We quantify factuality using a
groundedness metric, and we find that our approach enables the model to
generate responses grounded in known sources, rather than responses that merely
sound plausible. Finally, we explore the use of LaMDA in the domains of
education and content recommendations, and analyze their helpfulness and role
consistency.
</summary>
    <author>
      <name>Romal Thoppilan</name>
    </author>
    <author>
      <name>Daniel De Freitas</name>
    </author>
    <author>
      <name>Jamie Hall</name>
    </author>
    <author>
      <name>Noam Shazeer</name>
    </author>
    <author>
      <name>Apoorv Kulshreshtha</name>
    </author>
    <author>
      <name>Heng-Tze Cheng</name>
    </author>
    <author>
      <name>Alicia Jin</name>
    </author>
    <author>
      <name>Taylor Bos</name>
    </author>
    <author>
      <name>Leslie Baker</name>
    </author>
    <author>
      <name>Yu Du</name>
    </author>
    <author>
      <name>YaGuang Li</name>
    </author>
    <author>
      <name>Hongrae Lee</name>
    </author>
    <author>
      <name>Huaixiu Steven Zheng</name>
    </author>
    <author>
      <name>Amin Ghafouri</name>
    </author>
    <author>
      <name>Marcelo Menegali</name>
    </author>
    <author>
      <name>Yanping Huang</name>
    </author>
    <author>
      <name>Maxim Krikun</name>
    </author>
    <author>
      <name>Dmitry Lepikhin</name>
    </author>
    <author>
      <name>James Qin</name>
    </author>
    <author>
      <name>Dehao Chen</name>
    </author>
    <author>
      <name>Yuanzhong Xu</name>
    </author>
    <author>
      <name>Zhifeng Chen</name>
    </author>
    <author>
      <name>Adam Roberts</name>
    </author>
    <author>
      <name>Maarten Bosma</name>
    </author>
    <author>
      <name>Vincent Zhao</name>
    </author>
    <author>
      <name>Yanqi Zhou</name>
    </author>
    <author>
      <name>Chung-Ching Chang</name>
    </author>
    <author>
      <name>Igor Krivokon</name>
    </author>
    <author>
      <name>Will Rusch</name>
    </author>
    <author>
      <name>Marc Pickett</name>
    </author>
    <author>
      <name>Pranesh Srinivasan</name>
    </author>
    <author>
      <name>Laichee Man</name>
    </author>
    <author>
      <name>Kathleen Meier-Hellstern</name>
    </author>
    <author>
      <name>Meredith Ringel Morris</name>
    </author>
    <author>
      <name>Tulsee Doshi</name>
    </author>
    <author>
      <name>Renelito Delos Santos</name>
    </author>
    <author>
      <name>Toju Duke</name>
    </author>
    <author>
      <name>Johnny Soraker</name>
    </author>
    <author>
      <name>Ben Zevenbergen</name>
    </author>
    <author>
      <name>Vinodkumar Prabhakaran</name>
    </author>
    <author>
      <name>Mark Diaz</name>
    </author>
    <author>
      <name>Ben Hutchinson</name>
    </author>
    <author>
      <name>Kristen Olson</name>
    </author>
    <author>
      <name>Alejandra Molina</name>
    </author>
    <author>
      <name>Erin Hoffman-John</name>
    </author>
    <author>
      <name>Josh Lee</name>
    </author>
    <author>
      <name>Lora Aroyo</name>
    </author>
    <author>
      <name>Ravi Rajakumar</name>
    </author>
    <author>
      <name>Alena Butryna</name>
    </author>
    <author>
      <name>Matthew Lamm</name>
    </author>
    <author>
      <name>Viktoriya Kuzmina</name>
    </author>
    <author>
      <name>Joe Fenton</name>
    </author>
    <author>
      <name>Aaron Cohen</name>
    </author>
    <author>
      <name>Rachel Bernstein</name>
    </author>
    <author>
      <name>Ray Kurzweil</name>
    </author>
    <author>
      <name>Blaise Aguera-Arcas</name>
    </author>
    <author>
      <name>Claire Cui</name>
    </author>
    <author>
      <name>Marian Croak</name>
    </author>
    <author>
      <name>Ed Chi</name>
    </author>
    <author>
      <name>Quoc Le</name>
    </author>
    <link href="http://arxiv.org/abs/2201.08239v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.08239v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.09792v1</id>
    <updated>2022-01-24T16:42:56Z</updated>
    <published>2022-01-24T16:42:56Z</published>
    <title>Patches Are All You Need?</title>
    <summary>  Although convolutional networks have been the dominant architecture for
vision tasks for many years, recent experiments have shown that
Transformer-based models, most notably the Vision Transformer (ViT), may exceed
their performance in some settings. However, due to the quadratic runtime of
the self-attention layers in Transformers, ViTs require the use of patch
embeddings, which group together small regions of the image into single input
features, in order to be applied to larger image sizes. This raises a question:
Is the performance of ViTs due to the inherently-more-powerful Transformer
architecture, or is it at least partly due to using patches as the input
representation? In this paper, we present some evidence for the latter:
specifically, we propose the ConvMixer, an extremely simple model that is
similar in spirit to the ViT and the even-more-basic MLP-Mixer in that it
operates directly on patches as input, separates the mixing of spatial and
channel dimensions, and maintains equal size and resolution throughout the
network. In contrast, however, the ConvMixer uses only standard convolutions to
achieve the mixing steps. Despite its simplicity, we show that the ConvMixer
outperforms the ViT, MLP-Mixer, and some of their variants for similar
parameter counts and data set sizes, in addition to outperforming classical
vision models such as the ResNet. Our code is available at
https://github.com/locuslab/convmixer.
</summary>
    <author>
      <name>Asher Trockman</name>
    </author>
    <author>
      <name>J. Zico Kolter</name>
    </author>
    <link href="http://arxiv.org/abs/2201.09792v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.09792v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.11903v6</id>
    <updated>2023-01-10T23:07:57Z</updated>
    <published>2022-01-28T02:33:07Z</published>
    <title>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</title>
    <summary>  We explore how generating a chain of thought -- a series of intermediate
reasoning steps -- significantly improves the ability of large language models
to perform complex reasoning. In particular, we show how such reasoning
abilities emerge naturally in sufficiently large language models via a simple
method called chain of thought prompting, where a few chain of thought
demonstrations are provided as exemplars in prompting. Experiments on three
large language models show that chain of thought prompting improves performance
on a range of arithmetic, commonsense, and symbolic reasoning tasks. The
empirical gains can be striking. For instance, prompting a 540B-parameter
language model with just eight chain of thought exemplars achieves state of the
art accuracy on the GSM8K benchmark of math word problems, surpassing even
finetuned GPT-3 with a verifier.
</summary>
    <author>
      <name>Jason Wei</name>
    </author>
    <author>
      <name>Xuezhi Wang</name>
    </author>
    <author>
      <name>Dale Schuurmans</name>
    </author>
    <author>
      <name>Maarten Bosma</name>
    </author>
    <author>
      <name>Brian Ichter</name>
    </author>
    <author>
      <name>Fei Xia</name>
    </author>
    <author>
      <name>Ed Chi</name>
    </author>
    <author>
      <name>Quoc Le</name>
    </author>
    <author>
      <name>Denny Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/2201.11903v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.11903v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.01374v1</id>
    <updated>2022-02-03T02:26:40Z</updated>
    <published>2022-02-03T02:26:40Z</published>
    <title>mSLAM: Massively multilingual joint pre-training for speech and text</title>
    <summary>  We present mSLAM, a multilingual Speech and LAnguage Model that learns
cross-lingual cross-modal representations of speech and text by pre-training
jointly on large amounts of unlabeled speech and text in multiple languages.
mSLAM combines w2v-BERT pre-training on speech with SpanBERT pre-training on
character-level text, along with Connectionist Temporal Classification (CTC)
losses on paired speech and transcript data, to learn a single model capable of
learning from and representing both speech and text signals in a shared
representation space. We evaluate mSLAM on several downstream speech
understanding tasks and find that joint pre-training with text improves quality
on speech translation, speech intent classification and speech language-ID
while being competitive on multilingual ASR, when compared against speech-only
pre-training. Our speech translation model demonstrates zero-shot text
translation without seeing any text translation data, providing evidence for
cross-modal alignment of representations. mSLAM also benefits from multi-modal
fine-tuning, further improving the quality of speech translation by directly
leveraging text translation data during the fine-tuning process. Our empirical
analysis highlights several opportunities and challenges arising from
large-scale multimodal pre-training, suggesting directions for future research.
</summary>
    <author>
      <name>Ankur Bapna</name>
    </author>
    <author>
      <name>Colin Cherry</name>
    </author>
    <author>
      <name>Yu Zhang</name>
    </author>
    <author>
      <name>Ye Jia</name>
    </author>
    <author>
      <name>Melvin Johnson</name>
    </author>
    <author>
      <name>Yong Cheng</name>
    </author>
    <author>
      <name>Simran Khanuja</name>
    </author>
    <author>
      <name>Jason Riesa</name>
    </author>
    <author>
      <name>Alexis Conneau</name>
    </author>
    <link href="http://arxiv.org/abs/2202.01374v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.01374v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.08433v3</id>
    <updated>2024-07-02T04:06:53Z</updated>
    <published>2022-02-17T03:29:20Z</published>
    <title>ADD 2022: the First Audio Deep Synthesis Detection Challenge</title>
    <summary>  Audio deepfake detection is an emerging topic, which was included in the
ASVspoof 2021. However, the recent shared tasks have not covered many real-life
and challenging scenarios. The first Audio Deep synthesis Detection challenge
(ADD) was motivated to fill in the gap. The ADD 2022 includes three tracks:
low-quality fake audio detection (LF), partially fake audio detection (PF) and
audio fake game (FG). The LF track focuses on dealing with bona fide and fully
fake utterances with various real-world noises etc. The PF track aims to
distinguish the partially fake audio from the real. The FG track is a rivalry
game, which includes two tasks: an audio generation task and an audio fake
detection task. In this paper, we describe the datasets, evaluation metrics,
and protocols. We also report major findings that reflect the recent advances
in audio deepfake detection tasks.
</summary>
    <author>
      <name>Jiangyan Yi</name>
    </author>
    <author>
      <name>Ruibo Fu</name>
    </author>
    <author>
      <name>Jianhua Tao</name>
    </author>
    <author>
      <name>Shuai Nie</name>
    </author>
    <author>
      <name>Haoxin Ma</name>
    </author>
    <author>
      <name>Chenglong Wang</name>
    </author>
    <author>
      <name>Tao Wang</name>
    </author>
    <author>
      <name>Zhengkun Tian</name>
    </author>
    <author>
      <name>Xiaohui Zhang</name>
    </author>
    <author>
      <name>Ye Bai</name>
    </author>
    <author>
      <name>Cunhang Fan</name>
    </author>
    <author>
      <name>Shan Liang</name>
    </author>
    <author>
      <name>Shiming Wang</name>
    </author>
    <author>
      <name>Shuai Zhang</name>
    </author>
    <author>
      <name>Xinrui Yan</name>
    </author>
    <author>
      <name>Le Xu</name>
    </author>
    <author>
      <name>Zhengqi Wen</name>
    </author>
    <author>
      <name>Haizhou Li</name>
    </author>
    <author>
      <name>Zheng Lian</name>
    </author>
    <author>
      <name>Bin Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ICASSP 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2202.08433v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.08433v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.11214v1</id>
    <updated>2022-02-22T22:19:35Z</updated>
    <published>2022-02-22T22:19:35Z</published>
    <title>FourCastNet: A Global Data-driven High-resolution Weather Model using
  Adaptive Fourier Neural Operators</title>
    <summary>  FourCastNet, short for Fourier Forecasting Neural Network, is a global
data-driven weather forecasting model that provides accurate short to
medium-range global predictions at $0.25^{\circ}$ resolution. FourCastNet
accurately forecasts high-resolution, fast-timescale variables such as the
surface wind speed, precipitation, and atmospheric water vapor. It has
important implications for planning wind energy resources, predicting extreme
weather events such as tropical cyclones, extra-tropical cyclones, and
atmospheric rivers. FourCastNet matches the forecasting accuracy of the ECMWF
Integrated Forecasting System (IFS), a state-of-the-art Numerical Weather
Prediction (NWP) model, at short lead times for large-scale variables, while
outperforming IFS for variables with complex fine-scale structure, including
precipitation. FourCastNet generates a week-long forecast in less than 2
seconds, orders of magnitude faster than IFS. The speed of FourCastNet enables
the creation of rapid and inexpensive large-ensemble forecasts with thousands
of ensemble-members for improving probabilistic forecasting. We discuss how
data-driven deep learning models such as FourCastNet are a valuable addition to
the meteorology toolkit to aid and augment NWP models.
</summary>
    <author>
      <name>Jaideep Pathak</name>
    </author>
    <author>
      <name>Shashank Subramanian</name>
    </author>
    <author>
      <name>Peter Harrington</name>
    </author>
    <author>
      <name>Sanjeev Raja</name>
    </author>
    <author>
      <name>Ashesh Chattopadhyay</name>
    </author>
    <author>
      <name>Morteza Mardani</name>
    </author>
    <author>
      <name>Thorsten Kurth</name>
    </author>
    <author>
      <name>David Hall</name>
    </author>
    <author>
      <name>Zongyi Li</name>
    </author>
    <author>
      <name>Kamyar Azizzadenesheli</name>
    </author>
    <author>
      <name>Pedram Hassanzadeh</name>
    </author>
    <author>
      <name>Karthik Kashinath</name>
    </author>
    <author>
      <name>Animashree Anandkumar</name>
    </author>
    <link href="http://arxiv.org/abs/2202.11214v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.11214v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.ao-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ao-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.02053v2</id>
    <updated>2022-10-19T20:39:13Z</updated>
    <published>2022-03-03T22:53:54Z</published>
    <title>Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive
  Representation Learning</title>
    <summary>  We present modality gap, an intriguing geometric phenomenon of the
representation space of multi-modal models. Specifically, we show that
different data modalities (e.g. images and text) are embedded at arm's length
in their shared representation in multi-modal models such as CLIP. Our
systematic analysis demonstrates that this gap is caused by a combination of
model initialization and contrastive learning optimization. In model
initialization, we show empirically and theoretically that the representation
of a common deep neural network is restricted to a narrow cone. As a
consequence, in a multi-modal model with two encoders, the representations of
the two modalities are clearly apart when the model is initialized. During
optimization, contrastive learning keeps the different modalities separate by a
certain distance, which is influenced by the temperature parameter in the loss
function. Our experiments further demonstrate that varying the modality gap
distance has a significant impact in improving the model's downstream zero-shot
classification performance and fairness. Our code and data are available at
https://modalitygap.readthedocs.io/
</summary>
    <author>
      <name>Weixin Liang</name>
    </author>
    <author>
      <name>Yuhui Zhang</name>
    </author>
    <author>
      <name>Yongchan Kwon</name>
    </author>
    <author>
      <name>Serena Yeung</name>
    </author>
    <author>
      <name>James Zou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at NeurIPS 2022. Code and data are available at
  https://modalitygap.readthedocs.io/</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.02053v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.02053v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.02155v1</id>
    <updated>2022-03-04T07:04:42Z</updated>
    <published>2022-03-04T07:04:42Z</published>
    <title>Training language models to follow instructions with human feedback</title>
    <summary>  Making language models bigger does not inherently make them better at
following a user's intent. For example, large language models can generate
outputs that are untruthful, toxic, or simply not helpful to the user. In other
words, these models are not aligned with their users. In this paper, we show an
avenue for aligning language models with user intent on a wide range of tasks
by fine-tuning with human feedback. Starting with a set of labeler-written
prompts and prompts submitted through the OpenAI API, we collect a dataset of
labeler demonstrations of the desired model behavior, which we use to fine-tune
GPT-3 using supervised learning. We then collect a dataset of rankings of model
outputs, which we use to further fine-tune this supervised model using
reinforcement learning from human feedback. We call the resulting models
InstructGPT. In human evaluations on our prompt distribution, outputs from the
1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3,
despite having 100x fewer parameters. Moreover, InstructGPT models show
improvements in truthfulness and reductions in toxic output generation while
having minimal performance regressions on public NLP datasets. Even though
InstructGPT still makes simple mistakes, our results show that fine-tuning with
human feedback is a promising direction for aligning language models with human
intent.
</summary>
    <author>
      <name>Long Ouyang</name>
    </author>
    <author>
      <name>Jeff Wu</name>
    </author>
    <author>
      <name>Xu Jiang</name>
    </author>
    <author>
      <name>Diogo Almeida</name>
    </author>
    <author>
      <name>Carroll L. Wainwright</name>
    </author>
    <author>
      <name>Pamela Mishkin</name>
    </author>
    <author>
      <name>Chong Zhang</name>
    </author>
    <author>
      <name>Sandhini Agarwal</name>
    </author>
    <author>
      <name>Katarina Slama</name>
    </author>
    <author>
      <name>Alex Ray</name>
    </author>
    <author>
      <name>John Schulman</name>
    </author>
    <author>
      <name>Jacob Hilton</name>
    </author>
    <author>
      <name>Fraser Kelton</name>
    </author>
    <author>
      <name>Luke Miller</name>
    </author>
    <author>
      <name>Maddie Simens</name>
    </author>
    <author>
      <name>Amanda Askell</name>
    </author>
    <author>
      <name>Peter Welinder</name>
    </author>
    <author>
      <name>Paul Christiano</name>
    </author>
    <author>
      <name>Jan Leike</name>
    </author>
    <author>
      <name>Ryan Lowe</name>
    </author>
    <link href="http://arxiv.org/abs/2203.02155v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.02155v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.15556v1</id>
    <updated>2022-03-29T13:38:03Z</updated>
    <published>2022-03-29T13:38:03Z</published>
    <title>Training Compute-Optimal Large Language Models</title>
    <summary>  We investigate the optimal model size and number of tokens for training a
transformer language model under a given compute budget. We find that current
large language models are significantly undertrained, a consequence of the
recent focus on scaling language models whilst keeping the amount of training
data constant. By training over 400 language models ranging from 70 million to
over 16 billion parameters on 5 to 500 billion tokens, we find that for
compute-optimal training, the model size and the number of training tokens
should be scaled equally: for every doubling of model size the number of
training tokens should also be doubled. We test this hypothesis by training a
predicted compute-optimal model, Chinchilla, that uses the same compute budget
as Gopher but with 70B parameters and 4$\times$ more more data. Chinchilla
uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1
(178B), and Megatron-Turing NLG (530B) on a large range of downstream
evaluation tasks. This also means that Chinchilla uses substantially less
compute for fine-tuning and inference, greatly facilitating downstream usage.
As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5%
on the MMLU benchmark, greater than a 7% improvement over Gopher.
</summary>
    <author>
      <name>Jordan Hoffmann</name>
    </author>
    <author>
      <name>Sebastian Borgeaud</name>
    </author>
    <author>
      <name>Arthur Mensch</name>
    </author>
    <author>
      <name>Elena Buchatskaya</name>
    </author>
    <author>
      <name>Trevor Cai</name>
    </author>
    <author>
      <name>Eliza Rutherford</name>
    </author>
    <author>
      <name>Diego de Las Casas</name>
    </author>
    <author>
      <name>Lisa Anne Hendricks</name>
    </author>
    <author>
      <name>Johannes Welbl</name>
    </author>
    <author>
      <name>Aidan Clark</name>
    </author>
    <author>
      <name>Tom Hennigan</name>
    </author>
    <author>
      <name>Eric Noland</name>
    </author>
    <author>
      <name>Katie Millican</name>
    </author>
    <author>
      <name>George van den Driessche</name>
    </author>
    <author>
      <name>Bogdan Damoc</name>
    </author>
    <author>
      <name>Aurelia Guy</name>
    </author>
    <author>
      <name>Simon Osindero</name>
    </author>
    <author>
      <name>Karen Simonyan</name>
    </author>
    <author>
      <name>Erich Elsen</name>
    </author>
    <author>
      <name>Jack W. Rae</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Laurent Sifre</name>
    </author>
    <link href="http://arxiv.org/abs/2203.15556v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.15556v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.01691v2</id>
    <updated>2022-08-16T16:06:33Z</updated>
    <published>2022-04-04T17:57:11Z</published>
    <title>Do As I Can, Not As I Say: Grounding Language in Robotic Affordances</title>
    <summary>  Large language models can encode a wealth of semantic knowledge about the
world. Such knowledge could be extremely useful to robots aiming to act upon
high-level, temporally extended instructions expressed in natural language.
However, a significant weakness of language models is that they lack real-world
experience, which makes it difficult to leverage them for decision making
within a given embodiment. For example, asking a language model to describe how
to clean a spill might result in a reasonable narrative, but it may not be
applicable to a particular agent, such as a robot, that needs to perform this
task in a particular environment. We propose to provide real-world grounding by
means of pretrained skills, which are used to constrain the model to propose
natural language actions that are both feasible and contextually appropriate.
The robot can act as the language model's "hands and eyes," while the language
model supplies high-level semantic knowledge about the task. We show how
low-level skills can be combined with large language models so that the
language model provides high-level knowledge about the procedures for
performing complex and temporally-extended instructions, while value functions
associated with these skills provide the grounding necessary to connect this
knowledge to a particular physical environment. We evaluate our method on a
number of real-world robotic tasks, where we show the need for real-world
grounding and that this approach is capable of completing long-horizon,
abstract, natural language instructions on a mobile manipulator. The project's
website and the video can be found at https://say-can.github.io/.
</summary>
    <author>
      <name>Michael Ahn</name>
    </author>
    <author>
      <name>Anthony Brohan</name>
    </author>
    <author>
      <name>Noah Brown</name>
    </author>
    <author>
      <name>Yevgen Chebotar</name>
    </author>
    <author>
      <name>Omar Cortes</name>
    </author>
    <author>
      <name>Byron David</name>
    </author>
    <author>
      <name>Chelsea Finn</name>
    </author>
    <author>
      <name>Chuyuan Fu</name>
    </author>
    <author>
      <name>Keerthana Gopalakrishnan</name>
    </author>
    <author>
      <name>Karol Hausman</name>
    </author>
    <author>
      <name>Alex Herzog</name>
    </author>
    <author>
      <name>Daniel Ho</name>
    </author>
    <author>
      <name>Jasmine Hsu</name>
    </author>
    <author>
      <name>Julian Ibarz</name>
    </author>
    <author>
      <name>Brian Ichter</name>
    </author>
    <author>
      <name>Alex Irpan</name>
    </author>
    <author>
      <name>Eric Jang</name>
    </author>
    <author>
      <name>Rosario Jauregui Ruano</name>
    </author>
    <author>
      <name>Kyle Jeffrey</name>
    </author>
    <author>
      <name>Sally Jesmonth</name>
    </author>
    <author>
      <name>Nikhil J Joshi</name>
    </author>
    <author>
      <name>Ryan Julian</name>
    </author>
    <author>
      <name>Dmitry Kalashnikov</name>
    </author>
    <author>
      <name>Yuheng Kuang</name>
    </author>
    <author>
      <name>Kuang-Huei Lee</name>
    </author>
    <author>
      <name>Sergey Levine</name>
    </author>
    <author>
      <name>Yao Lu</name>
    </author>
    <author>
      <name>Linda Luu</name>
    </author>
    <author>
      <name>Carolina Parada</name>
    </author>
    <author>
      <name>Peter Pastor</name>
    </author>
    <author>
      <name>Jornell Quiambao</name>
    </author>
    <author>
      <name>Kanishka Rao</name>
    </author>
    <author>
      <name>Jarek Rettinghouse</name>
    </author>
    <author>
      <name>Diego Reyes</name>
    </author>
    <author>
      <name>Pierre Sermanet</name>
    </author>
    <author>
      <name>Nicolas Sievers</name>
    </author>
    <author>
      <name>Clayton Tan</name>
    </author>
    <author>
      <name>Alexander Toshev</name>
    </author>
    <author>
      <name>Vincent Vanhoucke</name>
    </author>
    <author>
      <name>Fei Xia</name>
    </author>
    <author>
      <name>Ted Xiao</name>
    </author>
    <author>
      <name>Peng Xu</name>
    </author>
    <author>
      <name>Sichun Xu</name>
    </author>
    <author>
      <name>Mengyuan Yan</name>
    </author>
    <author>
      <name>Andy Zeng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">See website at https://say-can.github.io/ V1. Initial Upload. V2.
  Added PaLM results. Added study about new capabilities (drawer manipulation,
  chain of thought prompting, multilingual instructions). Added an ablation
  study of language model size. Added an open-source version of \algname on a
  simulated tabletop environment. Improved readability</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.01691v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.01691v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.01697v4</id>
    <updated>2022-09-09T17:57:10Z</updated>
    <published>2022-04-04T17:59:44Z</published>
    <title>MaxViT: Multi-Axis Vision Transformer</title>
    <summary>  Transformers have recently gained significant attention in the computer
vision community. However, the lack of scalability of self-attention mechanisms
with respect to image size has limited their wide adoption in state-of-the-art
vision backbones. In this paper we introduce an efficient and scalable
attention model we call multi-axis attention, which consists of two aspects:
blocked local and dilated global attention. These design choices allow
global-local spatial interactions on arbitrary input resolutions with only
linear complexity. We also present a new architectural element by effectively
blending our proposed attention model with convolutions, and accordingly
propose a simple hierarchical vision backbone, dubbed MaxViT, by simply
repeating the basic building block over multiple stages. Notably, MaxViT is
able to ''see'' globally throughout the entire network, even in earlier,
high-resolution stages. We demonstrate the effectiveness of our model on a
broad spectrum of vision tasks. On image classification, MaxViT achieves
state-of-the-art performance under various settings: without extra data, MaxViT
attains 86.5% ImageNet-1K top-1 accuracy; with ImageNet-21K pre-training, our
model achieves 88.7% top-1 accuracy. For downstream tasks, MaxViT as a backbone
delivers favorable performance on object detection as well as visual aesthetic
assessment. We also show that our proposed model expresses strong generative
modeling capability on ImageNet, demonstrating the superior potential of MaxViT
blocks as a universal vision module. The source code and trained models will be
available at https://github.com/google-research/maxvit.
</summary>
    <author>
      <name>Zhengzhong Tu</name>
    </author>
    <author>
      <name>Hossein Talebi</name>
    </author>
    <author>
      <name>Han Zhang</name>
    </author>
    <author>
      <name>Feng Yang</name>
    </author>
    <author>
      <name>Peyman Milanfar</name>
    </author>
    <author>
      <name>Alan Bovik</name>
    </author>
    <author>
      <name>Yinxiao Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ECCV 2022; code: https://github.com/google-research/maxvit v1:
  initials; v2: added GAN visuals; v3: fixed ImageNet-1k acc typos for Maxvit @
  384</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.01697v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.01697v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.02311v5</id>
    <updated>2022-10-05T06:02:24Z</updated>
    <published>2022-04-05T16:11:45Z</published>
    <title>PaLM: Scaling Language Modeling with Pathways</title>
    <summary>  Large language models have been shown to achieve remarkable performance
across a variety of natural language tasks using few-shot learning, which
drastically reduces the number of task-specific training examples needed to
adapt the model to a particular application. To further our understanding of
the impact of scale on few-shot learning, we trained a 540-billion parameter,
densely activated, Transformer language model, which we call Pathways Language
Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML
system which enables highly efficient training across multiple TPU Pods. We
demonstrate continued benefits of scaling by achieving state-of-the-art
few-shot learning results on hundreds of language understanding and generation
benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough
performance, outperforming the finetuned state-of-the-art on a suite of
multi-step reasoning tasks, and outperforming average human performance on the
recently released BIG-bench benchmark. A significant number of BIG-bench tasks
showed discontinuous improvements from model scale, meaning that performance
steeply increased as we scaled to our largest model. PaLM also has strong
capabilities in multilingual tasks and source code generation, which we
demonstrate on a wide array of benchmarks. We additionally provide a
comprehensive analysis on bias and toxicity, and study the extent of training
data memorization with respect to model scale. Finally, we discuss the ethical
considerations related to large language models and discuss potential
mitigation strategies.
</summary>
    <author>
      <name>Aakanksha Chowdhery</name>
    </author>
    <author>
      <name>Sharan Narang</name>
    </author>
    <author>
      <name>Jacob Devlin</name>
    </author>
    <author>
      <name>Maarten Bosma</name>
    </author>
    <author>
      <name>Gaurav Mishra</name>
    </author>
    <author>
      <name>Adam Roberts</name>
    </author>
    <author>
      <name>Paul Barham</name>
    </author>
    <author>
      <name>Hyung Won Chung</name>
    </author>
    <author>
      <name>Charles Sutton</name>
    </author>
    <author>
      <name>Sebastian Gehrmann</name>
    </author>
    <author>
      <name>Parker Schuh</name>
    </author>
    <author>
      <name>Kensen Shi</name>
    </author>
    <author>
      <name>Sasha Tsvyashchenko</name>
    </author>
    <author>
      <name>Joshua Maynez</name>
    </author>
    <author>
      <name>Abhishek Rao</name>
    </author>
    <author>
      <name>Parker Barnes</name>
    </author>
    <author>
      <name>Yi Tay</name>
    </author>
    <author>
      <name>Noam Shazeer</name>
    </author>
    <author>
      <name>Vinodkumar Prabhakaran</name>
    </author>
    <author>
      <name>Emily Reif</name>
    </author>
    <author>
      <name>Nan Du</name>
    </author>
    <author>
      <name>Ben Hutchinson</name>
    </author>
    <author>
      <name>Reiner Pope</name>
    </author>
    <author>
      <name>James Bradbury</name>
    </author>
    <author>
      <name>Jacob Austin</name>
    </author>
    <author>
      <name>Michael Isard</name>
    </author>
    <author>
      <name>Guy Gur-Ari</name>
    </author>
    <author>
      <name>Pengcheng Yin</name>
    </author>
    <author>
      <name>Toju Duke</name>
    </author>
    <author>
      <name>Anselm Levskaya</name>
    </author>
    <author>
      <name>Sanjay Ghemawat</name>
    </author>
    <author>
      <name>Sunipa Dev</name>
    </author>
    <author>
      <name>Henryk Michalewski</name>
    </author>
    <author>
      <name>Xavier Garcia</name>
    </author>
    <author>
      <name>Vedant Misra</name>
    </author>
    <author>
      <name>Kevin Robinson</name>
    </author>
    <author>
      <name>Liam Fedus</name>
    </author>
    <author>
      <name>Denny Zhou</name>
    </author>
    <author>
      <name>Daphne Ippolito</name>
    </author>
    <author>
      <name>David Luan</name>
    </author>
    <author>
      <name>Hyeontaek Lim</name>
    </author>
    <author>
      <name>Barret Zoph</name>
    </author>
    <author>
      <name>Alexander Spiridonov</name>
    </author>
    <author>
      <name>Ryan Sepassi</name>
    </author>
    <author>
      <name>David Dohan</name>
    </author>
    <author>
      <name>Shivani Agrawal</name>
    </author>
    <author>
      <name>Mark Omernick</name>
    </author>
    <author>
      <name>Andrew M. Dai</name>
    </author>
    <author>
      <name>Thanumalayan Sankaranarayana Pillai</name>
    </author>
    <author>
      <name>Marie Pellat</name>
    </author>
    <author>
      <name>Aitor Lewkowycz</name>
    </author>
    <author>
      <name>Erica Moreira</name>
    </author>
    <author>
      <name>Rewon Child</name>
    </author>
    <author>
      <name>Oleksandr Polozov</name>
    </author>
    <author>
      <name>Katherine Lee</name>
    </author>
    <author>
      <name>Zongwei Zhou</name>
    </author>
    <author>
      <name>Xuezhi Wang</name>
    </author>
    <author>
      <name>Brennan Saeta</name>
    </author>
    <author>
      <name>Mark Diaz</name>
    </author>
    <author>
      <name>Orhan Firat</name>
    </author>
    <author>
      <name>Michele Catasta</name>
    </author>
    <author>
      <name>Jason Wei</name>
    </author>
    <author>
      <name>Kathy Meier-Hellstern</name>
    </author>
    <author>
      <name>Douglas Eck</name>
    </author>
    <author>
      <name>Jeff Dean</name>
    </author>
    <author>
      <name>Slav Petrov</name>
    </author>
    <author>
      <name>Noah Fiedel</name>
    </author>
    <link href="http://arxiv.org/abs/2204.02311v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.02311v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.03162v2</id>
    <updated>2022-04-22T18:54:25Z</updated>
    <published>2022-04-07T02:17:05Z</published>
    <title>Winoground: Probing Vision and Language Models for Visio-Linguistic
  Compositionality</title>
    <summary>  We present a novel task and dataset for evaluating the ability of vision and
language models to conduct visio-linguistic compositional reasoning, which we
call Winoground. Given two images and two captions, the goal is to match them
correctly - but crucially, both captions contain a completely identical set of
words, only in a different order. The dataset was carefully hand-curated by
expert annotators and is labeled with a rich set of fine-grained tags to assist
in analyzing model performance. We probe a diverse range of state-of-the-art
vision and language models and find that, surprisingly, none of them do much
better than chance. Evidently, these models are not as skilled at
visio-linguistic compositional reasoning as we might have hoped. We perform an
extensive analysis to obtain insights into how future work might try to
mitigate these models' shortcomings. We aim for Winoground to serve as a useful
evaluation set for advancing the state of the art and driving further progress
in the field. The dataset is available at
https://huggingface.co/datasets/facebook/winoground.
</summary>
    <author>
      <name>Tristan Thrush</name>
    </author>
    <author>
      <name>Ryan Jiang</name>
    </author>
    <author>
      <name>Max Bartolo</name>
    </author>
    <author>
      <name>Amanpreet Singh</name>
    </author>
    <author>
      <name>Adina Williams</name>
    </author>
    <author>
      <name>Douwe Kiela</name>
    </author>
    <author>
      <name>Candace Ross</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.03162v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.03162v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.03409v2</id>
    <updated>2022-07-01T19:53:30Z</updated>
    <published>2022-04-07T12:48:16Z</published>
    <title>MAESTRO: Matched Speech Text Representations through Modality Matching</title>
    <summary>  We present Maestro, a self-supervised training method to unify
representations learnt from speech and text modalities. Self-supervised
learning from speech signals aims to learn the latent structure inherent in the
signal, while self-supervised learning from text attempts to capture lexical
information. Learning aligned representations from unpaired speech and text
sequences is a challenging task. Previous work either implicitly enforced the
representations learnt from these two modalities to be aligned in the latent
space through multitasking and parameter sharing or explicitly through
conversion of modalities via speech synthesis. While the former suffers from
interference between the two modalities, the latter introduces additional
complexity. In this paper, we propose Maestro, a novel algorithm to learn
unified representations from both these modalities simultaneously that can
transfer to diverse downstream tasks such as Automated Speech Recognition (ASR)
and Speech Translation (ST). Maestro learns unified representations through
sequence alignment, duration prediction and matching embeddings in the learned
space through an aligned masked-language model loss. We establish a new
state-of-the-art (SOTA) on VoxPopuli multilingual ASR with a 8% relative
reduction in Word Error Rate (WER), multidomain SpeechStew ASR (3.7% relative)
and 21 languages to English multilingual ST on CoVoST 2 with an improvement of
2.8 BLEU averaged over 21 languages.
</summary>
    <author>
      <name>Zhehuai Chen</name>
    </author>
    <author>
      <name>Yu Zhang</name>
    </author>
    <author>
      <name>Andrew Rosenberg</name>
    </author>
    <author>
      <name>Bhuvana Ramabhadran</name>
    </author>
    <author>
      <name>Pedro Moreno</name>
    </author>
    <author>
      <name>Ankur Bapna</name>
    </author>
    <author>
      <name>Heiga Zen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by Interspeech 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.03409v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.03409v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T10" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.06125v1</id>
    <updated>2022-04-13T01:10:33Z</updated>
    <published>2022-04-13T01:10:33Z</published>
    <title>Hierarchical Text-Conditional Image Generation with CLIP Latents</title>
    <summary>  Contrastive models like CLIP have been shown to learn robust representations
of images that capture both semantics and style. To leverage these
representations for image generation, we propose a two-stage model: a prior
that generates a CLIP image embedding given a text caption, and a decoder that
generates an image conditioned on the image embedding. We show that explicitly
generating image representations improves image diversity with minimal loss in
photorealism and caption similarity. Our decoders conditioned on image
representations can also produce variations of an image that preserve both its
semantics and style, while varying the non-essential details absent from the
image representation. Moreover, the joint embedding space of CLIP enables
language-guided image manipulations in a zero-shot fashion. We use diffusion
models for the decoder and experiment with both autoregressive and diffusion
models for the prior, finding that the latter are computationally more
efficient and produce higher-quality samples.
</summary>
    <author>
      <name>Aditya Ramesh</name>
    </author>
    <author>
      <name>Prafulla Dhariwal</name>
    </author>
    <author>
      <name>Alex Nichol</name>
    </author>
    <author>
      <name>Casey Chu</name>
    </author>
    <author>
      <name>Mark Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2204.06125v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.06125v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.06745v1</id>
    <updated>2022-04-14T04:00:27Z</updated>
    <published>2022-04-14T04:00:27Z</published>
    <title>GPT-NeoX-20B: An Open-Source Autoregressive Language Model</title>
    <summary>  We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language
model trained on the Pile, whose weights will be made freely and openly
available to the public through a permissive license. It is, to the best of our
knowledge, the largest dense autoregressive model that has publicly available
weights at the time of submission. In this work, we describe \model{}'s
architecture and training and evaluate its performance on a range of
language-understanding, mathematics, and knowledge-based tasks. We find that
GPT-NeoX-20B is a particularly powerful few-shot reasoner and gains far more in
performance when evaluated five-shot than similarly sized GPT-3 and FairSeq
models. We open-source the training and evaluation code, as well as the model
weights, at https://github.com/EleutherAI/gpt-neox.
</summary>
    <author>
      <name>Sid Black</name>
    </author>
    <author>
      <name>Stella Biderman</name>
    </author>
    <author>
      <name>Eric Hallahan</name>
    </author>
    <author>
      <name>Quentin Anthony</name>
    </author>
    <author>
      <name>Leo Gao</name>
    </author>
    <author>
      <name>Laurence Golding</name>
    </author>
    <author>
      <name>Horace He</name>
    </author>
    <author>
      <name>Connor Leahy</name>
    </author>
    <author>
      <name>Kyle McDonell</name>
    </author>
    <author>
      <name>Jason Phang</name>
    </author>
    <author>
      <name>Michael Pieler</name>
    </author>
    <author>
      <name>USVSN Sai Prashanth</name>
    </author>
    <author>
      <name>Shivanshu Purohit</name>
    </author>
    <author>
      <name>Laria Reynolds</name>
    </author>
    <author>
      <name>Jonathan Tow</name>
    </author>
    <author>
      <name>Ben Wang</name>
    </author>
    <author>
      <name>Samuel Weinbach</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the Proceedings of the ACL Workshop on Challenges &amp;
  Perspectives in Creating Large Language Models</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.06745v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.06745v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.14198v2</id>
    <updated>2022-11-15T23:07:37Z</updated>
    <published>2022-04-29T16:29:01Z</published>
    <title>Flamingo: a Visual Language Model for Few-Shot Learning</title>
    <summary>  Building models that can be rapidly adapted to novel tasks using only a
handful of annotated examples is an open challenge for multimodal machine
learning research. We introduce Flamingo, a family of Visual Language Models
(VLM) with this ability. We propose key architectural innovations to: (i)
bridge powerful pretrained vision-only and language-only models, (ii) handle
sequences of arbitrarily interleaved visual and textual data, and (iii)
seamlessly ingest images or videos as inputs. Thanks to their flexibility,
Flamingo models can be trained on large-scale multimodal web corpora containing
arbitrarily interleaved text and images, which is key to endow them with
in-context few-shot learning capabilities. We perform a thorough evaluation of
our models, exploring and measuring their ability to rapidly adapt to a variety
of image and video tasks. These include open-ended tasks such as visual
question-answering, where the model is prompted with a question which it has to
answer; captioning tasks, which evaluate the ability to describe a scene or an
event; and close-ended tasks such as multiple-choice visual question-answering.
For tasks lying anywhere on this spectrum, a single Flamingo model can achieve
a new state of the art with few-shot learning, simply by prompting the model
with task-specific examples. On numerous benchmarks, Flamingo outperforms
models fine-tuned on thousands of times more task-specific data.
</summary>
    <author>
      <name>Jean-Baptiste Alayrac</name>
    </author>
    <author>
      <name>Jeff Donahue</name>
    </author>
    <author>
      <name>Pauline Luc</name>
    </author>
    <author>
      <name>Antoine Miech</name>
    </author>
    <author>
      <name>Iain Barr</name>
    </author>
    <author>
      <name>Yana Hasson</name>
    </author>
    <author>
      <name>Karel Lenc</name>
    </author>
    <author>
      <name>Arthur Mensch</name>
    </author>
    <author>
      <name>Katie Millican</name>
    </author>
    <author>
      <name>Malcolm Reynolds</name>
    </author>
    <author>
      <name>Roman Ring</name>
    </author>
    <author>
      <name>Eliza Rutherford</name>
    </author>
    <author>
      <name>Serkan Cabi</name>
    </author>
    <author>
      <name>Tengda Han</name>
    </author>
    <author>
      <name>Zhitao Gong</name>
    </author>
    <author>
      <name>Sina Samangooei</name>
    </author>
    <author>
      <name>Marianne Monteiro</name>
    </author>
    <author>
      <name>Jacob Menick</name>
    </author>
    <author>
      <name>Sebastian Borgeaud</name>
    </author>
    <author>
      <name>Andrew Brock</name>
    </author>
    <author>
      <name>Aida Nematzadeh</name>
    </author>
    <author>
      <name>Sahand Sharifzadeh</name>
    </author>
    <author>
      <name>Mikolaj Binkowski</name>
    </author>
    <author>
      <name>Ricardo Barreira</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Andrew Zisserman</name>
    </author>
    <author>
      <name>Karen Simonyan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">54 pages. In Proceedings of Neural Information Processing Systems
  (NeurIPS) 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.14198v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.14198v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.01580v1</id>
    <updated>2022-05-03T15:54:44Z</updated>
    <published>2022-05-03T15:54:44Z</published>
    <title>Better plain ViT baselines for ImageNet-1k</title>
    <summary>  It is commonly accepted that the Vision Transformer model requires
sophisticated regularization techniques to excel at ImageNet-1k scale data.
Surprisingly, we find this is not the case and standard data augmentation is
sufficient. This note presents a few minor modifications to the original Vision
Transformer (ViT) vanilla training setting that dramatically improve the
performance of plain ViT models. Notably, 90 epochs of training surpass 76%
top-1 accuracy in under seven hours on a TPUv3-8, similar to the classic
ResNet50 baseline, and 300 epochs of training reach 80% in less than one day.
</summary>
    <author>
      <name>Lucas Beyer</name>
    </author>
    <author>
      <name>Xiaohua Zhai</name>
    </author>
    <author>
      <name>Alexander Kolesnikov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code available at https://github.com/google-research/big_vision</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.01580v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.01580v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.01917v2</id>
    <updated>2022-06-14T00:48:04Z</updated>
    <published>2022-05-04T07:01:14Z</published>
    <title>CoCa: Contrastive Captioners are Image-Text Foundation Models</title>
    <summary>  Exploring large-scale pretrained foundation models is of significant interest
in computer vision because these models can be quickly transferred to many
downstream tasks. This paper presents Contrastive Captioner (CoCa), a
minimalist design to pretrain an image-text encoder-decoder foundation model
jointly with contrastive loss and captioning loss, thereby subsuming model
capabilities from contrastive approaches like CLIP and generative methods like
SimVLM. In contrast to standard encoder-decoder transformers where all decoder
layers attend to encoder outputs, CoCa omits cross-attention in the first half
of decoder layers to encode unimodal text representations, and cascades the
remaining decoder layers which cross-attend to the image encoder for multimodal
image-text representations. We apply a contrastive loss between unimodal image
and text embeddings, in addition to a captioning loss on the multimodal decoder
outputs which predicts text tokens autoregressively. By sharing the same
computational graph, the two training objectives are computed efficiently with
minimal overhead. CoCa is pretrained end-to-end and from scratch on both
web-scale alt-text data and annotated images by treating all labels simply as
text, seamlessly unifying natural language supervision for representation
learning. Empirically, CoCa achieves state-of-the-art performance with
zero-shot transfer or minimal task-specific adaptation on a broad range of
downstream tasks, spanning visual recognition (ImageNet, Kinetics-400/600/700,
Moments-in-Time), crossmodal retrieval (MSCOCO, Flickr30K, MSR-VTT), multimodal
understanding (VQA, SNLI-VE, NLVR2), and image captioning (MSCOCO, NoCaps).
Notably on ImageNet classification, CoCa obtains 86.3% zero-shot top-1
accuracy, 90.6% with a frozen encoder and learned classification head, and new
state-of-the-art 91.0% top-1 accuracy on ImageNet with a finetuned encoder.
</summary>
    <author>
      <name>Jiahui Yu</name>
    </author>
    <author>
      <name>Zirui Wang</name>
    </author>
    <author>
      <name>Vijay Vasudevan</name>
    </author>
    <author>
      <name>Legg Yeung</name>
    </author>
    <author>
      <name>Mojtaba Seyedhosseini</name>
    </author>
    <author>
      <name>Yonghui Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.01917v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.01917v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.06175v3</id>
    <updated>2022-11-11T10:04:29Z</updated>
    <published>2022-05-12T16:03:26Z</published>
    <title>A Generalist Agent</title>
    <summary>  Inspired by progress in large-scale language modeling, we apply a similar
approach towards building a single generalist agent beyond the realm of text
outputs. The agent, which we refer to as Gato, works as a multi-modal,
multi-task, multi-embodiment generalist policy. The same network with the same
weights can play Atari, caption images, chat, stack blocks with a real robot
arm and much more, deciding based on its context whether to output text, joint
torques, button presses, or other tokens. In this report we describe the model
and the data, and document the current capabilities of Gato.
</summary>
    <author>
      <name>Scott Reed</name>
    </author>
    <author>
      <name>Konrad Zolna</name>
    </author>
    <author>
      <name>Emilio Parisotto</name>
    </author>
    <author>
      <name>Sergio Gomez Colmenarejo</name>
    </author>
    <author>
      <name>Alexander Novikov</name>
    </author>
    <author>
      <name>Gabriel Barth-Maron</name>
    </author>
    <author>
      <name>Mai Gimenez</name>
    </author>
    <author>
      <name>Yury Sulsky</name>
    </author>
    <author>
      <name>Jackie Kay</name>
    </author>
    <author>
      <name>Jost Tobias Springenberg</name>
    </author>
    <author>
      <name>Tom Eccles</name>
    </author>
    <author>
      <name>Jake Bruce</name>
    </author>
    <author>
      <name>Ali Razavi</name>
    </author>
    <author>
      <name>Ashley Edwards</name>
    </author>
    <author>
      <name>Nicolas Heess</name>
    </author>
    <author>
      <name>Yutian Chen</name>
    </author>
    <author>
      <name>Raia Hadsell</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Mahyar Bordbar</name>
    </author>
    <author>
      <name>Nando de Freitas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at TMLR, 42 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Transactions on Machine Learning Research, 11/2022,
  https://openreview.net/forum?id=1ikK0kHjvj</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2205.06175v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.06175v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.10343v2</id>
    <updated>2022-10-14T17:39:04Z</updated>
    <published>2022-05-20T17:56:17Z</published>
    <title>Towards Understanding Grokking: An Effective Theory of Representation
  Learning</title>
    <summary>  We aim to understand grokking, a phenomenon where models generalize long
after overfitting their training set. We present both a microscopic analysis
anchored by an effective theory and a macroscopic analysis of phase diagrams
describing learning performance across hyperparameters. We find that
generalization originates from structured representations whose training
dynamics and dependence on training set size can be predicted by our effective
theory in a toy setting. We observe empirically the presence of four learning
phases: comprehension, grokking, memorization, and confusion. We find
representation learning to occur only in a "Goldilocks zone" (including
comprehension and grokking) between memorization and confusion. We find on
transformers the grokking phase stays closer to the memorization phase
(compared to the comprehension phase), leading to delayed generalization. The
Goldilocks phase is reminiscent of "intelligence from starvation" in Darwinian
evolution, where resource limitations drive discovery of more efficient
solutions. This study not only provides intuitive explanations of the origin of
grokking, but also highlights the usefulness of physics-inspired tools, e.g.,
effective theories and phase diagrams, for understanding deep learning.
</summary>
    <author>
      <name>Ziming Liu</name>
    </author>
    <author>
      <name>Ouail Kitouni</name>
    </author>
    <author>
      <name>Niklas Nolte</name>
    </author>
    <author>
      <name>Eric J. Michaud</name>
    </author>
    <author>
      <name>Max Tegmark</name>
    </author>
    <author>
      <name>Mike Williams</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by NeurIPS 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.10343v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.10343v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.class-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.11487v1</id>
    <updated>2022-05-23T17:42:53Z</updated>
    <published>2022-05-23T17:42:53Z</published>
    <title>Photorealistic Text-to-Image Diffusion Models with Deep Language
  Understanding</title>
    <summary>  We present Imagen, a text-to-image diffusion model with an unprecedented
degree of photorealism and a deep level of language understanding. Imagen
builds on the power of large transformer language models in understanding text
and hinges on the strength of diffusion models in high-fidelity image
generation. Our key discovery is that generic large language models (e.g. T5),
pretrained on text-only corpora, are surprisingly effective at encoding text
for image synthesis: increasing the size of the language model in Imagen boosts
both sample fidelity and image-text alignment much more than increasing the
size of the image diffusion model. Imagen achieves a new state-of-the-art FID
score of 7.27 on the COCO dataset, without ever training on COCO, and human
raters find Imagen samples to be on par with the COCO data itself in image-text
alignment. To assess text-to-image models in greater depth, we introduce
DrawBench, a comprehensive and challenging benchmark for text-to-image models.
With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP,
Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen
over other models in side-by-side comparisons, both in terms of sample quality
and image-text alignment. See https://imagen.research.google/ for an overview
of the results.
</summary>
    <author>
      <name>Chitwan Saharia</name>
    </author>
    <author>
      <name>William Chan</name>
    </author>
    <author>
      <name>Saurabh Saxena</name>
    </author>
    <author>
      <name>Lala Li</name>
    </author>
    <author>
      <name>Jay Whang</name>
    </author>
    <author>
      <name>Emily Denton</name>
    </author>
    <author>
      <name>Seyed Kamyar Seyed Ghasemipour</name>
    </author>
    <author>
      <name>Burcu Karagol Ayan</name>
    </author>
    <author>
      <name>S. Sara Mahdavi</name>
    </author>
    <author>
      <name>Rapha Gontijo Lopes</name>
    </author>
    <author>
      <name>Tim Salimans</name>
    </author>
    <author>
      <name>Jonathan Ho</name>
    </author>
    <author>
      <name>David J Fleet</name>
    </author>
    <author>
      <name>Mohammad Norouzi</name>
    </author>
    <link href="http://arxiv.org/abs/2205.11487v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.11487v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.14100v5</id>
    <updated>2022-12-15T19:21:35Z</updated>
    <published>2022-05-27T17:03:38Z</published>
    <title>GIT: A Generative Image-to-text Transformer for Vision and Language</title>
    <summary>  In this paper, we design and train a Generative Image-to-text Transformer,
GIT, to unify vision-language tasks such as image/video captioning and question
answering. While generative models provide a consistent network architecture
between pre-training and fine-tuning, existing work typically contains complex
structures (uni/multi-modal encoder/decoder) and depends on external modules
such as object detectors/taggers and optical character recognition (OCR). In
GIT, we simplify the architecture as one image encoder and one text decoder
under a single language modeling task. We also scale up the pre-training data
and the model size to boost the model performance. Without bells and whistles,
our GIT establishes new state of the arts on 12 challenging benchmarks with a
large margin. For instance, our model surpasses the human performance for the
first time on TextCaps (138.2 vs. 125.5 in CIDEr). Furthermore, we present a
new scheme of generation-based image classification and scene text recognition,
achieving decent performance on standard benchmarks. Codes are released at
\url{https://github.com/microsoft/GenerativeImage2Text}.
</summary>
    <author>
      <name>Jianfeng Wang</name>
    </author>
    <author>
      <name>Zhengyuan Yang</name>
    </author>
    <author>
      <name>Xiaowei Hu</name>
    </author>
    <author>
      <name>Linjie Li</name>
    </author>
    <author>
      <name>Kevin Lin</name>
    </author>
    <author>
      <name>Zhe Gan</name>
    </author>
    <author>
      <name>Zicheng Liu</name>
    </author>
    <author>
      <name>Ce Liu</name>
    </author>
    <author>
      <name>Lijuan Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2205.14100v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.14100v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.00364v2</id>
    <updated>2022-10-11T13:20:30Z</updated>
    <published>2022-06-01T10:03:24Z</published>
    <title>Elucidating the Design Space of Diffusion-Based Generative Models</title>
    <summary>  We argue that the theory and practice of diffusion-based generative models
are currently unnecessarily convoluted and seek to remedy the situation by
presenting a design space that clearly separates the concrete design choices.
This lets us identify several changes to both the sampling and training
processes, as well as preconditioning of the score networks. Together, our
improvements yield new state-of-the-art FID of 1.79 for CIFAR-10 in a
class-conditional setting and 1.97 in an unconditional setting, with much
faster sampling (35 network evaluations per image) than prior designs. To
further demonstrate their modular nature, we show that our design changes
dramatically improve both the efficiency and quality obtainable with
pre-trained score networks from previous work, including improving the FID of a
previously trained ImageNet-64 model from 2.07 to near-SOTA 1.55, and after
re-training with our proposed improvements to a new SOTA of 1.36.
</summary>
    <author>
      <name>Tero Karras</name>
    </author>
    <author>
      <name>Miika Aittala</name>
    </author>
    <author>
      <name>Timo Aila</name>
    </author>
    <author>
      <name>Samuli Laine</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NeurIPS 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.00364v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.00364v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.04615v3</id>
    <updated>2023-06-12T17:51:15Z</updated>
    <published>2022-06-09T17:05:34Z</published>
    <title>Beyond the Imitation Game: Quantifying and extrapolating the
  capabilities of language models</title>
    <summary>  Language models demonstrate both quantitative improvement and new qualitative
capabilities with increasing scale. Despite their potentially transformative
impact, these new capabilities are as yet poorly characterized. In order to
inform future research, prepare for disruptive new model capabilities, and
ameliorate socially harmful effects, it is vital that we understand the present
and near-future capabilities and limitations of language models. To address
this challenge, we introduce the Beyond the Imitation Game benchmark
(BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450
authors across 132 institutions. Task topics are diverse, drawing problems from
linguistics, childhood development, math, common-sense reasoning, biology,
physics, social bias, software development, and beyond. BIG-bench focuses on
tasks that are believed to be beyond the capabilities of current language
models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense
transformer architectures, and Switch-style sparse transformers on BIG-bench,
across model sizes spanning millions to hundreds of billions of parameters. In
addition, a team of human expert raters performed all tasks in order to provide
a strong baseline. Findings include: model performance and calibration both
improve with scale, but are poor in absolute terms (and when compared with
rater performance); performance is remarkably similar across model classes,
though with benefits from sparsity; tasks that improve gradually and
predictably commonly involve a large knowledge or memorization component,
whereas tasks that exhibit "breakthrough" behavior at a critical scale often
involve multiple steps or components, or brittle metrics; social bias typically
increases with scale in settings with ambiguous context, but this can be
improved with prompting.
</summary>
    <author>
      <name>Aarohi Srivastava</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Abhinav Rastogi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Abhishek Rao</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Abu Awal Md Shoeb</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Abubakar Abid</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Adam Fisch</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Adam R. Brown</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Adam Santoro</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Aditya Gupta</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Adrià Garriga-Alonso</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Agnieszka Kluska</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Aitor Lewkowycz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Akshat Agarwal</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Alethea Power</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Alex Ray</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Alex Warstadt</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Alexander W. Kocurek</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ali Safaya</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ali Tazarv</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Alice Xiang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Alicia Parrish</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Allen Nie</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Aman Hussain</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Amanda Askell</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Amanda Dsouza</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ambrose Slone</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ameet Rahane</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Anantharaman S. Iyer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Anders Andreassen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Andrea Madotto</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Andrea Santilli</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Andreas Stuhlmüller</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Andrew Dai</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Andrew La</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Andrew Lampinen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Andy Zou</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Angela Jiang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Angelica Chen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Anh Vuong</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Animesh Gupta</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Anna Gottardi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Antonio Norelli</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Anu Venkatesh</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Arash Gholamidavoodi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Arfa Tabassum</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Arul Menezes</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Arun Kirubarajan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Asher Mullokandov</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ashish Sabharwal</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Austin Herrick</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Avia Efrat</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Aykut Erdem</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ayla Karakaş</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>B. Ryan Roberts</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Bao Sheng Loe</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Barret Zoph</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Bartłomiej Bojanowski</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Batuhan Özyurt</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Behnam Hedayatnia</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Behnam Neyshabur</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Benjamin Inden</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Benno Stein</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Berk Ekmekci</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Bill Yuchen Lin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Blake Howald</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Bryan Orinion</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Cameron Diao</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Cameron Dour</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Catherine Stinson</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Cedrick Argueta</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>César Ferri Ramírez</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Chandan Singh</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Charles Rathkopf</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Chenlin Meng</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Chitta Baral</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Chiyu Wu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Chris Callison-Burch</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Chris Waites</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Christian Voigt</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Christopher D. Manning</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Christopher Potts</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Cindy Ramirez</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Clara E. Rivera</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Clemencia Siro</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Colin Raffel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Courtney Ashcraft</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Cristina Garbacea</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Damien Sileo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Dan Garrette</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Dan Hendrycks</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Dan Kilman</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Dan Roth</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Daniel Freeman</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Daniel Khashabi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Daniel Levy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Daniel Moseguí González</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Danielle Perszyk</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Danny Hernandez</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Danqi Chen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Daphne Ippolito</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Dar Gilboa</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>David Dohan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>David Drakard</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>David Jurgens</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Debajyoti Datta</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Deep Ganguli</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Denis Emelin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Denis Kleyko</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Deniz Yuret</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Derek Chen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Derek Tam</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Dieuwke Hupkes</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Diganta Misra</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Dilyar Buzan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Dimitri Coelho Mollo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Diyi Yang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Dong-Ho Lee</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Dylan Schrader</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ekaterina Shutova</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ekin Dogus Cubuk</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Elad Segal</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Eleanor Hagerman</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Elizabeth Barnes</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Elizabeth Donoway</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ellie Pavlick</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Emanuele Rodola</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Emma Lam</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Eric Chu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Eric Tang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Erkut Erdem</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ernie Chang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ethan A. Chi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ethan Dyer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ethan Jerzak</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ethan Kim</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Eunice Engefu Manyasi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Evgenii Zheltonozhskii</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Fanyue Xia</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Fatemeh Siar</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Fernando Martínez-Plumed</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Francesca Happé</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Francois Chollet</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Frieda Rong</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Gaurav Mishra</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Genta Indra Winata</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Gerard de Melo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Germán Kruszewski</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Giambattista Parascandolo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Giorgio Mariani</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Gloria Wang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Gonzalo Jaimovitch-López</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Gregor Betz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Guy Gur-Ari</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Hana Galijasevic</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Hannah Kim</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Hannah Rashkin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Hannaneh Hajishirzi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Harsh Mehta</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Hayden Bogar</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Henry Shevlin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Hinrich Schütze</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Hiromu Yakura</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Hongming Zhang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Hugh Mee Wong</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ian Ng</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Isaac Noble</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jaap Jumelet</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jack Geissinger</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jackson Kernion</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jacob Hilton</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jaehoon Lee</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jaime Fernández Fisac</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>James B. Simon</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>James Koppel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>James Zheng</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>James Zou</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jan Kocoń</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jana Thompson</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Janelle Wingfield</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jared Kaplan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jarema Radom</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jascha Sohl-Dickstein</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jason Phang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jason Wei</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jason Yosinski</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jekaterina Novikova</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jelle Bosscher</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jennifer Marsh</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jeremy Kim</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jeroen Taal</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jesse Engel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jesujoba Alabi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jiacheng Xu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jiaming Song</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jillian Tang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Joan Waweru</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>John Burden</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>John Miller</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>John U. Balis</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jonathan Batchelder</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jonathan Berant</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jörg Frohberg</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jos Rozen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Jose Hernandez-Orallo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Joseph Boudeman</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Joseph Guerr</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Joseph Jones</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Joshua B. Tenenbaum</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Joshua S. Rule</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Joyce Chua</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Kamil Kanclerz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Karen Livescu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Karl Krauth</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Karthik Gopalakrishnan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Katerina Ignatyeva</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Katja Markert</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Kaustubh D. Dhole</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Kevin Gimpel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Kevin Omondi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Kory Mathewson</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Kristen Chiafullo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ksenia Shkaruta</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Kumar Shridhar</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Kyle McDonell</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Kyle Richardson</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Laria Reynolds</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Leo Gao</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Li Zhang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Liam Dugan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Lianhui Qin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Lidia Contreras-Ochando</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Louis-Philippe Morency</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Luca Moschella</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Lucas Lam</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Lucy Noble</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ludwig Schmidt</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Luheng He</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Luis Oliveros Colón</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Luke Metz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Lütfi Kerem Şenel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Maarten Bosma</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Maarten Sap</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Maartje ter Hoeve</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Maheen Farooqi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Manaal Faruqui</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Mantas Mazeika</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Marco Baturan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Marco Marelli</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Marco Maru</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Maria Jose Ramírez Quintana</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Marie Tolkiehn</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Mario Giulianelli</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Martha Lewis</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Martin Potthast</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Matthew L. Leavitt</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Matthias Hagen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Mátyás Schubert</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Medina Orduna Baitemirova</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Melody Arnaud</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Melvin McElrath</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Michael A. Yee</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Michael Cohen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Michael Gu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Michael Ivanitskiy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Michael Starritt</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Michael Strube</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Michał Swędrowski</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Michele Bevilacqua</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Michihiro Yasunaga</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Mihir Kale</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Mike Cain</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Mimee Xu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Mirac Suzgun</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Mitch Walker</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Mo Tiwari</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Mohit Bansal</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Moin Aminnaseri</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Mor Geva</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Mozhdeh Gheini</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Mukund Varma T</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Nanyun Peng</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Nathan A. Chi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Nayeon Lee</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Neta Gur-Ari Krakover</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Nicholas Cameron</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Nicholas Roberts</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Nick Doiron</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Nicole Martinez</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Nikita Nangia</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Niklas Deckers</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Niklas Muennighoff</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Nitish Shirish Keskar</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Niveditha S. Iyer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Noah Constant</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Noah Fiedel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Nuan Wen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Oliver Zhang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Omar Agha</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Omar Elbaghdadi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Omer Levy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Owain Evans</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Pablo Antonio Moreno Casares</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Parth Doshi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Pascale Fung</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Paul Pu Liang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Paul Vicol</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Pegah Alipoormolabashi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Peiyuan Liao</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Percy Liang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Peter Chang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Peter Eckersley</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Phu Mon Htut</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Pinyu Hwang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Piotr Miłkowski</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Piyush Patil</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Pouya Pezeshkpour</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Priti Oli</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Qiaozhu Mei</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Qing Lyu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Qinlang Chen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Rabin Banjade</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Rachel Etta Rudolph</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Raefer Gabriel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Rahel Habacker</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ramon Risco</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Raphaël Millière</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Rhythm Garg</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Richard Barnes</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Rif A. Saurous</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Riku Arakawa</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Robbe Raymaekers</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Robert Frank</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Rohan Sikand</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Roman Novak</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Roman Sitelew</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ronan LeBras</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Rosanne Liu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Rowan Jacobs</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Rui Zhang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ryan Chi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ryan Lee</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ryan Stovall</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Ryan Teehan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Rylan Yang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Sahib Singh</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Saif M. Mohammad</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Sajant Anand</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Sam Dillavou</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Sam Shleifer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Sam Wiseman</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Samuel Gruetter</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Samuel R. Bowman</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Samuel S. Schoenholz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Sanghyun Han</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Sanjeev Kwatra</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Sarah A. Rous</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Sarik Ghazarian</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Sayan Ghosh</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Sean Casey</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Sebastian Bischoff</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Sebastian Gehrmann</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Sebastian Schuster</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Sepideh Sadeghi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Shadi Hamdan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Sharon Zhou</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Shashank Srivastava</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Sherry Shi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Shikhar Singh</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Shima Asaadi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Shixiang Shane Gu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Shubh Pachchigar</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Shubham Toshniwal</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name>Shyam Upadhyay</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name> Shyamolima</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shammie</arxiv:affiliation>
    </author>
    <author>
      <name> Debnath</name>
    </author>
    <author>
      <name>Siamak Shakeri</name>
    </author>
    <author>
      <name>Simon Thormeyer</name>
    </author>
    <author>
      <name>Simone Melzi</name>
    </author>
    <author>
      <name>Siva Reddy</name>
    </author>
    <author>
      <name>Sneha Priscilla Makini</name>
    </author>
    <author>
      <name>Soo-Hwan Lee</name>
    </author>
    <author>
      <name>Spencer Torene</name>
    </author>
    <author>
      <name>Sriharsha Hatwar</name>
    </author>
    <author>
      <name>Stanislas Dehaene</name>
    </author>
    <author>
      <name>Stefan Divic</name>
    </author>
    <author>
      <name>Stefano Ermon</name>
    </author>
    <author>
      <name>Stella Biderman</name>
    </author>
    <author>
      <name>Stephanie Lin</name>
    </author>
    <author>
      <name>Stephen Prasad</name>
    </author>
    <author>
      <name>Steven T. Piantadosi</name>
    </author>
    <author>
      <name>Stuart M. Shieber</name>
    </author>
    <author>
      <name>Summer Misherghi</name>
    </author>
    <author>
      <name>Svetlana Kiritchenko</name>
    </author>
    <author>
      <name>Swaroop Mishra</name>
    </author>
    <author>
      <name>Tal Linzen</name>
    </author>
    <author>
      <name>Tal Schuster</name>
    </author>
    <author>
      <name>Tao Li</name>
    </author>
    <author>
      <name>Tao Yu</name>
    </author>
    <author>
      <name>Tariq Ali</name>
    </author>
    <author>
      <name>Tatsu Hashimoto</name>
    </author>
    <author>
      <name>Te-Lin Wu</name>
    </author>
    <author>
      <name>Théo Desbordes</name>
    </author>
    <author>
      <name>Theodore Rothschild</name>
    </author>
    <author>
      <name>Thomas Phan</name>
    </author>
    <author>
      <name>Tianle Wang</name>
    </author>
    <author>
      <name>Tiberius Nkinyili</name>
    </author>
    <author>
      <name>Timo Schick</name>
    </author>
    <author>
      <name>Timofei Kornev</name>
    </author>
    <author>
      <name>Titus Tunduny</name>
    </author>
    <author>
      <name>Tobias Gerstenberg</name>
    </author>
    <author>
      <name>Trenton Chang</name>
    </author>
    <author>
      <name>Trishala Neeraj</name>
    </author>
    <author>
      <name>Tushar Khot</name>
    </author>
    <author>
      <name>Tyler Shultz</name>
    </author>
    <author>
      <name>Uri Shaham</name>
    </author>
    <author>
      <name>Vedant Misra</name>
    </author>
    <author>
      <name>Vera Demberg</name>
    </author>
    <author>
      <name>Victoria Nyamai</name>
    </author>
    <author>
      <name>Vikas Raunak</name>
    </author>
    <author>
      <name>Vinay Ramasesh</name>
    </author>
    <author>
      <name>Vinay Uday Prabhu</name>
    </author>
    <author>
      <name>Vishakh Padmakumar</name>
    </author>
    <author>
      <name>Vivek Srikumar</name>
    </author>
    <author>
      <name>William Fedus</name>
    </author>
    <author>
      <name>William Saunders</name>
    </author>
    <author>
      <name>William Zhang</name>
    </author>
    <author>
      <name>Wout Vossen</name>
    </author>
    <author>
      <name>Xiang Ren</name>
    </author>
    <author>
      <name>Xiaoyu Tong</name>
    </author>
    <author>
      <name>Xinran Zhao</name>
    </author>
    <author>
      <name>Xinyi Wu</name>
    </author>
    <author>
      <name>Xudong Shen</name>
    </author>
    <author>
      <name>Yadollah Yaghoobzadeh</name>
    </author>
    <author>
      <name>Yair Lakretz</name>
    </author>
    <author>
      <name>Yangqiu Song</name>
    </author>
    <author>
      <name>Yasaman Bahri</name>
    </author>
    <author>
      <name>Yejin Choi</name>
    </author>
    <author>
      <name>Yichi Yang</name>
    </author>
    <author>
      <name>Yiding Hao</name>
    </author>
    <author>
      <name>Yifu Chen</name>
    </author>
    <author>
      <name>Yonatan Belinkov</name>
    </author>
    <author>
      <name>Yu Hou</name>
    </author>
    <author>
      <name>Yufang Hou</name>
    </author>
    <author>
      <name>Yuntao Bai</name>
    </author>
    <author>
      <name>Zachary Seid</name>
    </author>
    <author>
      <name>Zhuoye Zhao</name>
    </author>
    <author>
      <name>Zijian Wang</name>
    </author>
    <author>
      <name>Zijie J. Wang</name>
    </author>
    <author>
      <name>Zirui Wang</name>
    </author>
    <author>
      <name>Ziyi Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 17 figures + references and appendices, repo:
  https://github.com/google/BIG-bench</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Transactions on Machine Learning Research, May/2022,
  https://openreview.net/forum?id=uyTL5Bvosj</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2206.04615v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.04615v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.13170v1</id>
    <updated>2022-06-27T10:27:28Z</updated>
    <published>2022-06-27T10:27:28Z</published>
    <title>Measuring and Improving the Use of Graph Information in Graph Neural
  Networks</title>
    <summary>  Graph neural networks (GNNs) have been widely used for representation
learning on graph data. However, there is limited understanding on how much
performance GNNs actually gain from graph data. This paper introduces a
context-surrounding GNN framework and proposes two smoothness metrics to
measure the quantity and quality of information obtained from graph data. A new
GNN model, called CS-GNN, is then designed to improve the use of graph
information based on the smoothness values of a graph. CS-GNN is shown to
achieve better performance than existing methods in different types of real
graphs.
</summary>
    <author>
      <name>Yifan Hou</name>
    </author>
    <author>
      <name>Jian Zhang</name>
    </author>
    <author>
      <name>James Cheng</name>
    </author>
    <author>
      <name>Kaili Ma</name>
    </author>
    <author>
      <name>Richard T. B. Ma</name>
    </author>
    <author>
      <name>Hongzhi Chen</name>
    </author>
    <author>
      <name>Ming-Chang Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been published in ICLR 2020. Code and Dataset can be
  found here: https://github.com/yifan-h/CS-GNN</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.13170v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.13170v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.14858v2</id>
    <updated>2022-07-01T02:15:12Z</updated>
    <published>2022-06-29T18:54:49Z</published>
    <title>Solving Quantitative Reasoning Problems with Language Models</title>
    <summary>  Language models have achieved remarkable performance on a wide range of tasks
that require natural language understanding. Nevertheless, state-of-the-art
models have generally struggled with tasks that require quantitative reasoning,
such as solving mathematics, science, and engineering problems at the college
level. To help close this gap, we introduce Minerva, a large language model
pretrained on general natural language data and further trained on technical
content. The model achieves state-of-the-art performance on technical
benchmarks without the use of external tools. We also evaluate our model on
over two hundred undergraduate-level problems in physics, biology, chemistry,
economics, and other sciences that require quantitative reasoning, and find
that the model can correctly answer nearly a third of them.
</summary>
    <author>
      <name>Aitor Lewkowycz</name>
    </author>
    <author>
      <name>Anders Andreassen</name>
    </author>
    <author>
      <name>David Dohan</name>
    </author>
    <author>
      <name>Ethan Dyer</name>
    </author>
    <author>
      <name>Henryk Michalewski</name>
    </author>
    <author>
      <name>Vinay Ramasesh</name>
    </author>
    <author>
      <name>Ambrose Slone</name>
    </author>
    <author>
      <name>Cem Anil</name>
    </author>
    <author>
      <name>Imanol Schlag</name>
    </author>
    <author>
      <name>Theo Gutman-Solo</name>
    </author>
    <author>
      <name>Yuhuai Wu</name>
    </author>
    <author>
      <name>Behnam Neyshabur</name>
    </author>
    <author>
      <name>Guy Gur-Ari</name>
    </author>
    <author>
      <name>Vedant Misra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 5 figures + references and appendices</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.14858v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.14858v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.12598v1</id>
    <updated>2022-07-26T01:42:07Z</updated>
    <published>2022-07-26T01:42:07Z</published>
    <title>Classifier-Free Diffusion Guidance</title>
    <summary>  Classifier guidance is a recently introduced method to trade off mode
coverage and sample fidelity in conditional diffusion models post training, in
the same spirit as low temperature sampling or truncation in other types of
generative models. Classifier guidance combines the score estimate of a
diffusion model with the gradient of an image classifier and thereby requires
training an image classifier separate from the diffusion model. It also raises
the question of whether guidance can be performed without a classifier. We show
that guidance can be indeed performed by a pure generative model without such a
classifier: in what we call classifier-free guidance, we jointly train a
conditional and an unconditional diffusion model, and we combine the resulting
conditional and unconditional score estimates to attain a trade-off between
sample quality and diversity similar to that obtained using classifier
guidance.
</summary>
    <author>
      <name>Jonathan Ho</name>
    </author>
    <author>
      <name>Tim Salimans</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A short version of this paper appeared in the NeurIPS 2021 Workshop
  on Deep Generative Models and Downstream Applications:
  https://openreview.net/pdf?id=qw8AKxfYbI</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.12598v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.12598v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.09392v1</id>
    <updated>2022-08-19T15:18:39Z</updated>
    <published>2022-08-19T15:18:39Z</published>
    <title>Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise</title>
    <summary>  Standard diffusion models involve an image transform -- adding Gaussian noise
-- and an image restoration operator that inverts this degradation. We observe
that the generative behavior of diffusion models is not strongly dependent on
the choice of image degradation, and in fact an entire family of generative
models can be constructed by varying this choice. Even when using completely
deterministic degradations (e.g., blur, masking, and more), the training and
test-time update rules that underlie diffusion models can be easily generalized
to create generative models. The success of these fully deterministic models
calls into question the community's understanding of diffusion models, which
relies on noise in either gradient Langevin dynamics or variational inference,
and paves the way for generalized diffusion models that invert arbitrary
processes. Our code is available at
https://github.com/arpitbansal297/Cold-Diffusion-Models
</summary>
    <author>
      <name>Arpit Bansal</name>
    </author>
    <author>
      <name>Eitan Borgnia</name>
    </author>
    <author>
      <name>Hong-Min Chu</name>
    </author>
    <author>
      <name>Jie S. Li</name>
    </author>
    <author>
      <name>Hamid Kazemi</name>
    </author>
    <author>
      <name>Furong Huang</name>
    </author>
    <author>
      <name>Micah Goldblum</name>
    </author>
    <author>
      <name>Jonas Geiping</name>
    </author>
    <author>
      <name>Tom Goldstein</name>
    </author>
    <link href="http://arxiv.org/abs/2208.09392v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.09392v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.10442v2</id>
    <updated>2022-08-31T02:26:45Z</updated>
    <published>2022-08-22T16:55:04Z</published>
    <title>Image as a Foreign Language: BEiT Pretraining for All Vision and
  Vision-Language Tasks</title>
    <summary>  A big convergence of language, vision, and multimodal pretraining is
emerging. In this work, we introduce a general-purpose multimodal foundation
model BEiT-3, which achieves state-of-the-art transfer performance on both
vision and vision-language tasks. Specifically, we advance the big convergence
from three aspects: backbone architecture, pretraining task, and model scaling
up. We introduce Multiway Transformers for general-purpose modeling, where the
modular architecture enables both deep fusion and modality-specific encoding.
Based on the shared backbone, we perform masked "language" modeling on images
(Imglish), texts (English), and image-text pairs ("parallel sentences") in a
unified manner. Experimental results show that BEiT-3 obtains state-of-the-art
performance on object detection (COCO), semantic segmentation (ADE20K), image
classification (ImageNet), visual reasoning (NLVR2), visual question answering
(VQAv2), image captioning (COCO), and cross-modal retrieval (Flickr30K, COCO).
</summary>
    <author>
      <name>Wenhui Wang</name>
    </author>
    <author>
      <name>Hangbo Bao</name>
    </author>
    <author>
      <name>Li Dong</name>
    </author>
    <author>
      <name>Johan Bjorck</name>
    </author>
    <author>
      <name>Zhiliang Peng</name>
    </author>
    <author>
      <name>Qiang Liu</name>
    </author>
    <author>
      <name>Kriti Aggarwal</name>
    </author>
    <author>
      <name>Owais Khan Mohammed</name>
    </author>
    <author>
      <name>Saksham Singhal</name>
    </author>
    <author>
      <name>Subhojit Som</name>
    </author>
    <author>
      <name>Furu Wei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.10442v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.10442v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.12242v2</id>
    <updated>2023-03-15T17:52:27Z</updated>
    <published>2022-08-25T17:45:49Z</published>
    <title>DreamBooth: Fine Tuning Text-to-Image Diffusion Models for
  Subject-Driven Generation</title>
    <summary>  Large text-to-image models achieved a remarkable leap in the evolution of AI,
enabling high-quality and diverse synthesis of images from a given text prompt.
However, these models lack the ability to mimic the appearance of subjects in a
given reference set and synthesize novel renditions of them in different
contexts. In this work, we present a new approach for "personalization" of
text-to-image diffusion models. Given as input just a few images of a subject,
we fine-tune a pretrained text-to-image model such that it learns to bind a
unique identifier with that specific subject. Once the subject is embedded in
the output domain of the model, the unique identifier can be used to synthesize
novel photorealistic images of the subject contextualized in different scenes.
By leveraging the semantic prior embedded in the model with a new autogenous
class-specific prior preservation loss, our technique enables synthesizing the
subject in diverse scenes, poses, views and lighting conditions that do not
appear in the reference images. We apply our technique to several
previously-unassailable tasks, including subject recontextualization,
text-guided view synthesis, and artistic rendering, all while preserving the
subject's key features. We also provide a new dataset and evaluation protocol
for this new task of subject-driven generation. Project page:
https://dreambooth.github.io/
</summary>
    <author>
      <name>Nataniel Ruiz</name>
    </author>
    <author>
      <name>Yuanzhen Li</name>
    </author>
    <author>
      <name>Varun Jampani</name>
    </author>
    <author>
      <name>Yael Pritch</name>
    </author>
    <author>
      <name>Michael Rubinstein</name>
    </author>
    <author>
      <name>Kfir Aberman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at CVPR 2023. Project page: https://dreambooth.github.io/</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.12242v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.12242v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.12415v1</id>
    <updated>2022-08-26T03:13:21Z</updated>
    <published>2022-08-26T03:13:21Z</published>
    <title>MuLan: A Joint Embedding of Music Audio and Natural Language</title>
    <summary>  Music tagging and content-based retrieval systems have traditionally been
constructed using pre-defined ontologies covering a rigid set of music
attributes or text queries. This paper presents MuLan: a first attempt at a new
generation of acoustic models that link music audio directly to unconstrained
natural language music descriptions. MuLan takes the form of a two-tower, joint
audio-text embedding model trained using 44 million music recordings (370K
hours) and weakly-associated, free-form text annotations. Through its
compatibility with a wide range of music genres and text styles (including
conventional music tags), the resulting audio-text representation subsumes
existing ontologies while graduating to true zero-shot functionalities. We
demonstrate the versatility of the MuLan embeddings with a range of experiments
including transfer learning, zero-shot music tagging, language understanding in
the music domain, and cross-modal retrieval applications.
</summary>
    <author>
      <name>Qingqing Huang</name>
    </author>
    <author>
      <name>Aren Jansen</name>
    </author>
    <author>
      <name>Joonseok Lee</name>
    </author>
    <author>
      <name>Ravi Ganti</name>
    </author>
    <author>
      <name>Judith Yue Li</name>
    </author>
    <author>
      <name>Daniel P. W. Ellis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ISMIR 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.12415v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.12415v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.03143v2</id>
    <updated>2023-07-26T03:52:36Z</updated>
    <published>2022-09-07T13:40:08Z</published>
    <title>AudioLM: a Language Modeling Approach to Audio Generation</title>
    <summary>  We introduce AudioLM, a framework for high-quality audio generation with
long-term consistency. AudioLM maps the input audio to a sequence of discrete
tokens and casts audio generation as a language modeling task in this
representation space. We show how existing audio tokenizers provide different
trade-offs between reconstruction quality and long-term structure, and we
propose a hybrid tokenization scheme to achieve both objectives. Namely, we
leverage the discretized activations of a masked language model pre-trained on
audio to capture long-term structure and the discrete codes produced by a
neural audio codec to achieve high-quality synthesis. By training on large
corpora of raw audio waveforms, AudioLM learns to generate natural and coherent
continuations given short prompts. When trained on speech, and without any
transcript or annotation, AudioLM generates syntactically and semantically
plausible speech continuations while also maintaining speaker identity and
prosody for unseen speakers. Furthermore, we demonstrate how our approach
extends beyond speech by generating coherent piano music continuations, despite
being trained without any symbolic representation of music.
</summary>
    <author>
      <name>Zalán Borsos</name>
    </author>
    <author>
      <name>Raphaël Marinier</name>
    </author>
    <author>
      <name>Damien Vincent</name>
    </author>
    <author>
      <name>Eugene Kharitonov</name>
    </author>
    <author>
      <name>Olivier Pietquin</name>
    </author>
    <author>
      <name>Matt Sharifi</name>
    </author>
    <author>
      <name>Dominik Roblek</name>
    </author>
    <author>
      <name>Olivier Teboul</name>
    </author>
    <author>
      <name>David Grangier</name>
    </author>
    <author>
      <name>Marco Tagliasacchi</name>
    </author>
    <author>
      <name>Neil Zeghidour</name>
    </author>
    <link href="http://arxiv.org/abs/2209.03143v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.03143v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.06794v4</id>
    <updated>2023-06-05T17:55:12Z</updated>
    <published>2022-09-14T17:24:07Z</published>
    <title>PaLI: A Jointly-Scaled Multilingual Language-Image Model</title>
    <summary>  Effective scaling and a flexible task interface enable large language models
to excel at many tasks. We present PaLI (Pathways Language and Image model), a
model that extends this approach to the joint modeling of language and vision.
PaLI generates text based on visual and textual inputs, and with this interface
performs many vision, language, and multimodal tasks, in many languages. To
train PaLI, we make use of large pre-trained encoder-decoder language models
and Vision Transformers (ViTs). This allows us to capitalize on their existing
capabilities and leverage the substantial cost of training them. We find that
joint scaling of the vision and language components is important. Since
existing Transformers for language are much larger than their vision
counterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the
benefits from even larger-capacity vision models. To train PaLI, we create a
large multilingual mix of pretraining tasks, based on a new image-text training
set containing 10B images and texts in over 100 languages. PaLI achieves
state-of-the-art in multiple vision and language tasks (such as captioning,
visual question-answering, scene-text understanding), while retaining a simple,
modular, and scalable design.
</summary>
    <author>
      <name>Xi Chen</name>
    </author>
    <author>
      <name>Xiao Wang</name>
    </author>
    <author>
      <name>Soravit Changpinyo</name>
    </author>
    <author>
      <name>AJ Piergiovanni</name>
    </author>
    <author>
      <name>Piotr Padlewski</name>
    </author>
    <author>
      <name>Daniel Salz</name>
    </author>
    <author>
      <name>Sebastian Goodman</name>
    </author>
    <author>
      <name>Adam Grycner</name>
    </author>
    <author>
      <name>Basil Mustafa</name>
    </author>
    <author>
      <name>Lucas Beyer</name>
    </author>
    <author>
      <name>Alexander Kolesnikov</name>
    </author>
    <author>
      <name>Joan Puigcerver</name>
    </author>
    <author>
      <name>Nan Ding</name>
    </author>
    <author>
      <name>Keran Rong</name>
    </author>
    <author>
      <name>Hassan Akbari</name>
    </author>
    <author>
      <name>Gaurav Mishra</name>
    </author>
    <author>
      <name>Linting Xue</name>
    </author>
    <author>
      <name>Ashish Thapliyal</name>
    </author>
    <author>
      <name>James Bradbury</name>
    </author>
    <author>
      <name>Weicheng Kuo</name>
    </author>
    <author>
      <name>Mojtaba Seyedhosseini</name>
    </author>
    <author>
      <name>Chao Jia</name>
    </author>
    <author>
      <name>Burcu Karagol Ayan</name>
    </author>
    <author>
      <name>Carlos Riquelme</name>
    </author>
    <author>
      <name>Andreas Steiner</name>
    </author>
    <author>
      <name>Anelia Angelova</name>
    </author>
    <author>
      <name>Xiaohua Zhai</name>
    </author>
    <author>
      <name>Neil Houlsby</name>
    </author>
    <author>
      <name>Radu Soricut</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICLR 2023 (Notable-top-5%)</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.06794v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.06794v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.14792v1</id>
    <updated>2022-09-29T13:59:46Z</updated>
    <published>2022-09-29T13:59:46Z</published>
    <title>Make-A-Video: Text-to-Video Generation without Text-Video Data</title>
    <summary>  We propose Make-A-Video -- an approach for directly translating the
tremendous recent progress in Text-to-Image (T2I) generation to Text-to-Video
(T2V). Our intuition is simple: learn what the world looks like and how it is
described from paired text-image data, and learn how the world moves from
unsupervised video footage. Make-A-Video has three advantages: (1) it
accelerates training of the T2V model (it does not need to learn visual and
multimodal representations from scratch), (2) it does not require paired
text-video data, and (3) the generated videos inherit the vastness (diversity
in aesthetic, fantastical depictions, etc.) of today's image generation models.
We design a simple yet effective way to build on T2I models with novel and
effective spatial-temporal modules. First, we decompose the full temporal U-Net
and attention tensors and approximate them in space and time. Second, we design
a spatial temporal pipeline to generate high resolution and frame rate videos
with a video decoder, interpolation model and two super resolution models that
can enable various applications besides T2V. In all aspects, spatial and
temporal resolution, faithfulness to text, and quality, Make-A-Video sets the
new state-of-the-art in text-to-video generation, as determined by both
qualitative and quantitative measures.
</summary>
    <author>
      <name>Uriel Singer</name>
    </author>
    <author>
      <name>Adam Polyak</name>
    </author>
    <author>
      <name>Thomas Hayes</name>
    </author>
    <author>
      <name>Xi Yin</name>
    </author>
    <author>
      <name>Jie An</name>
    </author>
    <author>
      <name>Songyang Zhang</name>
    </author>
    <author>
      <name>Qiyuan Hu</name>
    </author>
    <author>
      <name>Harry Yang</name>
    </author>
    <author>
      <name>Oron Ashual</name>
    </author>
    <author>
      <name>Oran Gafni</name>
    </author>
    <author>
      <name>Devi Parikh</name>
    </author>
    <author>
      <name>Sonal Gupta</name>
    </author>
    <author>
      <name>Yaniv Taigman</name>
    </author>
    <link href="http://arxiv.org/abs/2209.14792v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.14792v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.14988v1</id>
    <updated>2022-09-29T17:50:40Z</updated>
    <published>2022-09-29T17:50:40Z</published>
    <title>DreamFusion: Text-to-3D using 2D Diffusion</title>
    <summary>  Recent breakthroughs in text-to-image synthesis have been driven by diffusion
models trained on billions of image-text pairs. Adapting this approach to 3D
synthesis would require large-scale datasets of labeled 3D data and efficient
architectures for denoising 3D data, neither of which currently exist. In this
work, we circumvent these limitations by using a pretrained 2D text-to-image
diffusion model to perform text-to-3D synthesis. We introduce a loss based on
probability density distillation that enables the use of a 2D diffusion model
as a prior for optimization of a parametric image generator. Using this loss in
a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a
Neural Radiance Field, or NeRF) via gradient descent such that its 2D
renderings from random angles achieve a low loss. The resulting 3D model of the
given text can be viewed from any angle, relit by arbitrary illumination, or
composited into any 3D environment. Our approach requires no 3D training data
and no modifications to the image diffusion model, demonstrating the
effectiveness of pretrained image diffusion models as priors.
</summary>
    <author>
      <name>Ben Poole</name>
    </author>
    <author>
      <name>Ajay Jain</name>
    </author>
    <author>
      <name>Jonathan T. Barron</name>
    </author>
    <author>
      <name>Ben Mildenhall</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">see project page at https://dreamfusion3d.github.io/</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.14988v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.14988v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.15352v2</id>
    <updated>2023-03-05T09:14:16Z</updated>
    <published>2022-09-30T10:17:05Z</published>
    <title>AudioGen: Textually Guided Audio Generation</title>
    <summary>  We tackle the problem of generating audio samples conditioned on descriptive
text captions. In this work, we propose AaudioGen, an auto-regressive
generative model that generates audio samples conditioned on text inputs.
AudioGen operates on a learnt discrete audio representation. The task of
text-to-audio generation poses multiple challenges. Due to the way audio
travels through a medium, differentiating ``objects'' can be a difficult task
(e.g., separating multiple people simultaneously speaking). This is further
complicated by real-world recording conditions (e.g., background noise,
reverberation, etc.). Scarce text annotations impose another constraint,
limiting the ability to scale models. Finally, modeling high-fidelity audio
requires encoding audio at high sampling rate, leading to extremely long
sequences. To alleviate the aforementioned challenges we propose an
augmentation technique that mixes different audio samples, driving the model to
internally learn to separate multiple sources. We curated 10 datasets
containing different types of audio and text annotations to handle the scarcity
of text-audio data points. For faster inference, we explore the use of
multi-stream modeling, allowing the use of shorter sequences while maintaining
a similar bitrate and perceptual quality. We apply classifier-free guidance to
improve adherence to text. Comparing to the evaluated baselines, AudioGen
outperforms over both objective and subjective metrics. Finally, we explore the
ability of the proposed method to generate audio continuation conditionally and
unconditionally. Samples: https://felixkreuk.github.io/audiogen
</summary>
    <author>
      <name>Felix Kreuk</name>
    </author>
    <author>
      <name>Gabriel Synnaeve</name>
    </author>
    <author>
      <name>Adam Polyak</name>
    </author>
    <author>
      <name>Uriel Singer</name>
    </author>
    <author>
      <name>Alexandre Défossez</name>
    </author>
    <author>
      <name>Jade Copet</name>
    </author>
    <author>
      <name>Devi Parikh</name>
    </author>
    <author>
      <name>Yaniv Taigman</name>
    </author>
    <author>
      <name>Yossi Adi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ICLR 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.15352v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.15352v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.02186v3</id>
    <updated>2023-04-12T02:34:03Z</updated>
    <published>2022-10-05T12:19:51Z</published>
    <title>TimesNet: Temporal 2D-Variation Modeling for General Time Series
  Analysis</title>
    <summary>  Time series analysis is of immense importance in extensive applications, such
as weather forecasting, anomaly detection, and action recognition. This paper
focuses on temporal variation modeling, which is the common key problem of
extensive analysis tasks. Previous methods attempt to accomplish this directly
from the 1D time series, which is extremely challenging due to the intricate
temporal patterns. Based on the observation of multi-periodicity in time
series, we ravel out the complex temporal variations into the multiple
intraperiod- and interperiod-variations. To tackle the limitations of 1D time
series in representation capability, we extend the analysis of temporal
variations into the 2D space by transforming the 1D time series into a set of
2D tensors based on multiple periods. This transformation can embed the
intraperiod- and interperiod-variations into the columns and rows of the 2D
tensors respectively, making the 2D-variations to be easily modeled by 2D
kernels. Technically, we propose the TimesNet with TimesBlock as a task-general
backbone for time series analysis. TimesBlock can discover the
multi-periodicity adaptively and extract the complex temporal variations from
transformed 2D tensors by a parameter-efficient inception block. Our proposed
TimesNet achieves consistent state-of-the-art in five mainstream time series
analysis tasks, including short- and long-term forecasting, imputation,
classification, and anomaly detection. Code is available at this repository:
https://github.com/thuml/TimesNet.
</summary>
    <author>
      <name>Haixu Wu</name>
    </author>
    <author>
      <name>Tengge Hu</name>
    </author>
    <author>
      <name>Yong Liu</name>
    </author>
    <author>
      <name>Hang Zhou</name>
    </author>
    <author>
      <name>Jianmin Wang</name>
    </author>
    <author>
      <name>Mingsheng Long</name>
    </author>
    <link href="http://arxiv.org/abs/2210.02186v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.02186v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.03142v3</id>
    <updated>2023-04-12T21:23:35Z</updated>
    <published>2022-10-06T18:03:56Z</published>
    <title>On Distillation of Guided Diffusion Models</title>
    <summary>  Classifier-free guided diffusion models have recently been shown to be highly
effective at high-resolution image generation, and they have been widely used
in large-scale diffusion frameworks including DALLE-2, Stable Diffusion and
Imagen. However, a downside of classifier-free guided diffusion models is that
they are computationally expensive at inference time since they require
evaluating two diffusion models, a class-conditional model and an unconditional
model, tens to hundreds of times. To deal with this limitation, we propose an
approach to distilling classifier-free guided diffusion models into models that
are fast to sample from: Given a pre-trained classifier-free guided model, we
first learn a single model to match the output of the combined conditional and
unconditional models, and then we progressively distill that model to a
diffusion model that requires much fewer sampling steps. For standard diffusion
models trained on the pixel-space, our approach is able to generate images
visually comparable to that of the original model using as few as 4 sampling
steps on ImageNet 64x64 and CIFAR-10, achieving FID/IS scores comparable to
that of the original model while being up to 256 times faster to sample from.
For diffusion models trained on the latent-space (e.g., Stable Diffusion), our
approach is able to generate high-fidelity images using as few as 1 to 4
denoising steps, accelerating inference by at least 10-fold compared to
existing methods on ImageNet 256x256 and LAION datasets. We further demonstrate
the effectiveness of our approach on text-guided image editing and inpainting,
where our distilled model is able to generate high-quality results using as few
as 2-4 denoising steps.
</summary>
    <author>
      <name>Chenlin Meng</name>
    </author>
    <author>
      <name>Robin Rombach</name>
    </author>
    <author>
      <name>Ruiqi Gao</name>
    </author>
    <author>
      <name>Diederik P. Kingma</name>
    </author>
    <author>
      <name>Stefano Ermon</name>
    </author>
    <author>
      <name>Jonathan Ho</name>
    </author>
    <author>
      <name>Tim Salimans</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2023, Award candidate</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.03142v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.03142v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.03629v3</id>
    <updated>2023-03-10T01:00:17Z</updated>
    <published>2022-10-06T01:00:32Z</published>
    <title>ReAct: Synergizing Reasoning and Acting in Language Models</title>
    <summary>  While large language models (LLMs) have demonstrated impressive capabilities
across tasks in language understanding and interactive decision making, their
abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.
action plan generation) have primarily been studied as separate topics. In this
paper, we explore the use of LLMs to generate both reasoning traces and
task-specific actions in an interleaved manner, allowing for greater synergy
between the two: reasoning traces help the model induce, track, and update
action plans as well as handle exceptions, while actions allow it to interface
with external sources, such as knowledge bases or environments, to gather
additional information. We apply our approach, named ReAct, to a diverse set of
language and decision making tasks and demonstrate its effectiveness over
state-of-the-art baselines, as well as improved human interpretability and
trustworthiness over methods without reasoning or acting components.
Concretely, on question answering (HotpotQA) and fact verification (Fever),
ReAct overcomes issues of hallucination and error propagation prevalent in
chain-of-thought reasoning by interacting with a simple Wikipedia API, and
generates human-like task-solving trajectories that are more interpretable than
baselines without reasoning traces. On two interactive decision making
benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and
reinforcement learning methods by an absolute success rate of 34% and 10%
respectively, while being prompted with only one or two in-context examples.
Project site with code: https://react-lm.github.io
</summary>
    <author>
      <name>Shunyu Yao</name>
    </author>
    <author>
      <name>Jeffrey Zhao</name>
    </author>
    <author>
      <name>Dian Yu</name>
    </author>
    <author>
      <name>Nan Du</name>
    </author>
    <author>
      <name>Izhak Shafran</name>
    </author>
    <author>
      <name>Karthik Narasimhan</name>
    </author>
    <author>
      <name>Yuan Cao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">v3 is the ICLR camera ready version with some typos fixed. Project
  site with code: https://react-lm.github.io</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.03629v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.03629v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.08402v1</id>
    <updated>2022-10-16T00:08:18Z</updated>
    <published>2022-10-16T00:08:18Z</published>
    <title>LAION-5B: An open large-scale dataset for training next generation
  image-text models</title>
    <summary>  Groundbreaking language-vision architectures like CLIP and DALL-E proved the
utility of training on large amounts of noisy image-text data, without relying
on expensive accurate labels used in standard vision unimodal supervised
learning. The resulting models showed capabilities of strong text-guided image
generation and transfer to downstream tasks, while performing remarkably at
zero-shot classification with noteworthy out-of-distribution robustness. Since
then, large-scale language-vision models like ALIGN, BASIC, GLIDE, Flamingo and
Imagen made further improvements. Studying the training and capabilities of
such models requires datasets containing billions of image-text pairs. Until
now, no datasets of this size have been made openly available for the broader
research community. To address this problem and democratize research on
large-scale multi-modal models, we present LAION-5B - a dataset consisting of
5.85 billion CLIP-filtered image-text pairs, of which 2.32B contain English
language. We show successful replication and fine-tuning of foundational models
like CLIP, GLIDE and Stable Diffusion using the dataset, and discuss further
experiments enabled with an openly available dataset of this scale.
Additionally we provide several nearest neighbor indices, an improved
web-interface for dataset exploration and subset generation, and detection
scores for watermark, NSFW, and toxic content detection. Announcement page
https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/
</summary>
    <author>
      <name>Christoph Schuhmann</name>
    </author>
    <author>
      <name>Romain Beaumont</name>
    </author>
    <author>
      <name>Richard Vencu</name>
    </author>
    <author>
      <name>Cade Gordon</name>
    </author>
    <author>
      <name>Ross Wightman</name>
    </author>
    <author>
      <name>Mehdi Cherti</name>
    </author>
    <author>
      <name>Theo Coombes</name>
    </author>
    <author>
      <name>Aarush Katta</name>
    </author>
    <author>
      <name>Clayton Mullis</name>
    </author>
    <author>
      <name>Mitchell Wortsman</name>
    </author>
    <author>
      <name>Patrick Schramowski</name>
    </author>
    <author>
      <name>Srivatsa Kundurthy</name>
    </author>
    <author>
      <name>Katherine Crowson</name>
    </author>
    <author>
      <name>Ludwig Schmidt</name>
    </author>
    <author>
      <name>Robert Kaczmarczyk</name>
    </author>
    <author>
      <name>Jenia Jitsev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36th Conference on Neural Information Processing Systems (NeurIPS
  2022), Track on Datasets and Benchmarks. OpenReview:
  https://openreview.net/forum?id=M3Y74vmsMcY</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.08402v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.08402v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.09276v3</id>
    <updated>2023-03-20T15:58:50Z</updated>
    <published>2022-10-17T17:27:32Z</published>
    <title>Imagic: Text-Based Real Image Editing with Diffusion Models</title>
    <summary>  Text-conditioned image editing has recently attracted considerable interest.
However, most methods are currently either limited to specific editing types
(e.g., object overlay, style transfer), or apply to synthetically generated
images, or require multiple input images of a common object. In this paper we
demonstrate, for the very first time, the ability to apply complex (e.g.,
non-rigid) text-guided semantic edits to a single real image. For example, we
can change the posture and composition of one or multiple objects inside an
image, while preserving its original characteristics. Our method can make a
standing dog sit down or jump, cause a bird to spread its wings, etc. -- each
within its single high-resolution natural image provided by the user. Contrary
to previous work, our proposed method requires only a single input image and a
target text (the desired edit). It operates on real images, and does not
require any additional inputs (such as image masks or additional views of the
object). Our method, which we call "Imagic", leverages a pre-trained
text-to-image diffusion model for this task. It produces a text embedding that
aligns with both the input image and the target text, while fine-tuning the
diffusion model to capture the image-specific appearance. We demonstrate the
quality and versatility of our method on numerous inputs from various domains,
showcasing a plethora of high quality complex semantic image edits, all within
a single unified framework.
</summary>
    <author>
      <name>Bahjat Kawar</name>
    </author>
    <author>
      <name>Shiran Zada</name>
    </author>
    <author>
      <name>Oran Lang</name>
    </author>
    <author>
      <name>Omer Tov</name>
    </author>
    <author>
      <name>Huiwen Chang</name>
    </author>
    <author>
      <name>Tali Dekel</name>
    </author>
    <author>
      <name>Inbar Mosseri</name>
    </author>
    <author>
      <name>Michal Irani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page: https://imagic-editing.github.io/</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.09276v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.09276v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.13438v1</id>
    <updated>2022-10-24T17:52:02Z</updated>
    <published>2022-10-24T17:52:02Z</published>
    <title>High Fidelity Neural Audio Compression</title>
    <summary>  We introduce a state-of-the-art real-time, high-fidelity, audio codec
leveraging neural networks. It consists in a streaming encoder-decoder
architecture with quantized latent space trained in an end-to-end fashion. We
simplify and speed-up the training by using a single multiscale spectrogram
adversary that efficiently reduces artifacts and produce high-quality samples.
We introduce a novel loss balancer mechanism to stabilize training: the weight
of a loss now defines the fraction of the overall gradient it should represent,
thus decoupling the choice of this hyper-parameter from the typical scale of
the loss. Finally, we study how lightweight Transformer models can be used to
further compress the obtained representation by up to 40%, while staying faster
than real time. We provide a detailed description of the key design choices of
the proposed model including: training objective, architectural changes and a
study of various perceptual loss functions. We present an extensive subjective
evaluation (MUSHRA tests) together with an ablation study for a range of
bandwidths and audio domains, including speech, noisy-reverberant speech, and
music. Our approach is superior to the baselines methods across all evaluated
settings, considering both 24 kHz monophonic and 48 kHz stereophonic audio.
Code and models are available at github.com/facebookresearch/encodec.
</summary>
    <author>
      <name>Alexandre Défossez</name>
    </author>
    <author>
      <name>Jade Copet</name>
    </author>
    <author>
      <name>Gabriel Synnaeve</name>
    </author>
    <author>
      <name>Yossi Adi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.13438v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.13438v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.05100v4</id>
    <updated>2023-06-27T09:57:58Z</updated>
    <published>2022-11-09T18:48:09Z</published>
    <title>BLOOM: A 176B-Parameter Open-Access Multilingual Language Model</title>
    <summary>  Large language models (LLMs) have been shown to be able to perform new tasks
based on a few demonstrations or natural language instructions. While these
capabilities have led to widespread adoption, most LLMs are developed by
resource-rich organizations and are frequently kept from the public. As a step
towards democratizing this powerful technology, we present BLOOM, a
176B-parameter open-access language model designed and built thanks to a
collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer
language model that was trained on the ROOTS corpus, a dataset comprising
hundreds of sources in 46 natural and 13 programming languages (59 in total).
We find that BLOOM achieves competitive performance on a wide variety of
benchmarks, with stronger results after undergoing multitask prompted
finetuning. To facilitate future research and applications using LLMs, we
publicly release our models and code under the Responsible AI License.
</summary>
    <author>
      <name>BigScience Workshop</name>
    </author>
    <author>
      <name> :</name>
    </author>
    <author>
      <name>Teven Le Scao</name>
    </author>
    <author>
      <name>Angela Fan</name>
    </author>
    <author>
      <name>Christopher Akiki</name>
    </author>
    <author>
      <name>Ellie Pavlick</name>
    </author>
    <author>
      <name>Suzana Ilić</name>
    </author>
    <author>
      <name>Daniel Hesslow</name>
    </author>
    <author>
      <name>Roman Castagné</name>
    </author>
    <author>
      <name>Alexandra Sasha Luccioni</name>
    </author>
    <author>
      <name>François Yvon</name>
    </author>
    <author>
      <name>Matthias Gallé</name>
    </author>
    <author>
      <name>Jonathan Tow</name>
    </author>
    <author>
      <name>Alexander M. Rush</name>
    </author>
    <author>
      <name>Stella Biderman</name>
    </author>
    <author>
      <name>Albert Webson</name>
    </author>
    <author>
      <name>Pawan Sasanka Ammanamanchi</name>
    </author>
    <author>
      <name>Thomas Wang</name>
    </author>
    <author>
      <name>Benoît Sagot</name>
    </author>
    <author>
      <name>Niklas Muennighoff</name>
    </author>
    <author>
      <name>Albert Villanova del Moral</name>
    </author>
    <author>
      <name>Olatunji Ruwase</name>
    </author>
    <author>
      <name>Rachel Bawden</name>
    </author>
    <author>
      <name>Stas Bekman</name>
    </author>
    <author>
      <name>Angelina McMillan-Major</name>
    </author>
    <author>
      <name>Iz Beltagy</name>
    </author>
    <author>
      <name>Huu Nguyen</name>
    </author>
    <author>
      <name>Lucile Saulnier</name>
    </author>
    <author>
      <name>Samson Tan</name>
    </author>
    <author>
      <name>Pedro Ortiz Suarez</name>
    </author>
    <author>
      <name>Victor Sanh</name>
    </author>
    <author>
      <name>Hugo Laurençon</name>
    </author>
    <author>
      <name>Yacine Jernite</name>
    </author>
    <author>
      <name>Julien Launay</name>
    </author>
    <author>
      <name>Margaret Mitchell</name>
    </author>
    <author>
      <name>Colin Raffel</name>
    </author>
    <author>
      <name>Aaron Gokaslan</name>
    </author>
    <author>
      <name>Adi Simhi</name>
    </author>
    <author>
      <name>Aitor Soroa</name>
    </author>
    <author>
      <name>Alham Fikri Aji</name>
    </author>
    <author>
      <name>Amit Alfassy</name>
    </author>
    <author>
      <name>Anna Rogers</name>
    </author>
    <author>
      <name>Ariel Kreisberg Nitzav</name>
    </author>
    <author>
      <name>Canwen Xu</name>
    </author>
    <author>
      <name>Chenghao Mou</name>
    </author>
    <author>
      <name>Chris Emezue</name>
    </author>
    <author>
      <name>Christopher Klamm</name>
    </author>
    <author>
      <name>Colin Leong</name>
    </author>
    <author>
      <name>Daniel van Strien</name>
    </author>
    <author>
      <name>David Ifeoluwa Adelani</name>
    </author>
    <author>
      <name>Dragomir Radev</name>
    </author>
    <author>
      <name>Eduardo González Ponferrada</name>
    </author>
    <author>
      <name>Efrat Levkovizh</name>
    </author>
    <author>
      <name>Ethan Kim</name>
    </author>
    <author>
      <name>Eyal Bar Natan</name>
    </author>
    <author>
      <name>Francesco De Toni</name>
    </author>
    <author>
      <name>Gérard Dupont</name>
    </author>
    <author>
      <name>Germán Kruszewski</name>
    </author>
    <author>
      <name>Giada Pistilli</name>
    </author>
    <author>
      <name>Hady Elsahar</name>
    </author>
    <author>
      <name>Hamza Benyamina</name>
    </author>
    <author>
      <name>Hieu Tran</name>
    </author>
    <author>
      <name>Ian Yu</name>
    </author>
    <author>
      <name>Idris Abdulmumin</name>
    </author>
    <author>
      <name>Isaac Johnson</name>
    </author>
    <author>
      <name>Itziar Gonzalez-Dios</name>
    </author>
    <author>
      <name>Javier de la Rosa</name>
    </author>
    <author>
      <name>Jenny Chim</name>
    </author>
    <author>
      <name>Jesse Dodge</name>
    </author>
    <author>
      <name>Jian Zhu</name>
    </author>
    <author>
      <name>Jonathan Chang</name>
    </author>
    <author>
      <name>Jörg Frohberg</name>
    </author>
    <author>
      <name>Joseph Tobing</name>
    </author>
    <author>
      <name>Joydeep Bhattacharjee</name>
    </author>
    <author>
      <name>Khalid Almubarak</name>
    </author>
    <author>
      <name>Kimbo Chen</name>
    </author>
    <author>
      <name>Kyle Lo</name>
    </author>
    <author>
      <name>Leandro Von Werra</name>
    </author>
    <author>
      <name>Leon Weber</name>
    </author>
    <author>
      <name>Long Phan</name>
    </author>
    <author>
      <name>Loubna Ben allal</name>
    </author>
    <author>
      <name>Ludovic Tanguy</name>
    </author>
    <author>
      <name>Manan Dey</name>
    </author>
    <author>
      <name>Manuel Romero Muñoz</name>
    </author>
    <author>
      <name>Maraim Masoud</name>
    </author>
    <author>
      <name>María Grandury</name>
    </author>
    <author>
      <name>Mario Šaško</name>
    </author>
    <author>
      <name>Max Huang</name>
    </author>
    <author>
      <name>Maximin Coavoux</name>
    </author>
    <author>
      <name>Mayank Singh</name>
    </author>
    <author>
      <name>Mike Tian-Jian Jiang</name>
    </author>
    <author>
      <name>Minh Chien Vu</name>
    </author>
    <author>
      <name>Mohammad A. Jauhar</name>
    </author>
    <author>
      <name>Mustafa Ghaleb</name>
    </author>
    <author>
      <name>Nishant Subramani</name>
    </author>
    <author>
      <name>Nora Kassner</name>
    </author>
    <author>
      <name>Nurulaqilla Khamis</name>
    </author>
    <author>
      <name>Olivier Nguyen</name>
    </author>
    <author>
      <name>Omar Espejel</name>
    </author>
    <author>
      <name>Ona de Gibert</name>
    </author>
    <author>
      <name>Paulo Villegas</name>
    </author>
    <author>
      <name>Peter Henderson</name>
    </author>
    <author>
      <name>Pierre Colombo</name>
    </author>
    <author>
      <name>Priscilla Amuok</name>
    </author>
    <author>
      <name>Quentin Lhoest</name>
    </author>
    <author>
      <name>Rheza Harliman</name>
    </author>
    <author>
      <name>Rishi Bommasani</name>
    </author>
    <author>
      <name>Roberto Luis López</name>
    </author>
    <author>
      <name>Rui Ribeiro</name>
    </author>
    <author>
      <name>Salomey Osei</name>
    </author>
    <author>
      <name>Sampo Pyysalo</name>
    </author>
    <author>
      <name>Sebastian Nagel</name>
    </author>
    <author>
      <name>Shamik Bose</name>
    </author>
    <author>
      <name>Shamsuddeen Hassan Muhammad</name>
    </author>
    <author>
      <name>Shanya Sharma</name>
    </author>
    <author>
      <name>Shayne Longpre</name>
    </author>
    <author>
      <name>Somaieh Nikpoor</name>
    </author>
    <author>
      <name>Stanislav Silberberg</name>
    </author>
    <author>
      <name>Suhas Pai</name>
    </author>
    <author>
      <name>Sydney Zink</name>
    </author>
    <author>
      <name>Tiago Timponi Torrent</name>
    </author>
    <author>
      <name>Timo Schick</name>
    </author>
    <author>
      <name>Tristan Thrush</name>
    </author>
    <author>
      <name>Valentin Danchev</name>
    </author>
    <author>
      <name>Vassilina Nikoulina</name>
    </author>
    <author>
      <name>Veronika Laippala</name>
    </author>
    <author>
      <name>Violette Lepercq</name>
    </author>
    <author>
      <name>Vrinda Prabhu</name>
    </author>
    <author>
      <name>Zaid Alyafeai</name>
    </author>
    <author>
      <name>Zeerak Talat</name>
    </author>
    <author>
      <name>Arun Raja</name>
    </author>
    <author>
      <name>Benjamin Heinzerling</name>
    </author>
    <author>
      <name>Chenglei Si</name>
    </author>
    <author>
      <name>Davut Emre Taşar</name>
    </author>
    <author>
      <name>Elizabeth Salesky</name>
    </author>
    <author>
      <name>Sabrina J. Mielke</name>
    </author>
    <author>
      <name>Wilson Y. Lee</name>
    </author>
    <author>
      <name>Abheesht Sharma</name>
    </author>
    <author>
      <name>Andrea Santilli</name>
    </author>
    <author>
      <name>Antoine Chaffin</name>
    </author>
    <author>
      <name>Arnaud Stiegler</name>
    </author>
    <author>
      <name>Debajyoti Datta</name>
    </author>
    <author>
      <name>Eliza Szczechla</name>
    </author>
    <author>
      <name>Gunjan Chhablani</name>
    </author>
    <author>
      <name>Han Wang</name>
    </author>
    <author>
      <name>Harshit Pandey</name>
    </author>
    <author>
      <name>Hendrik Strobelt</name>
    </author>
    <author>
      <name>Jason Alan Fries</name>
    </author>
    <author>
      <name>Jos Rozen</name>
    </author>
    <author>
      <name>Leo Gao</name>
    </author>
    <author>
      <name>Lintang Sutawika</name>
    </author>
    <author>
      <name>M Saiful Bari</name>
    </author>
    <author>
      <name>Maged S. Al-shaibani</name>
    </author>
    <author>
      <name>Matteo Manica</name>
    </author>
    <author>
      <name>Nihal Nayak</name>
    </author>
    <author>
      <name>Ryan Teehan</name>
    </author>
    <author>
      <name>Samuel Albanie</name>
    </author>
    <author>
      <name>Sheng Shen</name>
    </author>
    <author>
      <name>Srulik Ben-David</name>
    </author>
    <author>
      <name>Stephen H. Bach</name>
    </author>
    <author>
      <name>Taewoon Kim</name>
    </author>
    <author>
      <name>Tali Bers</name>
    </author>
    <author>
      <name>Thibault Fevry</name>
    </author>
    <author>
      <name>Trishala Neeraj</name>
    </author>
    <author>
      <name>Urmish Thakker</name>
    </author>
    <author>
      <name>Vikas Raunak</name>
    </author>
    <author>
      <name>Xiangru Tang</name>
    </author>
    <author>
      <name>Zheng-Xin Yong</name>
    </author>
    <author>
      <name>Zhiqing Sun</name>
    </author>
    <author>
      <name>Shaked Brody</name>
    </author>
    <author>
      <name>Yallow Uri</name>
    </author>
    <author>
      <name>Hadar Tojarieh</name>
    </author>
    <author>
      <name>Adam Roberts</name>
    </author>
    <author>
      <name>Hyung Won Chung</name>
    </author>
    <author>
      <name>Jaesung Tae</name>
    </author>
    <author>
      <name>Jason Phang</name>
    </author>
    <author>
      <name>Ofir Press</name>
    </author>
    <author>
      <name>Conglong Li</name>
    </author>
    <author>
      <name>Deepak Narayanan</name>
    </author>
    <author>
      <name>Hatim Bourfoune</name>
    </author>
    <author>
      <name>Jared Casper</name>
    </author>
    <author>
      <name>Jeff Rasley</name>
    </author>
    <author>
      <name>Max Ryabinin</name>
    </author>
    <author>
      <name>Mayank Mishra</name>
    </author>
    <author>
      <name>Minjia Zhang</name>
    </author>
    <author>
      <name>Mohammad Shoeybi</name>
    </author>
    <author>
      <name>Myriam Peyrounette</name>
    </author>
    <author>
      <name>Nicolas Patry</name>
    </author>
    <author>
      <name>Nouamane Tazi</name>
    </author>
    <author>
      <name>Omar Sanseviero</name>
    </author>
    <author>
      <name>Patrick von Platen</name>
    </author>
    <author>
      <name>Pierre Cornette</name>
    </author>
    <author>
      <name>Pierre François Lavallée</name>
    </author>
    <author>
      <name>Rémi Lacroix</name>
    </author>
    <author>
      <name>Samyam Rajbhandari</name>
    </author>
    <author>
      <name>Sanchit Gandhi</name>
    </author>
    <author>
      <name>Shaden Smith</name>
    </author>
    <author>
      <name>Stéphane Requena</name>
    </author>
    <author>
      <name>Suraj Patil</name>
    </author>
    <author>
      <name>Tim Dettmers</name>
    </author>
    <author>
      <name>Ahmed Baruwa</name>
    </author>
    <author>
      <name>Amanpreet Singh</name>
    </author>
    <author>
      <name>Anastasia Cheveleva</name>
    </author>
    <author>
      <name>Anne-Laure Ligozat</name>
    </author>
    <author>
      <name>Arjun Subramonian</name>
    </author>
    <author>
      <name>Aurélie Névéol</name>
    </author>
    <author>
      <name>Charles Lovering</name>
    </author>
    <author>
      <name>Dan Garrette</name>
    </author>
    <author>
      <name>Deepak Tunuguntla</name>
    </author>
    <author>
      <name>Ehud Reiter</name>
    </author>
    <author>
      <name>Ekaterina Taktasheva</name>
    </author>
    <author>
      <name>Ekaterina Voloshina</name>
    </author>
    <author>
      <name>Eli Bogdanov</name>
    </author>
    <author>
      <name>Genta Indra Winata</name>
    </author>
    <author>
      <name>Hailey Schoelkopf</name>
    </author>
    <author>
      <name>Jan-Christoph Kalo</name>
    </author>
    <author>
      <name>Jekaterina Novikova</name>
    </author>
    <author>
      <name>Jessica Zosa Forde</name>
    </author>
    <author>
      <name>Jordan Clive</name>
    </author>
    <author>
      <name>Jungo Kasai</name>
    </author>
    <author>
      <name>Ken Kawamura</name>
    </author>
    <author>
      <name>Liam Hazan</name>
    </author>
    <author>
      <name>Marine Carpuat</name>
    </author>
    <author>
      <name>Miruna Clinciu</name>
    </author>
    <author>
      <name>Najoung Kim</name>
    </author>
    <author>
      <name>Newton Cheng</name>
    </author>
    <author>
      <name>Oleg Serikov</name>
    </author>
    <author>
      <name>Omer Antverg</name>
    </author>
    <author>
      <name>Oskar van der Wal</name>
    </author>
    <author>
      <name>Rui Zhang</name>
    </author>
    <author>
      <name>Ruochen Zhang</name>
    </author>
    <author>
      <name>Sebastian Gehrmann</name>
    </author>
    <author>
      <name>Shachar Mirkin</name>
    </author>
    <author>
      <name>Shani Pais</name>
    </author>
    <author>
      <name>Tatiana Shavrina</name>
    </author>
    <author>
      <name>Thomas Scialom</name>
    </author>
    <author>
      <name>Tian Yun</name>
    </author>
    <author>
      <name>Tomasz Limisiewicz</name>
    </author>
    <author>
      <name>Verena Rieser</name>
    </author>
    <author>
      <name>Vitaly Protasov</name>
    </author>
    <author>
      <name>Vladislav Mikhailov</name>
    </author>
    <author>
      <name>Yada Pruksachatkun</name>
    </author>
    <author>
      <name>Yonatan Belinkov</name>
    </author>
    <author>
      <name>Zachary Bamberger</name>
    </author>
    <author>
      <name>Zdeněk Kasner</name>
    </author>
    <author>
      <name>Alice Rueda</name>
    </author>
    <author>
      <name>Amanda Pestana</name>
    </author>
    <author>
      <name>Amir Feizpour</name>
    </author>
    <author>
      <name>Ammar Khan</name>
    </author>
    <author>
      <name>Amy Faranak</name>
    </author>
    <author>
      <name>Ana Santos</name>
    </author>
    <author>
      <name>Anthony Hevia</name>
    </author>
    <author>
      <name>Antigona Unldreaj</name>
    </author>
    <author>
      <name>Arash Aghagol</name>
    </author>
    <author>
      <name>Arezoo Abdollahi</name>
    </author>
    <author>
      <name>Aycha Tammour</name>
    </author>
    <author>
      <name>Azadeh HajiHosseini</name>
    </author>
    <author>
      <name>Bahareh Behroozi</name>
    </author>
    <author>
      <name>Benjamin Ajibade</name>
    </author>
    <author>
      <name>Bharat Saxena</name>
    </author>
    <author>
      <name>Carlos Muñoz Ferrandis</name>
    </author>
    <author>
      <name>Daniel McDuff</name>
    </author>
    <author>
      <name>Danish Contractor</name>
    </author>
    <author>
      <name>David Lansky</name>
    </author>
    <author>
      <name>Davis David</name>
    </author>
    <author>
      <name>Douwe Kiela</name>
    </author>
    <author>
      <name>Duong A. Nguyen</name>
    </author>
    <author>
      <name>Edward Tan</name>
    </author>
    <author>
      <name>Emi Baylor</name>
    </author>
    <author>
      <name>Ezinwanne Ozoani</name>
    </author>
    <author>
      <name>Fatima Mirza</name>
    </author>
    <author>
      <name>Frankline Ononiwu</name>
    </author>
    <author>
      <name>Habib Rezanejad</name>
    </author>
    <author>
      <name>Hessie Jones</name>
    </author>
    <author>
      <name>Indrani Bhattacharya</name>
    </author>
    <author>
      <name>Irene Solaiman</name>
    </author>
    <author>
      <name>Irina Sedenko</name>
    </author>
    <author>
      <name>Isar Nejadgholi</name>
    </author>
    <author>
      <name>Jesse Passmore</name>
    </author>
    <author>
      <name>Josh Seltzer</name>
    </author>
    <author>
      <name>Julio Bonis Sanz</name>
    </author>
    <author>
      <name>Livia Dutra</name>
    </author>
    <author>
      <name>Mairon Samagaio</name>
    </author>
    <author>
      <name>Maraim Elbadri</name>
    </author>
    <author>
      <name>Margot Mieskes</name>
    </author>
    <author>
      <name>Marissa Gerchick</name>
    </author>
    <author>
      <name>Martha Akinlolu</name>
    </author>
    <author>
      <name>Michael McKenna</name>
    </author>
    <author>
      <name>Mike Qiu</name>
    </author>
    <author>
      <name>Muhammed Ghauri</name>
    </author>
    <author>
      <name>Mykola Burynok</name>
    </author>
    <author>
      <name>Nafis Abrar</name>
    </author>
    <author>
      <name>Nazneen Rajani</name>
    </author>
    <author>
      <name>Nour Elkott</name>
    </author>
    <author>
      <name>Nour Fahmy</name>
    </author>
    <author>
      <name>Olanrewaju Samuel</name>
    </author>
    <author>
      <name>Ran An</name>
    </author>
    <author>
      <name>Rasmus Kromann</name>
    </author>
    <author>
      <name>Ryan Hao</name>
    </author>
    <author>
      <name>Samira Alizadeh</name>
    </author>
    <author>
      <name>Sarmad Shubber</name>
    </author>
    <author>
      <name>Silas Wang</name>
    </author>
    <author>
      <name>Sourav Roy</name>
    </author>
    <author>
      <name>Sylvain Viguier</name>
    </author>
    <author>
      <name>Thanh Le</name>
    </author>
    <author>
      <name>Tobi Oyebade</name>
    </author>
    <author>
      <name>Trieu Le</name>
    </author>
    <author>
      <name>Yoyo Yang</name>
    </author>
    <author>
      <name>Zach Nguyen</name>
    </author>
    <author>
      <name>Abhinav Ramesh Kashyap</name>
    </author>
    <author>
      <name>Alfredo Palasciano</name>
    </author>
    <author>
      <name>Alison Callahan</name>
    </author>
    <author>
      <name>Anima Shukla</name>
    </author>
    <author>
      <name>Antonio Miranda-Escalada</name>
    </author>
    <author>
      <name>Ayush Singh</name>
    </author>
    <author>
      <name>Benjamin Beilharz</name>
    </author>
    <author>
      <name>Bo Wang</name>
    </author>
    <author>
      <name>Caio Brito</name>
    </author>
    <author>
      <name>Chenxi Zhou</name>
    </author>
    <author>
      <name>Chirag Jain</name>
    </author>
    <author>
      <name>Chuxin Xu</name>
    </author>
    <author>
      <name>Clémentine Fourrier</name>
    </author>
    <author>
      <name>Daniel León Periñán</name>
    </author>
    <author>
      <name>Daniel Molano</name>
    </author>
    <author>
      <name>Dian Yu</name>
    </author>
    <author>
      <name>Enrique Manjavacas</name>
    </author>
    <author>
      <name>Fabio Barth</name>
    </author>
    <author>
      <name>Florian Fuhrimann</name>
    </author>
    <author>
      <name>Gabriel Altay</name>
    </author>
    <author>
      <name>Giyaseddin Bayrak</name>
    </author>
    <author>
      <name>Gully Burns</name>
    </author>
    <author>
      <name>Helena U. Vrabec</name>
    </author>
    <author>
      <name>Imane Bello</name>
    </author>
    <author>
      <name>Ishani Dash</name>
    </author>
    <author>
      <name>Jihyun Kang</name>
    </author>
    <author>
      <name>John Giorgi</name>
    </author>
    <author>
      <name>Jonas Golde</name>
    </author>
    <author>
      <name>Jose David Posada</name>
    </author>
    <author>
      <name>Karthik Rangasai Sivaraman</name>
    </author>
    <author>
      <name>Lokesh Bulchandani</name>
    </author>
    <author>
      <name>Lu Liu</name>
    </author>
    <author>
      <name>Luisa Shinzato</name>
    </author>
    <author>
      <name>Madeleine Hahn de Bykhovetz</name>
    </author>
    <author>
      <name>Maiko Takeuchi</name>
    </author>
    <author>
      <name>Marc Pàmies</name>
    </author>
    <author>
      <name>Maria A Castillo</name>
    </author>
    <author>
      <name>Marianna Nezhurina</name>
    </author>
    <author>
      <name>Mario Sänger</name>
    </author>
    <author>
      <name>Matthias Samwald</name>
    </author>
    <author>
      <name>Michael Cullan</name>
    </author>
    <author>
      <name>Michael Weinberg</name>
    </author>
    <author>
      <name>Michiel De Wolf</name>
    </author>
    <author>
      <name>Mina Mihaljcic</name>
    </author>
    <author>
      <name>Minna Liu</name>
    </author>
    <author>
      <name>Moritz Freidank</name>
    </author>
    <author>
      <name>Myungsun Kang</name>
    </author>
    <author>
      <name>Natasha Seelam</name>
    </author>
    <author>
      <name>Nathan Dahlberg</name>
    </author>
    <author>
      <name>Nicholas Michio Broad</name>
    </author>
    <author>
      <name>Nikolaus Muellner</name>
    </author>
    <author>
      <name>Pascale Fung</name>
    </author>
    <author>
      <name>Patrick Haller</name>
    </author>
    <author>
      <name>Ramya Chandrasekhar</name>
    </author>
    <author>
      <name>Renata Eisenberg</name>
    </author>
    <author>
      <name>Robert Martin</name>
    </author>
    <author>
      <name>Rodrigo Canalli</name>
    </author>
    <author>
      <name>Rosaline Su</name>
    </author>
    <author>
      <name>Ruisi Su</name>
    </author>
    <author>
      <name>Samuel Cahyawijaya</name>
    </author>
    <author>
      <name>Samuele Garda</name>
    </author>
    <author>
      <name>Shlok S Deshmukh</name>
    </author>
    <author>
      <name>Shubhanshu Mishra</name>
    </author>
    <author>
      <name>Sid Kiblawi</name>
    </author>
    <author>
      <name>Simon Ott</name>
    </author>
    <author>
      <name>Sinee Sang-aroonsiri</name>
    </author>
    <author>
      <name>Srishti Kumar</name>
    </author>
    <author>
      <name>Stefan Schweter</name>
    </author>
    <author>
      <name>Sushil Bharati</name>
    </author>
    <author>
      <name>Tanmay Laud</name>
    </author>
    <author>
      <name>Théo Gigant</name>
    </author>
    <author>
      <name>Tomoya Kainuma</name>
    </author>
    <author>
      <name>Wojciech Kusa</name>
    </author>
    <author>
      <name>Yanis Labrak</name>
    </author>
    <author>
      <name>Yash Shailesh Bajaj</name>
    </author>
    <author>
      <name>Yash Venkatraman</name>
    </author>
    <author>
      <name>Yifan Xu</name>
    </author>
    <author>
      <name>Yingxin Xu</name>
    </author>
    <author>
      <name>Yu Xu</name>
    </author>
    <author>
      <name>Zhe Tan</name>
    </author>
    <author>
      <name>Zhongli Xie</name>
    </author>
    <author>
      <name>Zifan Ye</name>
    </author>
    <author>
      <name>Mathilde Bras</name>
    </author>
    <author>
      <name>Younes Belkada</name>
    </author>
    <author>
      <name>Thomas Wolf</name>
    </author>
    <link href="http://arxiv.org/abs/2211.05100v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.05100v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.09788v2</id>
    <updated>2023-08-19T10:03:29Z</updated>
    <published>2022-11-17T18:56:19Z</published>
    <title>DiffusionDet: Diffusion Model for Object Detection</title>
    <summary>  We propose DiffusionDet, a new framework that formulates object detection as
a denoising diffusion process from noisy boxes to object boxes. During the
training stage, object boxes diffuse from ground-truth boxes to random
distribution, and the model learns to reverse this noising process. In
inference, the model refines a set of randomly generated boxes to the output
results in a progressive way. Our work possesses an appealing property of
flexibility, which enables the dynamic number of boxes and iterative
evaluation. The extensive experiments on the standard benchmarks show that
DiffusionDet achieves favorable performance compared to previous
well-established detectors. For example, DiffusionDet achieves 5.3 AP and 4.8
AP gains when evaluated with more boxes and iteration steps, under a zero-shot
transfer setting from COCO to CrowdHuman. Our code is available at
https://github.com/ShoufaChen/DiffusionDet.
</summary>
    <author>
      <name>Shoufa Chen</name>
    </author>
    <author>
      <name>Peize Sun</name>
    </author>
    <author>
      <name>Yibing Song</name>
    </author>
    <author>
      <name>Ping Luo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICCV2023 (Oral), Camera-ready</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.09788v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.09788v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.09800v2</id>
    <updated>2023-01-18T17:31:52Z</updated>
    <published>2022-11-17T18:58:43Z</published>
    <title>InstructPix2Pix: Learning to Follow Image Editing Instructions</title>
    <summary>  We propose a method for editing images from human instructions: given an
input image and a written instruction that tells the model what to do, our
model follows these instructions to edit the image. To obtain training data for
this problem, we combine the knowledge of two large pretrained models -- a
language model (GPT-3) and a text-to-image model (Stable Diffusion) -- to
generate a large dataset of image editing examples. Our conditional diffusion
model, InstructPix2Pix, is trained on our generated data, and generalizes to
real images and user-written instructions at inference time. Since it performs
edits in the forward pass and does not require per example fine-tuning or
inversion, our model edits images quickly, in a matter of seconds. We show
compelling editing results for a diverse collection of input images and written
instructions.
</summary>
    <author>
      <name>Tim Brooks</name>
    </author>
    <author>
      <name>Aleksander Holynski</name>
    </author>
    <author>
      <name>Alexei A. Efros</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page with code:
  https://www.timothybrooks.com/instruct-pix2pix</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.09800v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.09800v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.10440v2</id>
    <updated>2023-03-25T17:32:25Z</updated>
    <published>2022-11-18T18:59:59Z</published>
    <title>Magic3D: High-Resolution Text-to-3D Content Creation</title>
    <summary>  DreamFusion has recently demonstrated the utility of a pre-trained
text-to-image diffusion model to optimize Neural Radiance Fields (NeRF),
achieving remarkable text-to-3D synthesis results. However, the method has two
inherent limitations: (a) extremely slow optimization of NeRF and (b)
low-resolution image space supervision on NeRF, leading to low-quality 3D
models with a long processing time. In this paper, we address these limitations
by utilizing a two-stage optimization framework. First, we obtain a coarse
model using a low-resolution diffusion prior and accelerate with a sparse 3D
hash grid structure. Using the coarse representation as the initialization, we
further optimize a textured 3D mesh model with an efficient differentiable
renderer interacting with a high-resolution latent diffusion model. Our method,
dubbed Magic3D, can create high quality 3D mesh models in 40 minutes, which is
2x faster than DreamFusion (reportedly taking 1.5 hours on average), while also
achieving higher resolution. User studies show 61.7% raters to prefer our
approach over DreamFusion. Together with the image-conditioned generation
capabilities, we provide users with new ways to control 3D synthesis, opening
up new avenues to various creative applications.
</summary>
    <author>
      <name>Chen-Hsuan Lin</name>
    </author>
    <author>
      <name>Jun Gao</name>
    </author>
    <author>
      <name>Luming Tang</name>
    </author>
    <author>
      <name>Towaki Takikawa</name>
    </author>
    <author>
      <name>Xiaohui Zeng</name>
    </author>
    <author>
      <name>Xun Huang</name>
    </author>
    <author>
      <name>Karsten Kreis</name>
    </author>
    <author>
      <name>Sanja Fidler</name>
    </author>
    <author>
      <name>Ming-Yu Liu</name>
    </author>
    <author>
      <name>Tsung-Yi Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to CVPR 2023 as highlight. Project website:
  https://research.nvidia.com/labs/dir/magic3d</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.10440v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.10440v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.04356v1</id>
    <updated>2022-12-06T18:46:04Z</updated>
    <published>2022-12-06T18:46:04Z</published>
    <title>Robust Speech Recognition via Large-Scale Weak Supervision</title>
    <summary>  We study the capabilities of speech processing systems trained simply to
predict large amounts of transcripts of audio on the internet. When scaled to
680,000 hours of multilingual and multitask supervision, the resulting models
generalize well to standard benchmarks and are often competitive with prior
fully supervised results but in a zero-shot transfer setting without the need
for any fine-tuning. When compared to humans, the models approach their
accuracy and robustness. We are releasing models and inference code to serve as
a foundation for further work on robust speech processing.
</summary>
    <author>
      <name>Alec Radford</name>
    </author>
    <author>
      <name>Jong Wook Kim</name>
    </author>
    <author>
      <name>Tao Xu</name>
    </author>
    <author>
      <name>Greg Brockman</name>
    </author>
    <author>
      <name>Christine McLeavey</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <link href="http://arxiv.org/abs/2212.04356v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.04356v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.04488v2</id>
    <updated>2023-06-20T16:26:38Z</updated>
    <published>2022-12-08T18:57:02Z</published>
    <title>Multi-Concept Customization of Text-to-Image Diffusion</title>
    <summary>  While generative models produce high-quality images of concepts learned from
a large-scale database, a user often wishes to synthesize instantiations of
their own concepts (for example, their family, pets, or items). Can we teach a
model to quickly acquire a new concept, given a few examples? Furthermore, can
we compose multiple new concepts together? We propose Custom Diffusion, an
efficient method for augmenting existing text-to-image models. We find that
only optimizing a few parameters in the text-to-image conditioning mechanism is
sufficiently powerful to represent new concepts while enabling fast tuning (~6
minutes). Additionally, we can jointly train for multiple concepts or combine
multiple fine-tuned models into one via closed-form constrained optimization.
Our fine-tuned model generates variations of multiple new concepts and
seamlessly composes them with existing concepts in novel settings. Our method
outperforms or performs on par with several baselines and concurrent works in
both qualitative and quantitative evaluations while being memory and
computationally efficient.
</summary>
    <author>
      <name>Nupur Kumari</name>
    </author>
    <author>
      <name>Bingliang Zhang</name>
    </author>
    <author>
      <name>Richard Zhang</name>
    </author>
    <author>
      <name>Eli Shechtman</name>
    </author>
    <author>
      <name>Jun-Yan Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Updated v2 with results on the new CustomConcept101 dataset
  https://www.cs.cmu.edu/~custom-diffusion/dataset.html Project webpage:
  https://www.cs.cmu.edu/~custom-diffusion</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.04488v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.04488v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.06817v2</id>
    <updated>2023-08-11T17:45:27Z</updated>
    <published>2022-12-13T18:55:15Z</published>
    <title>RT-1: Robotics Transformer for Real-World Control at Scale</title>
    <summary>  By transferring knowledge from large, diverse, task-agnostic datasets, modern
machine learning models can solve specific downstream tasks either zero-shot or
with small task-specific datasets to a high level of performance. While this
capability has been demonstrated in other fields such as computer vision,
natural language processing or speech recognition, it remains to be shown in
robotics, where the generalization capabilities of the models are particularly
critical due to the difficulty of collecting real-world robotic data. We argue
that one of the keys to the success of such general robotic models lies with
open-ended task-agnostic training, combined with high-capacity architectures
that can absorb all of the diverse, robotic data. In this paper, we present a
model class, dubbed Robotics Transformer, that exhibits promising scalable
model properties. We verify our conclusions in a study of different model
classes and their ability to generalize as a function of the data size, model
size, and data diversity based on a large-scale data collection on real robots
performing real-world tasks. The project's website and videos can be found at
robotics-transformer1.github.io
</summary>
    <author>
      <name>Anthony Brohan</name>
    </author>
    <author>
      <name>Noah Brown</name>
    </author>
    <author>
      <name>Justice Carbajal</name>
    </author>
    <author>
      <name>Yevgen Chebotar</name>
    </author>
    <author>
      <name>Joseph Dabis</name>
    </author>
    <author>
      <name>Chelsea Finn</name>
    </author>
    <author>
      <name>Keerthana Gopalakrishnan</name>
    </author>
    <author>
      <name>Karol Hausman</name>
    </author>
    <author>
      <name>Alex Herzog</name>
    </author>
    <author>
      <name>Jasmine Hsu</name>
    </author>
    <author>
      <name>Julian Ibarz</name>
    </author>
    <author>
      <name>Brian Ichter</name>
    </author>
    <author>
      <name>Alex Irpan</name>
    </author>
    <author>
      <name>Tomas Jackson</name>
    </author>
    <author>
      <name>Sally Jesmonth</name>
    </author>
    <author>
      <name>Nikhil J Joshi</name>
    </author>
    <author>
      <name>Ryan Julian</name>
    </author>
    <author>
      <name>Dmitry Kalashnikov</name>
    </author>
    <author>
      <name>Yuheng Kuang</name>
    </author>
    <author>
      <name>Isabel Leal</name>
    </author>
    <author>
      <name>Kuang-Huei Lee</name>
    </author>
    <author>
      <name>Sergey Levine</name>
    </author>
    <author>
      <name>Yao Lu</name>
    </author>
    <author>
      <name>Utsav Malla</name>
    </author>
    <author>
      <name>Deeksha Manjunath</name>
    </author>
    <author>
      <name>Igor Mordatch</name>
    </author>
    <author>
      <name>Ofir Nachum</name>
    </author>
    <author>
      <name>Carolina Parada</name>
    </author>
    <author>
      <name>Jodilyn Peralta</name>
    </author>
    <author>
      <name>Emily Perez</name>
    </author>
    <author>
      <name>Karl Pertsch</name>
    </author>
    <author>
      <name>Jornell Quiambao</name>
    </author>
    <author>
      <name>Kanishka Rao</name>
    </author>
    <author>
      <name>Michael Ryoo</name>
    </author>
    <author>
      <name>Grecia Salazar</name>
    </author>
    <author>
      <name>Pannag Sanketi</name>
    </author>
    <author>
      <name>Kevin Sayed</name>
    </author>
    <author>
      <name>Jaspiar Singh</name>
    </author>
    <author>
      <name>Sumedh Sontakke</name>
    </author>
    <author>
      <name>Austin Stone</name>
    </author>
    <author>
      <name>Clayton Tan</name>
    </author>
    <author>
      <name>Huong Tran</name>
    </author>
    <author>
      <name>Vincent Vanhoucke</name>
    </author>
    <author>
      <name>Steve Vega</name>
    </author>
    <author>
      <name>Quan Vuong</name>
    </author>
    <author>
      <name>Fei Xia</name>
    </author>
    <author>
      <name>Ted Xiao</name>
    </author>
    <author>
      <name>Peng Xu</name>
    </author>
    <author>
      <name>Sichun Xu</name>
    </author>
    <author>
      <name>Tianhe Yu</name>
    </author>
    <author>
      <name>Brianna Zitkovich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">See website at robotics-transformer1.github.io</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.06817v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.06817v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.09748v2</id>
    <updated>2023-03-02T09:06:55Z</updated>
    <published>2022-12-19T18:59:58Z</published>
    <title>Scalable Diffusion Models with Transformers</title>
    <summary>  We explore a new class of diffusion models based on the transformer
architecture. We train latent diffusion models of images, replacing the
commonly-used U-Net backbone with a transformer that operates on latent
patches. We analyze the scalability of our Diffusion Transformers (DiTs)
through the lens of forward pass complexity as measured by Gflops. We find that
DiTs with higher Gflops -- through increased transformer depth/width or
increased number of input tokens -- consistently have lower FID. In addition to
possessing good scalability properties, our largest DiT-XL/2 models outperform
all prior diffusion models on the class-conditional ImageNet 512x512 and
256x256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter.
</summary>
    <author>
      <name>William Peebles</name>
    </author>
    <author>
      <name>Saining Xie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code, project page and videos available at
  https://www.wpeebles.com/DiT</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.09748v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.09748v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.13138v1</id>
    <updated>2022-12-26T14:28:24Z</updated>
    <published>2022-12-26T14:28:24Z</published>
    <title>Large Language Models Encode Clinical Knowledge</title>
    <summary>  Large language models (LLMs) have demonstrated impressive capabilities in
natural language understanding and generation, but the quality bar for medical
and clinical applications is high. Today, attempts to assess models' clinical
knowledge typically rely on automated evaluations on limited benchmarks. There
is no standard to evaluate model predictions and reasoning across a breadth of
tasks. To address this, we present MultiMedQA, a benchmark combining six
existing open question answering datasets spanning professional medical exams,
research, and consumer queries; and HealthSearchQA, a new free-response dataset
of medical questions searched online. We propose a framework for human
evaluation of model answers along multiple axes including factuality,
precision, possible harm, and bias. In addition, we evaluate PaLM (a
540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM, on
MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves
state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA,
MedMCQA, PubMedQA, MMLU clinical topics), including 67.6% accuracy on MedQA (US
Medical License Exam questions), surpassing prior state-of-the-art by over 17%.
However, human evaluation reveals key gaps in Flan-PaLM responses. To resolve
this we introduce instruction prompt tuning, a parameter-efficient approach for
aligning LLMs to new domains using a few exemplars. The resulting model,
Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show
that comprehension, recall of knowledge, and medical reasoning improve with
model scale and instruction prompt tuning, suggesting the potential utility of
LLMs in medicine. Our human evaluations reveal important limitations of today's
models, reinforcing the importance of both evaluation frameworks and method
development in creating safe, helpful LLM models for clinical applications.
</summary>
    <author>
      <name>Karan Singhal</name>
    </author>
    <author>
      <name>Shekoofeh Azizi</name>
    </author>
    <author>
      <name>Tao Tu</name>
    </author>
    <author>
      <name>S. Sara Mahdavi</name>
    </author>
    <author>
      <name>Jason Wei</name>
    </author>
    <author>
      <name>Hyung Won Chung</name>
    </author>
    <author>
      <name>Nathan Scales</name>
    </author>
    <author>
      <name>Ajay Tanwani</name>
    </author>
    <author>
      <name>Heather Cole-Lewis</name>
    </author>
    <author>
      <name>Stephen Pfohl</name>
    </author>
    <author>
      <name>Perry Payne</name>
    </author>
    <author>
      <name>Martin Seneviratne</name>
    </author>
    <author>
      <name>Paul Gamble</name>
    </author>
    <author>
      <name>Chris Kelly</name>
    </author>
    <author>
      <name>Nathaneal Scharli</name>
    </author>
    <author>
      <name>Aakanksha Chowdhery</name>
    </author>
    <author>
      <name>Philip Mansfield</name>
    </author>
    <author>
      <name>Blaise Aguera y Arcas</name>
    </author>
    <author>
      <name>Dale Webster</name>
    </author>
    <author>
      <name>Greg S. Corrado</name>
    </author>
    <author>
      <name>Yossi Matias</name>
    </author>
    <author>
      <name>Katherine Chou</name>
    </author>
    <author>
      <name>Juraj Gottweis</name>
    </author>
    <author>
      <name>Nenad Tomasev</name>
    </author>
    <author>
      <name>Yun Liu</name>
    </author>
    <author>
      <name>Alvin Rajkomar</name>
    </author>
    <author>
      <name>Joelle Barral</name>
    </author>
    <author>
      <name>Christopher Semturs</name>
    </author>
    <author>
      <name>Alan Karthikesalingam</name>
    </author>
    <author>
      <name>Vivek Natarajan</name>
    </author>
    <link href="http://arxiv.org/abs/2212.13138v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.13138v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.00704v1</id>
    <updated>2023-01-02T14:43:38Z</updated>
    <published>2023-01-02T14:43:38Z</published>
    <title>Muse: Text-To-Image Generation via Masked Generative Transformers</title>
    <summary>  We present Muse, a text-to-image Transformer model that achieves
state-of-the-art image generation performance while being significantly more
efficient than diffusion or autoregressive models. Muse is trained on a masked
modeling task in discrete token space: given the text embedding extracted from
a pre-trained large language model (LLM), Muse is trained to predict randomly
masked image tokens. Compared to pixel-space diffusion models, such as Imagen
and DALL-E 2, Muse is significantly more efficient due to the use of discrete
tokens and requiring fewer sampling iterations; compared to autoregressive
models, such as Parti, Muse is more efficient due to the use of parallel
decoding. The use of a pre-trained LLM enables fine-grained language
understanding, translating to high-fidelity image generation and the
understanding of visual concepts such as objects, their spatial relationships,
pose, cardinality etc. Our 900M parameter model achieves a new SOTA on CC3M,
with an FID score of 6.06. The Muse 3B parameter model achieves an FID of 7.88
on zero-shot COCO evaluation, along with a CLIP score of 0.32. Muse also
directly enables a number of image editing applications without the need to
fine-tune or invert the model: inpainting, outpainting, and mask-free editing.
More results are available at https://muse-model.github.io
</summary>
    <author>
      <name>Huiwen Chang</name>
    </author>
    <author>
      <name>Han Zhang</name>
    </author>
    <author>
      <name>Jarred Barber</name>
    </author>
    <author>
      <name>AJ Maschinot</name>
    </author>
    <author>
      <name>Jose Lezama</name>
    </author>
    <author>
      <name>Lu Jiang</name>
    </author>
    <author>
      <name>Ming-Hsuan Yang</name>
    </author>
    <author>
      <name>Kevin Murphy</name>
    </author>
    <author>
      <name>William T. Freeman</name>
    </author>
    <author>
      <name>Michael Rubinstein</name>
    </author>
    <author>
      <name>Yuanzhen Li</name>
    </author>
    <author>
      <name>Dilip Krishnan</name>
    </author>
    <link href="http://arxiv.org/abs/2301.00704v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.00704v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.02111v1</id>
    <updated>2023-01-05T15:37:15Z</updated>
    <published>2023-01-05T15:37:15Z</published>
    <title>Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers</title>
    <summary>  We introduce a language modeling approach for text to speech synthesis (TTS).
Specifically, we train a neural codec language model (called Vall-E) using
discrete codes derived from an off-the-shelf neural audio codec model, and
regard TTS as a conditional language modeling task rather than continuous
signal regression as in previous work. During the pre-training stage, we scale
up the TTS training data to 60K hours of English speech which is hundreds of
times larger than existing systems. Vall-E emerges in-context learning
capabilities and can be used to synthesize high-quality personalized speech
with only a 3-second enrolled recording of an unseen speaker as an acoustic
prompt. Experiment results show that Vall-E significantly outperforms the
state-of-the-art zero-shot TTS system in terms of speech naturalness and
speaker similarity. In addition, we find Vall-E could preserve the speaker's
emotion and acoustic environment of the acoustic prompt in synthesis. See
https://aka.ms/valle for demos of our work.
</summary>
    <author>
      <name>Chengyi Wang</name>
    </author>
    <author>
      <name>Sanyuan Chen</name>
    </author>
    <author>
      <name>Yu Wu</name>
    </author>
    <author>
      <name>Ziqiang Zhang</name>
    </author>
    <author>
      <name>Long Zhou</name>
    </author>
    <author>
      <name>Shujie Liu</name>
    </author>
    <author>
      <name>Zhuo Chen</name>
    </author>
    <author>
      <name>Yanqing Liu</name>
    </author>
    <author>
      <name>Huaming Wang</name>
    </author>
    <author>
      <name>Jinyu Li</name>
    </author>
    <author>
      <name>Lei He</name>
    </author>
    <author>
      <name>Sheng Zhao</name>
    </author>
    <author>
      <name>Furu Wei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Working in progress</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.02111v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.02111v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.04104v2</id>
    <updated>2024-04-17T17:41:20Z</updated>
    <published>2023-01-10T18:12:16Z</published>
    <title>Mastering Diverse Domains through World Models</title>
    <summary>  Developing a general algorithm that learns to solve tasks across a wide range
of applications has been a fundamental challenge in artificial intelligence.
Although current reinforcement learning algorithms can be readily applied to
tasks similar to what they have been developed for, configuring them for new
application domains requires significant human expertise and experimentation.
We present DreamerV3, a general algorithm that outperforms specialized methods
across over 150 diverse tasks, with a single configuration. Dreamer learns a
model of the environment and improves its behavior by imagining future
scenarios. Robustness techniques based on normalization, balancing, and
transformations enable stable learning across domains. Applied out of the box,
Dreamer is the first algorithm to collect diamonds in Minecraft from scratch
without human data or curricula. This achievement has been posed as a
significant challenge in artificial intelligence that requires exploring
farsighted strategies from pixels and sparse rewards in an open world. Our work
allows solving challenging control problems without extensive experimentation,
making reinforcement learning broadly applicable.
</summary>
    <author>
      <name>Danijar Hafner</name>
    </author>
    <author>
      <name>Jurgis Pasukonis</name>
    </author>
    <author>
      <name>Jimmy Ba</name>
    </author>
    <author>
      <name>Timothy Lillicrap</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Website: https://danijar.com/dreamerv3</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.04104v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.04104v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.11305v2</id>
    <updated>2023-07-23T04:18:36Z</updated>
    <published>2023-01-26T18:44:06Z</published>
    <title>DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability
  Curvature</title>
    <summary>  The increasing fluency and widespread usage of large language models (LLMs)
highlight the desirability of corresponding tools aiding detection of
LLM-generated text. In this paper, we identify a property of the structure of
an LLM's probability function that is useful for such detection. Specifically,
we demonstrate that text sampled from an LLM tends to occupy negative curvature
regions of the model's log probability function. Leveraging this observation,
we then define a new curvature-based criterion for judging if a passage is
generated from a given LLM. This approach, which we call DetectGPT, does not
require training a separate classifier, collecting a dataset of real or
generated passages, or explicitly watermarking generated text. It uses only log
probabilities computed by the model of interest and random perturbations of the
passage from another generic pre-trained language model (e.g., T5). We find
DetectGPT is more discriminative than existing zero-shot methods for model
sample detection, notably improving detection of fake news articles generated
by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline
to 0.95 AUROC for DetectGPT. See https://ericmitchell.ai/detectgpt for code,
data, and other project information.
</summary>
    <author>
      <name>Eric Mitchell</name>
    </author>
    <author>
      <name>Yoonho Lee</name>
    </author>
    <author>
      <name>Alexander Khazatsky</name>
    </author>
    <author>
      <name>Christopher D. Manning</name>
    </author>
    <author>
      <name>Chelsea Finn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.11305v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.11305v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.11325v1</id>
    <updated>2023-01-26T18:58:53Z</updated>
    <published>2023-01-26T18:58:53Z</published>
    <title>MusicLM: Generating Music From Text</title>
    <summary>  We introduce MusicLM, a model generating high-fidelity music from text
descriptions such as "a calming violin melody backed by a distorted guitar
riff". MusicLM casts the process of conditional music generation as a
hierarchical sequence-to-sequence modeling task, and it generates music at 24
kHz that remains consistent over several minutes. Our experiments show that
MusicLM outperforms previous systems both in audio quality and adherence to the
text description. Moreover, we demonstrate that MusicLM can be conditioned on
both text and a melody in that it can transform whistled and hummed melodies
according to the style described in a text caption. To support future research,
we publicly release MusicCaps, a dataset composed of 5.5k music-text pairs,
with rich text descriptions provided by human experts.
</summary>
    <author>
      <name>Andrea Agostinelli</name>
    </author>
    <author>
      <name>Timo I. Denk</name>
    </author>
    <author>
      <name>Zalán Borsos</name>
    </author>
    <author>
      <name>Jesse Engel</name>
    </author>
    <author>
      <name>Mauro Verzetti</name>
    </author>
    <author>
      <name>Antoine Caillon</name>
    </author>
    <author>
      <name>Qingqing Huang</name>
    </author>
    <author>
      <name>Aren Jansen</name>
    </author>
    <author>
      <name>Adam Roberts</name>
    </author>
    <author>
      <name>Marco Tagliasacchi</name>
    </author>
    <author>
      <name>Matt Sharifi</name>
    </author>
    <author>
      <name>Neil Zeghidour</name>
    </author>
    <author>
      <name>Christian Frank</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Supplementary material at
  https://google-research.github.io/seanet/musiclm/examples and
  https://kaggle.com/datasets/googleai/musiccaps</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.11325v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.11325v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.12503v3</id>
    <updated>2023-09-09T15:27:58Z</updated>
    <published>2023-01-29T17:48:17Z</published>
    <title>AudioLDM: Text-to-Audio Generation with Latent Diffusion Models</title>
    <summary>  Text-to-audio (TTA) system has recently gained attention for its ability to
synthesize general audio based on text descriptions. However, previous studies
in TTA have limited generation quality with high computational costs. In this
study, we propose AudioLDM, a TTA system that is built on a latent space to
learn the continuous audio representations from contrastive language-audio
pretraining (CLAP) latents. The pretrained CLAP models enable us to train LDMs
with audio embedding while providing text embedding as a condition during
sampling. By learning the latent representations of audio signals and their
compositions without modeling the cross-modal relationship, AudioLDM is
advantageous in both generation quality and computational efficiency. Trained
on AudioCaps with a single GPU, AudioLDM achieves state-of-the-art TTA
performance measured by both objective and subjective metrics (e.g., frechet
distance). Moreover, AudioLDM is the first TTA system that enables various
text-guided audio manipulations (e.g., style transfer) in a zero-shot fashion.
Our implementation and demos are available at https://audioldm.github.io.
</summary>
    <author>
      <name>Haohe Liu</name>
    </author>
    <author>
      <name>Zehua Chen</name>
    </author>
    <author>
      <name>Yi Yuan</name>
    </author>
    <author>
      <name>Xinhao Mei</name>
    </author>
    <author>
      <name>Xubo Liu</name>
    </author>
    <author>
      <name>Danilo Mandic</name>
    </author>
    <author>
      <name>Wenwu Wang</name>
    </author>
    <author>
      <name>Mark D. Plumbley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ICML 2023. Demo and implementation at
  https://audioldm.github.io. Evaluation toolbox at
  https://github.com/haoheliu/audioldm_eval</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.12503v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.12503v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.01318v1</id>
    <updated>2023-02-02T18:44:11Z</updated>
    <published>2023-02-02T18:44:11Z</published>
    <title>Accelerating Large Language Model Decoding with Speculative Sampling</title>
    <summary>  We present speculative sampling, an algorithm for accelerating transformer
decoding by enabling the generation of multiple tokens from each transformer
call. Our algorithm relies on the observation that the latency of parallel
scoring of short continuations, generated by a faster but less powerful draft
model, is comparable to that of sampling a single token from the larger target
model. This is combined with a novel modified rejection sampling scheme which
preserves the distribution of the target model within hardware numerics. We
benchmark speculative sampling with Chinchilla, a 70 billion parameter language
model, achieving a 2-2.5x decoding speedup in a distributed setup, without
compromising the sample quality or making modifications to the model itself.
</summary>
    <author>
      <name>Charlie Chen</name>
    </author>
    <author>
      <name>Sebastian Borgeaud</name>
    </author>
    <author>
      <name>Geoffrey Irving</name>
    </author>
    <author>
      <name>Jean-Baptiste Lespiau</name>
    </author>
    <author>
      <name>Laurent Sifre</name>
    </author>
    <author>
      <name>John Jumper</name>
    </author>
    <link href="http://arxiv.org/abs/2302.01318v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.01318v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.02662v4</id>
    <updated>2024-10-17T08:00:06Z</updated>
    <published>2023-02-06T10:01:08Z</published>
    <title>Grounding Large Language Models in Interactive Environments with Online
  Reinforcement Learning</title>
    <summary>  Recent works successfully leveraged Large Language Models' (LLM) abilities to
capture abstract knowledge about world's physics to solve decision-making
problems. Yet, the alignment between LLMs' knowledge and the environment can be
wrong and limit functional competence due to lack of grounding. In this paper,
we study an approach (named GLAM) to achieve this alignment through functional
grounding: we consider an agent using an LLM as a policy that is progressively
updated as the agent interacts with the environment, leveraging online
Reinforcement Learning to improve its performance to solve goals. Using an
interactive textual environment designed to study higher-level forms of
functional grounding, and a set of spatial and navigation tasks, we study
several scientific questions: 1) Can LLMs boost sample efficiency for online
learning of various RL tasks? 2) How can it boost different forms of
generalization? 3) What is the impact of online learning? We study these
questions by functionally grounding several variants (size, architecture) of
FLAN-T5.
</summary>
    <author>
      <name>Thomas Carta</name>
    </author>
    <author>
      <name>Clément Romac</name>
    </author>
    <author>
      <name>Thomas Wolf</name>
    </author>
    <author>
      <name>Sylvain Lamprier</name>
    </author>
    <author>
      <name>Olivier Sigaud</name>
    </author>
    <author>
      <name>Pierre-Yves Oudeyer</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">PMLR 202 (2023):3676-3713</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2302.02662v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.02662v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.02948v4</id>
    <updated>2023-05-31T10:52:56Z</updated>
    <published>2023-02-06T17:30:22Z</published>
    <title>Efficient Online Reinforcement Learning with Offline Data</title>
    <summary>  Sample efficiency and exploration remain major challenges in online
reinforcement learning (RL). A powerful approach that can be applied to address
these issues is the inclusion of offline data, such as prior trajectories from
a human expert or a sub-optimal exploration policy. Previous methods have
relied on extensive modifications and additional complexity to ensure the
effective use of this data. Instead, we ask: can we simply apply existing
off-policy methods to leverage offline data when learning online? In this work,
we demonstrate that the answer is yes; however, a set of minimal but important
changes to existing off-policy RL algorithms are required to achieve reliable
performance. We extensively ablate these design choices, demonstrating the key
factors that most affect performance, and arrive at a set of recommendations
that practitioners can readily apply, whether their data comprise a small
number of expert demonstrations or large volumes of sub-optimal trajectories.
We see that correct application of these simple recommendations can provide a
$\mathbf{2.5\times}$ improvement over existing approaches across a diverse set
of competitive benchmarks, with no additional computational overhead. We have
released our code at https://github.com/ikostrikov/rlpd.
</summary>
    <author>
      <name>Philip J. Ball</name>
    </author>
    <author>
      <name>Laura Smith</name>
    </author>
    <author>
      <name>Ilya Kostrikov</name>
    </author>
    <author>
      <name>Sergey Levine</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Short Presentation at ICML 2023; to reproduce our results and use our
  codebase, see https://github.com/ikostrikov/rlpd</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.02948v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.02948v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.03011v1</id>
    <updated>2023-02-06T18:50:23Z</updated>
    <published>2023-02-06T18:50:23Z</published>
    <title>Structure and Content-Guided Video Synthesis with Diffusion Models</title>
    <summary>  Text-guided generative diffusion models unlock powerful image creation and
editing tools. While these have been extended to video generation, current
approaches that edit the content of existing footage while retaining structure
require expensive re-training for every input or rely on error-prone
propagation of image edits across frames. In this work, we present a structure
and content-guided video diffusion model that edits videos based on visual or
textual descriptions of the desired output. Conflicts between user-provided
content edits and structure representations occur due to insufficient
disentanglement between the two aspects. As a solution, we show that training
on monocular depth estimates with varying levels of detail provides control
over structure and content fidelity. Our model is trained jointly on images and
videos which also exposes explicit control of temporal consistency through a
novel guidance method. Our experiments demonstrate a wide variety of successes;
fine-grained control over output characteristics, customization based on a few
reference images, and a strong user preference towards results by our model.
</summary>
    <author>
      <name>Patrick Esser</name>
    </author>
    <author>
      <name>Johnathan Chiu</name>
    </author>
    <author>
      <name>Parmida Atighehchian</name>
    </author>
    <author>
      <name>Jonathan Granskog</name>
    </author>
    <author>
      <name>Anastasis Germanidis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page at https://research.runwayml.com/gen1</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.03011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.03011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.04761v1</id>
    <updated>2023-02-09T16:49:57Z</updated>
    <published>2023-02-09T16:49:57Z</published>
    <title>Toolformer: Language Models Can Teach Themselves to Use Tools</title>
    <summary>  Language models (LMs) exhibit remarkable abilities to solve new tasks from
just a few examples or textual instructions, especially at scale. They also,
paradoxically, struggle with basic functionality, such as arithmetic or factual
lookup, where much simpler and smaller models excel. In this paper, we show
that LMs can teach themselves to use external tools via simple APIs and achieve
the best of both worlds. We introduce Toolformer, a model trained to decide
which APIs to call, when to call them, what arguments to pass, and how to best
incorporate the results into future token prediction. This is done in a
self-supervised way, requiring nothing more than a handful of demonstrations
for each API. We incorporate a range of tools, including a calculator, a Q\&amp;A
system, two different search engines, a translation system, and a calendar.
Toolformer achieves substantially improved zero-shot performance across a
variety of downstream tasks, often competitive with much larger models, without
sacrificing its core language modeling abilities.
</summary>
    <author>
      <name>Timo Schick</name>
    </author>
    <author>
      <name>Jane Dwivedi-Yu</name>
    </author>
    <author>
      <name>Roberto Dessì</name>
    </author>
    <author>
      <name>Roberta Raileanu</name>
    </author>
    <author>
      <name>Maria Lomeli</name>
    </author>
    <author>
      <name>Luke Zettlemoyer</name>
    </author>
    <author>
      <name>Nicola Cancedda</name>
    </author>
    <author>
      <name>Thomas Scialom</name>
    </author>
    <link href="http://arxiv.org/abs/2302.04761v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.04761v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.05442v1</id>
    <updated>2023-02-10T18:58:21Z</updated>
    <published>2023-02-10T18:58:21Z</published>
    <title>Scaling Vision Transformers to 22 Billion Parameters</title>
    <summary>  The scaling of Transformers has driven breakthrough capabilities for language
models. At present, the largest large language models (LLMs) contain upwards of
100B parameters. Vision Transformers (ViT) have introduced the same
architecture to image and video modelling, but these have not yet been
successfully scaled to nearly the same degree; the largest dense ViT contains
4B parameters (Chen et al., 2022). We present a recipe for highly efficient and
stable training of a 22B-parameter ViT (ViT-22B) and perform a wide variety of
experiments on the resulting model. When evaluated on downstream tasks (often
with a lightweight linear model on frozen features), ViT-22B demonstrates
increasing performance with scale. We further observe other interesting
benefits of scale, including an improved tradeoff between fairness and
performance, state-of-the-art alignment to human visual perception in terms of
shape/texture bias, and improved robustness. ViT-22B demonstrates the potential
for "LLM-like" scaling in vision, and provides key steps towards getting there.
</summary>
    <author>
      <name>Mostafa Dehghani</name>
    </author>
    <author>
      <name>Josip Djolonga</name>
    </author>
    <author>
      <name>Basil Mustafa</name>
    </author>
    <author>
      <name>Piotr Padlewski</name>
    </author>
    <author>
      <name>Jonathan Heek</name>
    </author>
    <author>
      <name>Justin Gilmer</name>
    </author>
    <author>
      <name>Andreas Steiner</name>
    </author>
    <author>
      <name>Mathilde Caron</name>
    </author>
    <author>
      <name>Robert Geirhos</name>
    </author>
    <author>
      <name>Ibrahim Alabdulmohsin</name>
    </author>
    <author>
      <name>Rodolphe Jenatton</name>
    </author>
    <author>
      <name>Lucas Beyer</name>
    </author>
    <author>
      <name>Michael Tschannen</name>
    </author>
    <author>
      <name>Anurag Arnab</name>
    </author>
    <author>
      <name>Xiao Wang</name>
    </author>
    <author>
      <name>Carlos Riquelme</name>
    </author>
    <author>
      <name>Matthias Minderer</name>
    </author>
    <author>
      <name>Joan Puigcerver</name>
    </author>
    <author>
      <name>Utku Evci</name>
    </author>
    <author>
      <name>Manoj Kumar</name>
    </author>
    <author>
      <name>Sjoerd van Steenkiste</name>
    </author>
    <author>
      <name>Gamaleldin F. Elsayed</name>
    </author>
    <author>
      <name>Aravindh Mahendran</name>
    </author>
    <author>
      <name>Fisher Yu</name>
    </author>
    <author>
      <name>Avital Oliver</name>
    </author>
    <author>
      <name>Fantine Huot</name>
    </author>
    <author>
      <name>Jasmijn Bastings</name>
    </author>
    <author>
      <name>Mark Patrick Collier</name>
    </author>
    <author>
      <name>Alexey Gritsenko</name>
    </author>
    <author>
      <name>Vighnesh Birodkar</name>
    </author>
    <author>
      <name>Cristina Vasconcelos</name>
    </author>
    <author>
      <name>Yi Tay</name>
    </author>
    <author>
      <name>Thomas Mensink</name>
    </author>
    <author>
      <name>Alexander Kolesnikov</name>
    </author>
    <author>
      <name>Filip Pavetić</name>
    </author>
    <author>
      <name>Dustin Tran</name>
    </author>
    <author>
      <name>Thomas Kipf</name>
    </author>
    <author>
      <name>Mario Lučić</name>
    </author>
    <author>
      <name>Xiaohua Zhai</name>
    </author>
    <author>
      <name>Daniel Keysers</name>
    </author>
    <author>
      <name>Jeremiah Harmsen</name>
    </author>
    <author>
      <name>Neil Houlsby</name>
    </author>
    <link href="http://arxiv.org/abs/2302.05442v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.05442v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.05543v3</id>
    <updated>2023-11-26T22:26:12Z</updated>
    <published>2023-02-10T23:12:37Z</published>
    <title>Adding Conditional Control to Text-to-Image Diffusion Models</title>
    <summary>  We present ControlNet, a neural network architecture to add spatial
conditioning controls to large, pretrained text-to-image diffusion models.
ControlNet locks the production-ready large diffusion models, and reuses their
deep and robust encoding layers pretrained with billions of images as a strong
backbone to learn a diverse set of conditional controls. The neural
architecture is connected with "zero convolutions" (zero-initialized
convolution layers) that progressively grow the parameters from zero and ensure
that no harmful noise could affect the finetuning. We test various conditioning
controls, eg, edges, depth, segmentation, human pose, etc, with Stable
Diffusion, using single or multiple conditions, with or without prompts. We
show that the training of ControlNets is robust with small (&lt;50k) and large
(&gt;1m) datasets. Extensive results show that ControlNet may facilitate wider
applications to control image diffusion models.
</summary>
    <author>
      <name>Lvmin Zhang</name>
    </author>
    <author>
      <name>Anyi Rao</name>
    </author>
    <author>
      <name>Maneesh Agrawala</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Codes and Supplementary Material:
  https://github.com/lllyasviel/ControlNet</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.05543v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.05543v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.06675v4</id>
    <updated>2023-05-08T21:49:57Z</updated>
    <published>2023-02-13T20:27:30Z</published>
    <title>Symbolic Discovery of Optimization Algorithms</title>
    <summary>  We present a method to formulate algorithm discovery as program search, and
apply it to discover optimization algorithms for deep neural network training.
We leverage efficient search techniques to explore an infinite and sparse
program space. To bridge the large generalization gap between proxy and target
tasks, we also introduce program selection and simplification strategies. Our
method discovers a simple and effective optimization algorithm, $\textbf{Lion}$
($\textit{Evo$\textbf{L}$ved S$\textbf{i}$gn M$\textbf{o}$me$\textbf{n}$tum}$).
It is more memory-efficient than Adam as it only keeps track of the momentum.
Different from adaptive optimizers, its update has the same magnitude for each
parameter calculated through the sign operation. We compare Lion with widely
used optimizers, such as Adam and Adafactor, for training a variety of models
on different tasks. On image classification, Lion boosts the accuracy of ViT by
up to 2% on ImageNet and saves up to 5x the pre-training compute on JFT. On
vision-language contrastive learning, we achieve 88.3% $\textit{zero-shot}$ and
91.1% $\textit{fine-tuning}$ accuracy on ImageNet, surpassing the previous best
results by 2% and 0.1%, respectively. On diffusion models, Lion outperforms
Adam by achieving a better FID score and reducing the training compute by up to
2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion
exhibits a similar or better performance compared to Adam. Our analysis of Lion
reveals that its performance gain grows with the training batch size. It also
requires a smaller learning rate than Adam due to the larger norm of the update
produced by the sign function. Additionally, we examine the limitations of Lion
and identify scenarios where its improvements are small or not statistically
significant. Lion is also successfully deployed in production systems such as
Google search ads CTR model.
</summary>
    <author>
      <name>Xiangning Chen</name>
    </author>
    <author>
      <name>Chen Liang</name>
    </author>
    <author>
      <name>Da Huang</name>
    </author>
    <author>
      <name>Esteban Real</name>
    </author>
    <author>
      <name>Kaiyuan Wang</name>
    </author>
    <author>
      <name>Yao Liu</name>
    </author>
    <author>
      <name>Hieu Pham</name>
    </author>
    <author>
      <name>Xuanyi Dong</name>
    </author>
    <author>
      <name>Thang Luong</name>
    </author>
    <author>
      <name>Cho-Jui Hsieh</name>
    </author>
    <author>
      <name>Yifeng Lu</name>
    </author>
    <author>
      <name>Quoc V. Le</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, Lion is successfully deployed in production systems. We
  also add comparison with other automatically discovered optimizers</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.06675v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.06675v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.13971v1</id>
    <updated>2023-02-27T17:11:15Z</updated>
    <published>2023-02-27T17:11:15Z</published>
    <title>LLaMA: Open and Efficient Foundation Language Models</title>
    <summary>  We introduce LLaMA, a collection of foundation language models ranging from
7B to 65B parameters. We train our models on trillions of tokens, and show that
it is possible to train state-of-the-art models using publicly available
datasets exclusively, without resorting to proprietary and inaccessible
datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks,
and LLaMA-65B is competitive with the best models, Chinchilla-70B and
PaLM-540B. We release all our models to the research community.
</summary>
    <author>
      <name>Hugo Touvron</name>
    </author>
    <author>
      <name>Thibaut Lavril</name>
    </author>
    <author>
      <name>Gautier Izacard</name>
    </author>
    <author>
      <name>Xavier Martinet</name>
    </author>
    <author>
      <name>Marie-Anne Lachaux</name>
    </author>
    <author>
      <name>Timothée Lacroix</name>
    </author>
    <author>
      <name>Baptiste Rozière</name>
    </author>
    <author>
      <name>Naman Goyal</name>
    </author>
    <author>
      <name>Eric Hambro</name>
    </author>
    <author>
      <name>Faisal Azhar</name>
    </author>
    <author>
      <name>Aurelien Rodriguez</name>
    </author>
    <author>
      <name>Armand Joulin</name>
    </author>
    <author>
      <name>Edouard Grave</name>
    </author>
    <author>
      <name>Guillaume Lample</name>
    </author>
    <link href="http://arxiv.org/abs/2302.13971v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.13971v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.14045v2</id>
    <updated>2023-03-01T11:04:51Z</updated>
    <published>2023-02-27T18:55:27Z</published>
    <title>Language Is Not All You Need: Aligning Perception with Language Models</title>
    <summary>  A big convergence of language, multimodal perception, action, and world
modeling is a key step toward artificial general intelligence. In this work, we
introduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive
general modalities, learn in context (i.e., few-shot), and follow instructions
(i.e., zero-shot). Specifically, we train Kosmos-1 from scratch on web-scale
multimodal corpora, including arbitrarily interleaved text and images,
image-caption pairs, and text data. We evaluate various settings, including
zero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range
of tasks without any gradient updates or finetuning. Experimental results show
that Kosmos-1 achieves impressive performance on (i) language understanding,
generation, and even OCR-free NLP (directly fed with document images), (ii)
perception-language tasks, including multimodal dialogue, image captioning,
visual question answering, and (iii) vision tasks, such as image recognition
with descriptions (specifying classification via text instructions). We also
show that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge
from language to multimodal, and from multimodal to language. In addition, we
introduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning
capability of MLLMs.
</summary>
    <author>
      <name>Shaohan Huang</name>
    </author>
    <author>
      <name>Li Dong</name>
    </author>
    <author>
      <name>Wenhui Wang</name>
    </author>
    <author>
      <name>Yaru Hao</name>
    </author>
    <author>
      <name>Saksham Singhal</name>
    </author>
    <author>
      <name>Shuming Ma</name>
    </author>
    <author>
      <name>Tengchao Lv</name>
    </author>
    <author>
      <name>Lei Cui</name>
    </author>
    <author>
      <name>Owais Khan Mohammed</name>
    </author>
    <author>
      <name>Barun Patra</name>
    </author>
    <author>
      <name>Qiang Liu</name>
    </author>
    <author>
      <name>Kriti Aggarwal</name>
    </author>
    <author>
      <name>Zewen Chi</name>
    </author>
    <author>
      <name>Johan Bjorck</name>
    </author>
    <author>
      <name>Vishrav Chaudhary</name>
    </author>
    <author>
      <name>Subhojit Som</name>
    </author>
    <author>
      <name>Xia Song</name>
    </author>
    <author>
      <name>Furu Wei</name>
    </author>
    <link href="http://arxiv.org/abs/2302.14045v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.14045v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.00001v1</id>
    <updated>2023-02-27T22:09:35Z</updated>
    <published>2023-02-27T22:09:35Z</published>
    <title>Reward Design with Language Models</title>
    <summary>  Reward design in reinforcement learning (RL) is challenging since specifying
human notions of desired behavior may be difficult via reward functions or
require many expert demonstrations. Can we instead cheaply design rewards using
a natural language interface? This paper explores how to simplify reward design
by prompting a large language model (LLM) such as GPT-3 as a proxy reward
function, where the user provides a textual prompt containing a few examples
(few-shot) or a description (zero-shot) of the desired behavior. Our approach
leverages this proxy reward function in an RL framework. Specifically, users
specify a prompt once at the beginning of training. During training, the LLM
evaluates an RL agent's behavior against the desired behavior described by the
prompt and outputs a corresponding reward signal. The RL agent then uses this
reward to update its behavior. We evaluate whether our approach can train
agents aligned with user objectives in the Ultimatum Game, matrix games, and
the DealOrNoDeal negotiation task. In all three tasks, we show that RL agents
trained with our framework are well-aligned with the user's objectives and
outperform RL agents trained with reward functions learned via supervised
learning
</summary>
    <author>
      <name>Minae Kwon</name>
    </author>
    <author>
      <name>Sang Michael Xie</name>
    </author>
    <author>
      <name>Kalesha Bullard</name>
    </author>
    <author>
      <name>Dorsa Sadigh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Learning Representations (ICLR) 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.00001v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.00001v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.01037v3</id>
    <updated>2023-09-25T01:20:23Z</updated>
    <published>2023-03-02T07:47:18Z</published>
    <title>Google USM: Scaling Automatic Speech Recognition Beyond 100 Languages</title>
    <summary>  We introduce the Universal Speech Model (USM), a single large model that
performs automatic speech recognition (ASR) across 100+ languages. This is
achieved by pre-training the encoder of the model on a large unlabeled
multilingual dataset of 12 million (M) hours spanning over 300 languages, and
fine-tuning on a smaller labeled dataset. We use multilingual pre-training with
random-projection quantization and speech-text modality matching to achieve
state-of-the-art performance on downstream multilingual ASR and speech-to-text
translation tasks. We also demonstrate that despite using a labeled training
set 1/7-th the size of that used for the Whisper model, our model exhibits
comparable or better performance on both in-domain and out-of-domain speech
recognition tasks across many languages.
</summary>
    <author>
      <name>Yu Zhang</name>
    </author>
    <author>
      <name>Wei Han</name>
    </author>
    <author>
      <name>James Qin</name>
    </author>
    <author>
      <name>Yongqiang Wang</name>
    </author>
    <author>
      <name>Ankur Bapna</name>
    </author>
    <author>
      <name>Zhehuai Chen</name>
    </author>
    <author>
      <name>Nanxin Chen</name>
    </author>
    <author>
      <name>Bo Li</name>
    </author>
    <author>
      <name>Vera Axelrod</name>
    </author>
    <author>
      <name>Gary Wang</name>
    </author>
    <author>
      <name>Zhong Meng</name>
    </author>
    <author>
      <name>Ke Hu</name>
    </author>
    <author>
      <name>Andrew Rosenberg</name>
    </author>
    <author>
      <name>Rohit Prabhavalkar</name>
    </author>
    <author>
      <name>Daniel S. Park</name>
    </author>
    <author>
      <name>Parisa Haghani</name>
    </author>
    <author>
      <name>Jason Riesa</name>
    </author>
    <author>
      <name>Ginger Perng</name>
    </author>
    <author>
      <name>Hagen Soltau</name>
    </author>
    <author>
      <name>Trevor Strohman</name>
    </author>
    <author>
      <name>Bhuvana Ramabhadran</name>
    </author>
    <author>
      <name>Tara Sainath</name>
    </author>
    <author>
      <name>Pedro Moreno</name>
    </author>
    <author>
      <name>Chung-Cheng Chiu</name>
    </author>
    <author>
      <name>Johan Schalkwyk</name>
    </author>
    <author>
      <name>Françoise Beaufays</name>
    </author>
    <author>
      <name>Yonghui Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 7 figures, 8 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.01037v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.01037v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.03378v1</id>
    <updated>2023-03-06T18:58:06Z</updated>
    <published>2023-03-06T18:58:06Z</published>
    <title>PaLM-E: An Embodied Multimodal Language Model</title>
    <summary>  Large language models excel at a wide range of complex tasks. However,
enabling general inference in the real world, e.g., for robotics problems,
raises the challenge of grounding. We propose embodied language models to
directly incorporate real-world continuous sensor modalities into language
models and thereby establish the link between words and percepts. Input to our
embodied language model are multi-modal sentences that interleave visual,
continuous state estimation, and textual input encodings. We train these
encodings end-to-end, in conjunction with a pre-trained large language model,
for multiple embodied tasks including sequential robotic manipulation planning,
visual question answering, and captioning. Our evaluations show that PaLM-E, a
single large embodied multimodal model, can address a variety of embodied
reasoning tasks, from a variety of observation modalities, on multiple
embodiments, and further, exhibits positive transfer: the model benefits from
diverse joint training across internet-scale language, vision, and
visual-language domains. Our largest model, PaLM-E-562B with 562B parameters,
in addition to being trained on robotics tasks, is a visual-language generalist
with state-of-the-art performance on OK-VQA, and retains generalist language
capabilities with increasing scale.
</summary>
    <author>
      <name>Danny Driess</name>
    </author>
    <author>
      <name>Fei Xia</name>
    </author>
    <author>
      <name>Mehdi S. M. Sajjadi</name>
    </author>
    <author>
      <name>Corey Lynch</name>
    </author>
    <author>
      <name>Aakanksha Chowdhery</name>
    </author>
    <author>
      <name>Brian Ichter</name>
    </author>
    <author>
      <name>Ayzaan Wahid</name>
    </author>
    <author>
      <name>Jonathan Tompson</name>
    </author>
    <author>
      <name>Quan Vuong</name>
    </author>
    <author>
      <name>Tianhe Yu</name>
    </author>
    <author>
      <name>Wenlong Huang</name>
    </author>
    <author>
      <name>Yevgen Chebotar</name>
    </author>
    <author>
      <name>Pierre Sermanet</name>
    </author>
    <author>
      <name>Daniel Duckworth</name>
    </author>
    <author>
      <name>Sergey Levine</name>
    </author>
    <author>
      <name>Vincent Vanhoucke</name>
    </author>
    <author>
      <name>Karol Hausman</name>
    </author>
    <author>
      <name>Marc Toussaint</name>
    </author>
    <author>
      <name>Klaus Greff</name>
    </author>
    <author>
      <name>Andy Zeng</name>
    </author>
    <author>
      <name>Igor Mordatch</name>
    </author>
    <author>
      <name>Pete Florence</name>
    </author>
    <link href="http://arxiv.org/abs/2303.03378v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.03378v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.04671v1</id>
    <updated>2023-03-08T15:50:02Z</updated>
    <published>2023-03-08T15:50:02Z</published>
    <title>Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation
  Models</title>
    <summary>  ChatGPT is attracting a cross-field interest as it provides a language
interface with remarkable conversational competency and reasoning capabilities
across many domains. However, since ChatGPT is trained with languages, it is
currently not capable of processing or generating images from the visual world.
At the same time, Visual Foundation Models, such as Visual Transformers or
Stable Diffusion, although showing great visual understanding and generation
capabilities, they are only experts on specific tasks with one-round fixed
inputs and outputs. To this end, We build a system called \textbf{Visual
ChatGPT}, incorporating different Visual Foundation Models, to enable the user
to interact with ChatGPT by 1) sending and receiving not only languages but
also images 2) providing complex visual questions or visual editing
instructions that require the collaboration of multiple AI models with
multi-steps. 3) providing feedback and asking for corrected results. We design
a series of prompts to inject the visual model information into ChatGPT,
considering models of multiple inputs/outputs and models that require visual
feedback. Experiments show that Visual ChatGPT opens the door to investigating
the visual roles of ChatGPT with the help of Visual Foundation Models. Our
system is publicly available at
\url{https://github.com/microsoft/visual-chatgpt}.
</summary>
    <author>
      <name>Chenfei Wu</name>
    </author>
    <author>
      <name>Shengming Yin</name>
    </author>
    <author>
      <name>Weizhen Qi</name>
    </author>
    <author>
      <name>Xiaodong Wang</name>
    </author>
    <author>
      <name>Zecheng Tang</name>
    </author>
    <author>
      <name>Nan Duan</name>
    </author>
    <link href="http://arxiv.org/abs/2303.04671v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.04671v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.05511v2</id>
    <updated>2023-06-19T07:01:08Z</updated>
    <published>2023-03-09T18:59:47Z</published>
    <title>Scaling up GANs for Text-to-Image Synthesis</title>
    <summary>  The recent success of text-to-image synthesis has taken the world by storm
and captured the general public's imagination. From a technical standpoint, it
also marked a drastic change in the favored architecture to design generative
image models. GANs used to be the de facto choice, with techniques like
StyleGAN. With DALL-E 2, auto-regressive and diffusion models became the new
standard for large-scale generative models overnight. This rapid shift raises a
fundamental question: can we scale up GANs to benefit from large datasets like
LAION? We find that na\"Ively increasing the capacity of the StyleGAN
architecture quickly becomes unstable. We introduce GigaGAN, a new GAN
architecture that far exceeds this limit, demonstrating GANs as a viable option
for text-to-image synthesis. GigaGAN offers three major advantages. First, it
is orders of magnitude faster at inference time, taking only 0.13 seconds to
synthesize a 512px image. Second, it can synthesize high-resolution images, for
example, 16-megapixel pixels in 3.66 seconds. Finally, GigaGAN supports various
latent space editing applications such as latent interpolation, style mixing,
and vector arithmetic operations.
</summary>
    <author>
      <name>Minguk Kang</name>
    </author>
    <author>
      <name>Jun-Yan Zhu</name>
    </author>
    <author>
      <name>Richard Zhang</name>
    </author>
    <author>
      <name>Jaesik Park</name>
    </author>
    <author>
      <name>Eli Shechtman</name>
    </author>
    <author>
      <name>Sylvain Paris</name>
    </author>
    <author>
      <name>Taesung Park</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2023. Project webpage at https://mingukkang.github.io/GigaGAN/</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.05511v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.05511v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.06296v2</id>
    <updated>2023-07-25T17:42:37Z</updated>
    <published>2023-03-11T03:30:47Z</published>
    <title>Stabilizing Transformer Training by Preventing Attention Entropy
  Collapse</title>
    <summary>  Training stability is of great importance to Transformers. In this work, we
investigate the training dynamics of Transformers by examining the evolution of
the attention layers. In particular, we track the attention entropy for each
attention head during the course of training, which is a proxy for model
sharpness. We identify a common pattern across different architectures and
tasks, where low attention entropy is accompanied by high training instability,
which can take the form of oscillating loss or divergence. We denote the
pathologically low attention entropy, corresponding to highly concentrated
attention scores, as $\textit{entropy collapse}$. As a remedy, we propose
$\sigma$Reparam, a simple and efficient solution where we reparametrize all
linear layers with spectral normalization and an additional learned scalar. We
demonstrate that $\sigma$Reparam successfully prevents entropy collapse in the
attention layers, promoting more stable training. Additionally, we prove a
tight lower bound of the attention entropy, which decreases exponentially fast
with the spectral norm of the attention logits, providing additional motivation
for our approach. We conduct experiments with $\sigma$Reparam on image
classification, image self-supervised learning, machine translation, speech
recognition, and language modeling tasks. We show that $\sigma$Reparam provides
stability and robustness with respect to the choice of hyperparameters, going
so far as enabling training (a) a Vision Transformer {to competitive
performance} without warmup, weight decay, layer normalization or adaptive
optimizers; (b) deep architectures in machine translation and (c) speech
recognition to competitive performance without warmup and adaptive optimizers.
Code is available at \url{https://github.com/apple/ml-sigma-reparam}.
</summary>
    <author>
      <name>Shuangfei Zhai</name>
    </author>
    <author>
      <name>Tatiana Likhomanenko</name>
    </author>
    <author>
      <name>Etai Littwin</name>
    </author>
    <author>
      <name>Dan Busbridge</name>
    </author>
    <author>
      <name>Jason Ramapuram</name>
    </author>
    <author>
      <name>Yizhe Zhang</name>
    </author>
    <author>
      <name>Jiatao Gu</name>
    </author>
    <author>
      <name>Josh Susskind</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In International Conference on Machine Learning (pp. 40770-40803).
  PMLR. 2023</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2303.06296v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.06296v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.12712v5</id>
    <updated>2023-04-13T20:41:31Z</updated>
    <published>2023-03-22T16:51:28Z</published>
    <title>Sparks of Artificial General Intelligence: Early experiments with GPT-4</title>
    <summary>  Artificial intelligence (AI) researchers have been developing and refining
large language models (LLMs) that exhibit remarkable capabilities across a
variety of domains and tasks, challenging our understanding of learning and
cognition. The latest model developed by OpenAI, GPT-4, was trained using an
unprecedented scale of compute and data. In this paper, we report on our
investigation of an early version of GPT-4, when it was still in active
development by OpenAI. We contend that (this early version of) GPT-4 is part of
a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that
exhibit more general intelligence than previous AI models. We discuss the
rising capabilities and implications of these models. We demonstrate that,
beyond its mastery of language, GPT-4 can solve novel and difficult tasks that
span mathematics, coding, vision, medicine, law, psychology and more, without
needing any special prompting. Moreover, in all of these tasks, GPT-4's
performance is strikingly close to human-level performance, and often vastly
surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's
capabilities, we believe that it could reasonably be viewed as an early (yet
still incomplete) version of an artificial general intelligence (AGI) system.
In our exploration of GPT-4, we put special emphasis on discovering its
limitations, and we discuss the challenges ahead for advancing towards deeper
and more comprehensive versions of AGI, including the possible need for
pursuing a new paradigm that moves beyond next-word prediction. We conclude
with reflections on societal influences of the recent technological leap and
future research directions.
</summary>
    <author>
      <name>Sébastien Bubeck</name>
    </author>
    <author>
      <name>Varun Chandrasekaran</name>
    </author>
    <author>
      <name>Ronen Eldan</name>
    </author>
    <author>
      <name>Johannes Gehrke</name>
    </author>
    <author>
      <name>Eric Horvitz</name>
    </author>
    <author>
      <name>Ece Kamar</name>
    </author>
    <author>
      <name>Peter Lee</name>
    </author>
    <author>
      <name>Yin Tat Lee</name>
    </author>
    <author>
      <name>Yuanzhi Li</name>
    </author>
    <author>
      <name>Scott Lundberg</name>
    </author>
    <author>
      <name>Harsha Nori</name>
    </author>
    <author>
      <name>Hamid Palangi</name>
    </author>
    <author>
      <name>Marco Tulio Ribeiro</name>
    </author>
    <author>
      <name>Yi Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2303.12712v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.12712v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.17564v3</id>
    <updated>2023-12-21T06:21:11Z</updated>
    <published>2023-03-30T17:30:36Z</published>
    <title>BloombergGPT: A Large Language Model for Finance</title>
    <summary>  The use of NLP in the realm of financial technology is broad and complex,
with applications ranging from sentiment analysis and named entity recognition
to question answering. Large Language Models (LLMs) have been shown to be
effective on a variety of tasks; however, no LLM specialized for the financial
domain has been reported in literature. In this work, we present BloombergGPT,
a 50 billion parameter language model that is trained on a wide range of
financial data. We construct a 363 billion token dataset based on Bloomberg's
extensive data sources, perhaps the largest domain-specific dataset yet,
augmented with 345 billion tokens from general purpose datasets. We validate
BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite
of internal benchmarks that most accurately reflect our intended usage. Our
mixed dataset training leads to a model that outperforms existing models on
financial tasks by significant margins without sacrificing performance on
general LLM benchmarks. Additionally, we explain our modeling choices, training
process, and evaluation methodology. We release Training Chronicles (Appendix
C) detailing our experience in training BloombergGPT.
</summary>
    <author>
      <name>Shijie Wu</name>
    </author>
    <author>
      <name>Ozan Irsoy</name>
    </author>
    <author>
      <name>Steven Lu</name>
    </author>
    <author>
      <name>Vadim Dabravolski</name>
    </author>
    <author>
      <name>Mark Dredze</name>
    </author>
    <author>
      <name>Sebastian Gehrmann</name>
    </author>
    <author>
      <name>Prabhanjan Kambadur</name>
    </author>
    <author>
      <name>David Rosenberg</name>
    </author>
    <author>
      <name>Gideon Mann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Updated to include Training Chronicles (Appendix C)</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.17564v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.17564v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.17580v4</id>
    <updated>2023-12-03T18:17:21Z</updated>
    <published>2023-03-30T17:48:28Z</published>
    <title>HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging
  Face</title>
    <summary>  Solving complicated AI tasks with different domains and modalities is a key
step toward artificial general intelligence. While there are numerous AI models
available for various domains and modalities, they cannot handle complicated AI
tasks autonomously. Considering large language models (LLMs) have exhibited
exceptional abilities in language understanding, generation, interaction, and
reasoning, we advocate that LLMs could act as a controller to manage existing
AI models to solve complicated AI tasks, with language serving as a generic
interface to empower this. Based on this philosophy, we present HuggingGPT, an
LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI
models in machine learning communities (e.g., Hugging Face) to solve AI tasks.
Specifically, we use ChatGPT to conduct task planning when receiving a user
request, select models according to their function descriptions available in
Hugging Face, execute each subtask with the selected AI model, and summarize
the response according to the execution results. By leveraging the strong
language capability of ChatGPT and abundant AI models in Hugging Face,
HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different
modalities and domains and achieve impressive results in language, vision,
speech, and other challenging tasks, which paves a new way towards the
realization of artificial general intelligence.
</summary>
    <author>
      <name>Yongliang Shen</name>
    </author>
    <author>
      <name>Kaitao Song</name>
    </author>
    <author>
      <name>Xu Tan</name>
    </author>
    <author>
      <name>Dongsheng Li</name>
    </author>
    <author>
      <name>Weiming Lu</name>
    </author>
    <author>
      <name>Yueting Zhuang</name>
    </author>
    <link href="http://arxiv.org/abs/2303.17580v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.17580v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.02643v1</id>
    <updated>2023-04-05T17:59:46Z</updated>
    <published>2023-04-05T17:59:46Z</published>
    <title>Segment Anything</title>
    <summary>  We introduce the Segment Anything (SA) project: a new task, model, and
dataset for image segmentation. Using our efficient model in a data collection
loop, we built the largest segmentation dataset to date (by far), with over 1
billion masks on 11M licensed and privacy respecting images. The model is
designed and trained to be promptable, so it can transfer zero-shot to new
image distributions and tasks. We evaluate its capabilities on numerous tasks
and find that its zero-shot performance is impressive -- often competitive with
or even superior to prior fully supervised results. We are releasing the
Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and
11M images at https://segment-anything.com to foster research into foundation
models for computer vision.
</summary>
    <author>
      <name>Alexander Kirillov</name>
    </author>
    <author>
      <name>Eric Mintun</name>
    </author>
    <author>
      <name>Nikhila Ravi</name>
    </author>
    <author>
      <name>Hanzi Mao</name>
    </author>
    <author>
      <name>Chloe Rolland</name>
    </author>
    <author>
      <name>Laura Gustafson</name>
    </author>
    <author>
      <name>Tete Xiao</name>
    </author>
    <author>
      <name>Spencer Whitehead</name>
    </author>
    <author>
      <name>Alexander C. Berg</name>
    </author>
    <author>
      <name>Wan-Yen Lo</name>
    </author>
    <author>
      <name>Piotr Dollár</name>
    </author>
    <author>
      <name>Ross Girshick</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project web-page: https://segment-anything.com</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.02643v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.02643v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.03277v1</id>
    <updated>2023-04-06T17:58:09Z</updated>
    <published>2023-04-06T17:58:09Z</published>
    <title>Instruction Tuning with GPT-4</title>
    <summary>  Prior work has shown that finetuning large language models (LLMs) using
machine-generated instruction-following data enables such models to achieve
remarkable zero-shot capabilities on new tasks, and no human-written
instructions are needed. In this paper, we present the first attempt to use
GPT-4 to generate instruction-following data for LLM finetuning. Our early
experiments on instruction-tuned LLaMA models show that the 52K English and
Chinese instruction-following data generated by GPT-4 leads to superior
zero-shot performance on new tasks to the instruction-following data generated
by previous state-of-the-art models. We also collect feedback and comparison
data from GPT-4 to enable a comprehensive evaluation and reward model training.
We make our data generated using GPT-4 as well as our codebase publicly
available.
</summary>
    <author>
      <name>Baolin Peng</name>
    </author>
    <author>
      <name>Chunyuan Li</name>
    </author>
    <author>
      <name>Pengcheng He</name>
    </author>
    <author>
      <name>Michel Galley</name>
    </author>
    <author>
      <name>Jianfeng Gao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages. Work in progress. Project page:
  https://instruction-tuning-with-gpt-4.github.io</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.03277v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.03277v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.03442v2</id>
    <updated>2023-08-06T00:21:19Z</updated>
    <published>2023-04-07T01:55:19Z</published>
    <title>Generative Agents: Interactive Simulacra of Human Behavior</title>
    <summary>  Believable proxies of human behavior can empower interactive applications
ranging from immersive environments to rehearsal spaces for interpersonal
communication to prototyping tools. In this paper, we introduce generative
agents--computational software agents that simulate believable human behavior.
Generative agents wake up, cook breakfast, and head to work; artists paint,
while authors write; they form opinions, notice each other, and initiate
conversations; they remember and reflect on days past as they plan the next
day. To enable generative agents, we describe an architecture that extends a
large language model to store a complete record of the agent's experiences
using natural language, synthesize those memories over time into higher-level
reflections, and retrieve them dynamically to plan behavior. We instantiate
generative agents to populate an interactive sandbox environment inspired by
The Sims, where end users can interact with a small town of twenty five agents
using natural language. In an evaluation, these generative agents produce
believable individual and emergent social behaviors: for example, starting with
only a single user-specified notion that one agent wants to throw a Valentine's
Day party, the agents autonomously spread invitations to the party over the
next two days, make new acquaintances, ask each other out on dates to the
party, and coordinate to show up for the party together at the right time. We
demonstrate through ablation that the components of our agent
architecture--observation, planning, and reflection--each contribute critically
to the believability of agent behavior. By fusing large language models with
computational, interactive agents, this work introduces architectural and
interaction patterns for enabling believable simulations of human behavior.
</summary>
    <author>
      <name>Joon Sung Park</name>
    </author>
    <author>
      <name>Joseph C. O'Brien</name>
    </author>
    <author>
      <name>Carrie J. Cai</name>
    </author>
    <author>
      <name>Meredith Ringel Morris</name>
    </author>
    <author>
      <name>Percy Liang</name>
    </author>
    <author>
      <name>Michael S. Bernstein</name>
    </author>
    <link href="http://arxiv.org/abs/2304.03442v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.03442v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.07193v2</id>
    <updated>2024-02-02T10:24:09Z</updated>
    <published>2023-04-14T15:12:19Z</published>
    <title>DINOv2: Learning Robust Visual Features without Supervision</title>
    <summary>  The recent breakthroughs in natural language processing for model pretraining
on large quantities of data have opened the way for similar foundation models
in computer vision. These models could greatly simplify the use of images in
any system by producing all-purpose visual features, i.e., features that work
across image distributions and tasks without finetuning. This work shows that
existing pretraining methods, especially self-supervised methods, can produce
such features if trained on enough curated data from diverse sources. We
revisit existing approaches and combine different techniques to scale our
pretraining in terms of data and model size. Most of the technical
contributions aim at accelerating and stabilizing the training at scale. In
terms of data, we propose an automatic pipeline to build a dedicated, diverse,
and curated image dataset instead of uncurated data, as typically done in the
self-supervised literature. In terms of models, we train a ViT model
(Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of
smaller models that surpass the best available all-purpose features, OpenCLIP
(Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.
</summary>
    <author>
      <name>Maxime Oquab</name>
    </author>
    <author>
      <name>Timothée Darcet</name>
    </author>
    <author>
      <name>Théo Moutakanni</name>
    </author>
    <author>
      <name>Huy Vo</name>
    </author>
    <author>
      <name>Marc Szafraniec</name>
    </author>
    <author>
      <name>Vasil Khalidov</name>
    </author>
    <author>
      <name>Pierre Fernandez</name>
    </author>
    <author>
      <name>Daniel Haziza</name>
    </author>
    <author>
      <name>Francisco Massa</name>
    </author>
    <author>
      <name>Alaaeldin El-Nouby</name>
    </author>
    <author>
      <name>Mahmoud Assran</name>
    </author>
    <author>
      <name>Nicolas Ballas</name>
    </author>
    <author>
      <name>Wojciech Galuba</name>
    </author>
    <author>
      <name>Russell Howes</name>
    </author>
    <author>
      <name>Po-Yao Huang</name>
    </author>
    <author>
      <name>Shang-Wen Li</name>
    </author>
    <author>
      <name>Ishan Misra</name>
    </author>
    <author>
      <name>Michael Rabbat</name>
    </author>
    <author>
      <name>Vasu Sharma</name>
    </author>
    <author>
      <name>Gabriel Synnaeve</name>
    </author>
    <author>
      <name>Hu Xu</name>
    </author>
    <author>
      <name>Hervé Jegou</name>
    </author>
    <author>
      <name>Julien Mairal</name>
    </author>
    <author>
      <name>Patrick Labatut</name>
    </author>
    <author>
      <name>Armand Joulin</name>
    </author>
    <author>
      <name>Piotr Bojanowski</name>
    </author>
    <link href="http://arxiv.org/abs/2304.07193v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.07193v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.08466v1</id>
    <updated>2023-04-17T17:42:29Z</updated>
    <published>2023-04-17T17:42:29Z</published>
    <title>Synthetic Data from Diffusion Models Improves ImageNet Classification</title>
    <summary>  Deep generative models are becoming increasingly powerful, now generating
diverse high fidelity photo-realistic samples given text prompts. Have they
reached the point where models of natural images can be used for generative
data augmentation, helping to improve challenging discriminative tasks? We show
that large-scale text-to image diffusion models can be fine-tuned to produce
class conditional models with SOTA FID (1.76 at 256x256 resolution) and
Inception Score (239 at 256x256). The model also yields a new SOTA in
Classification Accuracy Scores (64.96 for 256x256 generative samples, improving
to 69.24 for 1024x1024 samples). Augmenting the ImageNet training set with
samples from the resulting models yields significant improvements in ImageNet
classification accuracy over strong ResNet and Vision Transformer baselines.
</summary>
    <author>
      <name>Shekoofeh Azizi</name>
    </author>
    <author>
      <name>Simon Kornblith</name>
    </author>
    <author>
      <name>Chitwan Saharia</name>
    </author>
    <author>
      <name>Mohammad Norouzi</name>
    </author>
    <author>
      <name>David J. Fleet</name>
    </author>
    <link href="http://arxiv.org/abs/2304.08466v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.08466v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.08485v2</id>
    <updated>2023-12-11T17:46:14Z</updated>
    <published>2023-04-17T17:59:25Z</published>
    <title>Visual Instruction Tuning</title>
    <summary>  Instruction tuning large language models (LLMs) using machine-generated
instruction-following data has improved zero-shot capabilities on new tasks,
but the idea is less explored in the multimodal field. In this paper, we
present the first attempt to use language-only GPT-4 to generate multimodal
language-image instruction-following data. By instruction tuning on such
generated data, we introduce LLaVA: Large Language and Vision Assistant, an
end-to-end trained large multimodal model that connects a vision encoder and
LLM for general-purpose visual and language understanding.Our early experiments
show that LLaVA demonstrates impressive multimodel chat abilities, sometimes
exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and
yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal
instruction-following dataset. When fine-tuned on Science QA, the synergy of
LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make
GPT-4 generated visual instruction tuning data, our model and code base
publicly available.
</summary>
    <author>
      <name>Haotian Liu</name>
    </author>
    <author>
      <name>Chunyuan Li</name>
    </author>
    <author>
      <name>Qingyang Wu</name>
    </author>
    <author>
      <name>Yong Jae Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NeurIPS 2023 Oral; project page: https://llava-vl.github.io/</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.08485v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.08485v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.08818v2</id>
    <updated>2023-12-28T03:31:59Z</updated>
    <published>2023-04-18T08:30:32Z</published>
    <title>Align your Latents: High-Resolution Video Synthesis with Latent
  Diffusion Models</title>
    <summary>  Latent Diffusion Models (LDMs) enable high-quality image synthesis while
avoiding excessive compute demands by training a diffusion model in a
compressed lower-dimensional latent space. Here, we apply the LDM paradigm to
high-resolution video generation, a particularly resource-intensive task. We
first pre-train an LDM on images only; then, we turn the image generator into a
video generator by introducing a temporal dimension to the latent space
diffusion model and fine-tuning on encoded image sequences, i.e., videos.
Similarly, we temporally align diffusion model upsamplers, turning them into
temporally consistent video super resolution models. We focus on two relevant
real-world applications: Simulation of in-the-wild driving data and creative
content creation with text-to-video modeling. In particular, we validate our
Video LDM on real driving videos of resolution 512 x 1024, achieving
state-of-the-art performance. Furthermore, our approach can easily leverage
off-the-shelf pre-trained image LDMs, as we only need to train a temporal
alignment model in that case. Doing so, we turn the publicly available,
state-of-the-art text-to-image LDM Stable Diffusion into an efficient and
expressive text-to-video model with resolution up to 1280 x 2048. We show that
the temporal layers trained in this way generalize to different fine-tuned
text-to-image LDMs. Utilizing this property, we show the first results for
personalized text-to-video generation, opening exciting directions for future
content creation. Project page:
https://research.nvidia.com/labs/toronto-ai/VideoLDM/
</summary>
    <author>
      <name>Andreas Blattmann</name>
    </author>
    <author>
      <name>Robin Rombach</name>
    </author>
    <author>
      <name>Huan Ling</name>
    </author>
    <author>
      <name>Tim Dockhorn</name>
    </author>
    <author>
      <name>Seung Wook Kim</name>
    </author>
    <author>
      <name>Sanja Fidler</name>
    </author>
    <author>
      <name>Karsten Kreis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Conference on Computer Vision and Pattern Recognition (CVPR) 2023.
  Project page: https://research.nvidia.com/labs/toronto-ai/VideoLDM/</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.08818v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.08818v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.12306v3</id>
    <updated>2024-04-01T16:18:16Z</updated>
    <published>2023-04-24T17:56:12Z</published>
    <title>Segment Anything in Medical Images</title>
    <summary>  Medical image segmentation is a critical component in clinical practice,
facilitating accurate diagnosis, treatment planning, and disease monitoring.
However, existing methods, often tailored to specific modalities or disease
types, lack generalizability across the diverse spectrum of medical image
segmentation tasks. Here we present MedSAM, a foundation model designed for
bridging this gap by enabling universal medical image segmentation. The model
is developed on a large-scale medical image dataset with 1,570,263 image-mask
pairs, covering 10 imaging modalities and over 30 cancer types. We conduct a
comprehensive evaluation on 86 internal validation tasks and 60 external
validation tasks, demonstrating better accuracy and robustness than
modality-wise specialist models. By delivering accurate and efficient
segmentation across a wide spectrum of tasks, MedSAM holds significant
potential to expedite the evolution of diagnostic tools and the personalization
of treatment plans.
</summary>
    <author>
      <name>Jun Ma</name>
    </author>
    <author>
      <name>Yuting He</name>
    </author>
    <author>
      <name>Feifei Li</name>
    </author>
    <author>
      <name>Lin Han</name>
    </author>
    <author>
      <name>Chenyu You</name>
    </author>
    <author>
      <name>Bo Wang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1038/s41467-024-44824-z</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1038/s41467-024-44824-z" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Nature Communications 15, 654 (2024)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2304.12306v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.12306v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.12995v1</id>
    <updated>2023-04-25T17:05:38Z</updated>
    <published>2023-04-25T17:05:38Z</published>
    <title>AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking
  Head</title>
    <summary>  Large language models (LLMs) have exhibited remarkable capabilities across a
variety of domains and tasks, challenging our understanding of learning and
cognition. Despite the recent success, current LLMs are not capable of
processing complex audio information or conducting spoken conversations (like
Siri or Alexa). In this work, we propose a multi-modal AI system named
AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to
process complex audio information and solve numerous understanding and
generation tasks; and 2) the input/output interface (ASR, TTS) to support
spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of
human intention understanding and cooperation with foundation models, we
outline the principles and processes and test AudioGPT in terms of consistency,
capability, and robustness. Experimental results demonstrate the capabilities
of AudioGPT in solving AI tasks with speech, music, sound, and talking head
understanding and generation in multi-round dialogues, which empower humans to
create rich and diverse audio content with unprecedented ease. Our system is
publicly available at \url{https://github.com/AIGC-Audio/AudioGPT}.
</summary>
    <author>
      <name>Rongjie Huang</name>
    </author>
    <author>
      <name>Mingze Li</name>
    </author>
    <author>
      <name>Dongchao Yang</name>
    </author>
    <author>
      <name>Jiatong Shi</name>
    </author>
    <author>
      <name>Xuankai Chang</name>
    </author>
    <author>
      <name>Zhenhui Ye</name>
    </author>
    <author>
      <name>Yuning Wu</name>
    </author>
    <author>
      <name>Zhiqing Hong</name>
    </author>
    <author>
      <name>Jiawei Huang</name>
    </author>
    <author>
      <name>Jinglin Liu</name>
    </author>
    <author>
      <name>Yi Ren</name>
    </author>
    <author>
      <name>Zhou Zhao</name>
    </author>
    <author>
      <name>Shinji Watanabe</name>
    </author>
    <link href="http://arxiv.org/abs/2304.12995v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.12995v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.10403v3</id>
    <updated>2023-09-13T20:35:45Z</updated>
    <published>2023-05-17T17:46:53Z</published>
    <title>PaLM 2 Technical Report</title>
    <summary>  We introduce PaLM 2, a new state-of-the-art language model that has better
multilingual and reasoning capabilities and is more compute-efficient than its
predecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture
of objectives. Through extensive evaluations on English and multilingual
language, and reasoning tasks, we demonstrate that PaLM 2 has significantly
improved quality on downstream tasks across different model sizes, while
simultaneously exhibiting faster and more efficient inference compared to PaLM.
This improved efficiency enables broader deployment while also allowing the
model to respond faster, for a more natural pace of interaction. PaLM 2
demonstrates robust reasoning capabilities exemplified by large improvements
over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable
performance on a suite of responsible AI evaluations, and enables
inference-time control over toxicity without additional overhead or impact on
other capabilities. Overall, PaLM 2 achieves state-of-the-art performance
across a diverse set of tasks and capabilities.
  When discussing the PaLM 2 family, it is important to distinguish between
pre-trained models (of various sizes), fine-tuned variants of these models, and
the user-facing products that use these models. In particular, user-facing
products typically include additional pre- and post-processing steps.
Additionally, the underlying models may evolve over time. Therefore, one should
not expect the performance of user-facing products to exactly match the results
reported in this report.
</summary>
    <author>
      <name>Rohan Anil</name>
    </author>
    <author>
      <name>Andrew M. Dai</name>
    </author>
    <author>
      <name>Orhan Firat</name>
    </author>
    <author>
      <name>Melvin Johnson</name>
    </author>
    <author>
      <name>Dmitry Lepikhin</name>
    </author>
    <author>
      <name>Alexandre Passos</name>
    </author>
    <author>
      <name>Siamak Shakeri</name>
    </author>
    <author>
      <name>Emanuel Taropa</name>
    </author>
    <author>
      <name>Paige Bailey</name>
    </author>
    <author>
      <name>Zhifeng Chen</name>
    </author>
    <author>
      <name>Eric Chu</name>
    </author>
    <author>
      <name>Jonathan H. Clark</name>
    </author>
    <author>
      <name>Laurent El Shafey</name>
    </author>
    <author>
      <name>Yanping Huang</name>
    </author>
    <author>
      <name>Kathy Meier-Hellstern</name>
    </author>
    <author>
      <name>Gaurav Mishra</name>
    </author>
    <author>
      <name>Erica Moreira</name>
    </author>
    <author>
      <name>Mark Omernick</name>
    </author>
    <author>
      <name>Kevin Robinson</name>
    </author>
    <author>
      <name>Sebastian Ruder</name>
    </author>
    <author>
      <name>Yi Tay</name>
    </author>
    <author>
      <name>Kefan Xiao</name>
    </author>
    <author>
      <name>Yuanzhong Xu</name>
    </author>
    <author>
      <name>Yujing Zhang</name>
    </author>
    <author>
      <name>Gustavo Hernandez Abrego</name>
    </author>
    <author>
      <name>Junwhan Ahn</name>
    </author>
    <author>
      <name>Jacob Austin</name>
    </author>
    <author>
      <name>Paul Barham</name>
    </author>
    <author>
      <name>Jan Botha</name>
    </author>
    <author>
      <name>James Bradbury</name>
    </author>
    <author>
      <name>Siddhartha Brahma</name>
    </author>
    <author>
      <name>Kevin Brooks</name>
    </author>
    <author>
      <name>Michele Catasta</name>
    </author>
    <author>
      <name>Yong Cheng</name>
    </author>
    <author>
      <name>Colin Cherry</name>
    </author>
    <author>
      <name>Christopher A. Choquette-Choo</name>
    </author>
    <author>
      <name>Aakanksha Chowdhery</name>
    </author>
    <author>
      <name>Clément Crepy</name>
    </author>
    <author>
      <name>Shachi Dave</name>
    </author>
    <author>
      <name>Mostafa Dehghani</name>
    </author>
    <author>
      <name>Sunipa Dev</name>
    </author>
    <author>
      <name>Jacob Devlin</name>
    </author>
    <author>
      <name>Mark Díaz</name>
    </author>
    <author>
      <name>Nan Du</name>
    </author>
    <author>
      <name>Ethan Dyer</name>
    </author>
    <author>
      <name>Vlad Feinberg</name>
    </author>
    <author>
      <name>Fangxiaoyu Feng</name>
    </author>
    <author>
      <name>Vlad Fienber</name>
    </author>
    <author>
      <name>Markus Freitag</name>
    </author>
    <author>
      <name>Xavier Garcia</name>
    </author>
    <author>
      <name>Sebastian Gehrmann</name>
    </author>
    <author>
      <name>Lucas Gonzalez</name>
    </author>
    <author>
      <name>Guy Gur-Ari</name>
    </author>
    <author>
      <name>Steven Hand</name>
    </author>
    <author>
      <name>Hadi Hashemi</name>
    </author>
    <author>
      <name>Le Hou</name>
    </author>
    <author>
      <name>Joshua Howland</name>
    </author>
    <author>
      <name>Andrea Hu</name>
    </author>
    <author>
      <name>Jeffrey Hui</name>
    </author>
    <author>
      <name>Jeremy Hurwitz</name>
    </author>
    <author>
      <name>Michael Isard</name>
    </author>
    <author>
      <name>Abe Ittycheriah</name>
    </author>
    <author>
      <name>Matthew Jagielski</name>
    </author>
    <author>
      <name>Wenhao Jia</name>
    </author>
    <author>
      <name>Kathleen Kenealy</name>
    </author>
    <author>
      <name>Maxim Krikun</name>
    </author>
    <author>
      <name>Sneha Kudugunta</name>
    </author>
    <author>
      <name>Chang Lan</name>
    </author>
    <author>
      <name>Katherine Lee</name>
    </author>
    <author>
      <name>Benjamin Lee</name>
    </author>
    <author>
      <name>Eric Li</name>
    </author>
    <author>
      <name>Music Li</name>
    </author>
    <author>
      <name>Wei Li</name>
    </author>
    <author>
      <name>YaGuang Li</name>
    </author>
    <author>
      <name>Jian Li</name>
    </author>
    <author>
      <name>Hyeontaek Lim</name>
    </author>
    <author>
      <name>Hanzhao Lin</name>
    </author>
    <author>
      <name>Zhongtao Liu</name>
    </author>
    <author>
      <name>Frederick Liu</name>
    </author>
    <author>
      <name>Marcello Maggioni</name>
    </author>
    <author>
      <name>Aroma Mahendru</name>
    </author>
    <author>
      <name>Joshua Maynez</name>
    </author>
    <author>
      <name>Vedant Misra</name>
    </author>
    <author>
      <name>Maysam Moussalem</name>
    </author>
    <author>
      <name>Zachary Nado</name>
    </author>
    <author>
      <name>John Nham</name>
    </author>
    <author>
      <name>Eric Ni</name>
    </author>
    <author>
      <name>Andrew Nystrom</name>
    </author>
    <author>
      <name>Alicia Parrish</name>
    </author>
    <author>
      <name>Marie Pellat</name>
    </author>
    <author>
      <name>Martin Polacek</name>
    </author>
    <author>
      <name>Alex Polozov</name>
    </author>
    <author>
      <name>Reiner Pope</name>
    </author>
    <author>
      <name>Siyuan Qiao</name>
    </author>
    <author>
      <name>Emily Reif</name>
    </author>
    <author>
      <name>Bryan Richter</name>
    </author>
    <author>
      <name>Parker Riley</name>
    </author>
    <author>
      <name>Alex Castro Ros</name>
    </author>
    <author>
      <name>Aurko Roy</name>
    </author>
    <author>
      <name>Brennan Saeta</name>
    </author>
    <author>
      <name>Rajkumar Samuel</name>
    </author>
    <author>
      <name>Renee Shelby</name>
    </author>
    <author>
      <name>Ambrose Slone</name>
    </author>
    <author>
      <name>Daniel Smilkov</name>
    </author>
    <author>
      <name>David R. So</name>
    </author>
    <author>
      <name>Daniel Sohn</name>
    </author>
    <author>
      <name>Simon Tokumine</name>
    </author>
    <author>
      <name>Dasha Valter</name>
    </author>
    <author>
      <name>Vijay Vasudevan</name>
    </author>
    <author>
      <name>Kiran Vodrahalli</name>
    </author>
    <author>
      <name>Xuezhi Wang</name>
    </author>
    <author>
      <name>Pidong Wang</name>
    </author>
    <author>
      <name>Zirui Wang</name>
    </author>
    <author>
      <name>Tao Wang</name>
    </author>
    <author>
      <name>John Wieting</name>
    </author>
    <author>
      <name>Yuhuai Wu</name>
    </author>
    <author>
      <name>Kelvin Xu</name>
    </author>
    <author>
      <name>Yunhan Xu</name>
    </author>
    <author>
      <name>Linting Xue</name>
    </author>
    <author>
      <name>Pengcheng Yin</name>
    </author>
    <author>
      <name>Jiahui Yu</name>
    </author>
    <author>
      <name>Qiao Zhang</name>
    </author>
    <author>
      <name>Steven Zheng</name>
    </author>
    <author>
      <name>Ce Zheng</name>
    </author>
    <author>
      <name>Weikang Zhou</name>
    </author>
    <author>
      <name>Denny Zhou</name>
    </author>
    <author>
      <name>Slav Petrov</name>
    </author>
    <author>
      <name>Yonghui Wu</name>
    </author>
    <link href="http://arxiv.org/abs/2305.10403v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.10403v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.10601v2</id>
    <updated>2023-12-03T22:50:35Z</updated>
    <published>2023-05-17T23:16:17Z</published>
    <title>Tree of Thoughts: Deliberate Problem Solving with Large Language Models</title>
    <summary>  Language models are increasingly being deployed for general problem solving
across a wide range of tasks, but are still confined to token-level,
left-to-right decision-making processes during inference. This means they can
fall short in tasks that require exploration, strategic lookahead, or where
initial decisions play a pivotal role. To surmount these challenges, we
introduce a new framework for language model inference, Tree of Thoughts (ToT),
which generalizes over the popular Chain of Thought approach to prompting
language models, and enables exploration over coherent units of text (thoughts)
that serve as intermediate steps toward problem solving. ToT allows LMs to
perform deliberate decision making by considering multiple different reasoning
paths and self-evaluating choices to decide the next course of action, as well
as looking ahead or backtracking when necessary to make global choices. Our
experiments show that ToT significantly enhances language models'
problem-solving abilities on three novel tasks requiring non-trivial planning
or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in
Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of
tasks, our method achieved a success rate of 74%. Code repo with all prompts:
https://github.com/princeton-nlp/tree-of-thought-llm.
</summary>
    <author>
      <name>Shunyu Yao</name>
    </author>
    <author>
      <name>Dian Yu</name>
    </author>
    <author>
      <name>Jeffrey Zhao</name>
    </author>
    <author>
      <name>Izhak Shafran</name>
    </author>
    <author>
      <name>Thomas L. Griffiths</name>
    </author>
    <author>
      <name>Yuan Cao</name>
    </author>
    <author>
      <name>Karthik Narasimhan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NeurIPS 2023 camera ready version. Code repo with all prompts:
  https://github.com/princeton-nlp/tree-of-thought-llm</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.10601v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.10601v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.10973v2</id>
    <updated>2024-07-17T10:27:55Z</updated>
    <published>2023-05-18T13:41:25Z</published>
    <title>Drag Your GAN: Interactive Point-based Manipulation on the Generative
  Image Manifold</title>
    <summary>  Synthesizing visual content that meets users' needs often requires flexible
and precise controllability of the pose, shape, expression, and layout of the
generated objects. Existing approaches gain controllability of generative
adversarial networks (GANs) via manually annotated training data or a prior 3D
model, which often lack flexibility, precision, and generality. In this work,
we study a powerful yet much less explored way of controlling GANs, that is, to
"drag" any points of the image to precisely reach target points in a
user-interactive manner, as shown in Fig.1. To achieve this, we propose
DragGAN, which consists of two main components: 1) a feature-based motion
supervision that drives the handle point to move towards the target position,
and 2) a new point tracking approach that leverages the discriminative
generator features to keep localizing the position of the handle points.
Through DragGAN, anyone can deform an image with precise control over where
pixels go, thus manipulating the pose, shape, expression, and layout of diverse
categories such as animals, cars, humans, landscapes, etc. As these
manipulations are performed on the learned generative image manifold of a GAN,
they tend to produce realistic outputs even for challenging scenarios such as
hallucinating occluded content and deforming shapes that consistently follow
the object's rigidity. Both qualitative and quantitative comparisons
demonstrate the advantage of DragGAN over prior approaches in the tasks of
image manipulation and point tracking. We also showcase the manipulation of
real images through GAN inversion.
</summary>
    <author>
      <name>Xingang Pan</name>
    </author>
    <author>
      <name>Ayush Tewari</name>
    </author>
    <author>
      <name>Thomas Leimkühler</name>
    </author>
    <author>
      <name>Lingjie Liu</name>
    </author>
    <author>
      <name>Abhimitra Meka</name>
    </author>
    <author>
      <name>Christian Theobalt</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3588432.3591500</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3588432.3591500" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to SIGGRAPH 2023. Project page:
  https://vcai.mpi-inf.mpg.de/projects/DragGAN/</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.10973v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.10973v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.11206v1</id>
    <updated>2023-05-18T17:45:22Z</updated>
    <published>2023-05-18T17:45:22Z</published>
    <title>LIMA: Less Is More for Alignment</title>
    <summary>  Large language models are trained in two stages: (1) unsupervised pretraining
from raw text, to learn general-purpose representations, and (2) large scale
instruction tuning and reinforcement learning, to better align to end tasks and
user preferences. We measure the relative importance of these two stages by
training LIMA, a 65B parameter LLaMa language model fine-tuned with the
standard supervised loss on only 1,000 carefully curated prompts and responses,
without any reinforcement learning or human preference modeling. LIMA
demonstrates remarkably strong performance, learning to follow specific
response formats from only a handful of examples in the training data,
including complex queries that range from planning trip itineraries to
speculating about alternate history. Moreover, the model tends to generalize
well to unseen tasks that did not appear in the training data. In a controlled
human study, responses from LIMA are either equivalent or strictly preferred to
GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard
and 65% versus DaVinci003, which was trained with human feedback. Taken
together, these results strongly suggest that almost all knowledge in large
language models is learned during pretraining, and only limited instruction
tuning data is necessary to teach models to produce high quality output.
</summary>
    <author>
      <name>Chunting Zhou</name>
    </author>
    <author>
      <name>Pengfei Liu</name>
    </author>
    <author>
      <name>Puxin Xu</name>
    </author>
    <author>
      <name>Srini Iyer</name>
    </author>
    <author>
      <name>Jiao Sun</name>
    </author>
    <author>
      <name>Yuning Mao</name>
    </author>
    <author>
      <name>Xuezhe Ma</name>
    </author>
    <author>
      <name>Avia Efrat</name>
    </author>
    <author>
      <name>Ping Yu</name>
    </author>
    <author>
      <name>Lili Yu</name>
    </author>
    <author>
      <name>Susan Zhang</name>
    </author>
    <author>
      <name>Gargi Ghosh</name>
    </author>
    <author>
      <name>Mike Lewis</name>
    </author>
    <author>
      <name>Luke Zettlemoyer</name>
    </author>
    <author>
      <name>Omer Levy</name>
    </author>
    <link href="http://arxiv.org/abs/2305.11206v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.11206v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.13516v1</id>
    <updated>2023-05-22T22:09:41Z</updated>
    <published>2023-05-22T22:09:41Z</published>
    <title>Scaling Speech Technology to 1,000+ Languages</title>
    <summary>  Expanding the language coverage of speech technology has the potential to
improve access to information for many more people. However, current speech
technology is restricted to about one hundred languages which is a small
fraction of the over 7,000 languages spoken around the world. The Massively
Multilingual Speech (MMS) project increases the number of supported languages
by 10-40x, depending on the task. The main ingredients are a new dataset based
on readings of publicly available religious texts and effectively leveraging
self-supervised learning. We built pre-trained wav2vec 2.0 models covering
1,406 languages, a single multilingual automatic speech recognition model for
1,107 languages, speech synthesis models for the same number of languages, as
well as a language identification model for 4,017 languages. Experiments show
that our multilingual speech recognition model more than halves the word error
rate of Whisper on 54 languages of the FLEURS benchmark while being trained on
a small fraction of the labeled data.
</summary>
    <author>
      <name>Vineel Pratap</name>
    </author>
    <author>
      <name>Andros Tjandra</name>
    </author>
    <author>
      <name>Bowen Shi</name>
    </author>
    <author>
      <name>Paden Tomasello</name>
    </author>
    <author>
      <name>Arun Babu</name>
    </author>
    <author>
      <name>Sayani Kundu</name>
    </author>
    <author>
      <name>Ali Elkahky</name>
    </author>
    <author>
      <name>Zhaoheng Ni</name>
    </author>
    <author>
      <name>Apoorv Vyas</name>
    </author>
    <author>
      <name>Maryam Fazel-Zarandi</name>
    </author>
    <author>
      <name>Alexei Baevski</name>
    </author>
    <author>
      <name>Yossi Adi</name>
    </author>
    <author>
      <name>Xiaohui Zhang</name>
    </author>
    <author>
      <name>Wei-Ning Hsu</name>
    </author>
    <author>
      <name>Alexis Conneau</name>
    </author>
    <author>
      <name>Michael Auli</name>
    </author>
    <link href="http://arxiv.org/abs/2305.13516v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.13516v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.14314v1</id>
    <updated>2023-05-23T17:50:33Z</updated>
    <published>2023-05-23T17:50:33Z</published>
    <title>QLoRA: Efficient Finetuning of Quantized LLMs</title>
    <summary>  We present QLoRA, an efficient finetuning approach that reduces memory usage
enough to finetune a 65B parameter model on a single 48GB GPU while preserving
full 16-bit finetuning task performance. QLoRA backpropagates gradients through
a frozen, 4-bit quantized pretrained language model into Low Rank
Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all
previous openly released models on the Vicuna benchmark, reaching 99.3% of the
performance level of ChatGPT while only requiring 24 hours of finetuning on a
single GPU. QLoRA introduces a number of innovations to save memory without
sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is
information theoretically optimal for normally distributed weights (b) double
quantization to reduce the average memory footprint by quantizing the
quantization constants, and (c) paged optimziers to manage memory spikes. We
use QLoRA to finetune more than 1,000 models, providing a detailed analysis of
instruction following and chatbot performance across 8 instruction datasets,
multiple model types (LLaMA, T5), and model scales that would be infeasible to
run with regular finetuning (e.g. 33B and 65B parameter models). Our results
show that QLoRA finetuning on a small high-quality dataset leads to
state-of-the-art results, even when using smaller models than the previous
SoTA. We provide a detailed analysis of chatbot performance based on both human
and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable
alternative to human evaluation. Furthermore, we find that current chatbot
benchmarks are not trustworthy to accurately evaluate the performance levels of
chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to
ChatGPT. We release all of our models and code, including CUDA kernels for
4-bit training.
</summary>
    <author>
      <name>Tim Dettmers</name>
    </author>
    <author>
      <name>Artidoro Pagnoni</name>
    </author>
    <author>
      <name>Ari Holtzman</name>
    </author>
    <author>
      <name>Luke Zettlemoyer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended NeurIPS submission</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.14314v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.14314v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.16291v2</id>
    <updated>2023-10-19T16:27:03Z</updated>
    <published>2023-05-25T17:46:38Z</published>
    <title>Voyager: An Open-Ended Embodied Agent with Large Language Models</title>
    <summary>  We introduce Voyager, the first LLM-powered embodied lifelong learning agent
in Minecraft that continuously explores the world, acquires diverse skills, and
makes novel discoveries without human intervention. Voyager consists of three
key components: 1) an automatic curriculum that maximizes exploration, 2) an
ever-growing skill library of executable code for storing and retrieving
complex behaviors, and 3) a new iterative prompting mechanism that incorporates
environment feedback, execution errors, and self-verification for program
improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses
the need for model parameter fine-tuning. The skills developed by Voyager are
temporally extended, interpretable, and compositional, which compounds the
agent's abilities rapidly and alleviates catastrophic forgetting. Empirically,
Voyager shows strong in-context lifelong learning capability and exhibits
exceptional proficiency in playing Minecraft. It obtains 3.3x more unique
items, travels 2.3x longer distances, and unlocks key tech tree milestones up
to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill
library in a new Minecraft world to solve novel tasks from scratch, while other
techniques struggle to generalize. We open-source our full codebase and prompts
at https://voyager.minedojo.org/.
</summary>
    <author>
      <name>Guanzhi Wang</name>
    </author>
    <author>
      <name>Yuqi Xie</name>
    </author>
    <author>
      <name>Yunfan Jiang</name>
    </author>
    <author>
      <name>Ajay Mandlekar</name>
    </author>
    <author>
      <name>Chaowei Xiao</name>
    </author>
    <author>
      <name>Yuke Zhu</name>
    </author>
    <author>
      <name>Linxi Fan</name>
    </author>
    <author>
      <name>Anima Anandkumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project website and open-source codebase:
  https://voyager.minedojo.org/</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.16291v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.16291v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.16367v1</id>
    <updated>2023-05-25T11:36:52Z</updated>
    <published>2023-05-25T11:36:52Z</published>
    <title>Role-Play with Large Language Models</title>
    <summary>  As dialogue agents become increasingly human-like in their performance, it is
imperative that we develop effective ways to describe their behaviour in
high-level terms without falling into the trap of anthropomorphism. In this
paper, we foreground the concept of role-play. Casting dialogue agent behaviour
in terms of role-play allows us to draw on familiar folk psychological terms,
without ascribing human characteristics to language models they in fact lack.
Two important cases of dialogue agent behaviour are addressed this way, namely
(apparent) deception and (apparent) self-awareness.
</summary>
    <author>
      <name>Murray Shanahan</name>
    </author>
    <author>
      <name>Kyle McDonell</name>
    </author>
    <author>
      <name>Laria Reynolds</name>
    </author>
    <link href="http://arxiv.org/abs/2305.16367v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.16367v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.18290v3</id>
    <updated>2024-07-29T22:26:36Z</updated>
    <published>2023-05-29T17:57:46Z</published>
    <title>Direct Preference Optimization: Your Language Model is Secretly a Reward
  Model</title>
    <summary>  While large-scale unsupervised language models (LMs) learn broad world
knowledge and some reasoning skills, achieving precise control of their
behavior is difficult due to the completely unsupervised nature of their
training. Existing methods for gaining such steerability collect human labels
of the relative quality of model generations and fine-tune the unsupervised LM
to align with these preferences, often with reinforcement learning from human
feedback (RLHF). However, RLHF is a complex and often unstable procedure, first
fitting a reward model that reflects the human preferences, and then
fine-tuning the large unsupervised LM using reinforcement learning to maximize
this estimated reward without drifting too far from the original model. In this
paper we introduce a new parameterization of the reward model in RLHF that
enables extraction of the corresponding optimal policy in closed form, allowing
us to solve the standard RLHF problem with only a simple classification loss.
The resulting algorithm, which we call Direct Preference Optimization (DPO), is
stable, performant, and computationally lightweight, eliminating the need for
sampling from the LM during fine-tuning or performing significant
hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align
with human preferences as well as or better than existing methods. Notably,
fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of
generations, and matches or improves response quality in summarization and
single-turn dialogue while being substantially simpler to implement and train.
</summary>
    <author>
      <name>Rafael Rafailov</name>
    </author>
    <author>
      <name>Archit Sharma</name>
    </author>
    <author>
      <name>Eric Mitchell</name>
    </author>
    <author>
      <name>Stefano Ermon</name>
    </author>
    <author>
      <name>Christopher D. Manning</name>
    </author>
    <author>
      <name>Chelsea Finn</name>
    </author>
    <link href="http://arxiv.org/abs/2305.18290v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.18290v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.19466v2</id>
    <updated>2023-11-06T19:48:10Z</updated>
    <published>2023-05-31T00:29:55Z</published>
    <title>The Impact of Positional Encoding on Length Generalization in
  Transformers</title>
    <summary>  Length generalization, the ability to generalize from small training context
sizes to larger ones, is a critical challenge in the development of
Transformer-based language models. Positional encoding (PE) has been identified
as a major factor influencing length generalization, but the exact impact of
different PE schemes on extrapolation in downstream tasks remains unclear. In
this paper, we conduct a systematic empirical study comparing the length
generalization performance of decoder-only Transformers with five different
position encoding approaches including Absolute Position Embedding (APE), T5's
Relative PE, ALiBi, and Rotary, in addition to Transformers without positional
encoding (NoPE). Our evaluation encompasses a battery of reasoning and
mathematical tasks. Our findings reveal that the most commonly used positional
encoding methods, such as ALiBi, Rotary, and APE, are not well suited for
length generalization in downstream tasks. More importantly, NoPE outperforms
other explicit positional encoding methods while requiring no additional
computation. We theoretically demonstrate that NoPE can represent both absolute
and relative PEs, but when trained with SGD, it mostly resembles T5's relative
PE attention patterns. Finally, we find that scratchpad is not always helpful
to solve length generalization and its format highly impacts the model's
performance. Overall, our work suggests that explicit position embeddings are
not essential for decoder-only Transformers to generalize well to longer
sequences.
</summary>
    <author>
      <name>Amirhossein Kazemnejad</name>
    </author>
    <author>
      <name>Inkit Padhi</name>
    </author>
    <author>
      <name>Karthikeyan Natesan Ramamurthy</name>
    </author>
    <author>
      <name>Payel Das</name>
    </author>
    <author>
      <name>Siva Reddy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at NeurIPS 2023; 15 pages and 22 pages Appendix</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.19466v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.19466v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.03092v2</id>
    <updated>2023-06-12T20:50:07Z</updated>
    <published>2023-06-05T17:59:57Z</published>
    <title>Neuralangelo: High-Fidelity Neural Surface Reconstruction</title>
    <summary>  Neural surface reconstruction has been shown to be powerful for recovering
dense 3D surfaces via image-based neural rendering. However, current methods
struggle to recover detailed structures of real-world scenes. To address the
issue, we present Neuralangelo, which combines the representation power of
multi-resolution 3D hash grids with neural surface rendering. Two key
ingredients enable our approach: (1) numerical gradients for computing
higher-order derivatives as a smoothing operation and (2) coarse-to-fine
optimization on the hash grids controlling different levels of details. Even
without auxiliary inputs such as depth, Neuralangelo can effectively recover
dense 3D surface structures from multi-view images with fidelity significantly
surpassing previous methods, enabling detailed large-scale scene reconstruction
from RGB video captures.
</summary>
    <author>
      <name>Zhaoshuo Li</name>
    </author>
    <author>
      <name>Thomas Müller</name>
    </author>
    <author>
      <name>Alex Evans</name>
    </author>
    <author>
      <name>Russell H. Taylor</name>
    </author>
    <author>
      <name>Mathias Unberath</name>
    </author>
    <author>
      <name>Ming-Yu Liu</name>
    </author>
    <author>
      <name>Chen-Hsuan Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2023, project page:
  https://research.nvidia.com/labs/dir/neuralangelo</arxiv:comment>
    <link href="http://arxiv.org/abs/2306.03092v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.03092v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.05284v3</id>
    <updated>2024-01-30T04:49:16Z</updated>
    <published>2023-06-08T15:31:05Z</published>
    <title>Simple and Controllable Music Generation</title>
    <summary>  We tackle the task of conditional music generation. We introduce MusicGen, a
single Language Model (LM) that operates over several streams of compressed
discrete music representation, i.e., tokens. Unlike prior work, MusicGen is
comprised of a single-stage transformer LM together with efficient token
interleaving patterns, which eliminates the need for cascading several models,
e.g., hierarchically or upsampling. Following this approach, we demonstrate how
MusicGen can generate high-quality samples, both mono and stereo, while being
conditioned on textual description or melodic features, allowing better
controls over the generated output. We conduct extensive empirical evaluation,
considering both automatic and human studies, showing the proposed approach is
superior to the evaluated baselines on a standard text-to-music benchmark.
Through ablation studies, we shed light over the importance of each of the
components comprising MusicGen. Music samples, code, and models are available
at https://github.com/facebookresearch/audiocraft
</summary>
    <author>
      <name>Jade Copet</name>
    </author>
    <author>
      <name>Felix Kreuk</name>
    </author>
    <author>
      <name>Itai Gat</name>
    </author>
    <author>
      <name>Tal Remez</name>
    </author>
    <author>
      <name>David Kant</name>
    </author>
    <author>
      <name>Gabriel Synnaeve</name>
    </author>
    <author>
      <name>Yossi Adi</name>
    </author>
    <author>
      <name>Alexandre Défossez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at Neurips 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2306.05284v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.05284v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.12925v1</id>
    <updated>2023-06-22T14:37:54Z</updated>
    <published>2023-06-22T14:37:54Z</published>
    <title>AudioPaLM: A Large Language Model That Can Speak and Listen</title>
    <summary>  We introduce AudioPaLM, a large language model for speech understanding and
generation. AudioPaLM fuses text-based and speech-based language models, PaLM-2
[Anil et al., 2023] and AudioLM [Borsos et al., 2022], into a unified
multimodal architecture that can process and generate text and speech with
applications including speech recognition and speech-to-speech translation.
AudioPaLM inherits the capability to preserve paralinguistic information such
as speaker identity and intonation from AudioLM and the linguistic knowledge
present only in text large language models such as PaLM-2. We demonstrate that
initializing AudioPaLM with the weights of a text-only large language model
improves speech processing, successfully leveraging the larger quantity of text
training data used in pretraining to assist with the speech tasks. The
resulting model significantly outperforms existing systems for speech
translation tasks and has the ability to perform zero-shot speech-to-text
translation for many languages for which input/target language combinations
were not seen in training. AudioPaLM also demonstrates features of audio
language models, such as transferring a voice across languages based on a short
spoken prompt. We release examples of our method at
https://google-research.github.io/seanet/audiopalm/examples
</summary>
    <author>
      <name>Paul K. Rubenstein</name>
    </author>
    <author>
      <name>Chulayuth Asawaroengchai</name>
    </author>
    <author>
      <name>Duc Dung Nguyen</name>
    </author>
    <author>
      <name>Ankur Bapna</name>
    </author>
    <author>
      <name>Zalán Borsos</name>
    </author>
    <author>
      <name>Félix de Chaumont Quitry</name>
    </author>
    <author>
      <name>Peter Chen</name>
    </author>
    <author>
      <name>Dalia El Badawy</name>
    </author>
    <author>
      <name>Wei Han</name>
    </author>
    <author>
      <name>Eugene Kharitonov</name>
    </author>
    <author>
      <name>Hannah Muckenhirn</name>
    </author>
    <author>
      <name>Dirk Padfield</name>
    </author>
    <author>
      <name>James Qin</name>
    </author>
    <author>
      <name>Danny Rozenberg</name>
    </author>
    <author>
      <name>Tara Sainath</name>
    </author>
    <author>
      <name>Johan Schalkwyk</name>
    </author>
    <author>
      <name>Matt Sharifi</name>
    </author>
    <author>
      <name>Michelle Tadmor Ramanovich</name>
    </author>
    <author>
      <name>Marco Tagliasacchi</name>
    </author>
    <author>
      <name>Alexandru Tudor</name>
    </author>
    <author>
      <name>Mihajlo Velimirović</name>
    </author>
    <author>
      <name>Damien Vincent</name>
    </author>
    <author>
      <name>Jiahui Yu</name>
    </author>
    <author>
      <name>Yongqiang Wang</name>
    </author>
    <author>
      <name>Vicky Zayats</name>
    </author>
    <author>
      <name>Neil Zeghidour</name>
    </author>
    <author>
      <name>Yu Zhang</name>
    </author>
    <author>
      <name>Zhishuai Zhang</name>
    </author>
    <author>
      <name>Lukas Zilka</name>
    </author>
    <author>
      <name>Christian Frank</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical report</arxiv:comment>
    <link href="http://arxiv.org/abs/2306.12925v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.12925v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.15687v2</id>
    <updated>2023-10-19T13:23:28Z</updated>
    <published>2023-06-23T16:23:24Z</published>
    <title>Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale</title>
    <summary>  Large-scale generative models such as GPT and DALL-E have revolutionized the
research community. These models not only generate high fidelity outputs, but
are also generalists which can solve tasks not explicitly taught. In contrast,
speech generative models are still primitive in terms of scale and task
generalization. In this paper, we present Voicebox, the most versatile
text-guided generative model for speech at scale. Voicebox is a
non-autoregressive flow-matching model trained to infill speech, given audio
context and text, trained on over 50K hours of speech that are not filtered or
enhanced. Similar to GPT, Voicebox can perform many different tasks through
in-context learning, but is more flexible as it can also condition on future
context. Voicebox can be used for mono or cross-lingual zero-shot
text-to-speech synthesis, noise removal, content editing, style conversion, and
diverse sample generation. In particular, Voicebox outperforms the
state-of-the-art zero-shot TTS model VALL-E on both intelligibility (5.9% vs
1.9% word error rates) and audio similarity (0.580 vs 0.681) while being up to
20 times faster. Audio samples can be found in
\url{https://voicebox.metademolab.com}.
</summary>
    <author>
      <name>Matthew Le</name>
    </author>
    <author>
      <name>Apoorv Vyas</name>
    </author>
    <author>
      <name>Bowen Shi</name>
    </author>
    <author>
      <name>Brian Karrer</name>
    </author>
    <author>
      <name>Leda Sari</name>
    </author>
    <author>
      <name>Rashel Moritz</name>
    </author>
    <author>
      <name>Mary Williamson</name>
    </author>
    <author>
      <name>Vimal Manohar</name>
    </author>
    <author>
      <name>Yossi Adi</name>
    </author>
    <author>
      <name>Jay Mahadeokar</name>
    </author>
    <author>
      <name>Wei-Ning Hsu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to NeurIPS 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2306.15687v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.15687v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.01952v1</id>
    <updated>2023-07-04T23:04:57Z</updated>
    <published>2023-07-04T23:04:57Z</published>
    <title>SDXL: Improving Latent Diffusion Models for High-Resolution Image
  Synthesis</title>
    <summary>  We present SDXL, a latent diffusion model for text-to-image synthesis.
Compared to previous versions of Stable Diffusion, SDXL leverages a three times
larger UNet backbone: The increase of model parameters is mainly due to more
attention blocks and a larger cross-attention context as SDXL uses a second
text encoder. We design multiple novel conditioning schemes and train SDXL on
multiple aspect ratios. We also introduce a refinement model which is used to
improve the visual fidelity of samples generated by SDXL using a post-hoc
image-to-image technique. We demonstrate that SDXL shows drastically improved
performance compared the previous versions of Stable Diffusion and achieves
results competitive with those of black-box state-of-the-art image generators.
In the spirit of promoting open research and fostering transparency in large
model training and evaluation, we provide access to code and model weights at
https://github.com/Stability-AI/generative-models
</summary>
    <author>
      <name>Dustin Podell</name>
    </author>
    <author>
      <name>Zion English</name>
    </author>
    <author>
      <name>Kyle Lacey</name>
    </author>
    <author>
      <name>Andreas Blattmann</name>
    </author>
    <author>
      <name>Tim Dockhorn</name>
    </author>
    <author>
      <name>Jonas Müller</name>
    </author>
    <author>
      <name>Joe Penna</name>
    </author>
    <author>
      <name>Robin Rombach</name>
    </author>
    <link href="http://arxiv.org/abs/2307.01952v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.01952v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.10802v1</id>
    <updated>2023-07-20T12:10:29Z</updated>
    <published>2023-07-20T12:10:29Z</published>
    <title>Meta-Transformer: A Unified Framework for Multimodal Learning</title>
    <summary>  Multimodal learning aims to build models that can process and relate
information from multiple modalities. Despite years of development in this
field, it still remains challenging to design a unified network for processing
various modalities ($\textit{e.g.}$ natural language, 2D images, 3D point
clouds, audio, video, time series, tabular data) due to the inherent gaps among
them. In this work, we propose a framework, named Meta-Transformer, that
leverages a $\textbf{frozen}$ encoder to perform multimodal perception without
any paired multimodal training data. In Meta-Transformer, the raw input data
from various modalities are mapped into a shared token space, allowing a
subsequent encoder with frozen parameters to extract high-level semantic
features of the input data. Composed of three main components: a unified data
tokenizer, a modality-shared encoder, and task-specific heads for downstream
tasks, Meta-Transformer is the first framework to perform unified learning
across 12 modalities with unpaired data. Experiments on different benchmarks
reveal that Meta-Transformer can handle a wide range of tasks including
fundamental perception (text, image, point cloud, audio, video), practical
application (X-Ray, infrared, hyperspectral, and IMU), and data mining (graph,
tabular, and time-series). Meta-Transformer indicates a promising future for
developing unified multimodal intelligence with transformers. Code will be
available at https://github.com/invictus717/MetaTransformer
</summary>
    <author>
      <name>Yiyuan Zhang</name>
    </author>
    <author>
      <name>Kaixiong Gong</name>
    </author>
    <author>
      <name>Kaipeng Zhang</name>
    </author>
    <author>
      <name>Hongsheng Li</name>
    </author>
    <author>
      <name>Yu Qiao</name>
    </author>
    <author>
      <name>Wanli Ouyang</name>
    </author>
    <author>
      <name>Xiangyu Yue</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project website: https://kxgong.github.io/meta_transformer/</arxiv:comment>
    <link href="http://arxiv.org/abs/2307.10802v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.10802v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.15818v1</id>
    <updated>2023-07-28T21:18:02Z</updated>
    <published>2023-07-28T21:18:02Z</published>
    <title>RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic
  Control</title>
    <summary>  We study how vision-language models trained on Internet-scale data can be
incorporated directly into end-to-end robotic control to boost generalization
and enable emergent semantic reasoning. Our goal is to enable a single
end-to-end trained model to both learn to map robot observations to actions and
enjoy the benefits of large-scale pretraining on language and vision-language
data from the web. To this end, we propose to co-fine-tune state-of-the-art
vision-language models on both robotic trajectory data and Internet-scale
vision-language tasks, such as visual question answering. In contrast to other
approaches, we propose a simple, general recipe to achieve this goal: in order
to fit both natural language responses and robotic actions into the same
format, we express the actions as text tokens and incorporate them directly
into the training set of the model in the same way as natural language tokens.
We refer to such category of models as vision-language-action models (VLA) and
instantiate an example of such a model, which we call RT-2. Our extensive
evaluation (6k evaluation trials) shows that our approach leads to performant
robotic policies and enables RT-2 to obtain a range of emergent capabilities
from Internet-scale training. This includes significantly improved
generalization to novel objects, the ability to interpret commands not present
in the robot training data (such as placing an object onto a particular number
or icon), and the ability to perform rudimentary reasoning in response to user
commands (such as picking up the smallest or largest object, or the one closest
to another object). We further show that incorporating chain of thought
reasoning allows RT-2 to perform multi-stage semantic reasoning, for example
figuring out which object to pick up for use as an improvised hammer (a rock),
or which type of drink is best suited for someone who is tired (an energy
drink).
</summary>
    <author>
      <name>Anthony Brohan</name>
    </author>
    <author>
      <name>Noah Brown</name>
    </author>
    <author>
      <name>Justice Carbajal</name>
    </author>
    <author>
      <name>Yevgen Chebotar</name>
    </author>
    <author>
      <name>Xi Chen</name>
    </author>
    <author>
      <name>Krzysztof Choromanski</name>
    </author>
    <author>
      <name>Tianli Ding</name>
    </author>
    <author>
      <name>Danny Driess</name>
    </author>
    <author>
      <name>Avinava Dubey</name>
    </author>
    <author>
      <name>Chelsea Finn</name>
    </author>
    <author>
      <name>Pete Florence</name>
    </author>
    <author>
      <name>Chuyuan Fu</name>
    </author>
    <author>
      <name>Montse Gonzalez Arenas</name>
    </author>
    <author>
      <name>Keerthana Gopalakrishnan</name>
    </author>
    <author>
      <name>Kehang Han</name>
    </author>
    <author>
      <name>Karol Hausman</name>
    </author>
    <author>
      <name>Alexander Herzog</name>
    </author>
    <author>
      <name>Jasmine Hsu</name>
    </author>
    <author>
      <name>Brian Ichter</name>
    </author>
    <author>
      <name>Alex Irpan</name>
    </author>
    <author>
      <name>Nikhil Joshi</name>
    </author>
    <author>
      <name>Ryan Julian</name>
    </author>
    <author>
      <name>Dmitry Kalashnikov</name>
    </author>
    <author>
      <name>Yuheng Kuang</name>
    </author>
    <author>
      <name>Isabel Leal</name>
    </author>
    <author>
      <name>Lisa Lee</name>
    </author>
    <author>
      <name>Tsang-Wei Edward Lee</name>
    </author>
    <author>
      <name>Sergey Levine</name>
    </author>
    <author>
      <name>Yao Lu</name>
    </author>
    <author>
      <name>Henryk Michalewski</name>
    </author>
    <author>
      <name>Igor Mordatch</name>
    </author>
    <author>
      <name>Karl Pertsch</name>
    </author>
    <author>
      <name>Kanishka Rao</name>
    </author>
    <author>
      <name>Krista Reymann</name>
    </author>
    <author>
      <name>Michael Ryoo</name>
    </author>
    <author>
      <name>Grecia Salazar</name>
    </author>
    <author>
      <name>Pannag Sanketi</name>
    </author>
    <author>
      <name>Pierre Sermanet</name>
    </author>
    <author>
      <name>Jaspiar Singh</name>
    </author>
    <author>
      <name>Anikait Singh</name>
    </author>
    <author>
      <name>Radu Soricut</name>
    </author>
    <author>
      <name>Huong Tran</name>
    </author>
    <author>
      <name>Vincent Vanhoucke</name>
    </author>
    <author>
      <name>Quan Vuong</name>
    </author>
    <author>
      <name>Ayzaan Wahid</name>
    </author>
    <author>
      <name>Stefan Welker</name>
    </author>
    <author>
      <name>Paul Wohlhart</name>
    </author>
    <author>
      <name>Jialin Wu</name>
    </author>
    <author>
      <name>Fei Xia</name>
    </author>
    <author>
      <name>Ted Xiao</name>
    </author>
    <author>
      <name>Peng Xu</name>
    </author>
    <author>
      <name>Sichun Xu</name>
    </author>
    <author>
      <name>Tianhe Yu</name>
    </author>
    <author>
      <name>Brianna Zitkovich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Website: https://robotics-transformer.github.io/</arxiv:comment>
    <link href="http://arxiv.org/abs/2307.15818v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.15818v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.16789v2</id>
    <updated>2023-10-03T14:45:48Z</updated>
    <published>2023-07-31T15:56:53Z</published>
    <title>ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world
  APIs</title>
    <summary>  Despite the advancements of open-source large language models (LLMs), e.g.,
LLaMA, they remain significantly limited in tool-use capabilities, i.e., using
external tools (APIs) to fulfill human instructions. The reason is that current
instruction tuning largely focuses on basic language tasks but ignores the
tool-use domain. This is in contrast to the excellent tool-use capabilities of
state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap,
we introduce ToolLLM, a general tool-use framework encompassing data
construction, model training, and evaluation. We first present ToolBench, an
instruction-tuning dataset for tool use, which is constructed automatically
using ChatGPT. Specifically, the construction can be divided into three stages:
(i) API collection: we collect 16,464 real-world RESTful APIs spanning 49
categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to
generate diverse instructions involving these APIs, covering both single-tool
and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to
search for a valid solution path (chain of API calls) for each instruction. To
enhance the reasoning capabilities of LLMs, we develop a novel depth-first
search-based decision tree algorithm. It enables LLMs to evaluate multiple
reasoning traces and expand the search space. Moreover, to evaluate the
tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval.
Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it
with a neural API retriever to recommend appropriate APIs for each instruction.
Experiments show that ToolLLaMA demonstrates a remarkable ability to execute
complex instructions and generalize to unseen APIs, and exhibits comparable
performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot
generalization ability in an out-of-distribution tool-use dataset: APIBench.
</summary>
    <author>
      <name>Yujia Qin</name>
    </author>
    <author>
      <name>Shihao Liang</name>
    </author>
    <author>
      <name>Yining Ye</name>
    </author>
    <author>
      <name>Kunlun Zhu</name>
    </author>
    <author>
      <name>Lan Yan</name>
    </author>
    <author>
      <name>Yaxi Lu</name>
    </author>
    <author>
      <name>Yankai Lin</name>
    </author>
    <author>
      <name>Xin Cong</name>
    </author>
    <author>
      <name>Xiangru Tang</name>
    </author>
    <author>
      <name>Bill Qian</name>
    </author>
    <author>
      <name>Sihan Zhao</name>
    </author>
    <author>
      <name>Lauren Hong</name>
    </author>
    <author>
      <name>Runchu Tian</name>
    </author>
    <author>
      <name>Ruobing Xie</name>
    </author>
    <author>
      <name>Jie Zhou</name>
    </author>
    <author>
      <name>Mark Gerstein</name>
    </author>
    <author>
      <name>Dahai Li</name>
    </author>
    <author>
      <name>Zhiyuan Liu</name>
    </author>
    <author>
      <name>Maosong Sun</name>
    </author>
    <link href="http://arxiv.org/abs/2307.16789v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.16789v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.00352v7</id>
    <updated>2024-11-01T14:36:52Z</updated>
    <published>2023-08-01T07:49:10Z</published>
    <title>MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework</title>
    <summary>  Remarkable progress has been made on automated problem solving through
societies of agents based on large language models (LLMs). Existing LLM-based
multi-agent systems can already solve simple dialogue tasks. Solutions to more
complex tasks, however, are complicated through logic inconsistencies due to
cascading hallucinations caused by naively chaining LLMs. Here we introduce
MetaGPT, an innovative meta-programming framework incorporating efficient human
workflows into LLM-based multi-agent collaborations. MetaGPT encodes
Standardized Operating Procedures (SOPs) into prompt sequences for more
streamlined workflows, thus allowing agents with human-like domain expertise to
verify intermediate results and reduce errors. MetaGPT utilizes an assembly
line paradigm to assign diverse roles to various agents, efficiently breaking
down complex tasks into subtasks involving many agents working together. On
collaborative software engineering benchmarks, MetaGPT generates more coherent
solutions than previous chat-based multi-agent systems. Our project can be
found at https://github.com/geekan/MetaGPT
</summary>
    <author>
      <name>Sirui Hong</name>
    </author>
    <author>
      <name>Mingchen Zhuge</name>
    </author>
    <author>
      <name>Jiaqi Chen</name>
    </author>
    <author>
      <name>Xiawu Zheng</name>
    </author>
    <author>
      <name>Yuheng Cheng</name>
    </author>
    <author>
      <name>Ceyao Zhang</name>
    </author>
    <author>
      <name>Jinlin Wang</name>
    </author>
    <author>
      <name>Zili Wang</name>
    </author>
    <author>
      <name>Steven Ka Shing Yau</name>
    </author>
    <author>
      <name>Zijuan Lin</name>
    </author>
    <author>
      <name>Liyang Zhou</name>
    </author>
    <author>
      <name>Chenyu Ran</name>
    </author>
    <author>
      <name>Lingfeng Xiao</name>
    </author>
    <author>
      <name>Chenglin Wu</name>
    </author>
    <author>
      <name>Jürgen Schmidhuber</name>
    </author>
    <link href="http://arxiv.org/abs/2308.00352v7" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.00352v7" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.02151v3</id>
    <updated>2024-05-05T05:04:49Z</updated>
    <published>2023-08-04T06:14:23Z</published>
    <title>Retroformer: Retrospective Large Language Agents with Policy Gradient
  Optimization</title>
    <summary>  Recent months have seen the emergence of a powerful new trend in which large
language models (LLMs) are augmented to become autonomous language agents
capable of performing objective oriented multi-step tasks on their own, rather
than merely responding to queries from human users. Most existing language
agents, however, are not optimized using environment-specific rewards. Although
some agents enable iterative refinement through verbal feedback, they do not
reason and plan in ways that are compatible with gradient-based learning from
rewards. This paper introduces a principled framework for reinforcing large
language agents by learning a retrospective model, which automatically tunes
the language agent prompts from environment feedback through policy gradient.
Specifically, our proposed agent architecture learns from rewards across
multiple environments and tasks, for fine-tuning a pre-trained language model
which refines the language agent prompt by summarizing the root cause of prior
failed attempts and proposing action plans. Experimental results on various
tasks demonstrate that the language agents improve over time and that our
approach considerably outperforms baselines that do not properly leverage
gradients from the environment. This demonstrates that using policy gradient
optimization to improve language agents, for which we believe our work is one
of the first, seems promising and can be applied to optimize other models in
the agent architecture to enhance agent performances over time.
</summary>
    <author>
      <name>Weiran Yao</name>
    </author>
    <author>
      <name>Shelby Heinecke</name>
    </author>
    <author>
      <name>Juan Carlos Niebles</name>
    </author>
    <author>
      <name>Zhiwei Liu</name>
    </author>
    <author>
      <name>Yihao Feng</name>
    </author>
    <author>
      <name>Le Xue</name>
    </author>
    <author>
      <name>Rithesh Murthy</name>
    </author>
    <author>
      <name>Zeyuan Chen</name>
    </author>
    <author>
      <name>Jianguo Zhang</name>
    </author>
    <author>
      <name>Devansh Arpit</name>
    </author>
    <author>
      <name>Ran Xu</name>
    </author>
    <author>
      <name>Phil Mui</name>
    </author>
    <author>
      <name>Huan Wang</name>
    </author>
    <author>
      <name>Caiming Xiong</name>
    </author>
    <author>
      <name>Silvio Savarese</name>
    </author>
    <link href="http://arxiv.org/abs/2308.02151v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.02151v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.11596v3</id>
    <updated>2023-10-25T03:52:07Z</updated>
    <published>2023-08-22T17:44:18Z</published>
    <title>SeamlessM4T: Massively Multilingual &amp; Multimodal Machine Translation</title>
    <summary>  What does it take to create the Babel Fish, a tool that can help individuals
translate speech between any two languages? While recent breakthroughs in
text-based models have pushed machine translation coverage beyond 200
languages, unified speech-to-speech translation models have yet to achieve
similar strides. More specifically, conventional speech-to-speech translation
systems rely on cascaded systems that perform translation progressively,
putting high-performing unified systems out of reach. To address these gaps, we
introduce SeamlessM4T, a single model that supports speech-to-speech
translation, speech-to-text translation, text-to-speech translation,
text-to-text translation, and automatic speech recognition for up to 100
languages. To build this, we used 1 million hours of open speech audio data to
learn self-supervised speech representations with w2v-BERT 2.0. Subsequently,
we created a multimodal corpus of automatically aligned speech translations.
Filtered and combined with human-labeled and pseudo-labeled data, we developed
the first multilingual system capable of translating from and into English for
both speech and text. On FLEURS, SeamlessM4T sets a new standard for
translations into multiple target languages, achieving an improvement of 20%
BLEU over the previous SOTA in direct speech-to-text translation. Compared to
strong cascaded models, SeamlessM4T improves the quality of into-English
translation by 1.3 BLEU points in speech-to-text and by 2.6 ASR-BLEU points in
speech-to-speech. Tested for robustness, our system performs better against
background noises and speaker variations in speech-to-text tasks compared to
the current SOTA model. Critically, we evaluated SeamlessM4T on gender bias and
added toxicity to assess translation safety. Finally, all contributions in this
work are open-sourced and accessible at
https://github.com/facebookresearch/seamless_communication
</summary>
    <author>
      <name>Seamless Communication</name>
    </author>
    <author>
      <name>Loïc Barrault</name>
    </author>
    <author>
      <name>Yu-An Chung</name>
    </author>
    <author>
      <name>Mariano Cora Meglioli</name>
    </author>
    <author>
      <name>David Dale</name>
    </author>
    <author>
      <name>Ning Dong</name>
    </author>
    <author>
      <name>Paul-Ambroise Duquenne</name>
    </author>
    <author>
      <name>Hady Elsahar</name>
    </author>
    <author>
      <name>Hongyu Gong</name>
    </author>
    <author>
      <name>Kevin Heffernan</name>
    </author>
    <author>
      <name>John Hoffman</name>
    </author>
    <author>
      <name>Christopher Klaiber</name>
    </author>
    <author>
      <name>Pengwei Li</name>
    </author>
    <author>
      <name>Daniel Licht</name>
    </author>
    <author>
      <name>Jean Maillard</name>
    </author>
    <author>
      <name>Alice Rakotoarison</name>
    </author>
    <author>
      <name>Kaushik Ram Sadagopan</name>
    </author>
    <author>
      <name>Guillaume Wenzek</name>
    </author>
    <author>
      <name>Ethan Ye</name>
    </author>
    <author>
      <name>Bapi Akula</name>
    </author>
    <author>
      <name>Peng-Jen Chen</name>
    </author>
    <author>
      <name>Naji El Hachem</name>
    </author>
    <author>
      <name>Brian Ellis</name>
    </author>
    <author>
      <name>Gabriel Mejia Gonzalez</name>
    </author>
    <author>
      <name>Justin Haaheim</name>
    </author>
    <author>
      <name>Prangthip Hansanti</name>
    </author>
    <author>
      <name>Russ Howes</name>
    </author>
    <author>
      <name>Bernie Huang</name>
    </author>
    <author>
      <name>Min-Jae Hwang</name>
    </author>
    <author>
      <name>Hirofumi Inaguma</name>
    </author>
    <author>
      <name>Somya Jain</name>
    </author>
    <author>
      <name>Elahe Kalbassi</name>
    </author>
    <author>
      <name>Amanda Kallet</name>
    </author>
    <author>
      <name>Ilia Kulikov</name>
    </author>
    <author>
      <name>Janice Lam</name>
    </author>
    <author>
      <name>Daniel Li</name>
    </author>
    <author>
      <name>Xutai Ma</name>
    </author>
    <author>
      <name>Ruslan Mavlyutov</name>
    </author>
    <author>
      <name>Benjamin Peloquin</name>
    </author>
    <author>
      <name>Mohamed Ramadan</name>
    </author>
    <author>
      <name>Abinesh Ramakrishnan</name>
    </author>
    <author>
      <name>Anna Sun</name>
    </author>
    <author>
      <name>Kevin Tran</name>
    </author>
    <author>
      <name>Tuan Tran</name>
    </author>
    <author>
      <name>Igor Tufanov</name>
    </author>
    <author>
      <name>Vish Vogeti</name>
    </author>
    <author>
      <name>Carleigh Wood</name>
    </author>
    <author>
      <name>Yilin Yang</name>
    </author>
    <author>
      <name>Bokai Yu</name>
    </author>
    <author>
      <name>Pierre Andrews</name>
    </author>
    <author>
      <name>Can Balioglu</name>
    </author>
    <author>
      <name>Marta R. Costa-jussà</name>
    </author>
    <author>
      <name>Onur Celebi</name>
    </author>
    <author>
      <name>Maha Elbayad</name>
    </author>
    <author>
      <name>Cynthia Gao</name>
    </author>
    <author>
      <name>Francisco Guzmán</name>
    </author>
    <author>
      <name>Justine Kao</name>
    </author>
    <author>
      <name>Ann Lee</name>
    </author>
    <author>
      <name>Alexandre Mourachko</name>
    </author>
    <author>
      <name>Juan Pino</name>
    </author>
    <author>
      <name>Sravya Popuri</name>
    </author>
    <author>
      <name>Christophe Ropers</name>
    </author>
    <author>
      <name>Safiyyah Saleem</name>
    </author>
    <author>
      <name>Holger Schwenk</name>
    </author>
    <author>
      <name>Paden Tomasello</name>
    </author>
    <author>
      <name>Changhan Wang</name>
    </author>
    <author>
      <name>Jeff Wang</name>
    </author>
    <author>
      <name>Skyler Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2308.11596v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.11596v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.12950v3</id>
    <updated>2024-01-31T19:47:26Z</updated>
    <published>2023-08-24T17:39:13Z</published>
    <title>Code Llama: Open Foundation Models for Code</title>
    <summary>  We release Code Llama, a family of large language models for code based on
Llama 2 providing state-of-the-art performance among open models, infilling
capabilities, support for large input contexts, and zero-shot instruction
following ability for programming tasks. We provide multiple flavors to cover a
wide range of applications: foundation models (Code Llama), Python
specializations (Code Llama - Python), and instruction-following models (Code
Llama - Instruct) with 7B, 13B, 34B and 70B parameters each. All models are
trained on sequences of 16k tokens and show improvements on inputs with up to
100k tokens. 7B, 13B and 70B Code Llama and Code Llama - Instruct variants
support infilling based on surrounding content. Code Llama reaches
state-of-the-art performance among open models on several code benchmarks, with
scores of up to 67% and 65% on HumanEval and MBPP, respectively. Notably, Code
Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our
models outperform every other publicly available model on MultiPL-E. We release
Code Llama under a permissive license that allows for both research and
commercial use.
</summary>
    <author>
      <name>Baptiste Rozière</name>
    </author>
    <author>
      <name>Jonas Gehring</name>
    </author>
    <author>
      <name>Fabian Gloeckle</name>
    </author>
    <author>
      <name>Sten Sootla</name>
    </author>
    <author>
      <name>Itai Gat</name>
    </author>
    <author>
      <name>Xiaoqing Ellen Tan</name>
    </author>
    <author>
      <name>Yossi Adi</name>
    </author>
    <author>
      <name>Jingyu Liu</name>
    </author>
    <author>
      <name>Romain Sauvestre</name>
    </author>
    <author>
      <name>Tal Remez</name>
    </author>
    <author>
      <name>Jérémy Rapin</name>
    </author>
    <author>
      <name>Artyom Kozhevnikov</name>
    </author>
    <author>
      <name>Ivan Evtimov</name>
    </author>
    <author>
      <name>Joanna Bitton</name>
    </author>
    <author>
      <name>Manish Bhatt</name>
    </author>
    <author>
      <name>Cristian Canton Ferrer</name>
    </author>
    <author>
      <name>Aaron Grattafiori</name>
    </author>
    <author>
      <name>Wenhan Xiong</name>
    </author>
    <author>
      <name>Alexandre Défossez</name>
    </author>
    <author>
      <name>Jade Copet</name>
    </author>
    <author>
      <name>Faisal Azhar</name>
    </author>
    <author>
      <name>Hugo Touvron</name>
    </author>
    <author>
      <name>Louis Martin</name>
    </author>
    <author>
      <name>Nicolas Usunier</name>
    </author>
    <author>
      <name>Thomas Scialom</name>
    </author>
    <author>
      <name>Gabriel Synnaeve</name>
    </author>
    <link href="http://arxiv.org/abs/2308.12950v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.12950v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.12966v3</id>
    <updated>2023-10-13T02:41:28Z</updated>
    <published>2023-08-24T17:59:17Z</published>
    <title>Qwen-VL: A Versatile Vision-Language Model for Understanding,
  Localization, Text Reading, and Beyond</title>
    <summary>  In this work, we introduce the Qwen-VL series, a set of large-scale
vision-language models (LVLMs) designed to perceive and understand both texts
and images. Starting from the Qwen-LM as a foundation, we endow it with visual
capacity by the meticulously designed (i) visual receptor, (ii) input-output
interface, (iii) 3-stage training pipeline, and (iv) multilingual multimodal
cleaned corpus. Beyond the conventional image description and
question-answering, we implement the grounding and text-reading ability of
Qwen-VLs by aligning image-caption-box tuples. The resulting models, including
Qwen-VL and Qwen-VL-Chat, set new records for generalist models under similar
model scales on a broad range of visual-centric benchmarks (e.g., image
captioning, question answering, visual grounding) and different settings (e.g.,
zero-shot, few-shot). Moreover, on real-world dialog benchmarks, our
instruction-tuned Qwen-VL-Chat also demonstrates superiority compared to
existing vision-language chatbots. Code, demo and models are available at
https://github.com/QwenLM/Qwen-VL.
</summary>
    <author>
      <name>Jinze Bai</name>
    </author>
    <author>
      <name>Shuai Bai</name>
    </author>
    <author>
      <name>Shusheng Yang</name>
    </author>
    <author>
      <name>Shijie Wang</name>
    </author>
    <author>
      <name>Sinan Tan</name>
    </author>
    <author>
      <name>Peng Wang</name>
    </author>
    <author>
      <name>Junyang Lin</name>
    </author>
    <author>
      <name>Chang Zhou</name>
    </author>
    <author>
      <name>Jingren Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code, demo and models are available at
  https://github.com/QwenLM/Qwen-VL</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.12966v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.12966v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.16512v4</id>
    <updated>2024-04-18T04:12:32Z</updated>
    <published>2023-08-31T07:49:06Z</published>
    <title>MVDream: Multi-view Diffusion for 3D Generation</title>
    <summary>  We introduce MVDream, a diffusion model that is able to generate consistent
multi-view images from a given text prompt. Learning from both 2D and 3D data,
a multi-view diffusion model can achieve the generalizability of 2D diffusion
models and the consistency of 3D renderings. We demonstrate that such a
multi-view diffusion model is implicitly a generalizable 3D prior agnostic to
3D representations. It can be applied to 3D generation via Score Distillation
Sampling, significantly enhancing the consistency and stability of existing
2D-lifting methods. It can also learn new concepts from a few 2D examples, akin
to DreamBooth, but for 3D generation.
</summary>
    <author>
      <name>Yichun Shi</name>
    </author>
    <author>
      <name>Peng Wang</name>
    </author>
    <author>
      <name>Jianglong Ye</name>
    </author>
    <author>
      <name>Mai Long</name>
    </author>
    <author>
      <name>Kejie Li</name>
    </author>
    <author>
      <name>Xiao Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Reorganized for arXiv; Our project page is https://MV-Dream.github.io</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.16512v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.16512v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.00267v3</id>
    <updated>2024-09-03T14:01:54Z</updated>
    <published>2023-09-01T05:53:33Z</published>
    <title>RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with
  AI Feedback</title>
    <summary>  Reinforcement learning from human feedback (RLHF) has proven effective in
aligning large language models (LLMs) with human preferences, but gathering
high-quality preference labels is expensive. RL from AI Feedback (RLAIF),
introduced in Bai et al., offers a promising alternative that trains the reward
model (RM) on preferences generated by an off-the-shelf LLM. Across the tasks
of summarization, helpful dialogue generation, and harmless dialogue
generation, we show that RLAIF achieves comparable performance to RLHF.
Furthermore, we take a step towards "self-improvement" by demonstrating that
RLAIF can outperform a supervised fine-tuned baseline even when the AI labeler
is the same size as the policy, or even the exact same checkpoint as the
initial policy. Finally, we introduce direct-RLAIF (d-RLAIF) - a technique that
circumvents RM training by obtaining rewards directly from an off-the-shelf LLM
during RL, which achieves superior performance to canonical RLAIF. Our results
suggest that RLAIF can achieve performance on-par with using human feedback,
offering a potential solution to the scalability limitations of RLHF.
</summary>
    <author>
      <name>Harrison Lee</name>
    </author>
    <author>
      <name>Samrat Phatale</name>
    </author>
    <author>
      <name>Hassan Mansoor</name>
    </author>
    <author>
      <name>Thomas Mesnard</name>
    </author>
    <author>
      <name>Johan Ferret</name>
    </author>
    <author>
      <name>Kellie Lu</name>
    </author>
    <author>
      <name>Colton Bishop</name>
    </author>
    <author>
      <name>Ethan Hall</name>
    </author>
    <author>
      <name>Victor Carbune</name>
    </author>
    <author>
      <name>Abhinav Rastogi</name>
    </author>
    <author>
      <name>Sushant Prakash</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at ICML 2024</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 41st International Conference on Machine
  Learning, PMLR 235:26874-26901, 2024</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2309.00267v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.00267v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.03409v3</id>
    <updated>2024-04-15T07:50:32Z</updated>
    <published>2023-09-07T00:07:15Z</published>
    <title>Large Language Models as Optimizers</title>
    <summary>  Optimization is ubiquitous. While derivative-based algorithms have been
powerful tools for various problems, the absence of gradient imposes challenges
on many real-world applications. In this work, we propose Optimization by
PROmpting (OPRO), a simple and effective approach to leverage large language
models (LLMs) as optimizers, where the optimization task is described in
natural language. In each optimization step, the LLM generates new solutions
from the prompt that contains previously generated solutions with their values,
then the new solutions are evaluated and added to the prompt for the next
optimization step. We first showcase OPRO on linear regression and traveling
salesman problems, then move on to our main application in prompt optimization,
where the goal is to find instructions that maximize the task accuracy. With a
variety of LLMs, we demonstrate that the best prompts optimized by OPRO
outperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on
Big-Bench Hard tasks. Code at https://github.com/google-deepmind/opro.
</summary>
    <author>
      <name>Chengrun Yang</name>
    </author>
    <author>
      <name>Xuezhi Wang</name>
    </author>
    <author>
      <name>Yifeng Lu</name>
    </author>
    <author>
      <name>Hanxiao Liu</name>
    </author>
    <author>
      <name>Quoc V. Le</name>
    </author>
    <author>
      <name>Denny Zhou</name>
    </author>
    <author>
      <name>Xinyun Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICLR 2024; 42 pages, 26 figures, 15 tables. Code at
  https://github.com/google-deepmind/opro</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.03409v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.03409v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.16588v2</id>
    <updated>2024-04-12T09:38:33Z</updated>
    <published>2023-09-28T16:45:46Z</published>
    <title>Vision Transformers Need Registers</title>
    <summary>  Transformers have recently emerged as a powerful tool for learning visual
representations. In this paper, we identify and characterize artifacts in
feature maps of both supervised and self-supervised ViT networks. The artifacts
correspond to high-norm tokens appearing during inference primarily in
low-informative background areas of images, that are repurposed for internal
computations. We propose a simple yet effective solution based on providing
additional tokens to the input sequence of the Vision Transformer to fill that
role. We show that this solution fixes that problem entirely for both
supervised and self-supervised models, sets a new state of the art for
self-supervised visual models on dense visual prediction tasks, enables object
discovery methods with larger models, and most importantly leads to smoother
feature maps and attention maps for downstream visual processing.
</summary>
    <author>
      <name>Timothée Darcet</name>
    </author>
    <author>
      <name>Maxime Oquab</name>
    </author>
    <author>
      <name>Julien Mairal</name>
    </author>
    <author>
      <name>Piotr Bojanowski</name>
    </author>
    <link href="http://arxiv.org/abs/2309.16588v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.16588v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.17453v4</id>
    <updated>2024-04-07T00:56:53Z</updated>
    <published>2023-09-29T17:59:56Z</published>
    <title>Efficient Streaming Language Models with Attention Sinks</title>
    <summary>  Deploying Large Language Models (LLMs) in streaming applications such as
multi-round dialogue, where long interactions are expected, is urgently needed
but poses two major challenges. Firstly, during the decoding stage, caching
previous tokens' Key and Value states (KV) consumes extensive memory. Secondly,
popular LLMs cannot generalize to longer texts than the training sequence
length. Window attention, where only the most recent KVs are cached, is a
natural approach -- but we show that it fails when the text length surpasses
the cache size. We observe an interesting phenomenon, namely attention sink,
that keeping the KV of initial tokens will largely recover the performance of
window attention. In this paper, we first demonstrate that the emergence of
attention sink is due to the strong attention scores towards initial tokens as
a "sink" even if they are not semantically important. Based on the above
analysis, we introduce StreamingLLM, an efficient framework that enables LLMs
trained with a finite length attention window to generalize to infinite
sequence lengths without any fine-tuning. We show that StreamingLLM can enable
Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language
modeling with up to 4 million tokens and more. In addition, we discover that
adding a placeholder token as a dedicated attention sink during pre-training
can further improve streaming deployment. In streaming settings, StreamingLLM
outperforms the sliding window recomputation baseline by up to 22.2x speedup.
Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.
</summary>
    <author>
      <name>Guangxuan Xiao</name>
    </author>
    <author>
      <name>Yuandong Tian</name>
    </author>
    <author>
      <name>Beidi Chen</name>
    </author>
    <author>
      <name>Song Han</name>
    </author>
    <author>
      <name>Mike Lewis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICLR 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.17453v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.17453v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.12931v2</id>
    <updated>2024-04-30T21:35:53Z</updated>
    <published>2023-10-19T17:31:01Z</published>
    <title>Eureka: Human-Level Reward Design via Coding Large Language Models</title>
    <summary>  Large Language Models (LLMs) have excelled as high-level semantic planners
for sequential decision-making tasks. However, harnessing them to learn complex
low-level manipulation tasks, such as dexterous pen spinning, remains an open
problem. We bridge this fundamental gap and present Eureka, a human-level
reward design algorithm powered by LLMs. Eureka exploits the remarkable
zero-shot generation, code-writing, and in-context improvement capabilities of
state-of-the-art LLMs, such as GPT-4, to perform evolutionary optimization over
reward code. The resulting rewards can then be used to acquire complex skills
via reinforcement learning. Without any task-specific prompting or pre-defined
reward templates, Eureka generates reward functions that outperform expert
human-engineered rewards. In a diverse suite of 29 open-source RL environments
that include 10 distinct robot morphologies, Eureka outperforms human experts
on 83% of the tasks, leading to an average normalized improvement of 52%. The
generality of Eureka also enables a new gradient-free in-context learning
approach to reinforcement learning from human feedback (RLHF), readily
incorporating human inputs to improve the quality and the safety of the
generated rewards without model updating. Finally, using Eureka rewards in a
curriculum learning setting, we demonstrate for the first time, a simulated
Shadow Hand capable of performing pen spinning tricks, adeptly manipulating a
pen in circles at rapid speed.
</summary>
    <author>
      <name>Yecheng Jason Ma</name>
    </author>
    <author>
      <name>William Liang</name>
    </author>
    <author>
      <name>Guanzhi Wang</name>
    </author>
    <author>
      <name>De-An Huang</name>
    </author>
    <author>
      <name>Osbert Bastani</name>
    </author>
    <author>
      <name>Dinesh Jayaraman</name>
    </author>
    <author>
      <name>Yuke Zhu</name>
    </author>
    <author>
      <name>Linxi Fan</name>
    </author>
    <author>
      <name>Anima Anandkumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICLR 2024. Project website and open-source code:
  https://eureka-research.github.io/</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.12931v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.12931v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.06242v1</id>
    <updated>2023-11-10T18:59:08Z</updated>
    <published>2023-11-10T18:59:08Z</published>
    <title>Florence-2: Advancing a Unified Representation for a Variety of Vision
  Tasks</title>
    <summary>  We introduce Florence-2, a novel vision foundation model with a unified,
prompt-based representation for a variety of computer vision and
vision-language tasks. While existing large vision models excel in transfer
learning, they struggle to perform a diversity of tasks with simple
instructions, a capability that implies handling the complexity of various
spatial hierarchy and semantic granularity. Florence-2 was designed to take
text-prompt as task instructions and generate desirable results in text forms,
whether it be captioning, object detection, grounding or segmentation. This
multi-task learning setup demands large-scale, high-quality annotated data. To
this end, we co-developed FLD-5B that consists of 5.4 billion comprehensive
visual annotations on 126 million images, using an iterative strategy of
automated image annotation and model refinement. We adopted a
sequence-to-sequence structure to train Florence-2 to perform versatile and
comprehensive vision tasks. Extensive evaluations on numerous tasks
demonstrated Florence-2 to be a strong vision foundation model contender with
unprecedented zero-shot and fine-tuning capabilities.
</summary>
    <author>
      <name>Bin Xiao</name>
    </author>
    <author>
      <name>Haiping Wu</name>
    </author>
    <author>
      <name>Weijian Xu</name>
    </author>
    <author>
      <name>Xiyang Dai</name>
    </author>
    <author>
      <name>Houdong Hu</name>
    </author>
    <author>
      <name>Yumao Lu</name>
    </author>
    <author>
      <name>Michael Zeng</name>
    </author>
    <author>
      <name>Ce Liu</name>
    </author>
    <author>
      <name>Lu Yuan</name>
    </author>
    <link href="http://arxiv.org/abs/2311.06242v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.06242v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.14125v4</id>
    <updated>2024-06-04T17:25:20Z</updated>
    <published>2023-12-21T18:46:41Z</published>
    <title>VideoPoet: A Large Language Model for Zero-Shot Video Generation</title>
    <summary>  We present VideoPoet, a language model capable of synthesizing high-quality
video, with matching audio, from a large variety of conditioning signals.
VideoPoet employs a decoder-only transformer architecture that processes
multimodal inputs -- including images, videos, text, and audio. The training
protocol follows that of Large Language Models (LLMs), consisting of two
stages: pretraining and task-specific adaptation. During pretraining, VideoPoet
incorporates a mixture of multimodal generative objectives within an
autoregressive Transformer framework. The pretrained LLM serves as a foundation
that can be adapted for a range of video generation tasks. We present empirical
results demonstrating the model's state-of-the-art capabilities in zero-shot
video generation, specifically highlighting VideoPoet's ability to generate
high-fidelity motions. Project page: http://sites.research.google/videopoet/
</summary>
    <author>
      <name>Dan Kondratyuk</name>
    </author>
    <author>
      <name>Lijun Yu</name>
    </author>
    <author>
      <name>Xiuye Gu</name>
    </author>
    <author>
      <name>José Lezama</name>
    </author>
    <author>
      <name>Jonathan Huang</name>
    </author>
    <author>
      <name>Grant Schindler</name>
    </author>
    <author>
      <name>Rachel Hornung</name>
    </author>
    <author>
      <name>Vighnesh Birodkar</name>
    </author>
    <author>
      <name>Jimmy Yan</name>
    </author>
    <author>
      <name>Ming-Chang Chiu</name>
    </author>
    <author>
      <name>Krishna Somandepalli</name>
    </author>
    <author>
      <name>Hassan Akbari</name>
    </author>
    <author>
      <name>Yair Alon</name>
    </author>
    <author>
      <name>Yong Cheng</name>
    </author>
    <author>
      <name>Josh Dillon</name>
    </author>
    <author>
      <name>Agrim Gupta</name>
    </author>
    <author>
      <name>Meera Hahn</name>
    </author>
    <author>
      <name>Anja Hauth</name>
    </author>
    <author>
      <name>David Hendon</name>
    </author>
    <author>
      <name>Alonso Martinez</name>
    </author>
    <author>
      <name>David Minnen</name>
    </author>
    <author>
      <name>Mikhail Sirotenko</name>
    </author>
    <author>
      <name>Kihyuk Sohn</name>
    </author>
    <author>
      <name>Xuan Yang</name>
    </author>
    <author>
      <name>Hartwig Adam</name>
    </author>
    <author>
      <name>Ming-Hsuan Yang</name>
    </author>
    <author>
      <name>Irfan Essa</name>
    </author>
    <author>
      <name>Huisheng Wang</name>
    </author>
    <author>
      <name>David A. Ross</name>
    </author>
    <author>
      <name>Bryan Seybold</name>
    </author>
    <author>
      <name>Lu Jiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at ICML 2024; Project page:
  http://sites.research.google/videopoet/</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.14125v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.14125v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
