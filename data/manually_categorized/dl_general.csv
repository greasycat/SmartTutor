Year,Paper,Authors,Link
1951,A Stochastic Approximation Method,H. Robbins and S. Monro,Let denote the expected value at level of the response to a certain experiment. is assumed to be a monotone function of but is unknown to the experimenter  and it is desired to find the solution of the equation   where is a given constant. We give a method for making successive experiments at levels in such a way that will tend to in probability.
1958,The perceptron: A probabilistic model for information storage and organization in the brain., Rosenblatt F., To answer the questions of how information about the physical world is sensed  in what form is information remembered  and how does information retained in memory influence recognition and behavior  a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems
1986,Learning representations by back-propagating errors,David Rumelhart et al.,https://www.nature.com/articles/323533a0
1998,Efficient Backprop,Yann LeCun et al.,https://link.springer.com/chapter/10.1007/978-3-642-35289-8_3
2010,Understanding the difficulty of training deep feedforward neural networks,Xavier Glorot and Yoshua Bengio,Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained  since then several algorithms have been shown to successfully train them  with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks  to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value  which can drive especially the top hidden layer into saturation. Surprisingly  we find that saturated units can move out of saturation by themselves  albeit slowly  and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally  we study how activations and gradients vary across layers and during training  with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations  we propose a new initialization scheme that brings substantially faster convergence.
2011,Deep Sparse Rectifier Neural Networks,Xavier Glorot et al.,While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons  the latter work better for training multi-layer neural networks. This pa-per shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiable at zero  creating sparse representations with true zeros  which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data  deep rectifier net-works can reach their best performance with-out requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence  these results can be seen as a new milestone in the attempts at under-standing the difficulty in training deep but purely supervised neural networks  and closing the performance gap between neural net-works learnt with and without unsupervised pre-training.
2011,HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent,Feng Niu et al.,https://arxiv.org/abs/1106.5730
2014,Neural Machine Translation by Jointly Learning to Align and Translate,Dzmitry Bahdanau et al.,https://arxiv.org/pdf/1409.0473v7.pdf
2014,Dropout: A Simple Way to Prevent Neural Networks from Overfitting,Nitish Srivastava et al.,https://jmlr.org/papers/v15/srivastava14a.html
2015,Highway Networks,Rupesh Kumar Srivastava et al.,https://arxiv.org/abs/1505.00387
2015,Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,Sergey Ioffe and Christian Szegedy,https://arxiv.org/abs/1502.03167
2016,Gaussian Error Linear Units (GELUs),Dan Hendrycks and Kevin Gimpel,https://arxiv.org/pdf/1606.08415v4.pdf
2016,Categorical Reparameterization with Gumbel-Softmax,Eric Jang et al.,https://arxiv.org/abs/1611.01144v5
2016,On the Properties of the Softmax Function with Application in Game Theory and Reinforcement Learning,Bolin Gao and Lacra Pavel,https://arxiv.org/pdf/1704.00805.pdf
2017,Squeeze-and-Excitation Networks,Jie Hu et al.,https://arxiv.org/abs/1709.01507
2017,Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization,Xun Huang and Serge Belongie,https://arxiv.org/abs/1703.06868
2017,Understanding the Disharmony Between Dropout and Batch Normalization by Variance Shift,Xiang Li et al.,https://arxiv.org/pdf/1801.05134.pdf
